Issue Number,Issue Title,Issue Body
25517,AttributeError: module 'tensorflow.python.ops.losses.losses_impl' has no attribute 'ReductionV2',"Installing tensorflow 1.12.0 with `pip install tensorflow` results in the following error:

```
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 55, in <module>
    'tensorflow_estimator.python.estimator'))
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/tools/component_api_helper.py"", line 56, in package_hook
    child_pkg = importlib.import_module(child_package_str)
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v2 import estimator
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/_api/v2/estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v2.estimator import experimental
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/_api/v2/estimator/experimental/__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py"", line 67, in <module>
    from tensorflow_estimator.python.estimator.head import head_utils
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/head/head_utils.py"", line 23, in <module>
    from tensorflow_estimator.python.estimator.head import binary_class_head
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py"", line 24, in <module>
    from tensorflow.python.keras.utils import metrics_utils
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py"", line 31, in <module>
    from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions
  File ""/home/s.ivanov/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py"", line 144, in <module>
    weighted_losses, reduction=losses_impl.ReductionV2.SUM_OVER_BATCH_SIZE):
AttributeError: module 'tensorflow.python.ops.losses.losses_impl' has no attribute 'ReductionV2'
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip"
25516,Defining metrics within loss functions was possible in Keras!,"**System information**
- TensorFlow version: 1.12.0
- Are you willing to contribute it: (Yes/No)

In Keras, it was possible to define metrics inside loss functions using closures. This means that something like the following 

```
class Loss():
    def __init__(self):
        self.metrics = []
    
    def loss(self, y_true, y_pred):
        
        # the following can be some complicated intermediate stuff that
        # would otherwise require redundant code
        t1 = tf.reduce_mean(y_true-y_pred)
        m2 = tf.reduce_max(y_true-y_pred)
        m3 = tf.reduce_min(y_true-y_pred)
        
        def m1(y_true, y_pred):
            return t1
        self.metrics.append(m1)
        
        def make_fcn(tensor, name):
            f = lambda y_true, y_pred: tensor
            f.__name__ = name
            return f
        for m in ['m2', 'm3']:
            self.metrics.append(make_fcn(eval(m),m))
        
        return tf.reduce_sum((y_true-y_pred)**2)

output2_loss = Loss()

model.compile(
    optimizer=keras.optimizers.RMSprop(lr=0.001, decay=1e-6), 
    loss={'pred': 'categorical_crossentropy', 'output2': output2_loss.loss},
    metrics={'pred': ['accuracy'], 'output2': output2_loss.metrics}
)
```

works perfectly fine in normal Keras but does not work in TensorFlow Keras.

I assume that there is no clear specification when the loss functions are evaluated and it is also clear to me that the approach may not work with eager execution.

Is there any elegant mechanism or workaround to achieve a similar behaviour?
"
25513,Custom Reader Op results in undefined symbol: _ZTIN10tensorflow8OpKernelE on library load,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker container, also tried source
- TensorFlow version: 1.12.0
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9/7
- GPU model and memory: 1080ti


I have a custom Reader op that inherits ReaderBase class (tensorflow/core/framework/reader_base.h). The op compiled and worked as expected under TF 1.3 and earlier.

I have now upgraded to TF1.12.0. Shared library for the custom op builds successfully, but  loading it with `import tensorFlow as tf; tf.load_op_library()` fails due to missing symbols:
`tensorflow.python.framework.errors_impl.NotFoundError: ./libjson_record_reader_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE`

Actually, at first it was reporting undefined symbol ReaderBaseE, but now reporting OpKernelE.

I tried building the op in the following Docker containers with the following tags: 1.12.0, 1.12.0-devel, 1.12.0-gpu-devel.

I have also built TF from source with bazel 0.18 and installed via pip (also in tensorflow/tensorflow-devel-gpu container) with the same result as above.

Building custom op as per current instructions in the docs using g++. I tried building with g++ 5.5, 5.4, 4.8, 4.9 with the same result.
"
25512,Cannot access layer losses attribute under MirroredStrategy,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 18.04
- **TensorFlow installed from**: Binary
- **TensorFlow version**: 1.12.0
- **Python version**: 2.7.15
- **GPU model and memory**: 2 x TITAN V 12GB

### Describe the problem

I tried to run some of our code with multiple GPUs using MirroredStrategy instead of Horovod and noticed that it fails with a cryptic error in _tensor_conversion_mirrored. It seems like accessing the losses attribute of a layer doesn't work with MirroredStrategy and the documentation doesn't seem to explain why this shouldn't be possible.

### Source code / logs

The following snippet of code reproduces the error.

```python
import os
import shutil

import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)


def model_fn(features, labels, mode):
    inputs = tf.reshape(features[""x""], [-1, 28, 28, 1])

    conv = tf.layers.Conv2D(
        filters=8,
        kernel_size=3,
        padding=""same"",
        activation=""relu"",
        kernel_regularizer=""l2"",
    )
    x = conv(inputs)
    flat = tf.layers.flatten(x)
    logits = tf.layers.dense(inputs=flat, units=10)

    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    total_loss = loss + tf.add_n(conv.losses)

    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(
        loss=total_loss, global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode=mode, loss=total_loss, train_op=train_op)


def input_fn():
    mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
    data = mnist.train.images
    labels = np.asarray(mnist.train.labels, dtype=np.int32)
    dataset = tf.data.Dataset.from_tensor_slices(({""x"": data}, labels))
    dataset = dataset.batch(128)
    return dataset


def main(unused_argv):
    model_dir = os.path.expanduser(""~/experiments/regularization"")
    shutil.rmtree(model_dir, ignore_errors=True)
    os.makedirs(model_dir)

    strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
    config = tf.estimator.RunConfig(train_distribute=strategy)

    mnist_classifier = tf.estimator.Estimator(
        model_fn=model_fn, model_dir=model_dir, config=config
    )
    mnist_classifier.train(input_fn, steps=10)


if __name__ == ""__main__"":
    tf.app.run()
```"
25509,[tf.data] non-used attributes in some tf.data operation definitions,"Some attributes in [tf.data operation definition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/dataset_ops.cc) (e.g. `RangeDataset`, `TensorDataset`, `TensorSliceDataset`) do not seem to be used in other places. 

Taken [RangeDataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/dataset_ops.cc#L339-L340) as an example, the `output_types` and `output_shapes` are required, but these attributes are not used in [range_dataset_op.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/range_dataset_op.cc)."
25507,Cannot import tensorflow-gpu in jupyter notebook.,"Hi,

I am using the following conda environment and jupyter notebook.

python==3.5
tensorflow-gpu==1.4.0
cuda==8.0
cudnn==6.0

While I can correctly import tensorflow in the cmd line:
```
(tensorflow) [dushu@ip-172-20-149-210 Mask_RCNN]$ python
Python 3.5.5 |Anaconda, Inc.| (default, May 13 2018, 21:12:35)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as t
/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
>>> t.test
<module 'tensorflow.python.platform.test' from '/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/test.py'>
>>> t.test.gpu_device_name()
2019-02-05 04:08:32.393567: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-05 04:08:35.257250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-05 04:08:35.257629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2019-02-05 04:08:35.257662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
'/device:GPU:0'
```
 I can' t import it in the jupyter notebook:
```
ImportError: Traceback (most recent call last):
  File ""/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/dushu/anaconda3/envs/tensorflow/lib/python3.5/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

The jupyter seems cannot find cuda path. The other similar issues don' t work either.

Any help please ?"
25504,tf.gather produces Invalid argument error while the code is working on GPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.8.0-8-g23c2187 1.8.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Patch Version 9.2.88.1/ cudnn: 1.7.4
- GPU model and memory: Titan X (Pascal) 12GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
when I run tf.gather, it returns invalid argument error. I found a document saying that tf.gather returns zero when there is an invalid argument on GPU. Even though I am using GPU for running my code, it produces an invalid argument error when I put -1 as an input of tf.gather. 

**Describe the expected behavior**

**Code to reproduce the issue**
When I tested the same code on the terminal, it works.
I can say my tf.gather is in tower/cond/while
I think tower is the problem, since i used tf.gather in cond/while.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
/usr/bin/python3.5 /home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py --config MODE_MASK=True MODE_FPN=True BACKBONE.WEIGHTS=/home/blackfoot/git/tensorpack/examples/FasterRCNN/pretrained/COCO-R50FPN-MaskRCNN-Standard.npz
[0204 23:11:13 @logger.py:87] Argv: /home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py --config MODE_MASK=True MODE_FPN=True BACKBONE.WEIGHTS=/home/blackfoot/git/tensorpack/examples/FasterRCNN/pretrained/COCO-R50FPN-MaskRCNN-Standard.npz
[0204 23:11:13 @config.py:284] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
'FREEZE_AT': 2,
'NORM': 'FreezeBN',
'RESNET_NUM_BLOCKS': [3, 4, 6, 3],
'STRIDE_1X1': False,
'TF_PAD_MODE': False,
'WEIGHTS': '/home/blackfoot/git/tensorpack/examples/FasterRCNN/pretrained/COCO-R50FPN-MaskRCNN-Standard.npz'},
'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
[30.0, 30.0, 15.0, 15.0]],
'IOUS': [0.5, 0.6, 0.7]},
'DATA': {'BASEDIR': '/data1/coco',
'CLASS_NAMES': ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',
'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',
'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',
'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',
'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
'hair drier', 'toothbrush'],
'NUM_CATEGORY': 80,
'NUM_CLASS': 81,
'TRAIN': ['train2014', 'valminusminival2014'],
'VAL': ('minival2014',)},
'DEBUG': False,
'EPSILON': 0.01,
'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
'CASCADE': False,
'FRCNN_CONV_HEAD_DIM': 256,
'FRCNN_FC_HEAD_DIM': 1024,
'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
'NORM': 'None',
'NUM_CHANNEL': 256,
'PROPOSAL_MODE': 'Level',
'RESOLUTION_REQUIREMENT': 32},
'FRCNN': {'BATCH_PER_IM': 512,
'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
'FG_RATIO': 0.25,
'FG_THRESH': 0.5},
'IDN_FEAT_DIM': 256,
'MODE_FPN': True,
'MODE_MASK': True,
'MRCNN': {'HEAD_DIM': 256},
'PREPROC': {'MAX_SIZE': 1344.0,
'PIXEL_MEAN': [123.675, 116.28, 103.53],
'PIXEL_STD': [58.395, 57.12, 57.375],
'TEST_SHORT_EDGE_SIZE': 800,
'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
'ANCHOR_SIZES': (32, 64, 128, 256, 512),
'ANCHOR_STRIDE': 16,
'BATCH_PER_IM': 256,
'CROWD_OVERLAP_THRESH': 9.99,
'FG_RATIO': 0.5,
'HEAD_DIM': 1024,
'MIN_SIZE': 0,
'NEGATIVE_ANCHOR_THRESH': 0.3,
'NUM_ANCHOR': 15,
'POSITIVE_ANCHOR_THRESH': 0.7,
'PROPOSAL_NMS_THRESH': 0.7,
'TEST_PER_LEVEL_NMS_TOPK': 1000,
'TEST_POST_NMS_TOPK': 1000,
'TEST_PRE_NMS_TOPK': 6000,
'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
'TRAIN_POST_NMS_TOPK': 2000,
'TRAIN_PRE_NMS_TOPK': 12000},
'TEST': {'FRCNN_NMS_THRESH': 0.5,
'RESULTS_PER_IM': 100,
'RESULT_SCORE_THRESH': 0.05,
'RESULT_SCORE_THRESH_VIS': 0.3},
'TRAIN': {'BASE_LR': 0.01,
'DPP_SCORE_THRESH': 0.0,
'EVAL_PERIOD': 25,
'LABEL_LIM': 20,
'LR_SCHEDULE': [30000],
'NUM_GPUS': 1,
'QUAL_TOPN': 10,
'SIM_LR': 1,
'STARTING_EPOCH': 1,
'STEPS_PER_EPOCH': 500,
'WARMUP': 1000,
'WARMUP_INIT_LR': 0.0033000000000000004,
'WEIGHT_DECAY': 0.0001},
'TRAINER': 'replicated'}
[0204 23:11:13 @train.py:573] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0204 23:11:13 @train.py:574] LR Schedule (epochs, value): [(2, 0.01)]
loading annotations into memory...
Done (t=9.42s)
creating index...
index created!
[0204 23:11:24 @dataset.py:45] Instances loaded from /data1/coco/annotations/instances_train2014.json.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82783/82783 [00:18<00:00, 4515.45it/s]
[0204 23:11:42 @timer.py:48] Load Groundtruth Boxes for train2014 finished, time:18.3824sec.
loading annotations into memory...
Done (t=8.37s)
creating index...
index created!
[0204 23:11:51 @dataset.py:45] Instances loaded from /data1/coco/annotations/instances_valminusminival2014.json.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35504/35504 [00:08<00:00, 4344.10it/s]
[0204 23:12:00 @timer.py:48] Load Groundtruth Boxes for valminusminival2014 finished, time:8.2563sec.
[0204 23:12:09 @data.py:49] Ground-Truth Boxes:
| class | #box |
|:---------------|-------:|
| BG | 0 |
| person | 257221 |
| bicycle | 7056 |
| car | 43532 |
| motorcycle | 8654 |
| airplane | 5129 |
| bus | 6061 |
| train | 4570 |
| truck | 9970 |
| boat | 10575 |
| traffic light | 12834 |
| fire hydrant | 1865 |
| stop sign | 1983 |
| parking meter | 1283 |
| bench | 9820 |
| bird | 10512 |
| cat | 4766 |
| dog | 5500 |
| horse | 6567 |
| sheep | 9223 |
| cow | 8014 |
| elephant | 5484 |
| bear | 1294 |
| zebra | 5269 |
| giraffe | 5128 |
| backpack | 8713 |
| umbrella | 11265 |
| handbag | 12340 |
| tie | 6445 |
| suitcase | 6112 |
| frisbee | 2681 |
| skis | 6620 |
| snowboard | 2679 |
| sports ball | 6291 |
| kite | 8792 |
| baseball bat | 3273 |
| baseball glove | 3742 |
| skateboard | 5536 |
| surfboard | 6093 |
| tennis racket | 4803 |
| bottle | 24070 |
| wine glass | 7839 |
| cup | 20574 |
| fork | 5474 |
| knife | 7759 |
| spoon | 6159 |
| bowl | 14323 |
| banana | 9195 |
| apple | 5776 |
| sandwich | 4356 |
| orange | 6302 |
| broccoli | 7261 |
| carrot | 7757 |
| hot dog | 2883 |
| pizza | 5807 |
| donut | 7005 |
| cake | 6296 |
| chair | 38072 |
| couch | 5779 |
| potted plant | 8631 |
| bed | 4192 |
| dining table | 15695 |
| toilet | 4149 |
| tv | 5803 |
| laptop | 4960 |
| mouse | 2261 |
| remote | 5699 |
| keyboard | 2854 |
| cell phone | 6405 |
| microwave | 1672 |
| oven | 3334 |
| toaster | 225 |
| sink | 5609 |
| refrigerator | 2634 |
| book | 24077 |
| clock | 6319 |
| vase | 6577 |
| scissors | 1464 |
| teddy bear | 4729 |
| hair drier | 198 |
| toothbrush | 1945 |
| total | 849814 |
[0204 23:12:09 @data.py:294] Filtered 1021 images which contain no non-crowd groudtruth boxes. Total #images for training: 117266
[0204 23:12:09 @train.py:578] Total passes of the training set is: 2.0466
[0204 23:12:11 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0204 23:12:11 @training.py:109] Building graph for training tower 0 on device /gpu:0 ...
[0204 23:12:12 @registry.py:125] conv0 input: [None, 3, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] conv0 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] pool0 input: [None, 64, None, None]
[0204 23:12:12 @registry.py:133] pool0 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block0/conv1 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block0/conv1 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block0/conv2 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block0/conv2 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block0/conv3 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block0/conv3 output: [None, 256, None, None]
[0204 23:12:12 @registry.py:125] group0/block0/convshortcut input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block0/convshortcut output: [None, 256, None, None]
[0204 23:12:12 @registry.py:125] group0/block1/conv1 input: [None, 256, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block1/conv1 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block1/conv2 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block1/conv2 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block1/conv3 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block1/conv3 output: [None, 256, None, None]
[0204 23:12:12 @registry.py:125] group0/block2/conv1 input: [None, 256, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block2/conv1 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block2/conv2 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block2/conv2 output: [None, 64, None, None]
[0204 23:12:12 @registry.py:125] group0/block2/conv3 input: [None, 64, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group0/block2/conv3 output: [None, 256, None, None]
[0204 23:12:12 @registry.py:125] group1/block0/conv1 input: [None, 256, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group1/block0/conv1 output: [None, 128, None, None]
[0204 23:12:12 @registry.py:125] group1/block0/conv2 input: [None, 128, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group1/block0/conv2 output: [None, 128, None, None]
[0204 23:12:12 @registry.py:125] group1/block0/conv3 input: [None, 128, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group1/block0/conv3 output: [None, 512, None, None]
[0204 23:12:12 @registry.py:125] group1/block0/convshortcut input: [None, 256, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:12 @registry.py:133] group1/block0/convshortcut output: [None, 512, None, None]
[0204 23:12:12 @registry.py:125] group1/block1/conv1 input: [None, 512, None, None]
[0204 23:12:12 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block1/conv1 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block1/conv2 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block1/conv2 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block1/conv3 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block1/conv3 output: [None, 512, None, None]
[0204 23:12:13 @registry.py:125] group1/block2/conv1 input: [None, 512, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block2/conv1 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block2/conv2 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block2/conv2 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block2/conv3 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block2/conv3 output: [None, 512, None, None]
[0204 23:12:13 @registry.py:125] group1/block3/conv1 input: [None, 512, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block3/conv1 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block3/conv2 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block3/conv2 output: [None, 128, None, None]
[0204 23:12:13 @registry.py:125] group1/block3/conv3 input: [None, 128, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group1/block3/conv3 output: [None, 512, None, None]
[0204 23:12:13 @registry.py:125] group2/block0/conv1 input: [None, 512, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block0/conv1 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block0/conv2 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block0/conv2 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block0/conv3 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block0/conv3 output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block0/convshortcut input: [None, 512, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block0/convshortcut output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block1/conv1 input: [None, 1024, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block1/conv1 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block1/conv2 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block1/conv2 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block1/conv3 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block1/conv3 output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block2/conv1 input: [None, 1024, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block2/conv1 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block2/conv2 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block2/conv2 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block2/conv3 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block2/conv3 output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block3/conv1 input: [None, 1024, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block3/conv1 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block3/conv2 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block3/conv2 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block3/conv3 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block3/conv3 output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block4/conv1 input: [None, 1024, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block4/conv1 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block4/conv2 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block4/conv2 output: [None, 256, None, None]
[0204 23:12:13 @registry.py:125] group2/block4/conv3 input: [None, 256, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:13 @registry.py:133] group2/block4/conv3 output: [None, 1024, None, None]
[0204 23:12:13 @registry.py:125] group2/block5/conv1 input: [None, 1024, None, None]
[0204 23:12:13 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group2/block5/conv1 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] group2/block5/conv2 input: [None, 256, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group2/block5/conv2 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] group2/block5/conv3 input: [None, 256, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group2/block5/conv3 output: [None, 1024, None, None]
[0204 23:12:14 @registry.py:125] group3/block0/conv1 input: [None, 1024, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block0/conv1 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block0/conv2 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block0/conv2 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block0/conv3 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block0/conv3 output: [None, 2048, None, None]
[0204 23:12:14 @registry.py:125] group3/block0/convshortcut input: [None, 1024, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block0/convshortcut output: [None, 2048, None, None]
[0204 23:12:14 @registry.py:125] group3/block1/conv1 input: [None, 2048, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block1/conv1 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block1/conv2 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block1/conv2 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block1/conv3 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block1/conv3 output: [None, 2048, None, None]
[0204 23:12:14 @registry.py:125] group3/block2/conv1 input: [None, 2048, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block2/conv1 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block2/conv2 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block2/conv2 output: [None, 512, None, None]
[0204 23:12:14 @registry.py:125] group3/block2/conv3 input: [None, 512, None, None]
[0204 23:12:14 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:14 @registry.py:133] group3/block2/conv3 output: [None, 2048, None, None]
[0204 23:12:14 @registry.py:125] fpn input: [None, 256, None, None],[None, 512, None, None],[None, 1024, None, None],[None, 2048, None, None]
[0204 23:12:14 @registry.py:125] fpn/lateral_1x1_c2 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/lateral_1x1_c2 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/lateral_1x1_c3 input: [None, 512, None, None]
[0204 23:12:14 @registry.py:133] fpn/lateral_1x1_c3 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/lateral_1x1_c4 input: [None, 1024, None, None]
[0204 23:12:14 @registry.py:133] fpn/lateral_1x1_c4 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/lateral_1x1_c5 input: [None, 2048, None, None]
[0204 23:12:14 @registry.py:133] fpn/lateral_1x1_c5 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/upsample_lat5 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/upsample_lat5 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/upsample_lat4 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/upsample_lat4 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/upsample_lat3 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/upsample_lat3 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/posthoc_3x3_p2 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/posthoc_3x3_p2 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/posthoc_3x3_p3 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/posthoc_3x3_p3 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/posthoc_3x3_p4 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/posthoc_3x3_p4 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/posthoc_3x3_p5 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/posthoc_3x3_p5 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] fpn/maxpool_p6 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn/maxpool_p6 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] fpn output: [None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None]
[0204 23:12:14 @registry.py:125] rpn input: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] rpn/conv0 input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] rpn/conv0 output: [None, 256, None, None]
[0204 23:12:14 @registry.py:125] rpn/class input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] rpn/class output: [None, 3, None, None]
[0204 23:12:14 @registry.py:125] rpn/box input: [None, 256, None, None]
[0204 23:12:14 @registry.py:133] rpn/box output: [None, 12, None, None]
[0204 23:12:14 @registry.py:133] rpn output: [None, None, 3],[None, None, 3, 4]
[0204 23:12:18 @registry.py:125] fastrcnn input: [None, 256, 7, 7]
[0204 23:12:18 @registry.py:125] fastrcnn/fc6 input: [None, 256, 7, 7]
[0204 23:12:18 @registry.py:133] fastrcnn/fc6 output: [None, 1024]
[0204 23:12:18 @registry.py:125] fastrcnn/fc7 input: [None, 1024]
[0204 23:12:18 @registry.py:133] fastrcnn/fc7 output: [None, 1024]
[0204 23:12:18 @registry.py:133] fastrcnn output: [None, 1024]
[0204 23:12:18 @registry.py:125] fastrcnn/outputs input: [None, 1024]
[0204 23:12:18 @registry.py:125] fastrcnn/outputs/class input: [None, 1024]
[0204 23:12:18 @registry.py:133] fastrcnn/outputs/class output: [None, 81]
[0204 23:12:18 @registry.py:125] fastrcnn/outputs/box input: [None, 1024]
[0204 23:12:18 @registry.py:133] fastrcnn/outputs/box output: [None, 324]
[0204 23:12:18 @registry.py:133] fastrcnn/outputs output: [None, 81],[None, 81, 4]
[0204 23:12:19 @registry.py:125] maskrcnn input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:125] maskrcnn/fcn0 input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:133] maskrcnn/fcn0 output: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:125] maskrcnn/fcn1 input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:133] maskrcnn/fcn1 output: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:125] maskrcnn/fcn2 input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:133] maskrcnn/fcn2 output: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:125] maskrcnn/fcn3 input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:133] maskrcnn/fcn3 output: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:125] maskrcnn/deconv input: [None, 256, 14, 14]
[0204 23:12:19 @registry.py:133] maskrcnn/deconv output: [None, 256, 28, 28]
[0204 23:12:19 @registry.py:125] maskrcnn/conv input: [None, 256, 28, 28]
[0204 23:12:19 @registry.py:133] maskrcnn/conv output: [None, 80, 28, 28]
[0204 23:12:19 @registry.py:133] maskrcnn output: [None, 80, 28, 28]
[0204 23:12:19 @registry.py:125] idn/idn input: [None, 256, None, None]
[0204 23:12:19 @registry.py:125] idn/idn/conv0 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv0 output: [None, 128, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/conv1 input: [None, 128, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv1 output: [None, 128, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/conv2 input: [None, 128, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv2 output: [None, 256, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/conv3 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv3 output: [None, 256, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/pool0 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/pool0 output: [None, 256, None, None]
[0204 23:12:19 @registry.py:125] idn/idn/conv4 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv4 output: [None, 256, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/conv5 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv5 output: [None, 256, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:19 @registry.py:125] idn/idn/conv6 input: [None, 256, None, None]
[0204 23:12:19 @registry.py:133] idn/idn/conv6 output: [None, 256, None, None]
[0204 23:12:19 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:20 @registry.py:125] idn/idn/fc1 input: [None, 256, 15, 15]
[0204 23:12:20 @registry.py:133] idn/idn/fc1 output: [None, 1000]
[0204 23:12:20 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:20 @registry.py:125] idn/idn/fc2 input: [None, 1000]
[0204 23:12:20 @registry.py:133] idn/idn/fc2 output: [None, 1000]
[0204 23:12:20 @batch_norm.py:164] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0204 23:12:20 @registry.py:125] idn/idn/idn_feat input: [None, 1000]
[0204 23:12:20 @registry.py:133] idn/idn/idn_feat output: [None, 256]
[0204 23:12:20 @registry.py:133] idn/idn output: [None, 256]
[0204 23:12:20 @regularize.py:95] regularize_cost() found 10 variables to regularize.
[0204 23:12:20 @regularize.py:20] The following tensors will be regularized: idn/idn/conv0/W:0, idn/idn/conv1/W:0, idn/idn/conv2/W:0, idn/idn/conv3/W:0, idn/idn/conv4/W:0, idn/idn/conv5/W:0, idn/idn/conv6/W:0, idn/idn/fc1/W:0, idn/idn/fc2/W:0, idn/idn/idn_feat/W:0
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/Rank with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/add_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/mod_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/range_1/start with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/range_1/delta with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/range_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/ListDiff with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/concat/axis with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/concat with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/GatherV2/axis with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/GatherV2 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/Const with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/Prod with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/GatherV2_1/axis with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/GatherV2_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/Const_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_3_grad/Prod_1 with an op tower0/cond/Prod_3 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.pyðŸ’¯ UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/Rank with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/add_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/mod_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/range_1/start with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/range_1/delta with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/range_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/ListDiff with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/concat/axis with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/concat with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/GatherV2/axis with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/GatherV2 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/Const with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/Prod with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/GatherV2_1/axis with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/GatherV2_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/Const_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_2/Prod_grad/Prod_1 with an op tower0/cond/while_2/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/Rank with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/add_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/mod_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/range_1/start with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/range_1/delta with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/range_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/ListDiff with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/concat/axis with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/concat with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/GatherV2/axis with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/GatherV2 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/Const with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/Prod with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/GatherV2_1/axis with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/GatherV2_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/Const_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_1_grad/Prod_1 with an op tower0/cond/Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/Rank with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/add_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/mod_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/range_1/start with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/range_1/delta with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/range_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/ListDiff with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/concat/axis with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/concat with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/GatherV2/axis with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/GatherV2 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/Const with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/Prod with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/GatherV2_1/axis with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/GatherV2_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/Const_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_grad/Prod_1 with an op tower0/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/Rank with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/add_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/mod_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/range_1/start with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/range_1/delta with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/range_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/ListDiff with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/concat/axis with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/concat with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/GatherV2/axis with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/GatherV2 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/Const with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/Prod with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/GatherV2_1/axis with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/GatherV2_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/Const_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/Prod_2_grad/Prod_1 with an op tower0/cond/Prod_2 that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/Rank with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/add_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/mod_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/range_1/start with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/range_1/delta with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/range_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/ListDiff with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/concat/axis with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/concat with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/GatherV2/axis with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/GatherV2 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/Const with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/Prod with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/GatherV2_1/axis with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/GatherV2_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/Const_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while/Prod_grad/Prod_1 with an op tower0/cond/while/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/Rank with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/add_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/mod_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/range_1/start with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/range_1/delta with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/range_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/ListDiff with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/concat/axis with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/concat with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/GatherV2/axis with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/GatherV2 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/Const with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/Prod with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/GatherV2_1/axis with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/GatherV2_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/Const_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
WARNING:tensorflow:Tried to colocate tower0/gradients/tower0/cond/while_1/cond/Prod_grad/Prod_1 with an op tower0/cond/while_1/cond/Prod that had a different device: /device:CPU:0 vs /device:GPU:0. Postponing error-checking until all devices are assigned.
[0204 23:12:23 @training.py:347] 'sync_variables_from_main_tower' includes 0 operations.
[0204 23:12:23 @model_utils.py:64] Trainable Variables:
name shape dim

idn/idn/conv0/W:0 [3, 3, 256, 128] 294912
idn/idn/bn0/gamma:0 [128] 128
idn/idn/bn0/beta:0 [128] 128
idn/idn/conv1/W:0 [3, 3, 128, 128] 147456
idn/idn/bn1/gamma:0 [128] 128
idn/idn/bn1/beta:0 [128] 128
idn/idn/conv2/W:0 [3, 3, 128, 256] 294912
idn/idn/bn2/gamma:0 [256] 256
idn/idn/bn2/beta:0 [256] 256
idn/idn/conv3/W:0 [3, 3, 256, 256] 589824
idn/idn/bn3/gamma:0 [256] 256
idn/idn/bn3/beta:0 [256] 256
idn/idn/conv4/W:0 [3, 3, 256, 256] 589824
idn/idn/bn4/gamma:0 [256] 256
idn/idn/bn4/beta:0 [256] 256
idn/idn/conv5/W:0 [3, 3, 256, 256] 589824
idn/idn/bn5/gamma:0 [256] 256
idn/idn/bn5/beta:0 [256] 256
idn/idn/conv6/W:0 [3, 3, 256, 256] 589824
idn/idn/bn6/gamma:0 [256] 256
idn/idn/bn6/beta:0 [256] 256
idn/idn/fc1/W:0 [57600, 1000] 57600000
idn/idn/fc1/b:0 [1000] 1000
idn/idn/bn7/gamma:0 [1000] 1000
idn/idn/bn7/beta:0 [1000] 1000
idn/idn/fc2/W:0 [1000, 1000] 1000000
idn/idn/fc2/b:0 [1000] 1000
idn/idn/bn8/gamma:0 [1000] 1000
idn/idn/bn8/beta:0 [1000] 1000
idn/idn/idn_feat/W:0 [1000, 256] 256000
idn/idn/idn_feat/b:0 [256] 256
Total #vars=31, #params=61961904, size=236.37MB
[0204 23:12:23 @base.py:208] Setup callbacks graph ...
[0204 23:12:23 @argtools.py:146] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
[0204 23:12:24 @tower.py:130] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0204 23:12:30 @collection.py:151] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 561->892)
loading annotations into memory...
Done (t=0.51s)
creating index...
index created!
[0204 23:12:30 @dataset.py:45] Instances loaded from /data1/coco/annotations/instances_minival2014.json.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 250918.53it/s]
[0204 23:12:30 @timer.py:48] Load Groundtruth Boxes for minival2014 finished, time:0.0225sec.
loading annotations into memory...
Done (t=0.47s)
creating index...
index created!
[0204 23:12:31 @dataset.py:45] Instances loaded from /data1/coco/annotations/instances_minival2014.json.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 174367.43it/s]
[0204 23:12:31 @timer.py:48] Load Groundtruth Boxes for minival2014 finished, time:0.0315sec.
[0204 23:12:31 @summary.py:46] [MovingAverageSummary] 75 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0204 23:12:31 @summary.py:93] Summarizing collection 'summaries' of size 78.
[0204 23:12:35 @base.py:229] Creating the session ...
2019-02-04 23:12:35.297084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:09:00.0
totalMemory: 11.91GiB freeMemory: 11.55GiB
2019-02-04 23:12:35.297112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-02-04 23:12:35.514816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-04 23:12:35.514841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929] 0
2019-02-04 23:12:35.514846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0: N
2019-02-04 23:12:35.515089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12074 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:09:00.0, compute capability: 6.1)
[0204 23:12:39 @base.py:235] Initializing the session ...
[0204 23:12:39 @sessinit.py:204] Variables to restore from dict: group1/block2/conv1/bn/mean/EMA:0, group2/block1/conv1/bn/gamma:0, group2/block1/conv3/bn/variance/EMA:0, group0/block1/conv2/W:0, group2/block2/conv1/bn/gamma:0, rpn/class/b:0, group2/block5/conv1/W:0, group2/block0/conv1/bn/mean/EMA:0, rpn/class/W:0, group0/block0/convshortcut/W:0, maskrcnn/conv/W:0, conv0/bn/mean/EMA:0, group0/block0/convshortcut/bn/gamma:0, group1/block3/conv1/bn/mean/EMA:0, group0/block2/conv3/W:0, group1/block1/conv3/bn/mean/EMA:0, group0/block2/conv1/W:0, group2/block0/conv2/W:0, group1/block1/conv2/bn/gamma:0, group3/block0/conv1/bn/variance/EMA:0, fpn/posthoc_3x3_p2/b:0, group1/block0/conv2/bn/variance/EMA:0, group1/block3/conv3/bn/variance/EMA:0, group0/block1/conv1/bn/mean/EMA:0, group0/block2/conv1/bn/mean/EMA:0, group1/block2/conv2/bn/gamma:0, group0/block1/conv2/bn/beta:0, group0/block2/conv3/bn/mean/EMA:0, group2/block5/conv1/bn/variance/EMA:0, group2/block2/conv2/bn/beta:0, fastrcnn/outputs/box/W:0, fpn/posthoc_3x3_p2/W:0, group1/block3/conv1/W:0, group2/block5/conv2/bn/gamma:0, group0/block0/conv3/bn/variance/EMA:0, fpn/lateral_1x1_c3/b:0, group3/block2/conv2/bn/mean/EMA:0, group1/block3/conv1/bn/beta:0, group3/block2/conv1/bn/gamma:0, conv0/bn/gamma:0, group2/block4/conv3/W:0, group1/block0/convshortcut/bn/mean/EMA:0, group3/block0/conv1/W:0, group2/block3/conv1/bn/variance/EMA:0, group3/block0/convshortcut/bn/gamma:0, group3/block1/conv2/W:0, group1/block1/conv2/bn/mean/EMA:0, group3/block2/conv1/bn/variance/EMA:0, group1/block2/conv1/bn/gamma:0, group0/block0/conv3/bn/mean/EMA:0, maskrcnn/fcn3/W:0, group1/block0/conv1/bn/beta:0, group0/block0/conv1/bn/mean/EMA:0, rpn/box/b:0, group0/block2/conv1/bn/gamma:0, group0/block1/conv1/bn/variance/EMA:0, group3/block0/conv3/bn/beta:0, group2/block5/conv2/bn/beta:0, group2/block0/conv3/bn/gamma:0, group0/block0/conv2/W:0, group2/block0/convshortcut/bn/beta:0, maskrcnn/fcn1/W:0, group2/block1/conv2/bn/beta:0, fpn/posthoc_3x3_p5/b:0, group1/block2/conv3/bn/mean/EMA:0, group3/block0/conv3/bn/mean/EMA:0, group0/block0/convshortcut/bn/mean/EMA:0, group2/block0/convshortcut/W:0, group2/block4/conv2/bn/gamma:0, group2/block1/conv2/W:0, fpn/lateral_1x1_c2/W:0, group3/block0/conv2/bn/variance/EMA:0, group1/block3/conv2/W:0, group2/block0/convshortcut/bn/variance/EMA:0, group2/block3/conv3/W:0, group2/block5/conv1/bn/mean/EMA:0, group3/block2/conv1/W:0, group2/block1/conv2/bn/gamma:0, fpn/posthoc_3x3_p3/W:0, group1/block1/conv3/bn/beta:0, group1/block2/conv3/W:0, group1/block1/conv1/bn/beta:0, group2/block2/conv3/bn/mean/EMA:0, rpn/conv0/b:0, group2/block0/conv3/W:0, group2/block4/conv1/W:0, group2/block0/conv1/bn/variance/EMA:0, group2/block3/conv1/bn/mean/EMA:0, group1/block0/conv3/W:0, conv0/bn/variance/EMA:0, group2/block1/conv1/bn/beta:0, group0/block1/conv2/bn/gamma:0, group2/block4/conv3/bn/gamma:0, group2/block1/conv2/bn/mean/EMA:0, group2/block2/conv3/bn/gamma:0, group3/block0/conv3/bn/variance/EMA:0, group0/block1/conv3/W:0, group3/block1/conv2/bn/gamma:0, group2/block3/conv1/W:0, group2/block5/conv2/W:0, group1/block0/conv2/bn/beta:0, group2/block1/conv1/bn/variance/EMA:0, group3/block1/conv1/W:0, group2/block2/conv3/bn/variance/EMA:0, group1/block2/conv3/bn/beta:0, group1/block1/conv1/bn/gamma:0, group1/block2/conv1/bn/beta:0, group3/block0/convshortcut/bn/beta:0, group0/block0/conv1/W:0, maskrcnn/conv/b:0, group2/block3/conv3/bn/variance/EMA:0, group2/block3/conv3/bn/gamma:0, group2/block2/conv1/bn/mean/EMA:0, group1/block0/conv2/bn/gamma:0, group2/block5/conv3/bn/mean/EMA:0, fastrcnn/fc7/W:0, group1/block1/conv1/bn/mean/EMA:0, group2/block2/conv1/bn/beta:0, group0/block0/conv3/W:0, group3/block2/conv3/bn/mean/EMA:0, group2/block1/conv3/bn/beta:0, fpn/lateral_1x1_c2/b:0, maskrcnn/deconv/b:0, group2/block3/conv3/bn/mean/EMA:0, group2/block4/conv2/bn/mean/EMA:0, group3/block0/conv2/W:0, group1/block0/convshortcut/bn/gamma:0, maskrcnn/fcn3/b:0, group3/block0/convshortcut/bn/mean/EMA:0, group2/block2/conv1/W:0, group1/block0/conv2/bn/mean/EMA:0, group1/block0/convshortcut/bn/variance/EMA:0, group2/block0/conv2/bn/variance/EMA:0, group3/block0/conv1/bn/mean/EMA:0, group1/block2/conv1/W:0, group3/block2/conv3/bn/beta:0, group1/block1/conv2/bn/variance/EMA:0, group1/block1/conv2/bn/beta:0, group1/block0/conv1/bn/variance/EMA:0, group2/block0/conv2/bn/mean/EMA:0, group1/block1/conv1/W:0, group0/block2/conv1/bn/variance/EMA:0, group2/block2/conv3/bn/beta:0, group1/block0/conv3/bn/beta:0, group3/block1/conv3/W:0, group1/block0/conv2/W:0, group0/block1/conv3/bn/gamma:0, fpn/posthoc_3x3_p5/W:0, fastrcnn/fc6/W:0, group0/block2/conv2/bn/variance/EMA:0, group1/block0/conv3/bn/variance/EMA:0, group1/block1/conv3/W:0, group2/block3/conv2/W:0, group0/block1/conv2/bn/mean/EMA:0, group1/block0/convshortcut/bn/beta:0, fpn/lateral_1x1_c4/b:0, group2/block5/conv2/bn/mean/EMA:0, group0/block0/convshortcut/bn/beta:0, group1/block2/conv2/W:0, group3/block1/conv3/bn/beta:0, maskrcnn/fcn0/W:0, group0/block0/conv3/bn/beta:0, group2/block4/conv2/W:0, group3/block1/conv1/bn/mean/EMA:0, group2/block0/convshortcut/bn/gamma:0, group2/block1/conv3/bn/gamma:0, group3/block2/conv2/bn/beta:0, group2/block0/conv3/bn/mean/EMA:0, group3/block0/conv3/bn/gamma:0, group3/block0/conv2/bn/gamma:0, group2/block3/conv2/bn/beta:0, group2/block4/conv1/bn/mean/EMA:0, group2/block0/conv3/bn/beta:0, group1/block3/conv2/bn/gamma:0, fastrcnn/outputs/class/W:0, group2/block3/conv2/bn/gamma:0, fpn/lateral_1x1_c4/W:0, group2/block5/conv3/bn/gamma:0, group1/block1/conv3/bn/variance/EMA:0, group3/block2/conv2/W:0, fastrcnn/fc6/b:0, group2/block2/conv2/W:0, group1/block3/conv3/bn/beta:0, group3/block2/conv2/bn/gamma:0, group1/block3/conv2/bn/beta:0, group2/block5/conv2/bn/variance/EMA:0, fastrcnn/fc7/b:0, group0/block2/conv1/bn/beta:0, maskrcnn/fcn2/W:0, group3/block0/conv1/bn/gamma:0, group2/block2/conv1/bn/variance/EMA:0, group1/block0/conv3/bn/mean/EMA:0, group3/block1/conv2/bn/variance/EMA:0, group3/block2/conv1/bn/mean/EMA:0, group0/block1/conv3/bn/mean/EMA:0, group0/block0/conv2/bn/variance/EMA:0, group2/block2/conv2/bn/variance/EMA:0, group1/block2/conv2/bn/beta:0, group1/block0/conv1/bn/gamma:0, group1/block3/conv3/W:0, group1/block3/conv3/bn/gamma:0, group3/block1/conv1/bn/gamma:0, group2/block3/conv3/bn/beta:0, group3/block2/conv3/W:0, group0/block2/conv2/bn/gamma:0, group2/block3/conv2/bn/mean/EMA:0, group2/block4/conv3/bn/mean/EMA:0, group3/block1/conv3/bn/variance/EMA:0, group1/block3/conv1/bn/gamma:0, group2/block0/conv1/W:0, group0/block2/conv2/bn/mean/EMA:0, group2/block5/conv3/W:0, fpn/posthoc_3x3_p4/W:0, group2/block0/conv2/bn/gamma:0, group0/block2/conv3/bn/gamma:0, group3/block0/conv3/W:0, group0/block0/convshortcut/bn/variance/EMA:0, group1/block3/conv2/bn/mean/EMA:0, group2/block4/conv3/bn/beta:0, group2/block5/conv1/bn/gamma:0, group0/block0/conv1/bn/gamma:0, rpn/conv0/W:0, group3/block2/conv1/bn/beta:0, group3/block0/convshortcut/bn/variance/EMA:0, group2/block0/convshortcut/bn/mean/EMA:0, group1/block0/convshortcut/W:0, group1/block2/conv2/bn/variance/EMA:0, group1/block3/conv2/bn/variance/EMA:0, group2/block5/conv3/bn/variance/EMA:0, group3/block2/conv2/bn/variance/EMA:0, group1/block2/conv3/bn/variance/EMA:0, group2/block3/conv1/bn/beta:0, group2/block5/conv3/bn/beta:0, group2/block0/conv2/bn/beta:0, fpn/posthoc_3x3_p3/b:0, group0/block0/conv2/bn/beta:0, conv0/bn/beta:0, group3/block0/conv2/bn/mean/EMA:0, group2/block2/conv2/bn/gamma:0, group1/block1/conv1/bn/variance/EMA:0, group3/block1/conv1/bn/beta:0, group0/block0/conv2/bn/mean/EMA:0, fpn/lateral_1x1_c3/W:0, group2/block3/conv1/bn/gamma:0, group1/block0/conv3/bn/gamma:0, group0/block0/conv1/bn/beta:0, group0/block0/conv2/bn/gamma:0, rpn/box/W:0, group2/block4/conv2/bn/beta:0, maskrcnn/fcn0/b:0, group0/block2/conv2/bn/beta:0, group0/block1/conv1/W:0, group2/block3/conv2/bn/variance/EMA:0, group3/block2/conv3/bn/gamma:0, group2/block2/conv2/bn/mean/EMA:0, group0/block1/conv1/bn/beta:0, group1/block2/conv3/bn/gamma:0, group3/block2/conv3/bn/variance/EMA:0, group3/block1/conv1/bn/variance/EMA:0, group0/block2/conv3/bn/beta:0, group2/block4/conv2/bn/variance/EMA:0, maskrcnn/fcn1/b:0, group2/block4/conv1/bn/gamma:0, group2/block5/conv1/bn/beta:0, fastrcnn/outputs/box/b:0, group2/block0/conv1/bn/beta:0, group3/block0/convshortcut/W:0, group2/block4/conv3/bn/variance/EMA:0, group3/block1/conv2/bn/beta:0, fpn/lateral_1x1_c5/b:0, fpn/lateral_1x1_c5/W:0, group2/block4/conv1/bn/beta:0, group0/block1/conv3/bn/variance/EMA:0, group2/block1/conv2/bn/variance/EMA:0, group0/block2/conv3/bn/variance/EMA:0, group0/block1/conv2/bn/variance/EMA:0, maskrcnn/deconv/W:0, fastrcnn/outputs/class/b:0, group3/block1/conv2/bn/mean/EMA:0, group1/block3/conv3/bn/mean/EMA:0, group3/block1/conv3/bn/gamma:0, conv0/W:0, group0/block1/conv3/bn/beta:0, group3/block1/conv3/bn/mean/EMA:0, group1/block1/conv2/W:0, group0/block0/conv3/bn/gamma:0, group1/block0/conv1/bn/mean/EMA:0, group0/block1/conv1/bn/gamma:0, group2/block1/conv1/bn/mean/EMA:0, group1/block2/conv1/bn/variance/EMA:0, group1/block3/conv1/bn/variance/EMA:0, group1/block0/conv1/W:0, group2/block4/conv1/bn/variance/EMA:0, group3/block0/conv2/bn/beta:0, group1/block2/conv2/bn/mean/EMA:0, maskrcnn/fcn2/b:0, fpn/posthoc_3x3_p4/b:0, group1/block1/conv3/bn/gamma:0, group2/block1/conv3/W:0, group2/block1/conv3/bn/mean/EMA:0, group0/block2/conv2/W:0, group3/block0/conv1/bn/beta:0, group2/block0/conv3/bn/variance/EMA:0, group2/block0/conv1/bn/gamma:0, group0/block0/conv1/bn/variance/EMA:0, group2/block2/conv3/W:0, group2/block1/conv1/W:0
[0204 23:12:39 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: global_step, idn/idn/bn0/beta, idn/idn/bn0/gamma, idn/idn/bn0/mean/EMA, idn/idn/bn0/variance/EMA, idn/idn/bn1/beta, idn/idn/bn1/gamma, idn/idn/bn1/mean/EMA, idn/idn/bn1/variance/EMA, idn/idn/bn2/beta, idn/idn/bn2/gamma, idn/idn/bn2/mean/EMA, idn/idn/bn2/variance/EMA, idn/idn/bn3/beta, idn/idn/bn3/gamma, idn/idn/bn3/mean/EMA, idn/idn/bn3/variance/EMA, idn/idn/bn4/beta, idn/idn/bn4/gamma, idn/idn/bn4/mean/EMA, idn/idn/bn4/variance/EMA, idn/idn/bn5/beta, idn/idn/bn5/gamma, idn/idn/bn5/mean/EMA, idn/idn/bn5/variance/EMA, idn/idn/bn6/beta, idn/idn/bn6/gamma, idn/idn/bn6/mean/EMA, idn/idn/bn6/variance/EMA, idn/idn/bn7/beta, idn/idn/bn7/gamma, idn/idn/bn7/mean/EMA, idn/idn/bn7/variance/EMA, idn/idn/bn8/beta, idn/idn/bn8/gamma, idn/idn/bn8/mean/EMA, idn/idn/bn8/variance/EMA, idn/idn/conv0/W, idn/idn/conv1/W, idn/idn/conv2/W, idn/idn/conv3/W, idn/idn/conv4/W, idn/idn/conv5/W, idn/idn/conv6/W, idn/idn/fc1/W, idn/idn/fc1/b, idn/idn/fc2/W, idn/idn/fc2/b, idn/idn/idn_feat/W, idn/idn/idn_feat/b, learning_rate
[0204 23:12:39 @sessinit.py:217] Restoring 307 variables from dict ...
[0204 23:13:34 @base.py:242] Graph Finalized.
[0204 23:13:34 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...
[0204 23:13:34 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0204 23:13:36 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0204 23:13:36 @eval.py:250] [EvalCallback] Will evaluate every 25 epochs
[0204 23:13:36 @base.py:274] Start Epoch 1 ...
0%| |0/500[00:00<?,?it/s]2019-02-04 23:13:47.543598: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x137ae830
[0204 23:13:48 @param.py:161] [HyperParamSetter] At global_step=1, learning_rate changes from 0.003300 to 0.003307
0%| |1/500[00:12<1:41:44, 0.08it/s]
[0204 23:13:49 @input_source.py:176]2019-02-04 23:13:49.209144: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
EnqueueThread QueueInput/input_queue Exited.
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
return fn(*args)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
options, feed_dict, fetch_list, target_list, run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[40] = -1 is not in [0, 517)
[[Node: tower0/cond/while/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@tower0/gradients/concat_5""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tower0/cond/while/Identity_3, tower0/cond/while/strided_slice_2, tower0/cond/while/boolean_mask_1/GatherV2/axis/_1305)]]
[[Node: tower0/gradients/tower0/cond/while_2/Prod_grad/DynamicStitch/StackPushV2/_1668 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_11639...tackPushV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](%5E_clooptower0/cond/while_2/NextIteration_3/_690)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py"", line 620, in 
launch_train_with_config(traincfg, trainer)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/interface.py"", line 94, in launch_train_with_config
extra_callbacks=config.extra_callbacks)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/base.py"", line 343, in train_with_defaults
steps_per_epoch, starting_epoch, max_epoch)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/base.py"", line 315, in train
self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
File ""/home/blackfoot/git/tensorpack/tensorpack/utils/argtools.py"", line 176, in wrapper
return func(*args, **kwargs)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/base.py"", line 280, in main_loop
self.run_step() # implemented by subclass
File ""/home/blackfoot/git/tensorpack/tensorpack/train/base.py"", line 180, in run_step
self.hooked_sess.run(self.train_op)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 567, in run
run_metadata=run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1043, in run
run_metadata=run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1134, in run
raise six.reraise(*original_exc_info)
File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
raise value
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1119, in run
return self._sess.run(*args, **kwargs)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1191, in run
run_metadata=run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 971, in run
return self._sess.run(*args, **kwargs)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 900, in run
run_metadata_ptr)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
feed_dict_tensor, options, run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
run_metadata)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[40] = -1 is not in [0, 517)
[[Node: tower0/cond/while/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@tower0/gradients/concat_5""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tower0/cond/while/Identity_3, tower0/cond/while/strided_slice_2, tower0/cond/while/boolean_mask_1/GatherV2/axis/_1305)]]
[[Node: tower0/gradients/tower0/cond/while_2/Prod_grad/DynamicStitch/StackPushV2/_1668 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_11639...tackPushV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](%5E_clooptower0/cond/while_2/NextIteration_3/_690)]]

Caused by op 'tower0/cond/while/GatherV2', defined at:
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py"", line 620, in 
launch_train_with_config(traincfg, trainer)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/interface.py"", line 84, in launch_train_with_config
model._build_graph_get_cost, model.get_optimizer)
File ""/home/blackfoot/git/tensorpack/tensorpack/utils/argtools.py"", line 176, in wrapper
return func(*args, **kwargs)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/tower.py"", line 214, in setup_graph
train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/trainers.py"", line 193, in _setup_graph
grad_list = self._builder.call_for_each_tower(tower_fn)
File ""/home/blackfoot/git/tensorpack/tensorpack/graph_builder/training.py"", line 225, in call_for_each_tower
use_vs=[False] + [True] * (len(self.towers) - 1))
File ""/home/blackfoot/git/tensorpack/tensorpack/graph_builder/training.py"", line 121, in build_on_towers
return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
File ""/home/blackfoot/git/tensorpack/tensorpack/graph_builder/training.py"", line 116, in call_for_each_tower
ret.append(func())
File ""/home/blackfoot/git/tensorpack/tensorpack/train/tower.py"", line 266, in get_grad_fn
return compute_grad_from_inputs(*inputs)
File ""/home/blackfoot/git/tensorpack/tensorpack/train/tower.py"", line 245, in compute_grad_from_inputs
cost = get_cost_fn(*inputs)
File ""/home/blackfoot/git/tensorpack/tensorpack/tfutils/tower.py"", line 286, in **call**
output = self._tower_fn(*args)
File ""/home/blackfoot/git/tensorpack/tensorpack/graph_builder/model_desc.py"", line 262, in _build_graph_get_cost
ret = self.build_graph(*inputs)
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py"", line 166, in build_graph
lambda: zerof())
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
return func(*args, **kwargs)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2063, in cond
orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1913, in BuildCondBranch
original_result = fn()
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py"", line 165, in 
lambda: self.idn_layer(features[0], input_boxes),
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/train.py"", line 417, in idn_layer
ID_loss = cfg.TRAIN.SIM_LR * id_loss(idn_feat_sim, pIOU, dppLabel, intraDppLabel, clssLabel, quality)
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/model_dpp.py"", line 57, in id_loss
intra_denom = tf.divide(whilef(intra_clss_index, mbody), tf.cast(tf.shape(clssLabel)[0], tf.float32))
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/utils/tf_utils.py"", line 35, in whilef
return tf.while_loop(condition, body, params)[1]
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3224, in while_loop
result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2956, in BuildLoop
pred, body, original_loop_vars, loop_vars, shape_invariants)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2893, in _BuildLoop
body_result = body(*packed_vars_for_body)
File ""/home/blackfoot/git/tensorpack/examples/FasterRCNN/utils/tf_utils.py"", line 28, in mbody
Lc = tf.gather(tf.cast(tf.transpose(tf.gather(tf.cast(L,tf.float32), label[index])),tf.float32), label[index])
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 2736, in gather
return gen_array_ops.gather_v2(params, indices, axis, name=name)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 3065, in gather_v2
""GatherV2"", params=params, indices=indices, axis=axis, name=name)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
op_def=op_def)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
op_def=op_def)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in **init**
self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): indices[40] = -1 is not in [0, 517)
[[Node: tower0/cond/while/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@tower0/gradients/concat_5""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tower0/cond/while/Identity_3, tower0/cond/while/strided_slice_2, tower0/cond/while/boolean_mask_1/GatherV2/axis/_1305)]]
[[Node: tower0/gradients/tower0/cond/while_2/Prod_grad/DynamicStitch/StackPushV2/_1668 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_11639...tackPushV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](%5E_clooptower0/cond/while_2/NextIteration_3/_690)]]

MultiProcessMapDataZMQ successfully cleaned-up."
25499,Build tensorflow from source[centos],"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: 3.5
- Installed using virtualenv? pip? conda?: none
- Bazel version (if compiling from source): 0.18.0
- GCC/Compiler version (if compiling from source): GCC8.2.0
- CUDA/cuDNN version: cudatoolkit9.1/cudnn7.4.2
- GPU model and memory:
K20


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I've finished `./configure` and trying to `bazel test` or `bazel build` as instructed in manual.

When I tried to `bazel test` with `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`, I got the following error message [gist](https://gist.github.com/jw447/a191d193b17926fd98f23ce1232d3d28)


When I tried to 'bazel build' with `bazel build --config=opt --config=mkl --config=monolithic --config=cuda //tensorflow/tools/pip_package:build_pip_package`, I got the following error message [gist](https://gist.github.com/jw447/bdfb2aef5dddc2552958115e5cc21f4b).

For the second problem, installing a newer version of glibc is temporarily unavailable. I'm wondering if both questions are triggered by the same reason, the version of glibc?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25495,Inference speed slows down after using graph transform tool,"**System information**
- Have I written custom code: No, I used tensorflow's object detection api's officially released model `ssd-mobilenet-v1-fpn`.
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): built from source, with xla and mkl support, use nvcc as compiler.
- TensorFlow version (use command below): r1.12.0
- Python version: Python 3.6.4 |Anaconda
- Bazel version (if compiling from source): 0.18.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.4.2
- GPU model and memory: 3*NVIDIA 1080Ti, 

**Describe the current behavior**
I trained `ssd-mobilenet-v1-fpn` from tensorflow object detection api with default configurations. As its inference was pretty slow on CPU, I checked using tensorflow's profiler advise and found out the main time consumption was from batch normalization(took 140 ms in total).  Therefore, I tried to use [tensorflow's graph transform tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) to make if faster.

But the inference speed actually slowed down after the graph transformation. To be specific, inference takes 
* 0.53 seconds for original inference graph
* 0.56 seconds for optimized inference graph

Why is the case?

An additional information is that the size actually increases too: the original inference graph is 43MB and optimized inference graph is 44MB. Not sure it will be useful.

Here is the code I used:
```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=fpnssd/frozen_inference_graph.pb \
--out_graph=fpnssd/optimized_frozen_inference_graph.pb \
--inputs=""image_tensor:0"" \
--outputs=""detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0"" \
--transforms=""strip_unused_nodes() fold_constants(ignore_errors=false) fold_batch_norms fold_old_batch_norms""
```


**Describe the expected behavior**
I expect it to be quicker with optimized graph.

**Code to reproduce the issue**
```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=fpnssd/frozen_inference_graph.pb \
--out_graph=fpnssd/optimized_frozen_inference_graph.pb \
--inputs=""image_tensor:0"" \
--outputs=""detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0"" \
--transforms=""strip_unused_nodes() fold_constants(ignore_errors=false) fold_batch_norms fold_old_batch_norms""
```"
25492,"Executor failed to create kernel. Not found: No registered 'NGraphVariable' OpKernel for GPU devices compatible with node {{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()     .  Registered:  device='CPU'","### **Executor failed to create kernel. Not found: No registered 'NGraphVariable' OpKernel for GPU devices compatible with node {{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()     .  Registered:  device='CPU'**

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
Tensorflow GPU 1.11.0-rc1
- Python version:
python 3.6.6
- CUDA/cuDNN version:
cuda 10 cuDNN 7.2
- GPU model and memory:
Quadro M4000
8126MiB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

While giving the command for training tensorflow model _(tensorflow models_master/research/object_detection)_ for object detection, it gives an error given below

### **Command:**
python model_main.py --logtostderr --train_dir=training/ --pipeline_config_path=training/pipeline.config

### **Error:**
E tensorflow/core/common_runtime/executor.cc:623] Executor failed to create kernel. Not found: No registered 'NGraphVariable' OpKernel for GPU devices compatible with node {{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()
    .  Registered:  device='CPU'

     [[{{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'NGraphVariable' OpKernel for GPU devices compatible with node {{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()
    .  Registered:  device='CPU'

     [[{{node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta}} = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1183, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1217, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1408, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 287, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'NGraphVariable' OpKernel for GPU devices compatible with node node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py:277)  = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()
    .  Registered:  device='CPU'

     [[node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py:277)  = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta', defined at:
  File ""model_main.py"", line 109, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1183, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1213, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1171, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/model_lib.py"", line 284, in model_fn
    features[fields.InputDataFields.true_image_shape])
  File ""/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""/usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 2790, in separable_convolution2d
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 650, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 828, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py"", line 364, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 759, in __call__
    self.build(input_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 297, in build
    trainable=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py"", line 278, in add_weight
    getter=vs.get_variable)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 586, in add_weight
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 639, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1484, in get_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1234, in get_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 521, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 1922, in wrapped_custom_getter
    *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1749, in layer_variable_getter
    return _model_variable_getter(getter, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1740, in _model_variable_getter
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 350, in model_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 277, in variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1749, in layer_variable_getter
    return _model_variable_getter(getter, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1740, in _model_variable_getter
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 350, in model_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 277, in variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 492, in _true_getter
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 920, in _get_single_variable
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 145, in __call__
    return cls._variable_call(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 141, in _variable_call
    aggregation=aggregation)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 120, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2441, in default_variable_creator
    expected_shape=expected_shape, import_scope=import_scope)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 147, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 1104, in __init__
    constraint=constraint)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 1218, in _init_from_args
    name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py"", line 77, in variable_op_v2
    shared_name=shared_name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 1357, in variable_v2
    shared_name=shared_name, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

NotFoundError (see above for traceback): No registered 'NGraphVariable' OpKernel for GPU devices compatible with node node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py:277)  = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()
    .  Registered:  device='CPU'

     [[node FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm/beta (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py:277)  = NGraphVariable[_class=[""loc:@Assign""], container="""", dtype=DT_FLOAT, just_looking=false, shape=[64], shared_name="""", _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/GatherV2:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
It was originally created here:
  File ""model_main.py"", line 109, in <module>
    tf.app.run()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 356, in train
    return self  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1183, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1217, in _train_model_default
    saving_listeners)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py"", line 1408, in _train_with_estimator_spec
    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1121, in _create_session
    'the job. Error: %s', e)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session
    self.tf_sess = self._session_creator.create_session()  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session
    init_fn=self._scaffold.init_fn)  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 219, in finalize
    return self  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 268, in get_or_default
    return op  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 199, in default_ready_for_local_init_op
    variables.global_variables())  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))

What can be the solution to this?"
25490,Compilation fails on protobuf_archive,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 1.12
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A, compiling for CPU
- GPU model and memory: N/A, compiling for CPU

**Describe the problem**

Bazel build fails on _protobuf_archive_ with the following error:

> builtin variable 'REPOSITORY_NAME' is referenced before assignment.

I'm building for CPU only because I'm running on an old Xeon that does not have AVX. I've checked out the v1.12 tag and answered _**n**_ to every question on `./configure`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ git checkout v1.12.0
$ ./configure
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files
:
/home/vitor/tensorflow/tools/bazel.rc
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 942c71ae-f211-47ba-bbfe-b9a1b929dd5a
You have bazel 0.22.0 installed.
Please specify the location of python. [Default is /usr/bin/python]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more deta
ils.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
Configuration finished
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files
:
/home/vitor/tensorflow/tools/bazel.rc
Starting local Bazel server and connecting to it...
INFO: Invocation ID: f013afad-a38e-4b6d-a3bd-f2f48351c346
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//c
losure': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replace
ment.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//c
losure': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replace
ment.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
INFO: Elapsed time: 2.204s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    Fetching @io_bazel_rules_closure; fetching
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --incompatible_remove_native_http_archive=false
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files
:
/home/vitor/tensorflow/tools/bazel.rc
INFO: Invocation ID: 86bed03e-2ee9-40f3-a646-9637f9c77ac2
ERROR: /home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/BUILD:591:1: Traceback (most recent cal
l last):
        File ""/home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/BUILD"", line 591
                internal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)
        File ""/home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/protobuf.bzl"", line 269, in inte
rnal_gen_well_known_protos_java
                Label((""%s//protobuf_java"" % REPOSITOR...))
        File ""/home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/protobuf.bzl"", line 269, in Labe
l
                REPOSITORY_NAME
builtin variable 'REPOSITORY_NAME' is referenced before assignment.
ERROR: /home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/BUILD:373:1: Target '@protobuf_archive/
/:android' contains an error and its package is in error and referenced by '@protobuf_archive//:protoc'
ERROR: /home/vitor/.cache/bazel/_bazel_vitor/24c362ffbb1041b0533e8d298828b5ea/external/protobuf_archive/BUILD:373:1: Target '@protobuf_archive/
/:msvc' contains an error and its package is in error and referenced by '@protobuf_archive//:protoc'
ERROR: /home/vitor/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:25:1: Target '@protobuf_archive//:protobuf_python_genproto' contains
 an error and its package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:split_info_proto_py_genproto'
ERROR: /home/vitor/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:25:1: Target '@protobuf_archive//:protoc' contains an error and its
package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:split_info_proto_py_genproto'
ERROR: /home/vitor/tensorflow/tensorflow/contrib/boosted_trees/proto/BUILD:25:1: Target '@protobuf_archive//:protobuf_python' contains an error
 and its package is in error and referenced by '//tensorflow/contrib/boosted_trees/proto:split_info_proto_py'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 8.874s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (166 packages loaded, 2541 targets configured)
    currently loading: tensorflow/core/kernels ... (3 packages)
    Fetching @org_sqlite; fetching
    Fetching @swig; fetching
    Fetching @absl_py; fetching
    Fetching @com_google_absl; fetching
    Fetching @nsync; fetching
```

**Any other info / logs**

I've looked around for this error and I haven't found anything. Closest I got is https://github.com/tensorflow/tensorflow/issues/17709 which seems completely unrelated. I'm following [the documentation](https://www.tensorflow.org/install/source) ipsis litteris and I know nothing of the internals of TensorFlow so the error doesn't give me any clue as to what I can do."
25488,import_meta_graph raises KeyError: InfeedEnqueueTuple when importing a graph trained on TPU,"Function `import_meta_graph` fails with `KeyError: InfeedEnqueueTuple` when importing a meta file of a graph trained on TPU (and with `TPUEstimator`). For graphs trained on CPU (also with `TPUEstimator`) it works correctly. Is this expected behaviour? How can I load a graph with parameters trained on TPU for CPU evaluation?

I noticed a similar error also related to TPU in another Github project, but no solution: https://github.com/tensorflow/minigo/issues/426.

**System information**
- Reproduced on MacOS and on Colab.
- tensorflow 1.12.0 (from PyPI)
- Python 3.6
- CUDA/cuDNN/GPU not used

**Describe the current behavior**
Raises exception `KeyError: InfeedEnqueueTuple`.

**Describe the expected behavior**
Should load the graph without an exception.

**Code to reproduce the issue**
```
 tf.train.import_meta_graph(META_PATH, clear_devices=True)
```
where `META_PATH` is a path to a meta file saved with `TPUEstimator`.

**Other info / logs**
Full traceback:
```
---------------------------------------------------------------------------


KeyError                                  Traceback (most recent call last)


<ipython-input-9-962933bf6153> in <module>()
      1 
----> 2 tf.train.import_meta_graph(META_PATH, clear_devices=True)



/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)
   1672   """"""  # pylint: disable=g-doc-exception
   1673   return _import_meta_graph_with_return_elements(
-> 1674       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
   1675 
   1676 



/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)
   1694           import_scope=import_scope,
   1695           return_elements=return_elements,
-> 1696           **kwargs))
   1697 
   1698   saver = _create_saver_from_imported_meta_graph(



/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)
    804         input_map=input_map,
    805         producer_op_list=producer_op_list,
--> 806         return_elements=return_elements)
    807 
    808     # Restores all the other collections.



/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    486                 'in a future version' if date is None else ('after %s' % date),
    487                 instructions)
--> 488       return func(*args, **kwargs)
    489     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    490                                        _add_deprecated_arg_notice_to_docstring(



/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    389   if producer_op_list is not None:
    390     # TODO(skyewm): make a copy of graph_def so we're not mutating the argument?
--> 391     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
    392 
    393   graph = ops.get_default_graph()



/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
    156     # Remove any default attr values that aren't in op_def.
    157     if node.op in producer_op_dict:
--> 158       op_def = op_dict[node.op]
    159       producer_op_def = producer_op_dict[node.op]
    160       # We make a copy of node.attr to iterate through since we may modify

KeyError: 'InfeedEnqueueTuple'
```"
25487,    from utils import visualization_utils as vis_util ImportError: cannot import name 'visualization_utils',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
25486,"[Bug, Quantization, MKL] Rounding error of fake quantization not corresponds to rounding error of real quantization","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): C++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
There is considering quantized_conv2d() operation. If kernel type is quint8 than simple kernel will be performed. But if kernel type is qint8 than mkl kernel will be performed.
If I use not mkl kernel for quantized_conv2d() then rounding errors correspond both for fake quantization and for real quantization:
```
import tensorflow as tf
import numpy as np

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')
qW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
qAW = tf.dequantize(*qAW, mode='MIN_FIRST')
result = tf.reduce_mean(tf.abs(fqAW - qAW))

print('Result :', tf.Session().run(result))
```
```
Result : 2.017457e-07
```
But if I use mkl kernel for quantized_conv2d() than for all possible quantization modes combinations rounding errors aren't corresponding:
```
import tensorflow as tf
import numpy as np

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

modes = ['MIN_COMBINED', 'MIN_FIRST', 'SCALED']
modes_variants = [[i, j, k] for i in modes for j in modes for k in modes]
for variant in modes_variants:
    qA = tf.quantize(A, min_A, max_A, tf.quint8, mode=variant[0])
    qW = tf.quantize(W, min_W, max_W, tf.qint8, mode=variant[1])
    qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
    qAW = tf.dequantize(*qAW, mode=variant[2])
    result = tf.reduce_mean(tf.abs(fqAW - qAW))

    print('{:<50} - {:.4}'.format(str(variant), tf.Session().run(result)))
```
```
['MIN_COMBINED', 'MIN_COMBINED', 'MIN_COMBINED']   - 6.242
['MIN_COMBINED', 'MIN_COMBINED', 'MIN_FIRST']      - 6.926
['MIN_COMBINED', 'MIN_COMBINED', 'SCALED']         - 6.526
['MIN_COMBINED', 'MIN_FIRST', 'MIN_COMBINED']      - 6.111
['MIN_COMBINED', 'MIN_FIRST', 'MIN_FIRST']         - 6.051
['MIN_COMBINED', 'MIN_FIRST', 'SCALED']            - 6.575
['MIN_COMBINED', 'SCALED', 'MIN_COMBINED']         - 5.819
['MIN_COMBINED', 'SCALED', 'MIN_FIRST']            - 5.704
['MIN_COMBINED', 'SCALED', 'SCALED']               - 7.089
['MIN_FIRST', 'MIN_COMBINED', 'MIN_COMBINED']      - 6.403
['MIN_FIRST', 'MIN_COMBINED', 'MIN_FIRST']         - 6.296
['MIN_FIRST', 'MIN_COMBINED', 'SCALED']            - 5.376
['MIN_FIRST', 'MIN_FIRST', 'MIN_COMBINED']         - 5.7
['MIN_FIRST', 'MIN_FIRST', 'MIN_FIRST']            - 6.233
['MIN_FIRST', 'MIN_FIRST', 'SCALED']               - 6.769
['MIN_FIRST', 'SCALED', 'MIN_COMBINED']            - 6.188
['MIN_FIRST', 'SCALED', 'MIN_FIRST']               - 6.119
['MIN_FIRST', 'SCALED', 'SCALED']                  - 5.869
['SCALED', 'MIN_COMBINED', 'MIN_COMBINED']         - 1.38
['SCALED', 'MIN_COMBINED', 'MIN_FIRST']            - 1.395
['SCALED', 'MIN_COMBINED', 'SCALED']               - 1.383
['SCALED', 'MIN_FIRST', 'MIN_COMBINED']            - 1.389
['SCALED', 'MIN_FIRST', 'MIN_FIRST']               - 1.395
['SCALED', 'MIN_FIRST', 'SCALED']                  - 1.405
['SCALED', 'SCALED', 'MIN_COMBINED']               - 1.352
['SCALED', 'SCALED', 'MIN_FIRST']                  - 1.355
['SCALED', 'SCALED', 'SCALED']                     - 1.357
```
**Other info / logs**
Tensorflow is build by command 
```
bazel build --config=mkl --config=opt --copt=-DINTEL_MKL_QUANTIZED -k //tensorflow/tools/pip_package:build_pip_package
```
StackOverflow link:
```
https://stackoverflow.com/questions/54518253/rounding-error-of-fake-quantization-not-corresponds-to-rounding-error-of-real-qu
```"
25484,initialize variables from other tensor slow in tf1.12.0: comparison tf0.12 vs tf1.12.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- TensorFlow version (use command below): 1.12.0 and 0.12
- Python version: 3.5 (for tf1.12.0) and 2.7 (for tf0.12)

**Describe the current behavior**
I am trying to use weight normalization with data-dependent initialization as reported in Salimans Kingma 2016 https://arxiv.org/pdf/1602.07868.pdf
I use two approaches: 1 sharing the variables between initialization and convolution, 2 creating initializers that will be used in convolution to create the variables accordingly.

It seems there are substantial differences in tf0.12 and tf1.12.0 in terms of initialize a variable throught the value of another tensor.
For example, the following code (with 6 layers) is very slow in the newer tf1.12.0.
approx 2 sec in tf.0.12
approx 70 sec in tf1.12.0
The creation of the graph in tf.1.12.0 is progressively getting slower for each extra added layer..

**Describe the expected behavior**
I would expect both version to be equally fast, or the newer version to be faster. In any case I would not expect the creation of the graph to slow down at each extra added layer. Also I cannot understand while sharing variables should be slower than instantiating from initializer with constant tensor...

**Code to reproduce the issue**

```
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import tensorflow as tf
import numpy as np
from pprint import pprint
import time


network_architecture = {
    ""channels"" : 10,  # Size of z variables.
    ""num_layers"" : 6,  # Number of resnet blocks for each downsampling layer.
}

def initialize_conv2dwn_vars(x, kernel_shape, output_channels, stride, padding, init_scale=1.0, mask=None):

    input_shape = x.get_shape()
    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]
    stride_shape = [1, stride[0], stride[1], 1]

    v_inizializer = tf.random_normal_initializer(0, 0.05)
    v = tf.get_variable(""v"", filter_shape, tf.float32, v_inizializer)
#     see https://www.tensorflow.org/api_docs/python/tf/Variable#initialized_value
    v_aux = v.initialized_value()
    
    if mask is not None:  # used for auto-regressive convolutions.
        v_aux = mask * v_masked
    
    v_norm = tf.nn.l2_normalize(v_aux, [0, 1, 2])
    x_init = tf.nn.conv2d(x, v_norm, strides=stride_shape, padding=padding) # ***
    m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])
    scale_init = init_scale / tf.sqrt(v_init + 1e-10)

    h_aux = tf.reshape(scale_init, [1, 1, 1, -1]) * (x_init - tf.reshape(m_init, [1, 1, 1, -1]))

    g = tf.get_variable(""g"", initializer=tf.log(scale_init) / 3.0)
    b = tf.get_variable(""b"", initializer=-m_init * scale_init)
            
    return h_aux

def initializers_for_conv2dwn_vars(x, kernel_shape, output_channels, stride, padding, init_scale=1.0, mask=None):

    input_shape = x.get_shape()
    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]
    stride_shape = [1, stride[0], stride[1], 1]
    
    v_aux = tf.constant(np.random.normal(loc=0, scale=0.05, size=filter_shape), dtype=tf.float32, name=""v_aux"")

    if mask is not None:  # used for auto-regressive convolutions.
        v_aux = mask * v_masked
    
    v_norm = tf.nn.l2_normalize(v_aux, [0, 1, 2])
    x_init = tf.nn.conv2d(x, v_norm, strides=stride_shape, padding=padding) # ***
    m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])
    scale_init = init_scale / tf.sqrt(v_init + 1e-10)

    def g_inizializer(*args, **kwargs):
        return tf.log(scale_init) / 3.0
    
    def b_inizializer(*args, **kwargs):
        return -m_init * scale_init

    def v_inizializer(*args, **kwargs):
        return v_aux
    
    h_aux = tf.reshape(scale_init, [1, 1, 1, -1]) * (x_init - tf.reshape(m_init, [1, 1, 1, -1]))
            
    return {'v' : v_inizializer, 'g' : g_inizializer, 'b' : b_inizializer}, h_aux


def conv2dwn_reuse_vars(inputs, kernel_shape, output_channels, stride, padding, mask):

    input_shape = inputs.get_shape()
    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]
    stride_shape = [1, stride[0], stride[1], 1]
    print(""...ready v"")
    v = tf.get_variable(""v"", shape=filter_shape)
    print(""...ready g"")
    g = tf.get_variable(""g"", shape=[output_channels]) #initializer=initializers['g'],
    print(""...ready b"")
    b = tf.get_variable(""b"", shape=[output_channels]) # initializer=initializers['b'],
    print(""...done vars"")
    if mask is not None:
        v = mask * v

    # use weight normalization (Salimans & Kingma, 2016)
    w = tf.reshape(tf.exp(g), [1, 1, 1, output_channels]) * tf.nn.l2_normalize(v, [0, 1, 2])

    # calculate convolutional layer output
    b = tf.reshape(b, [1, 1, 1, -1])
    
    print(""...ready"")
    r = tf.nn.conv2d(inputs, w, stride_shape, padding) + b
    print(""...done"")
        
    return r

def conv2dwn_create_vars(inputs, initializers, kernel_shape, output_channels, stride, padding, mask):

    input_shape = inputs.get_shape()
    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]
    stride_shape = [1, stride[0], stride[1], 1]
    print(""...ready v"")
    v = tf.get_variable(""v"", shape=filter_shape, initializer=initializers['v'])
    print(""...ready g"")
    g = tf.get_variable(""g"", shape=[output_channels], initializer=initializers['g'])
    print(""...ready b"")
    b = tf.get_variable(""b"", shape=[output_channels], initializer=initializers['b'])
    print(""...done vars"")
    if mask is not None:
        v = mask * v

    # use weight normalization (Salimans & Kingma, 2016)
    w = tf.reshape(tf.exp(g), [1, 1, 1, output_channels]) * tf.nn.l2_normalize(v, [0, 1, 2])

    # calculate convolutional layer output
    b = tf.reshape(b, [1, 1, 1, -1])
    
    print(""...ready"")
    r = tf.nn.conv2d(inputs, w, stride_shape, padding) + b
    print(""...done"")
        
    return r


# In[8]:


# REUSE VARIABLES

def conv2d_weightnorm_layer(name, inputs, inputs_aux, n_channels, kernel_shape=(3,3), stride=(1,1), init_scale=1.0, mask=None):
    
    conv2dwn_kwargs = {""kernel_shape"" : kernel_shape,
                       ""stride"" : stride,
                       ""padding"" : 'SAME',
                       ""mask"" : mask
    }

    print(""creating layer "" + name)
    
    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):
        h_aux = initialize_conv2dwn_vars(inputs_aux,
                                      output_channels = n_channels,
                                      init_scale = init_scale,
                                      **conv2dwn_kwargs)

    print(""middle"")

    with tf.variable_scope(name, reuse=True):#, reuse=tf.AUTO_REUSE):
        h = conv2dwn_reuse_vars(inputs, output_channels = n_channels, **conv2dwn_kwargs)
        
    print(""done"")
    print(h, h_aux)
        
    return h, h_aux


class Network:
    
    def __init__(self, network_architecture):
        self._num_layers = network_architecture[""num_layers""]
        self._channels = network_architecture[""channels""]
    
    def _build_net(self, x):
        
        h, h_aux = conv2d_weightnorm_layer(""first_layer_conv"",
                                        x,
                                        x,
                                        self._channels,
                                        kernel_shape = (5,5),
                                        stride = (2,2)
                                       )

        print(""start loop"")
        for i in range(self._num_layers):
            print(""\n layer %d""%i)
            h, h_aux = conv2d_weightnorm_layer(""layer%d""%i,
                                    h,
                                    h_aux,
                                    self._channels,
                                    kernel_shape = (5,5),
                                    stride = (1,1)
                                   )
            print(""DONE layer %d \n""%i)
                
        return h, h_aux
    


# In[9]:

tf.reset_default_graph()

print(""\n\nGRAPH CREATION WITH WEIGHT SHARING...\n\n"")
t_i = time.time()

x = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])
net = Network(network_architecture)
output = net._build_net(x)

t_f = time.time()
print(""\nEND OF GRAPH CREATION WITH WEIGHT SHARING"")
print(""time : %g s\n\n""%(t_f - t_i))

# In[ ]:


g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
pprint([str(var.name)+"" ""+str(var.get_shape().as_list()) for var in g_vars])
print(len(g_vars))


# In[ ]:


# PASS INITIALIZERS

def conv2d_weightnorm_layer(name, inputs, inputs_aux, n_channels, kernel_shape=(3,3), stride=(1,1), init_scale=1.0, mask=None):
    
    conv2dwn_kwargs = {""kernel_shape"" : kernel_shape,
                       ""stride"" : stride,
                       ""padding"" : 'SAME',
                       ""mask"" : mask
    }

    print(""creating layer "" + name)
    
    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):
        initializers, h_aux = initializers_for_conv2dwn_vars(inputs_aux,
                                                  output_channels = n_channels,
                                                  init_scale = init_scale,
                                                  **conv2dwn_kwargs)
    
    print(""middle"")

    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):
        h = conv2dwn_create_vars(inputs,
                                initializers = initializers,
                                output_channels = n_channels,
                                **conv2dwn_kwargs)
        
    print(""done"")
    print(h, h_aux)
    
    return h, h_aux


class Network:
    
    def __init__(self, network_architecture):
        self._num_layers = network_architecture[""num_layers""]
        self._channels = network_architecture[""channels""]
    
    def _build_net(self, x):
        
        h, h_aux = conv2d_weightnorm_layer(""first_layer_conv"",
                                        x,
                                        x,
                                        self._channels,
                                        kernel_shape = (5,5),
                                        stride = (2,2)
                                       )

        print(""start loop"")
        for i in range(self._num_layers):
            print(""\n layer %d""%i)
            h, h_aux = conv2d_weightnorm_layer(""layer%d""%i,
                                    h,
                                    h_aux,
                                    self._channels,
                                    kernel_shape = (5,5),
                                    stride = (1,1)
                                   )
            print(""DONE layer %d \n""%i)
                
        return h, h_aux
    


# In[ ]:


tf.reset_default_graph()

print(""\n\nGRAPH CREATION WITH INITIALIZERS FROM OTHER TENSORS...\n\n"")
t_i = time.time()

x = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])
net = Network(network_architecture)
output = net._build_net(x)

t_f = time.time()
print(""\nEND OF GRAPH CREATION WITH INITIALIZERS FROM OTHER TENSORS"")
print(""time : %g s\n\n""%(t_f - t_i))

# In[ ]:
g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
pprint([str(var.name)+"" ""+str(var.get_shape().as_list()) for var in g_vars])
print(len(g_vars))

```


**Other info**
Also plotting the graph for the two tf versions give completely different results (see graphs attached). It seems in the newer version tf1.12.0 th layer will directly depend on all the previous layers, does this means that more connections will be created in the newer version?
Is this expected? Could you help me understand the mechanism behind and if there is a workaround to have a fast custom data-dependent initialization from a tensor?

IN TF0.12
![wn_tf0 12](https://user-images.githubusercontent.com/9975354/52204247-4e867080-287c-11e9-911f-af81c6ecddb9.png)

IN TF1.12.0
![wn_tf1 12 0](https://user-images.githubusercontent.com/9975354/52204250-50e8ca80-287c-11e9-8685-d3388a151364.png)
"
25481,Here is a list of operators for which you will need custom implementations: AddN.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source):  
tensorflow==1.12.0  
tf-nightly==1.13.0.dev20190126

**Describe the current behavior**
When I convert my custom model to TensorFlow Lite model format, I got the following error message and cannot convert it. My question is that what does ""AddN"" in this message mean. Another question is that why does this model include NEG and RELU. I don't use these layers in original model I create.

**Provide the text output from tflite_convert**
```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. 
If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). 
Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). 
Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, NEG, PRELU, RELU, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SUB, TRANSPOSE_CONV. 
Here is a list of operators for which you will need custom implementations: AddN.
```

This file is the target file that I tried to convert.  
[savedmodel.zip](https://github.com/tensorflow/tensorflow/files/2826784/savedmodel.zip)

**Any other info / logs**
This is the code of file conversion I tried.
```
tflite_convert --output_file tensorflow_lite/model.tflite --saved_model_dir savedmodel
```
"
25480,C- How to draw bounding-box using tensorflow c_api,"### In python 
I did object detection, Its working fine. Same thing I want to do in C

Here is the code to detect the object object in python

Calling boxes, like following:

```
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')
num_detections = detection_graph.get_tensor_by_name('num_detections:0')
```
similarly for scores, and classes.

Then just called them in session run.

```
(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})
```
Now Drawing the results of the detection

```
vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.80)
```
Now I can able to view the image and detect the object(face).

`cv2.imshow('Object detector', image)`

Question :

**C language**

Now my question is how can I Feed the image data's into graph and detect the object in that image as well as draw the rectangle box using C Programming.
"
25479,tensorflow build 1.13 build fails,"I need tensorflow 1.13 (1.12 or below doesn't fit my need.)
But for now I can only build from source.
command:
` bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
returned:
> **ERROR: /Users/cindy951357/Documents/pythonworkspace/sphinxenv/tensorflow-1.13.0-rc0/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
> Traceback (most recent call last):
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
>     from tensorflow.python.tools.api.generator import doc_srcs
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 72, in <module>
>     from tensorflow.python.ops.standard_ops import *
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py"", line 25, in <module>
>     from tensorflow.python import autograph
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py"", line 37, in <module>
>     from tensorflow.python.autograph.core.converter import ConversionOptions
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/core/converter.py"", line 74, in <module>
>     from tensorflow.python.autograph.pyct import cfg
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/cfg.py"", line 41, in <module>
>     from tensorflow.python.autograph.pyct import compiler
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/compiler.py"", line 30, in <module>
>     import astor
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/__init__.py"", line 14, in <module>
>     from .code_gen import to_source  # NOQA
>   File ""/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/code_gen.py"", line 311
>     def visit_FunctionDef(self, node, async=False):
>                                           ^
> SyntaxError: invalid syntax
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 8612.039s, Critical Path: 400.52s
> INFO: 9224 processes: 9224 local.
> FAILED: Build did NOT complete successfully**

[this discussion](https://github.com/tensorflow/tensorflow/issues/25151) says that we can wait for **rc2**.
But when will rc2 release?
Or can I use python3.6 to build tensorflow 1.13? 
"
25478,TF 1.11.0 Updating non-Variables?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes https://gist.github.com/dvisztempacct/43e738e1651ecf61323ae92cae41c94c
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu Bionic Derivative (System76 PopOS)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
```
pip3 install tensorflow==1.11.0
```
- TensorFlow version (use command below):
```
$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
v1.11.0-0-gc19e29306c 1.11.0
```
- Python version:
```
$ python3 --version
Python 3.6.7
```
- Bazel version (if compiling from source):
n/a
- GCC/Compiler version (if compiling from source):
n/a
- CUDA/cuDNN version:
```
$ dpkg -l | grep -iE 'cuda|cudnn'
ii  cuda-repo-ubuntu1604                             9.0.176-1                           amd64        cuda repository configuration files
ii  system76-cuda                                    0pop2                               amd64        NVIDIA CUDA Compiler / Libraries / Toolkit Metapackage
ii  system76-cuda-9.0                                0pop3                               amd64        NVIDIA CUDA 9.0 Compiler / Libraries / Toolkit
ii  system76-cuda-9.2                                0pop3                               amd64        NVIDIA CUDA 9.2 Compiler / Libraries / Toolkit
ii  system76-cudnn-9.0                               7.1.4~0pop1                         amd64        NVIDIA CUDA Deep Neural Network library (cuDNN) for CUDA 9.0
```
- GPU model and memory:
```
$ lspci | grep VGA
00:02.0 VGA compatible controller: Intel Corporation Device 3e9b
01:00.0 VGA compatible controller: NVIDIA Corporation GP104M [GeForce GTX 1070 Mobile] (rev a1)
$ nvidia-smi
Sun Feb  3 20:54:44 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   52C    P5    16W /  N/A |    682MiB /  8119MiB |     29%      Default |
+-------------------------------+----------------------+----------------------+
```

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Non-variables seem to be being updated.

**Describe the expected behavior**
Non-variables should be initialized when running `tf.global_variables_initializer` and otherwise not updated.

**Code to reproduce the issue**
https://gist.github.com/dvisztempacct/43e738e1651ecf61323ae92cae41c94c

**Other info / logs**
![screenshot from 2019-02-03 20-51-44](https://user-images.githubusercontent.com/37460069/52190947-d2961500-27f6-11e9-9585-27ad0a23432c.png)

I may simply be misunderstanding how `tf.random_normal` is supposed to work, but I had thought if I didn't initialize a `tf.Variable` using the tensor returned by `tf.random_normal` (as I do on line 12) that it would only be initialized by `tf.global_variables_initializer` or similar, and wouldn't be updated or reinitialized upon subsequent runs unless I included such an initializer.

Thanks!"
25476,Gradient calculation in eager mode,"Hi, 
I would like to know what exactly tape.gradient does in eager mode when a batch is passed to it. 
Does it aggregate all gradients? and if so, does it sum them up or average them or what?


Thanks"
25475,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. \n Failed to load the native TensorFlow runtime.,"
**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.8.0
- Python version: 3.6.8
- Installed using pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



I am trying to run an extractor on my pc. Keep getting error no matter what I try. I tried using python 3.7 and 3.6, 64 bit and 32 bit, using tensorflow 1.8 and 1.12, and always get the same error. Has anyone else seen this? 
![untitled](https://user-images.githubusercontent.com/46904845/52184742-108a3d80-27e5-11e9-8a44-ac1afc00342a.png)

*extra context: the program works on my regular laptop. I wanted to try it on my pc and let it do the hard ML stuff and I use my laptop for other stuff. This pc didnt have any software stuff, so i installed python, pycharm and all the packages.*


I'm a noob so it might be a silly mistake. But I spent 3 days doing everything I can and still nothing...


"
25474,Core dumped bfc allocator ,"**System information**
- Have I written custom code (yes):
- OS Platform and Distribution (Linux Ubuntu 18):
- TensorFlow installed from (binary):
- TensorFlow version (1.12.0):
- Python version: 3.6
- CUDA/cuDNN version: cuda9/ cudnn7
- GPU model and memory: 1080Ti 11GB

**Describe the current behavior**
I get the following message during inference on mask rcnn model.
```
freeMemory: 10.74GiB
2019-02-03 22:26:45.730704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-03 22:26:45.988762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-03 22:26:45.988797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-03 22:26:45.988803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-03 22:26:45.989126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10363 MB memory) -> physical GPU (device: 0, name: 1080Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2019-02-03 22:32:17.296211: F tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle 
Aborted (core dumped)
```

**Describe the expected behavior**
Expected behaviour would have been to run without issues.

**Code to reproduce the issue**
1. Download project [here](https://github.com/matterport/Mask_RCNN)
2. Run inference from live video feed with batch size = 1, i.e. one image at a time.

**Other info / logs**
Stack trace from gdb
```
[New Thread 0x7fff6bd33700 (LWP 25492)]
[New Thread 0x7fff63532700 (LWP 25493)]
[New Thread 0x7fff6b532700 (LWP 25494)]
[New Thread 0x7fff6ad31700 (LWP 25495)]
[New Thread 0x7fff6a530700 (LWP 25496)]
[New Thread 0x7fff69d2f700 (LWP 25497)]
[New Thread 0x7fff6952e700 (LWP 25498)]
[New Thread 0x7fff68d2d700 (LWP 25499)]
[New Thread 0x7fff63fff700 (LWP 25500)]
[New Thread 0x7fff62d31700 (LWP 25501)]
[New Thread 0x7fff62530700 (LWP 25502)]
[New Thread 0x7fff61d2f700 (LWP 25503)]
[New Thread 0x7fff6152e700 (LWP 25504)]
[New Thread 0x7fff60d2d700 (LWP 25505)]
[New Thread 0x7fff33fff700 (LWP 25506)]
[New Thread 0x7fff337fe700 (LWP 25507)]
[New Thread 0x7fff32ffd700 (LWP 25508)]
[New Thread 0x7fff327fc700 (LWP 25509)]
[New Thread 0x7fff31ffb700 (LWP 25512)]
[New Thread 0x7fff317fa700 (LWP 25513)]
[New Thread 0x7fff30ff9700 (LWP 25514)]
[New Thread 0x7fff11fff700 (LWP 25515)]
2019-02-03 22:50:14.393995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:65:00.0
2019-02-03 22:50:14.394052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-03 22:50:14.659094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-03 22:50:14.659133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-03 22:50:14.659139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-03 22:50:14.659452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10363 MB memory) -> physical GPU (device: 0, name: 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
[New Thread 0x7fff117fe700 (LWP 25516)]
[New Thread 0x7fff10ffd700 (LWP 25517)]
[New Thread 0x7ffef5fff700 (LWP 25518)]
[New Thread 0x7ffef57fe700 (LWP 25519)]
[New Thread 0x7ffef4ffd700 (LWP 25520)]
[New Thread 0x7ffee1fff700 (LWP 25521)]
[New Thread 0x7ffee17fe700 (LWP 25522)]
[New Thread 0x7ffee0ffd700 (LWP 25523)]
[New Thread 0x7ffebbfff700 (LWP 25524)]
[New Thread 0x7ffebb7fe700 (LWP 25525)]
[New Thread 0x7ffebaffd700 (LWP 25526)]
[New Thread 0x7ffeba7fc700 (LWP 25527)]
[New Thread 0x7ffeb9ffb700 (LWP 25528)]
[New Thread 0x7ffeb97fa700 (LWP 25529)]
[New Thread 0x7ffeb8ff9700 (LWP 25530)]
[New Thread 0x7ffea3fff700 (LWP 25531)]
[New Thread 0x7ffea37fe700 (LWP 25532)]
[New Thread 0x7ffea2ffd700 (LWP 25533)]
[New Thread 0x7ffea27fc700 (LWP 25534)]
[Thread 0x7ffea27fc700 (LWP 25534) exited]
[New Thread 0x7ffea27fc700 (LWP 25535)]
[Thread 0x7ffea27fc700 (LWP 25535) exited]
[New Thread 0x7ffea27fc700 (LWP 25536)]
[Thread 0x7ffea27fc700 (LWP 25536) exited]
[New Thread 0x7ffea27fc700 (LWP 25537)]
[Thread 0x7ffea27fc700 (LWP 25537) exited]
[New Thread 0x7ffea27fc700 (LWP 25538)]
[New Thread 0x7ffea27fc700 (LWP 25539)]
[Thread 0x7ffea27fc700 (LWP 25538) exited]
[Thread 0x7ffea27fc700 (LWP 25539) exited]
[New Thread 0x7ffea27fc700 (LWP 25540)]
[Thread 0x7ffea27fc700 (LWP 25540) exited]
[New Thread 0x7ffea27fc700 (LWP 25541)]
[Thread 0x7ffea27fc700 (LWP 25541) exited]
[New Thread 0x7ffea27fc700 (LWP 25560)]
[Thread 0x7ffea27fc700 (LWP 25560) exited]
[New Thread 0x7ffea27fc700 (LWP 25562)]
[Thread 0x7ffea27fc700 (LWP 25562) exited]
[New Thread 0x7ffea27fc700 (LWP 25563)]
[Thread 0x7ffea27fc700 (LWP 25563) exited]
[New Thread 0x7ffea27fc700 (LWP 25565)]
[Thread 0x7ffea27fc700 (LWP 25565) exited]
[New Thread 0x7ffea27fc700 (LWP 25566)]
[Thread 0x7ffea27fc700 (LWP 25566) exited]
[New Thread 0x7ffea27fc700 (LWP 25567)]
[Thread 0x7ffea27fc700 (LWP 25567) exited]
[New Thread 0x7ffea27fc700 (LWP 25568)]
[Thread 0x7ffea27fc700 (LWP 25568) exited]
[New Thread 0x7ffea27fc700 (LWP 25569)]
[Thread 0x7ffea27fc700 (LWP 25569) exited]
[New Thread 0x7ffea27fc700 (LWP 25570)]
[New Thread 0x7fff731e6800 (LWP 25571)]
[New Thread 0x7fff72de4880 (LWP 25572)]
[New Thread 0x7fff729e2900 (LWP 25573)]
[New Thread 0x7fff725e0980 (LWP 25574)]
[New Thread 0x7fff721dea00 (LWP 25575)]
[New Thread 0x7fff71ddca80 (LWP 25576)]
[New Thread 0x7fff719db700 (LWP 25577)]
[Thread 0x7fff719db700 (LWP 25577) exited]
[New Thread 0x7fff719db700 (LWP 25578)]
[Thread 0x7fff719db700 (LWP 25578) exited]
[New Thread 0x7fff719db700 (LWP 25579)]
[Thread 0x7fff719db700 (LWP 25579) exited]
[New Thread 0x7fff719db700 (LWP 25580)]
[Thread 0x7fff719db700 (LWP 25580) exited]
[New Thread 0x7fff719db700 (LWP 25581)]
[New Thread 0x7fff719db700 (LWP 25582)]
[Thread 0x7fff719db700 (LWP 25581) exited]
[Thread 0x7fff719db700 (LWP 25582) exited]
[New Thread 0x7fff719db700 (LWP 25583)]
[Thread 0x7fff719db700 (LWP 25583) exited]
[New Thread 0x7fff719db700 (LWP 25584)]
[Thread 0x7fff719db700 (LWP 25584) exited]
[New Thread 0x7fff719db700 (LWP 25588)]
[New Thread 0x7fff704f1700 (LWP 25589)]
[New Thread 0x7fff6fcf0700 (LWP 25590)]
[New Thread 0x7fff6f4ef700 (LWP 25591)]
[New Thread 0x7fff6ecee700 (LWP 25592)]
[New Thread 0x7fff6e4ed700 (LWP 25593)]
[New Thread 0x7fff6dcec700 (LWP 25594)]
[New Thread 0x7fff6d4eb700 (LWP 25595)]
[New Thread 0x7fff6ccea700 (LWP 25596)]
[New Thread 0x7ffea1ffb700 (LWP 25597)]
[New Thread 0x7ffea17fa700 (LWP 25598)]
[New Thread 0x7ffea0ff9700 (LWP 25599)]
[New Thread 0x7ffe8dfff700 (LWP 25600)]
[New Thread 0x7ffe8d7fe700 (LWP 25601)]
[New Thread 0x7ffe8cffd700 (LWP 25602)]
2019-02-03 22:57:34.463561: F tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) 

Thread 39 ""python"" received signal SIGABRT, Aborted.
[Switching to Thread 0x7ffeb8ff9700 (LWP 25530)]
0x00007ffff7b3fd7f in raise () from /usr/lib/libc.so.6
```
"
25473,tf.distribution.Normal memory leak on calls to sample() ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- TensorFlow installed from (source or binary):
pip3 installation
- TensorFlow version (use command below):
12.0
- Python version:
3.6.7

**Describe the current behavior**
Type in the code below and watch memory usage increase.

**Describe the expected behavior**
Type in the code below and see no memory usage increases.

**Code to reproduce the issue**

    import tensorflow as tf

    dist=tf.distributions.Normal(loc=0., scale=1.)

    for i in range(1_000_000):
        a=dist.sample()
"
25472,tf2.0 - tf.keras.optimizers.Optimizer.get_updates() does not work,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190203
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When keras optimizer's get_updates() function is called, it raises error ""RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.""

**Describe the expected behavior**
I expect it returns correct operators that can be used to build keras function that changes weights.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Dense(1, input_shape=(1,)))
model.compile(optimizer='adam', loss='mse')
loss = keras.losses.mse(model.input, model.output)
updates = model.optimizer.get_updates(params=model.trainable_weights, loss=loss)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback:
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-4-f4bb57df0d91> in <module>()
      5 loss = keras.losses.mse(model.input, model.output)
      6 model.optimizer.get_updates(
----> 7         params=model.trainable_weights, loss=loss)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in get_updates(self, loss, params)
    440 
    441   def get_updates(self, loss, params):
--> 442     grads = self.get_gradients(loss, params)
    443     grads_and_vars = list(zip(grads, params))
    444     self._assert_valid_dtypes([

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in get_gradients(self, loss, params)
    353     """"""
    354     loss = self._scale_loss(loss)
--> 355     grads = gradients.gradients(loss, params)
    356     if None in grads:
    357       raise ValueError(""An operation has `None` for gradient. ""

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,
    157         gate_gradients, aggregation_method, stop_gradients,
--> 158         unconnected_gradients)
    159   # pylint: enable=protected-access
    160 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    545   """"""Implementation of gradients().""""""
    546   if context.executing_eagerly():
--> 547     raise RuntimeError(""tf.gradients is not supported when eager execution ""
    548                        ""is enabled. Use tf.GradientTape instead."")
    549   if src_graph is None:

RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
```"
25470,Scoping bug with custom layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.12.0
- Python version:
3.6.8
- Bazel version (if compiling from source):
-
- GCC/Compiler version (if compiling from source):
-
- CUDA/cuDNN version:
Using Cpu
- GPU model and memory:

**Describe the current behavior**

I am trying to implement a noisy linear layer in tensorflow, inheriting from tf.keras.layers.Layer . Everything works fine except for reusing variables. This seems to stem from some issue with the scoping: Whenever i use the add_weight function from the superclass and a weight with the same name already existing, it seems to ignore the given reuse-flag in the scope and creates a new variable instead. Interestingly, it does not add a 1 to the variable name in the end as usual in similar cases, but rather adds the 1 to the scope name.
The Code below prints the following variables:

scope/noisy_dense/noisy_kernel:0
scope_1/noisy_dense/noisy_kernel:0
scope/my_variable:0

**Describe the expected behavior**
Instead, i'd expect the weights to be reused. The print would then only be 

scope/noisy_dense/noisy_kernel:0
scope/my_variable:0

**Code to reproduce the issue**

```
import tensorflow as tf

class NoisyDense(tf.keras.layers.Layer):
    def __init__(self,output_dim):
        self.output_dim=output_dim

        super(NoisyDense, self).__init__()

    def build(self, input_shape):
        self.input_dim = input_shape.as_list()[1]
        self.noisy_kernel = self.add_weight(name='noisy_kernel',shape=  (self.input_dim,self.output_dim))

def noisydense(inputs, units):

    layer = NoisyDense(units)

    return layer.apply(inputs)

inputs = tf.placeholder(tf.float32, shape=(1, 10),name=""inputs"")



scope=""scope""
with tf.variable_scope(scope):
    inputs3 = noisydense(inputs,
           1)
    my_variable = tf.get_variable(""my_variable"", [1, 2, 3],trainable=True)


with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
    inputs2 = noisydense(inputs,
           1)
    my_variable = tf.get_variable(""my_variable"", [1, 2, 3],trainable=True)

tvars = tf.trainable_variables()



init=tf.global_variables_initializer()    
with tf.Session() as sess:
    sess.run(init)
    tvars_vals = sess.run(tvars)

for var, val in zip(tvars, tvars_vals):
    print(var.name, val)
```


"
25469,No converter defined for Switch,"This is a similar issue like https://github.com/tensorflow/tensorflow/issues/23397
```
from tensorflow.python.framework import function
from tensorflow.python.ops.parallel_for.gradients import jacobian as tf_jacobian
import numpy as np
import numpy.matlib
import tensorflow as tf

print(tf.__version__)

my_graph = tf.Graph()
with my_graph.as_default():
    x = tf.placeholder(tf.float32, shape=(None,9))
    tf_is_training_ph = tf.placeholder(tf.bool, shape=())
    
    def computeFunc1D(x_1D, is_training):
        x = tf.reshape(x_1D, [1, 9])
        with tf.variable_scope('Test', reuse=tf.AUTO_REUSE):
            dense_out = tf.layers.dense(x, 3, name='dense')
            bn_out = tf.layers.batch_normalization(dense_out, training=is_training, name='bn')
        return tf.reshape(bn_out, [3])
    
    def tensor_jacobian(X, is_training):
        fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)
        J = tf.map_fn(fn_compute_jacobian_1datapt, X)
        return J
    
#    my_tf_jacobian = tensor_jacobian(x, True) # this is working
    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working

with tf.Session(graph=my_graph) as sess:
    tf.global_variables_initializer().run()
    
    x_ = 0.5 * np.random.random((2,9))
    
    [my_tf_jacobian_val
     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: True})
    
    print ""x_ = "", x_
    print ""my_tf_jacobian_val = "", my_tf_jacobian_val
    print ""my_tf_jacobian_val.shape = "", my_tf_jacobian_val.shape
```

This will result in:
```
1.12.0
Traceback (most recent call last):

  File ""<ipython-input-2-b20d7560d318>"", line 1, in <module>
    runfile('/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py', wdir='/home/amdgsutanto/Desktop')

  File ""/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py"", line 94, in execfile
    builtins.execfile(filename, *where)

  File ""/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py"", line 27, in <module>
    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working

  File ""/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py"", line 23, in tensor_jacobian
    J = tf.map_fn(fn_compute_jacobian_1datapt, X)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py"", line 494, in map_fn
    maximum_iterations=n)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3291, in while_loop
    return_same_structure)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3004, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2939, in _BuildLoop
    body_result = body(*packed_vars_for_body)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3260, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py"", line 483, in compute
    packed_fn_values = fn(packed_values)

  File ""/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py"", line 22, in <lambda>
    fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/gradients.py"", line 59, in jacobian
    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 129, in pfor
    outputs.append(converter.convert(loop_fn_output))

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1077, in convert
    output = self._convert_helper(y)

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1223, in _convert_helper
    ""which may run slower"" % (y_op.type, y_op, converted_inputs))

ValueError: No converter defined for Switch
name: ""map/while/loop_body/gradients/map/while/Test/bn/cond/Merge_grad/cond_grad""
op: ""Switch""
input: ""map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul""
input: ""map/while/Test/bn/cond/pred_id""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul""
    }
  }
}

inputs: [WrappedTensor(t=<tf.Tensor 'map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul/pfor/Mul:0' shape=(3, 3) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'map/while/Test/bn/cond/pred_id:0' shape=() dtype=bool>, is_stacked=False, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
```
Would you mind fixing this, similar like https://github.com/tensorflow/tensorflow/issues/23397 ?

P.S.: If the fix made it to nightly build, how to update it on my side? Is it just re-installing (e.g. with pip)? Thanks a lot!"
25468,ImportError: DLL load failed with error code -1073741795,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
25464,Unnecessary logging noise for non-authenticated GCS access,"**System information**

* Ubuntu 16.04
* Python 3.6
* TF 1.12

**Describe the current behavior**

Unnecessary access to GCP metadata endpoint, 10 retries, and lots of logging. 

**Describe the expected behavior**

Should not be trying to access GCP metadata endpoint.

**Code to reproduce the issue**

```
$ python
>> tf.gfile.Exists(""gs://tfds-data"")  # this is a public bucket, doesnâ€™t need any auth
2019-02-03 02:51:58.175696: I tensorflow/core/platform/cloud/retrying_utils.cc:73] The operation failed and will be automatically retried in 0.12376 seconds (attempt 1 out of 10), caused by: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'

...(more of the same)

2019-02-03 02:52:05.330143: I tensorflow/core/platform/cloud/retrying_utils.cc:73] The operation failed and will be automatically retried in 1.29527 seconds (attempt 10 out of 10), caused by: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'
2019-02-03 02:52:06.626410: W tensorflow/core/platform/cloud/google_auth_provider.cc:157] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".
```

(Linking issue tensorflow/datasets#38)"
25463,TensorFlow GCS access does not work from colab ,"**System information**
Using colab.research.google.com

**Describe the current behavior**

Hangs.

```
import tensorflow as tf
tf.io.gfile.exists(â€œgs://tfds-dataâ€)  # which is a public GCS bucket
```

**Describe the expected behavior**

Should not hang."
25462,tflite outputs don't match with tensorflow outputs for conv2d_transpose,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0 (using python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"")
Python version: Python 3.5.4 |Anaconda custom (64-bit)| (default, Nov 20 2017, 18:44:38)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): [GCC 7.2.0] on linux
CUDA/cuDNN version: N/A
GPU model and memory: N/A

**Describe the current behavior**
tflite outputs don't match with tensorflow outputs
**Describe the expected behavior**
tflite outputs is expected to match tensorflow outputs
**Code to reproduce the issue**
<pre>
import tensorflow as tf
import numpy as np

np.random.seed(1234)
tf.random.set_random_seed(1234)

def trans_conv1d(x,
                 num_filters,
                 filter_length,
                 stride):
    batch_size, length, num_input_channels = x.get_shape().as_list()
    x = tf.reshape(x, [batch_size, 1, length, num_input_channels])

    weights = tf.get_variable('W', shape=(1, filter_length, num_filters, num_input_channels))
    biases = tf.get_variable('b', shape=(num_filters,))

    y = tf.nn.conv2d_transpose(
        x,
        filter=weights,
        output_shape=(batch_size, 1, stride * length, num_filters),
        strides=(1, 1, stride, 1),
        padding='SAME',
        data_format='NHWC',
        name=""cnn2d"")
    y = tf.nn.bias_add(y, biases)
    return y

num_filters = 4
filter_length = 40
stride = 8
x = tf.placeholder(dtype = tf.float32, shape = [1, 96, 2], name = ""input"")
y = trans_conv1d(x, num_filters, filter_length, stride)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
input_data = np.array(np.random.rand(1, 96, 2), dtype=np.float32)
output_data_tf = sess.run(y, feed_dict={x:input_data})
converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [x], [y])
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
sess.close()

# tflite test
interpreter = tf.contrib.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model on the same input data.
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()
output_data_tflite = interpreter.get_tensor(output_details[0]['index'])
print(np.array_equal(output_data_tf, output_data_tflite))
</pre>
**Other info / logs**
False"
25461,Feature: Update the environment capture script for system information.,"The current [**environment capture script for TensorFlow**](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh) does not provide all of the system information required for filing bug or performance issues:

- **OS Platform and Distribution** (e.g., Linux Ubuntu 16.04)
- **Mobile device** (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device
- TensorFlow installed from (**source or binary**)
- **TensorFlow version** (use command below)
- **Python** version
- **Bazel** version (if compiling from source)
- **GCC/Compiler** version (if compiling from source)
- **CUDA/cuDNN** version
- **GPU model** and memory

The purpose of this issue would be to modify the environment capture script to print as much of the information above as possible to an end user's console, or to create an extension of the TensorFlow Python API (similar to fastai's [`collect_env.py`](https://github.com/fastai/fastai/blob/110d29d7a407409399c77916b2453e7408a3a450/fastai/utils/collect_env.py) that accomplishes the same goal."
25460,usage of Dataset.window causes TypeError when creating iterator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5

**Describe the current behavior**

Following code is a snippet from [Dataset.window documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window)
```
import tensorflow as tf
dataset = tf.data.Dataset.range(7).window(2)
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()
```

Calling `iterator.get_next()` throws following error:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/execute.py"", line 123, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py"", line 712, in as_dtype
    raise TypeError(""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <tensorflow.python.data.ops.dataset_ops._NestedDatasetComponent object at 0x7fdf882319b0> to a TensorFlow DType.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-29-624c3010a782>"", line 1, in <module>
```

**Describe the expected behavior**
```
next_element = iterator.get_next()
```
should create iterator object.

**Code to reproduce the issue**
See above.

"
25459,"Converting a keras Sequential CNN to estimator and training it removes all layers, then crashes for want of layers","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version: 2.7.15rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: Not installed
- GPU model and memory: GeForce GTX 1050 Ti, 4GB

**Describe the current behavior**

This issue was also mentioned in #21778, but I believe this warrants an issue of its own.

I ran into this problem trying tho make a CNN using the `fashion_mnist` dataset. Instead of training the model, invoking `train` on the model seems to drop all the layers of the original model, and then crash because there are no layers. 

The example below has this output:

````
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_l6v51cf
[<tensorflow.python.keras.layers.core.Reshape object at 0x7fcb25244a90>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fcb07f670f0>, <tensorflow.python.keras.layers.core.Flatten object at 0x7fcb07f7fa20>, <tensorflow.python.keras.layers.core.Dense object at 0x7fcb07f7fb00>]
WARNING:tensorflow:From /home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
[]
```` 

**Describe the expected behavior**
I would expect `train` to train the model without emptying the `layers`-array. Also, the deprecation warning probably shouldn't be there either.

**Code to reproduce the issue**
````
import numpy as np
import tensorflow as tf
from tensorflow import keras

network = keras.Sequential([
            keras.layers.Reshape((10, 10, 1)),
            keras.layers.Conv2D(filters=8, kernel_size=(4, 4), activation=tf.nn.elu),
            keras.layers.Flatten(),
            keras.layers.Dense(1, activation=tf.nn.relu)
        ])

network.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),
                loss='categorical_crossentropy',
                metric='accuracy')

estimator = tf.keras.estimator.model_to_estimator(keras_model=network)

train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x=np.random.rand(10, 100),
        y=np.random.rand(10, 1),
        num_epochs=1,
        shuffle=False)

print(network.layers)

try:
    estimator.train(input_fn=train_input_fn, steps=10)
except:
    print(network.layers)

````

**Other info / logs**
Without the try/except, the error thrown is 

````
Traceback (most recent call last):
  File ""/home/heikki/Koodaus/python_sketches/Estimator.py"", line 26, in <module>
    estimator.train(input_fn=train_input_fn, steps=10)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 278, in model_fn
    labels)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 201, in _clone_and_build_model
    optimizer_iterations=global_step)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/keras/models.py"", line 448, in clone_and_build_model
    _in_place_subclassed_model_reset(clone)
  File ""/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/keras/models.py"", line 318, in _in_place_subclassed_model_reset
    name = layers_to_names[layer]
KeyError: <tensorflow.python.keras.layers.core.Reshape object at 0x7fde28e5ca90>
````
"
25458,TF1.13.0rc0 cuda10: ImportError: libcublas.so.9.0: cannot open shared object file,"### System information
- **OS Platform and Distribution:  Linux Ubuntu 16.04 
- **TensorFlow installed from: Docker Tag 1.13.0rc0-gpu-py3
- **Python version**: 3.5
- **CUDA/cuDNN version**: cuda 10.0
- **GPU model and memory**: 2 * RTX 2080 TI and 2 * GTX 1080

### Describe the problem

With version 1.13.0rc0 (for gpu, py3) from dockerhub an error occurs on:

`import tensorflow as tf`

With version 1.13.0-dev20181228 (for gpu, py3) from dockerhub everything works well.

Environment variables seem to be ok:
LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64 

### Source code / logs
---------------------------------------------------------------------------
```
ImportError                               Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-59571c70a10d> in <module>
      8 get_ipython().run_line_magic('autoreload', '2')
      9 
---> 10 import tensorflow as tf
     11 print(tf.__version__)

/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
25454,multi gpu model error when trying to create model,"Please see also SO question about [this](https://stackoverflow.com/questions/54440168/tf-python-keras-utils-multi-gpu-model-error-on-initializing)

I am using python 3 with tensorflow and multiple gpu configuration, I try to use the [following example][1] to init the multi gpu model, I create a model, It's fine, compiling, running and training, but When I try to add this before the model compilation: 

    from tensorflow.python.keras.utils import multi_gpu_model
    model = multi_gpu_model(model, gpus=2, cpu_merge=False)

I get this error 

> TypeError: int() argument must be a string or a number, not
> 'TensorShape'

Note I am using tf with eager eval 

I found [this][2] reffering to use keras.utils.multi_gpu_model instead of tf.python.keras.utils.multi_gpu_model But when I do that I get this error instead: 

What am i missing here? 

> line 217, in multi_gpu_model
>     with tf.device(x.device): AttributeError: 'DeferredTensor' object has no attribute 'device'

the code for the model is 

    model = Sequential()
    model.add(Flatten(input_shape=(128, 128, 3)))
    model.add(Dense(100, activation=""sigmoid""))
    model.add(Dense(100, activation=""sigmoid""))


Thanks 


  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model
  [2]: https://github.com/keras-team/keras/issues/11408"
25453,Custom gradients at layer level instead of at function level,"I am implementing a customized layer in tensorflow. However, I failed to figure out a way to customize gradients in layers. I would like to add a function to calculate the gradients to weights and gradients to input by myself. Is there any way to do so?

The code should look like this
```
class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs

    def build(self, input_shape):
        self.kernel = self.add_variable(""kernel"", shape=[int(input_shape[-1]), self.num_outputs])

    def call(self, input):
        return tf.matmul(input, self.kernel)
    
    # The function to calculate the gradients by myself
    def _backprop(self,y_delta,x):
        w_delta=some_function(y_delta,x)
        x_delta=some_function2(y_delta,x)
        return w_delta, x_delta
````
"
25452,Memory detected less than real value,"I used tf-1.13-python. But when I ran 'tf.Session()', I found it had only 8.99GB free memory instead of 11GB, which was confirmed with 'nvidia-smi', task manager and GPU-Z.

CUDA:10.0
CUDNN:7.4.1
System:win 10


>>> tf.Session()
2019-02-02 16:02:17.833936: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: AVX2
2019-02-02 16:02:18.143034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.665
pciBusID: 0000:26:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019-02-02 16:02:18.148745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-02 16:02:19.078669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-02 16:02:19.081929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-02-02 16:02:19.083860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-02-02 16:02:19.086029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 8661 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus
 id: 0000:26:00.0, compute capability: 7.5)
"
25451,Incorrect Hann window calculation in spectrogram kernel,"**System information**
- Have I written custom code: No
- OS Platform and Distribution:  N/A
- Mobile device:  N/A
- TensorFlow installed from:  N/A
- TensorFlow version : master
- Python version:  N/A
- Bazel version:  N/A
- GCC/Compiler version:  N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory:  N/A

**Describe the current behavior**
In [tensorflow/core/kernel/spectrogram.cc](https://github.com/tensorflow/tensorflow/blob/d08271d73c9f7d399d16917b47fcfda74806dd02/tensorflow/core/kernels/spectrogram.cc#L34) Hann window is calculated as blow: 
`(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / window_length);`

**Describe the expected behavior**
According to definition of Hann window [https://en.wikipedia.org/wiki/Hann_function](https://en.wikipedia.org/wiki/Hann_function), which should be:
`(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / (window_length - 1));`

Please correct me if I missed anything."
25450,some issues about estimator distribute," Now i'd like to do some distribute training using estimator api.I have some questions about chief,worker,and ps host .
  In code,
   ```
 worker_hosts = FLAGS.worker_hosts.split("","")
      os.environ[""TF_CONFIG""] = json.dumps(
              {
                  ""cluster"": {""chief"": chief_host, ""ps"": ps_hosts, ""worker"": worker_hosts},
                  ""task"": {""type"": FLAGS.job_name, ""index"": FLAGS.task_id},
              }
          )
    print (""run_config.num_ps_replicas = %d"" % run_config.num_ps_replicas)
    print (""run_config.num_worker_replicas = %d"" % run_config.num_worker_replicas)
    print (""run_config.master = %s"" % run_config.master)
    print (""run_config.task_type = %s"" % run_config.task_type)
    print (""run_config.task_id = %d"" % run_config.task_id)
    print (""run_config.is_chief = %d"" % run_config.is_chief)
```
And I found ,chief host or ps host can be missed if task_type is chief or ps. And worker host must be specified if task_type is worker.Is is reasonable that if chief host missed?
`the command is python main.py  --job_name=""chief"" --task_id=0 ,
`
and then the log as below:
```
INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': [''], 'ps': ['']}, 'task': {'type': 'chief', 'index': 0}}
run_config.num_ps_replicas = 1
run_config.num_worker_replicas = 1
run_config.master = grpc://
run_config.task_type = chief
run_config.task_id = 0
run_config.is_chief = 1
start_step = 0
```
if task_type is chief,the ps and worker default num is 1?and if chief host is not specified,can it run directly?
 "
25448,Feature: modify the TF 2.0 upgrade script to convert .ipynb files. ,"Currently, the [TensorFlow 2.0 upgrade script](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) only supports Python (`.py`) files, not Jupyter notebooks (`.ipynb`). This can make converting documentation frustrating and time consuming.

The purpose of this issue is to **modify the upgrade script to accept a Jupyter notebook, convert it from TensorFlow 1.x to TF 2.0, and to export out a replacement notebook**.

For more information, contact @VikramTiwari, @JerryKurata, and @lc0."
25446,tf.ConfigProto usage on TF 2.0,"On TensorFlow 1.X, there are various important parameters set by passing `tf.ConfigProto` to `tf.Session(config=...)` or `tf.enable_eager_execution(config=...)`.  For example, to use `NCCL`, it is useful to set the visible GPUs for a session with `config.gpu_options.visible_device_list`.

My understanding is that TensorFlow 2.0 no longer has a way to set this configuration â€” both `tf.Session` and `tf.enable_eager_execution` are gone.  Is there an alternate way to set this config?


[Related StackOverflow question](https://stackoverflow.com/questions/54485110/whats-the-equivalent-of-initializing-a-tf-session-with-tf-configproto-in-tensor)
CC @girving @allenlavoie "
25445,Library Conversion: TF Ranking,"[**TensorFlow Ranking**](https://github.com/tensorflow/ranking) is a library for Learning-to-Rank (LTR) techniques on the TensorFlow platform. It contains the following components:

*   Commonly used loss functions including pointwise, pairwise, and listwise losses.
*   Commonly used ranking metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG).
*   [Multi-item (also known as groupwise) scoring functions](https://arxiv.org/abs/1811.04415).
*   [LambdaLoss](https://ai.google/research/pubs/pub47258) implementation for
    direct ranking metric optimization.
*   [Unbiased Learning-to-Rank](http://www.cs.cornell.edu/people/tj/publications/joachims_etal_17a.pdf)
    from biased feedback data.

The objective of this issue is to migrate TensorFlow Ranking to TF 2.0, with support for Keras models and Ranking Estimator. The key objectives are:

1. Removing [deprecated TensorFlow functions](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md).
2. Migrate to newer version of [feature columns](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column). `tfr.feature.encode_features` is probably no longer needed.
3. Shift from `tflearn` to `estimator`, and from `experiment_gn` to `train_and_evaluate`.
4. Support for model definition via Keras: Keras allows for object oriented definition of models, in contrast with function closures currently used in TF Ranking.
5. Support Ranking Estimator as a canned estimator."
25444,Implementation of WARP Loss or pairwise ranking losses,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Looking to implement warp loss, attempting to implement but getting gradient errors based on ops used. This loss has been implemented elsewhere such as pytorch (https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a) but many models are already built using tensorflow.

**Will this change the current api? How?**
Yes, I am looking to implement new loss functions for learning to rank problems.

**Who will benefit with this feature?**
All users
**Any Other info.**

I have begun to implement this feature but run into gradient computation problems. I am looking for help implementing the loss function.
"
25443,Bug in embedding_ops.py Leads to Crash when importing Frozen Wide and Deep model/graph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12, 1.13 and master
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): GCC 6.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
After successfully training and exporting the trained Wide and Deep model from here:
https://github.com/tensorflow/models/tree/master/official/wide_deep
Tried to freeze the exported model using freeze_graph.py. The frozen graph got generated without errors. However, when tried to load the frozen graph using the call tf.import_graph_def(graph_def), got the following error:
File ""../python2.7/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: Input 0 of node import/linear/linear_model/linear_model/linear_model/age_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup was passed float from import/linear/linear_model/age_bucketized/weights/part_0:0 incompatible with expected resource.

After inspecting the frozen graph, we found that ResourceGather Op is receiving float form Const node (which used to be VarHandleOP before freezing) but ResourceGather is expecting 'resource' data type.

The issue was resolved after changing this line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L693
by removing the if statement and calling convert_to_tensor() unconditionally.

**Describe the expected behavior**
The graph is expected to load and run successfully.
**Code to reproduce the issue**
https://github.com/tensorflow/models/tree/master/official/wide_deep

"
25442,"dynamic_decode reports error under eager_execution ? â€œValueError: The inequality of unknown TensorShapes is undefined.""","- TensorFlow version (use command below): 1.2


I have encountered a weird problem when transforming a usual seq2seq code into eager execution mode. After changing the placeholder input to numpy array,  by calling `tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)`
gives error â€œValueError: The inequality of unknown TensorShapes is undefined.""

Without activating eager execution error, everything is fine.

**Code to reproduce the issue**

The code requires two files from github: 
https://github.com/udacity/deep-learning/blob/master/seq2seq/data/letters_source.txt
and 
https://github.com/udacity/deep-learning/tree/master/seq2seq/data/letters_target.txt

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

from distutils.version import LooseVersion
from tensorflow.python.layers.core import Dense

assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'
print('TensorFlow Version: {}'.format(tf.__version__))
import numpy as np
import time
import tensorflow as tf

with open('data/letters_source.txt', 'r', encoding='utf-8') as f:
    source_data = f.read()

with open('data/letters_target.txt', 'r', encoding='utf-8') as f:
    target_data = f.read()

def extract_character_vocab(data):
    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']
    set_words = list(set([character for line in data.split('\n') for character in line]))
    int_to_vocab = {idx: word for idx, word in enumerate(special_words + set_words)}
    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}
    return int_to_vocab, vocab_to_int

source_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)
target_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)

source_int = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) 
               for letter in line] for line in source_data.split('\n')]
target_int = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) 
               for letter in line] + [target_letter_to_int['<EOS>']] for line in target_data.split('\n')] 

def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):
    for batch_i in range(0, len(sources)//batch_size):
        start_i = batch_i * batch_size
        sources_batch = sources[start_i:start_i + batch_size]
        targets_batch = targets[start_i:start_i + batch_size]
        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))
        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))
        pad_targets_lengths = []
        for target in pad_targets_batch:
            pad_targets_lengths.append(len(target))
        pad_source_lengths = []
        for source in pad_sources_batch:
            pad_source_lengths.append(len(source))
        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths
        
def pad_sentence_batch(sentence_batch, pad_int):
    max_sentence = max([len(sentence) for sentence in sentence_batch])
    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]

display_step = 50 # æ¯éš”50è½®è¾“å‡ºloss
epochs = 60
batch_size = 128
rnn_size = 50
num_layers = 2
encoding_embedding_size = 15
decoding_embedding_size = 15
learning_rate = 0.001
checkpoint = ""trained_model.ckpt"" 
train_source = source_int[batch_size:]
train_target = target_int[batch_size:]
valid_source = source_int[:batch_size]
valid_target = target_int[:batch_size]
(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,
                           source_letter_to_int['<PAD>'],target_letter_to_int['<PAD>']))
sess = tf.InteractiveSession()
tf.global_variables_initializer()

epoch_i = 1
batch_i = 0
(targets_batch, sources_batch, targets_lengths, sources_lengths) = next(get_batches(train_target, train_source, batch_size,source_letter_to_int['<PAD>'],target_letter_to_int['<PAD>']))

input_data = sources_batch
targets = targets_batch
lr = learning_rate
target_sequence_length = targets_lengths
source_sequence_length= sources_lengths
max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')

#Encoder
source_vocab_size = len(source_letter_to_int)
target_vocab_size = len(target_letter_to_int)
def get_lstm_cell(rnn_size):
    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
    return lstm_cell

encoder_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)
cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])
encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)

ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])
decoder_input = tf.concat([tf.fill([batch_size, 1], target_letter_to_int['<GO>']), ending], 1)

target_vocab_size   = len(target_letter_to_int)
decoder_embeddings  = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input) 
def get_decoder_cell(rnn_size):
    decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
    return decoder_cell
cell         = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])
output_layer = Dense(target_vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))

with tf.variable_scope(""decode""):
    training_helper  = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,sequence_length=target_sequence_length,time_major=False)
    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer) 
    training_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)
with tf.variable_scope(""decode"", reuse=True):
    start_tokens       = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')
    predicting_helper  = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,target_letter_to_int['<EOS>'])
    predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,predicting_helper,encoder_state,output_layer)
    predicting_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)

```
[letters_source.txt](https://github.com/tensorflow/tensorflow/files/2823741/letters_source.txt)
[letters_target.txt](https://github.com/tensorflow/tensorflow/files/2823742/letters_target.txt)

"
25441,Feed batch of images to Tensorflow model in Golang,"I have tried all the methods but did not succeed to make a batch processing of images in Go Tensorflow.

I have been following this test case for the inception model. I tried many things but did not work.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/example_inception_inference_test.go#L199

Could anyone help me? I really appreciate any help."
25440,Feeding Batch of  Images to session.Run() in Golang,"I have tried all the methods but did not succeed to make a batch processing of images in Go Tensorflow. 

I have been following this test case for the inception model. I tried many things but did not work.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/example_inception_inference_test.go#L199

Could anyone help me? I really appreciate any help.

Error:
```
Endpoint ""Placeholder:0"" fed more than once.
```
"
25439,"np.array(x), where x is a tuple or list of tensor(s), is unusable (eager execution)","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
b'v1.12.0-5845-g764109a352' 1.12.0
- Python version:
3.6.7
- Bazel version (if compiling from source):
Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0
Build label: 0.21.0
Build time: Wed Dec 19 12:58:44 2018 (1545224324)
- GCC/Compiler version (if compiling from source):
gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version:
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory:
GTX 1060 Max Q, 6gb VRAM

**Describe the current behavior**
If x is a list of any size, or a tuple of size >= 2, which contains tensors, np.array(x) is unusable as it runs exponentially longer than the tensor.numpy() function.

**Describe the expected behavior**
np.array(x) should run in a reasonable amount of time. Doing a simple list comprehension with tensor.numpy() shouldn't be faster.

**Code to reproduce the issue**

This shows the exponential time difference when x is a list containing a single tensor.
```
import numpy as np
import tensorflow as tf
import time

tf.enable_eager_execution()

x = [tf.zeros((32, 32, 12))]

start = time.time()
print(""temp"")
temp = np.array(x[0:1])
end = time.time()
print(end - start)

start = time.time()
print(""temp1"")
temp1 = np.array([x[0]])
end = time.time()
print(end - start)

start = time.time()
print(""temp2"")
temp2 = np.array([x[0].numpy()])
end = time.time()
print(end - start)

start = time.time()
print(""temp3"")
list = []
[list.append(t.numpy()) for t in x]
temp3 = np.array(list)
end = time.time()
print(end - start)

```
**Other info / logs**
Output:
temp
1.7468986511230469
temp1
1.7278409004211426
temp2
0.00018978118896484375
temp3
0.00024890899658203125

However, strangely enough, the problem only occurs with tuples when the length of the tuple is 2 or more. Changing x to the line below and running the same code generates similar results.
`x = (tf.zeros((32, 32, 12)), tf.zeros((32, 32, 12)))`

Why is it faster for me to use a list comprehension with t.numpy(), appending it to a list, and then run np.array(list), than it is to run np.array(x)?"
25435,2.0 Reference Models: ResNet V1.5 (TPU with Keras),"Deep residual networks, or ResNets for short, proposed the breakthrough idea of identity mappings in order to enable training of very deep convolutional neural networks. This issue will track the creation of 1 GPU and 8 GPU TF 2.0-compatible implementations of ResNet for the ImageNet dataset.

See the following papers for more background:

[1] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.

[2] [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/resnet)."
25434,2.0 Reference Models: Keras Application Set (TPU),"Keras Applications is the `applications` module of the Keras deep learning library. It provides model definitions and pre-trained weights for a number of popular architectures, such as VGG16, ResNet50, Xception, MobileNet, and more.

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.

For more information on tf.keras.applications, check [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications)."
25433,2.0 Reference Models: Transformer (TPU with dist strat and Keras),"**Transformer** is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.

See the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) for more background.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/transformer).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25432,2.0 Reference Models: NMT Model (TPU with dist strat and Keras),"**Sequence-to-sequence** (seq2seq) models ([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. 

This model will give TF 2.0 end users a full understanding of seq2seq models and show them how to build a competitive seq2seq model from scratch. We focus on the task of **Neural Machine Translation** (NMT), which was the very first [testbed for seq2seq models](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/nmt).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25430,bazel tests r1.13rc0,"**System information**
- OS Platform and Distribution: Linux Ubuntu (4.15.0-44-generic)
- TensorFlow installed from: Source
- TensorFlow version: r1.13 (e7f2979fc7bbbd491a5c1db2268d4ee67cc46f88)
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source) : gcc-7 (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.4.2.24-1+cuda10.0
- GPU model and memory: K1100M 2 GB



**Describe the problem**
Running the bazel tests for r1.13rc0 fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
#!/bin/bash

CONDA_ENV=""tf_dev_3.5""
PYTHON_VERSION=""3.5""
WORKDIR=""${HOME}/GitHub""
PYTHON_BIN=""${HOME}/anaconda3/envs/${CONDA_ENV}/bin/python""
TF_VERSION=""r1.13""

if [ ! -d ""${WORKDIR}/tensorflow"" ]; then
  git clone git@github.com:tensorflow/tensorflow.git
  git --git-dir ""${WORKDIR}""/tensorflow/.git checkout ${TF_VERSION}
fi
git --git-dir ""${WORKDIR}""/tensorflow/.git pull

export PYTHON_BIN_PATH=""${PYTHON_BIN}""
export PYTHON_LIB_PATH=""${HOME}/anaconda3/envs/${CONDA_ENV}/lib/python${PYTHON_VERSION}/site-packages""
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_ROCM=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""10.0""
export CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.0""
export TF_CUDNN_VERSION=""7""
export CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0""
export TF_NEED_TENSORRT=1
export TENSORRT_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
export TF_NCCL_VERSION=""2.4"" # 2.4.2
export TF_CUDA_COMPUTE_CAPABILITIES=""3.0""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-7"" #""/usr/bin/gcc"" #""$(which gcc)""
export TF_NEED_MPI=1
export MPI_HOME=""/usr/lib/x86_64-linux-gnu/openmpi""
export CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
export TF_SET_ANDROID_WORKSPACE=0

cd ""${WORKDIR}""/tensorflow/
./configure

bazel build --config=opt --config=cuda --config=mkl --config=ngraph //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package ""${WORKDIR}""/tensorflow_pkg

# ${PYTHON_BIN} -m pip uninstall tensorflow -y
# ${PYTHON_BIN} -m pip install ""${WORKDIR}""/tensorflow_pkg/tensorflow-*
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
 âœ˜ î‚° ~/GitHub/tensorflow î‚° î‚  r1.13 î‚° bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: e5417977-f3f0-4c98-a6a3-729508e1bc94
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
ERROR: /home/erik/GitHub/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: no such package '@com_google_protobuf//': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replacement.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
ERROR: Analysis of target '//tensorflow/core:example_java_proto' failed; build aborted: Analysis failed
INFO: Elapsed time: 30.254s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (410 packages loaded, 3273 targets configured)
```



```
 âœ˜ î‚° ~/GitHub/tensorflow î‚° î‚  r1.13 î‚° bazel test --incompatible_remove_native_http_archive=false -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: cbdcdc07-17f9-45a7-bb7e-ea87aeb364e1
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
ERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:597:1: Traceback (most recent call last):
	File ""/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD"", line 597
		internal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)
	File ""/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/protobuf.bzl"", line 266, in internal_gen_well_known_protos_java
		Label((""%s//protobuf_java"" % REPOSITOR...))
	File ""/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/protobuf.bzl"", line 266, in Label
		REPOSITORY_NAME
The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false
ERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:android' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows_msvc' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/erik/GitHub/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: Target '@com_google_protobuf//:protoc' contains an error and its package is in error
ERROR: Analysis of target '//tensorflow/core:example_java_proto' failed; build aborted: Analysis failed
INFO: Elapsed time: 73.304s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (483 packages loaded, 17762 targets configured)
```


```
 âœ˜ î‚° ~/GitHub/tensorflow î‚° î‚  r1.13 î‚° bazel test --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Invocation ID: 9421f3f2-53c7-4490-8dc6-675ac7d12ab2
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
ERROR: /home/erik/GitHub/tensorflow/tensorflow/contrib/BUILD:167:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'
ERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted
INFO: Elapsed time: 24.164s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (532 packages loaded, 72385 targets configured)
```


"
25429,TF 2.0: Release binaries for Python 3.5 and 3.7.,
25428,T,
25426,"Segmentation Fault with tf.io.decode_csv , numpy record_defaults and tensor input ","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.4 LTS on Windows Linux SubSystem
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
'2.0.0-preview' / ""b'v1.12.0-6503-g7cfe43a11d'""
- Python version:
Python 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: No


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" **=> this doesn't work, you mean tf.version.x**

**Describe the current behavior**
`Segmentation fault (core dumped)`
**Describe the expected behavior**
prints result
**Code to reproduce the issue**

    import numpy as np
    import tensorflow as tf
    record_defaults=np.zeros(5)    
    parsed_fields = tf.io.decode_csv(tf.constant('1,2,3,4,5'), record_defaults)



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


It's fine with either tf.constants for record defaults or plain string to decode, eg:

    parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)

    
    Numpy version: 
    numpy                     1.16.0          py36_blas_openblash1522bff_1000  [blas_openblas]  conda-forge

"
25425,tf.contrib.distribute performance issue with asymmetrical multi GPU setup,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04, Kernel 4.13.16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): tensorflow-gpu binary from pip
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 9.0 with NCCL 2.3.7-1
- GPU model and memory: 1080ti 12G + 1070ti 8G

**Description**
I've been trying to port the Google BERT language model to support multi-GPU transfer learning using the new tf.contrib.distribute API. My hardware setup is a 1070ti + 1080ti, and it's a bit strange because I'm getting similar performance between the dual-GPU and a single 1070ti.

To reproduce this issue I built a much simpler model (see link) with a timer. And here's the results I got (in secs):
3.0106631300004665 1070Ti + 1080Ti
2.8305218839959707 1070Ti
1.6665251980011817 1080Ti

I can imagine the 1080Ti being throttled back with a MirroredStrategy, but at least it should perform like a 1070Ti and with the two cards gives 1.6~1.8X of the performance of a 1070Ti? Maybe this is a load-balancing issue with the NCCL?

**Code to reproduce the issue**
https://drive.google.com/file/d/15nJlVV9PWzKcF1AI24H2XSfzzCAhwQ5f/view?usp=sharing

(Change line 30 to specify different GPUs)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25423,BeamSearchDecoder.step bug with lengths_to_add OHE dtype ,"Hello,
I have problem with calling .step on BeamSearchDecoder obect:

```
/home/ms/venv36/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in step(self, time, inputs, state, name)
    673           end_token=end_token,
    674           length_penalty_weight=length_penalty_weight,
--> 675           coverage_penalty_weight=coverage_penalty_weight)
    676 
    677       finished = beam_search_state.finished

/home/ms/venv36/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in _beam_search_step(time, logits, next_cell_state, beam_state, batch_size, beam_width, end_token, length_penalty_weight, coverage_penalty_weight)
    729       on_value=np.int64(0),
    730       off_value=np.int64(1),
--> 731       dtype=dtypes.int64)
    732   add_mask = math_ops.to_int64(not_finished)
    733   lengths_to_add *= array_ops.expand_dims(add_mask, 2)

/home/ms/venv36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in one_hot(indices, depth, on_value, off_value, axis, dtype, name)
   2418         if on_exists and on_dtype != dtype:
   2419           raise TypeError(""dtype {0} of on_value does not match ""
-> 2420                           ""dtype parameter {1}"".format(on_dtype, dtype))
   2421         if off_exists and off_dtype != dtype:
   2422           raise TypeError(""dtype {0} of off_value does not match ""

TypeError: dtype <dtype: 'int32'> of on_value does not match dtype parameter <dtype: 'int64'>
```
I started this code with Eager Execution. I think, this problem depended with bad on_value, off_value type.  


**System information**
- Tensorflow 1.12:
- Ubuntu 18.04:
- TensorFlow installed from https://pypi.org/project/tensorflow-gpu/
- Python version: 3.6
- CUDA version: 9.0
- GPU model and memory: GTX 1080, 8gb

**Code to reproduce the issue**
```
#in model:
        self.beam_search_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
            cell=model.decoder_cell,
            embedding=model.embedding_decoder,
            start_tokens=tf.fill((batch_size.numpy(), ), tf.Variable(body_vocab['<SOS>'], dtype=tf.int32)),
            end_token=tf.Variable(body_vocab['<EOS>'], dtype=tf.int32),
            initial_state=tf.contrib.seq2seq.tile_batch(encoder_final_state, self.hparams.beaw_width),
            beam_width=self.hparams.beaw_width)


initial_finished, initial_inputs, initial_state = model.beam_search_decoder.initialize()
model.beam_search_decoder.step(0, 
                               initial_inputs, 
                               initial_state)
``` "
25421,Passing flattened tensor into RNN results in an error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No

- TensorFlow installed from (source or binary): pip install tensorflow-gpu

- TensorFlow version (use command below):  b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0

- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):  N/A
- CUDA/cuDNN version:  9.0
- GPU model and memory:  GeForce RTX 2070

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Passing flattened tensor into RNN results in an error:  ""Shape (608, ?) must have rank at least 3""
More:  I have been trying to find some way to squash down my data via CNNs.  I keep hitting strange dimension issues, where TF complains about None.

**Describe the expected behavior**
I would like to pass my flattened data into either a Dense or an RNN layer.

**Code to reproduce the issue**

from collections import namedtuple
import tensorflow as tf
import numpy as np

ConvArgs = namedtuple('ConvArgs',
                      'layerInput, numFilters, filterSize, stride, init, namePrefix')
ConvArgs.__new__.__defaults__ = (None,) * len(ConvArgs._fields)

# Set to false to see shape output of the final, flattened layer.
showError = True

def BuildConv2D(convArgs):
    channelAxis = 3
    filterShape = [convArgs.filterSize, 
                   convArgs.filterSize, 
                   convArgs.layerInput.get_shape()[channelAxis], 
                   convArgs.numFilters]

    filters = tf.get_variable(shape=filterShape, 
                              dtype=tf.float32,
                              initializer=convArgs.init,
                              name=convArgs.namePrefix + 'filters')
    
    conv = tf.nn.conv2d(input=convArgs.layerInput,
                        filter=filters, 
                        strides=[1,convArgs.stride,convArgs.stride,1], 
                        padding='VALID')

    activated = tf.nn.relu(conv)
    return activated

tf.reset_default_graph()

with tf.Session() as sess:
    data = tf.placeholder(dtype=tf.float32, 
                          shape=(None,150000,1), 
                          name='data')

    dataShape = tf.shape(data)


    numCols = 80
    numRows = int(150000 / numCols)
    data2D = tf.reshape(data, (tf.shape(data)[0], numRows, numCols, 1))

    kernelInit = tf.orthogonal_initializer()
    conv1 = BuildConv2D(
            ConvArgs(layerInput = data2D, 
                     numFilters = 8,
                     filterSize = 8,
                     stride = 4,
                     init = kernelInit,
                     namePrefix='c1'))

    conv2 = BuildConv2D(
            ConvArgs(layerInput = conv1, 
                     numFilters = 16,
                     filterSize = 8,
                     stride = 4,
                     init = kernelInit,
                     namePrefix='c2'))

    conv3 = BuildConv2D(
            ConvArgs(layerInput = conv2, 
                     numFilters = 16,
                     filterSize = 3,
                     stride = 3,
                     init = kernelInit,
                     namePrefix='c3'))

    c3_flattened = tf.layers.Flatten()(conv3)
 
    if(showError == True):
        lstmCell = tf.contrib.rnn.LSTMBlockCell(num_units=16)
    
        rawOutputs, final_state = tf.nn.dynamic_rnn(
                lstmCell, 
                c3_flattened, 
                dtype=tf.float32)

    sess.run(tf.global_variables_initializer())
    result = sess.run(c3_flattened, feed_dict={data:  np.random.randint(-5, 12, size=(2,150000,1))})
    print(result.shape)
    print(result[:, 0:20])
    
    if(showError == True):
        resultA, resultB = sess.run([rawOutputs, final_state], feed_dict={data:  np.random.randint(-5, 12, size=(2,150000,1))})
        print(resultA.shape)
        print(resultB.shape)

**Other info / logs**
When attempting to use a Dense layer, I ran into the issue described here:
https://github.com/tensorflow/tensorflow/issues/13348
"
25419,tf-nightly-gpu-2.0 import problem,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview
- TensorFlow version: tf-nightly-gpu-2.0-preview 2.0.0.dev20190201
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: virtual env with pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: Tesla k40d 11GB



**Describe the problem**
Cannot import tensorflow in python


**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

**Any other info / logs**
`>>> import tensorflow
Traceback (most recent call last):
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import encode_wav
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/net/scratch/people/plgmkowalski94/master/new/yolo3_tf2/tf2/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`
"
25418,`tf.contrib.framework.is_tensor` is not available/exported in 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190126
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

The [tf.contrib.framework.is_tensor](https://www.tensorflow.org/api_docs/python/tf/contrib/framework/is_tensor) function is not exported in TF 2.0, even though it is defined outside of contrib in the current master version, in [tensorflow/python/framework/tensor_util.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L935-L950).

It is neither in the [list of exported symbols](https://www.tensorflow.org/versions/r2.0/api_docs/python?hl=en), nor in [the tensorflow/addons repo](https://github.com/tensorflow/addons).

**Describe the expected behavior**

`is_tensor` should probably be exported in regular TF 2.0 API, since it is already defined outside of contrib and being used all over the place within the TF sources.

**Code to reproduce the issue**

N/A

**Other info / logs**

N/A"
25417, TensorRT loses defined shapes,"
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
Bazel version: N/A
GPU model and memory: P4 8G
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): docker
TensorFlow version (use command below): 1.12
Python version: 3.5
CUDA/cuDNN version: 10/7.4.1
Describe the problem

When tf.contrib.tensorrt compiles a graph which can be fully compiled it loses shape information.

https://github.com/tensorflow/tensorrt.git
just run the tensorrt/tftrt/examples/image-classification/image_classification.py with int8 parameter like the follow command and get the frozen model from graphs directory.

 python image_classification.py --model resnet_v1_50 --data_dir /data/ImageNetVal/ --precision fp32   --batch_size=1 --use_trt

the one frozen from tensorrt:
Tensor(""input:0"", shape=(?, 224, 224, 3), dtype=float32)
Tensor(""logits:0"", dtype=float32)

and the native one:

Tensor(""input:0"", shape=(?, 224, 224, 3), dtype=float32)
Tensor(""logits:0"", shape=(?, 1001), dtype=float32)

the difference is the shape in logits from tensorrt have been lost.

script as follow:

import tensorflow as tf
from tensorflow.contrib import tensorrt as trt
import os
import shutil
graph_pb = 'frozen_graph_resnet_v1_50_1_fp32_1.pb'

with tf.gfile.GFile(graph_pb, ""rb"") as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
sigs = {}
with tf.Session(graph=tf.Graph()) as sess:
    for n in graph_def.node:
        if n.name == ""logits"":
            print(n)
        if n.name == ""input"":
            print(n)

@joeyearsley "
25416,tf.keras.utils.multi  does not work with WGAN-GP Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
tf.keras.utils.multi  does not work with WGAN-GP Model
I also try on Keras(not tf.keras) and it was work!

**Code to reproduce the issue**
discriminator model code:
```
generator = gan_generator()
discriminator = gen_discriminator()


# Image input (real sample)
real_img = tf.keras.Input(shape=img_shape)
# Noise input
z_disc = tf.keras.Input(shape=(100,))
# Generate image based of noise (fake sample)
fake_img = generator(z_disc)

# Discriminator determines validity of the real and fake images
fake = discriminator(fake_img)
valid = discriminator(real_img)

# Construct weighted average between real and fake images
interpolated_img = RandomWeightedAverage()([real_img, fake_img])
# Determine validity of weighted sample
validity_interpolated = discriminator(interpolated_img)

# Use Python partial to provide loss function with additional
# 'averaged_samples' argument
partial_gp_loss = partial(gradient_penalty_loss,
                          averaged_samples=interpolated_img)
partial_gp_loss.__name__ = 'gradient_penalty'   # Keras requires function names

discriminator_model = tf.keras.Model(inputs=[real_img, z_disc], outputs=[valid, fake, validity_interpolated])
parallel_discriminator_model = tf.keras.utils.multi_gpu_model(discriminator_model, gpus=2)
```

generator & dircriminator body
```
def gan_generator(self):
    inputs = tf.keras.Input(shape=(100,))

    x = tf.keras.layers.Dense(7 * 7 * self.dims * 4, activation='relu', input_dim=self.latent_dim)(inputs)
    x = tf.keras.layers.Reshape((7, 7, self.dims * 4))(x)

    x = tf.keras.layers.UpSampling2D()(x)
    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, padding=""same"")(x)
    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)
    x = tf.keras.layers.Activation(""relu"")(x)

    x = tf.keras.layers.UpSampling2D()(x)
    x = tf.keras.layers.Conv2D(self.dims * 2, kernel_size=5, padding=""same"")(x)
    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)
    x = tf.keras.layers.Activation(""relu"")(x)

    # x = UpSampling2D()(x)
    # x = Conv2D(self.dims * 1, kernel_size=5, padding=""same"")(x)
    # x = BatchNormalization(momentum=0.8)(x)
    # x = Activation(""relu"")(x)

    x = tf.keras.layers.Conv2D(self.channels, kernel_size=5, padding=""same"")(x)
    x = tf.keras.layers.Activation(""tanh"")(x)

    return tf.keras.Model(inputs, x)

def gen_discriminator(self):
    inputs = tf.keras.Input(shape=self.img_shape)

    x = tf.keras.layers.Conv2D(self.dims, kernel_size=5, strides=2, padding=""same"", input_shape=self.img_shape)(inputs)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    x = tf.keras.layers.Conv2D(self.dims * 2, kernel_size=5, strides=2, padding=""same"")(x)
    x = tf.keras.layers.ZeroPadding2D(padding=((0, 1), (0, 1)))(x)
    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, strides=2, padding=""same"")(x)
    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    x = tf.keras.layers.Conv2D(self.dims * 4, kernel_size=5, strides=2, padding=""same"")(x)
    x = tf.keras.layers.BatchNormalization(momentum=0.8)(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.25)(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(1)(x)
    return tf.keras.Model(inputs, x)

```

**Other info / logs**
```
2019-02-01 10:23:31.330533: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-01 10:23:31.441144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 10:23:31.441617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.797
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.18GiB
2019-02-01 10:23:31.530899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 10:23:31.531436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.797
pciBusID: 0000:03:00.0
totalMemory: 7.93GiB freeMemory: 7.83GiB
2019-02-01 10:23:31.532281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-01 10:23:32.028009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-01 10:23:32.028044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-01 10:23:32.028053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-01 10:23:32.028061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-01 10:23:32.028301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6920 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-01 10:23:32.028599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7558 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:03:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/share/user/John/WGAN_GP/tf_main.py"", line 258, in <module>
    wgan = WGANGP()
  File ""/home/share/user/John/WGAN_GP/tf_main.py"", line 82, in __init__
    parallel_discriminator_model = tf.keras.utils.multi_gpu_model(self.discriminator_model, gpus=2)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/multi_gpu_utils.py"", line 252, in multi_gpu_model
    return Model(model.inputs, merged)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 121, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 80, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 268, in _init_graph_network
    self.inputs, self.outputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 1833, in _map_graph_network
    str(all_names.count(name)) + ' times in the model. '
ValueError: The name ""model_1"" is used 3 times in the model. All layer names should be unique.
```"
25415,There are some error when convert Mask RCNN model to .tflite,"### System information
- **What is the top-level directory of the model you are using**:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.12
- **Bazel version (if compiling from source)**: 0.22.0
- **CUDA/cuDNN version**: cuda9.0/cuDNN7.4
- **GPU model and memory**: GTX 1080/8G
- **Exact command to reproduce**:

### Describe the problem
i trained mask rcnn using object detection API, and frozen the mode successfully, but i encountered some problems when i convert  .pb file to .tflite file, following is my command

```
tflite_convert \
  --graph_def_file=/path/to/frozen_inference_graph.pb \
  --output_file=optimized_graph.lite \
  --output_format=TFLITE \
  --output_arrays=detection_masks,detection_classes,detection_boxes,detection_scores,num_detections \
  --inference_input_type=FLOAT \
  --input_shapes=1,224,224,3 \
  --input_arrays=image_tensor \
  --inference_type=FLOAT
```
the output_array is the same with output_node_names form research/object_detection/exporter.py
### Source code / logs
error
```
tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: CropAndResize_1\n2019-01-31 16:16:52.071330: I 
tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7262 operators, 13617 arrays (0 quantized)\n2019-01-31 16:16:52.843219: I 
tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 7208 operators, 13526 arrays (0 quantized)\n2019-01-31 16:16:54.116685: I 
tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7208 operators, 13526 arrays (0 quantized)\n2019-01-31 16:16:54.363824: F 
tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\nAborted (core dumped)\n'None
```
"
25414,Tensorflow 2.0 Preview - tf.function and tensorflow_dataset incompatibility,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): tf-nightly-2.0-preview
- Python version: 3.6
- CUDA/cuDNN version: 10
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**
Creating a dataset using tensorflow_dataset and passing it to a function decorated with `@tf.function` gives the error:
```
Traceback (most recent call last):
  File ""/home/emanuele/development/gan-toolbox/try2.py"", line 308, in <module>
    main()
  File ""/home/emanuele/development/gan-toolbox/try2.py"", line 304, in main
    _ = gan.train(dataset, G_opt, D_opt)
  File ""/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 398, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 460, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File ""/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 507, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 312, in call
    ctx=ctx)
  File ""/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Function Dataset_interleave_TFRecordExampleAdapter.dataset_from_filename_4 is not defined.
	 [[{{node ReduceDataset}}]] [Op:__inference_train_3786]
```


**Describe the expected behavior**
Not encountering the error. I think the problem is related to the magic behind tf.function. The issue is that there isn't a complete guide on how to use correctly tf.function.


**Code to reproduce the issue**
```python
""""""
Implement DCGAN using the new TF 2.0 API.

Also test tensorflow-datasets.

Celeb-A dataset.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from typing import Dict
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras as k
from tensorflow.python.ops import control_flow_util


control_flow_util.ENABLE_CONTROL_FLOW_V2 = True


@tf.function
def bce(x, label, label_smoothing=0.0):
    """"""Returns the discrete binary cross entropy between x and the discrete label
    Args:
        x: a 2D tensor
        label: the discrete label, aka, the distribution to match
        label_smoothing: if greater than zero, smooth the labels

    Returns:
        The binary cros entropy
    """"""
    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)


def min_max(positive, negative, label_smoothing=0.0):
    """"""Returns the discriminator (min max) loss
    Args:
        positive: the discriminator output for the positive class: 2D tensor
        negative: the discriminator output for the negative class: 2D tensor
        smooth: if greater than zero, appiles one-sided label smoothing
    Returns:
        The sum of 2 BCE
    """"""
    one = tf.constant(1.0)
    zero = tf.constant(0.0)
    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)
    return d_loss


class Generator(k.Model):
    def __init__(self):
        super(Generator, self).__init__()
        self.fc1 = k.layers.Dense(4 * 4 * 1024)
        self.batchnorm1 = k.layers.BatchNormalization()

        self.conv2 = k.layers.Conv2DTranspose(
            filters=512,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization()

        self.conv3 = k.layers.Conv2DTranspose(
            filters=256,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm3 = k.layers.BatchNormalization()

        self.conv4 = k.layers.Conv2DTranspose(
            filters=128,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm4 = k.layers.BatchNormalization()

        self.conv5 = k.layers.Conv2DTranspose(
            filters=3,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm5 = k.layers.BatchNormalization()

    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:
        x = self.fc1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.relu(x)
        x = tf.reshape(x, shape=(-1, 4, 4, 1024))

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv5(x)
        x = self.batchnorm5(x, training=training)

        x = tf.nn.tanh(x)
        return x


class Discriminator(k.Model):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=""same"")
        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm2 = k.layers.BatchNormalization()
        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm3 = k.layers.BatchNormalization()
        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm4 = k.layers.BatchNormalization()
        self.flatten = k.layers.Flatten()
        self.fc5 = k.layers.Dense(1)

    def call(self, x, training=True):
        x = self.conv1(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x)
        x = tf.nn.leaky_relu(x)

        x = self.flatten(x)
        x = self.fc5(x)
        return x


class GAN:
    def __init__(self, generator, discriminator, encoder=None):
        """"""
        GAN initializer.

        Args:
            generator: A ``tensorflow.keras.Model`` to use as Generator.
            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.
            encoder: A ``tensorflow.keras.Model`` to use as Encoder.

        Returns:
            Trained GAN model (?).

        """"""
        self.G = generator()
        self.D = discriminator()
        # self.E = encoder() if encoder is not None else None
        self.latent_vector_dims = 100

    @tf.function()
    def train(self, dataset, opt1, opt2):
        """"""
        Train.
        """"""
        step = 0
        for f in dataset:
            x = f[""image""]
            step += 1
            z = tf.random.normal((32, self.latent_vector_dims))

            # We record all the operations in the tape
            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                G_z = self.G(z, training=True)

                D_x = self.D(x, training=True)
                D_Gz = self.D(G_z, training=True)

                g_loss = bce(D_Gz, 1.0)
                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)

            # We retrieve the gradients from our records
            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)
            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)

            # Optimize and apply the gradients
            opt1.apply_gradients(zip(G_grads, self.G.trainable_variables))
            opt2.apply_gradients(zip(D_grads, self.D.trainable_variables))

            if step % 1 == 0:
                print(""--------------------------"")
                print(""STEP"", step)
                print(""D_LOSS"", d_loss)
                print(""G_LOSS:"", g_loss)
        return step


class InputPipeline:
    def __init__(
        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size
    ):
        self.batch_size = batch_size
        self.dataset_name = dataset
        self.epochs = epochs
        self.prefetched_items = prefetched_items
        self.shuffle_buffer = shuffle_buffer
        self.size = size

    def get_input_fn(self):
        """"""Input fn.""""""
        return self.input_fn

    def load_public_dataset(self):
        """"""
        Load one of the publicly available datasets, will merge together all the splits.

        Args:
            chosen_dataset: dataset to use.

        Return:
            The chosen dataset as a ``tf.data.Dataset``

        """"""
        # Construct a tf.data.Dataset
        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)
        return datasets

    def resize_images(self, features):
        """"""
        Overwrite the \""image\"" feature in order to resize them.

        Args:
            features: features dictionary.
            size: desired target size.

        Returns:
            Features with \""image\"" resized to the correct shape.

        """"""
        features[""image""] = tf.image.resize(features[""image""], self.size)
        return features

    def input_fn(self):
        dataset = self.load_public_dataset()
        dataset = (
            dataset.map(self.resize_images)
            .shuffle(self.shuffle_buffer)
            .batch(self.batch_size)
            .prefetch(self.prefetched_items)
            .repeat(self.epochs)
        )
        return dataset


def main():
    # See available datasets
    public_datasets = tfds.list_builders()

    gan = GAN(Generator, Discriminator)
    G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)
    D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)

    input_pipeline = InputPipeline(
        dataset=""celeb_a"",
        batch_size=32,
        epochs=2,
        prefetched_items=1,
        shuffle_buffer=1000,
        size=(64, 64),
    )
    dataset = input_pipeline.input_fn()
    _ = gan.train(dataset, G_opt, D_opt)


if __name__ == ""__main__"":
    main()
```

**Other info / logs**
If I remove the `@tf.function` annotation the code works as expected. 
If I create the dataset inside the annotate function the code works as expected.

Basically in the code below I create a td.data.Dataset using the new package tensorflow_dataset. Then I pass the created dataset to a function annotated with `tf.function` that should perform the training loop. The errors I get are not informative.
Unfortunately the tensorflow docs do not explain well how to use tf.function and the admitted operations.

"
25413,How can I do some process like segment_sum to a batch data in parrelel,"For example..
I got an embedding matrix as follow:
embed_matrix = [
[0,0,0,0],
[1,1,1,1],
[2,2,2,2],
]

I have another input which decides which embedding should I use for summation to get a new embedding. 
ids = [
[0,1,],
[1,2,],
[0,1,2]
]

And I want a function f as folow.
f(embed_matrix, ids) 
= [
 [0, 0, 0] + [1, 1, 1],
 [1, 1, 1] + [2, 2, 2],
 [0, 0, 0] + [1, 1, 1] + [2, 2, 2],
]
= [
[1,1,1],
[3,3,3],
[3,3,3],
]"
25412,Tensorflow Lite Multiple Choice Test example app,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I would like an example app where the user is presented with a multiple choice test, 4-5 questions. One of the questions should allow a numerical input with a decimal (float). The questions could be simple things like 2x+1=3... x=?
The example app would take these user inputs and classify the set of responses by a letter grade ""A"", ""B"", ""C"" or ""F"". After hitting a ""Submit"" button the app runs the model and displays a big letter grade on a new page. Then we give the user the option to re-do the test.

I could contribute a trained model from a sample data set that I could generate from test questions/answers in the desktop version of Tensorflow.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Mobile developers trying to get a handle on Tensorflow Lite. This would give a better starting point in Tensorflow Lite for passing other forms of inputs (not sensor based) through a pre-trained model. 

**Any Other info.**
This particular demo app would be more useful in its symbolic sense than the practical function."
25411,Tensorflow: ImportError: DLL load failed: The specified procedure could not be found.,"Hello,
Am a newbie and just started learning about deep learning and in the process,
I've installed Tensorflow on my Windows 10 laptop(cpu only no gpu) using Anaconda3 64bit and I have python3.6.2 installed on it. I verified the installation of Tensorflow at the command prompt by typing in for c:\'activate tensorflow' and typed python at (tensorflow) C:\python to get to python shell with >>> prompt and then I had print a  'Hello' string assigned to a variable and printed it by using Session() and it displayed  successfully.

However, when I opened up my jupyter notebook from Anaconda and typed the below command  
import tensorflow as tf
tf.__version__
to verify if everything was good, it gave the below errors.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-b6f7b46cc0dd> in <module>()
----> 1 import tensorflow as tf
      2 tf.__version__
ImportError: DLL load failed: The specified procedure could not be found.

I checked on google but couldn't find this error..all I saw was errors related to 'specific module could not be founded and some other error issues related to specific module. But, what am getting is 'specific procedure could not be found.

Appreciate if you can help me fix this burning issue..
Thank you!
-----------------------------------------------------------------------------------------
Error Log:
C:\Users\kshet\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

C:\Users\kshet\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py in <module>()
     57 
     58 # Protocol buffers
---> 59 from tensorflow.core.framework.graph_pb2 import *
     60 from tensorflow.core.framework.node_def_pb2 import *
     61 from tensorflow.core.framework.summary_pb2 import *

C:\Users\kshet\AppData\Roaming\Python\Python36\site-packages\tensorflow\core\framework\graph_pb2.py in <module>()
      4 import sys
      5 _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
----> 6 from google.protobuf import descriptor as _descriptor
      7 from google.protobuf import message as _message
      8 from google.protobuf import reflection as _reflection

C:\Users\kshet\AppData\Roaming\Python\Python36\site-packages\google\protobuf\descriptor.py in <module>()
     45   import binascii
     46   import os
---> 47   from google.protobuf.pyext import _message
     48   _USE_C_DESCRIPTORS = getattr(_message, '_USE_C_DESCRIPTORS', False)
     49 

ImportError: DLL load failed: The specified procedure could not be found.
"
25408,[Feature Request] complex64 and complex128 support for tf.decode_raw,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Latest (master branch)
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.decode_raw` does not support `complex64` and `complex128` yet, which causes issues when reading TFRecord files with complex data.

**Will this change the current api? How?** 
Little change needs to be made as the current algorithm is in a way type-agnostic: `REGISTER`ing `complex64` and `complex128` suffices.

**Who will benefit with this feature?**
Anyone handling serialized `complex64` and `complex128` data.

**Any Other info.**
PR #25409"
25406,error on  tf.estimator.train_and_evaluate,"I used  tf.estimator.train_and_evaluate and I try to train my model in a amazon linux machine with multi-gpu, I try batch_size of evulating with 64,256 and 512 , in all cases,  I get the following error:
Caused by op 'split_inputs/split_1', defined at:
  File ""esmm_test_multi_gpu_256eval.py"", line 342, in <module>
    tf.app.run(main=main)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""esmm_test_multi_gpu_256eval.py"", line 322, in main
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1320, in run
    run_metadata=run_metadata))
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 582, in after_run
    if self._save(run_context.session, global_step):
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 607, in _save
    if l.after_save(session, step):
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 912, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 478, in evaluate
    return _evaluate()
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 460, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1484, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1520, in _call_model_fn_eval
    features, labels, model_fn_lib.ModeKeys.EVAL, config)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 230, in replicated_model_fn
    features, labels, len(devices), device=consolidation_device)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 501, in _split_batch
    label_shards = split_dictionary(labels)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 486, in split_dictionary
    for i, shard in enumerate(array_ops.split(tensor, number_of_shards)):
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1327, in split
    axis=axis, num_split=num_or_size_splits, value=value, name=name)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8083, in split
    ""Split"", split_dim=axis, value=value, num_split=num_split, name=name)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 145) and num_split 8
	 [[node split_inputs/split_1 (defined at /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py:486)  = Split[T=DT_FLOAT, num_split=8, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](split_inputs/split/split_dim, IteratorGetNext:24)]]
	 [[{{node tower_1/my_model/ctr_model/input_layer/CategoryID_embedding/CategoryID_embedding_weights/embedding_lookup_sparse/_5309}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_10102...kup_sparse"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""]()]]"
25405,FP16 profermance on GTX1080,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows10
- TensorFlow installed from (source or binary):
Binary(PIP install)
- TensorFlow version (use command below):
1.12.0
- Python version:
3.6.2
- CUDA/cuDNN version:
CUDA 9.0 / cuDNN 7.3.1
- GPU model and memory:
GeForce GTX 1080, 8GB

**Describe the current behavior**
I try to evaluate the accuracy of fp16 precision on GTX 1080 GPU. As we known, GTX1080 doesn't have
tensor core to supporting FP16 calc, but my test reuslt shows that inference with fp16 is faster than fp32 by 20%-30%, could you give an explanation? 

Here is my operation step:
1. train a model using fp32 precision and save the checkpoint.
2. in order to inference with fp16, load the checkpoint and convert each param to fp16 type
```
    reader = tf.train.NewCheckpointReader(model_file)
    var_to_map = reader.get_variable_to_shape_map()

    val_f16 = {}
    for key, val in var_to_map.items():
        val_f16[key] = reader.get_tensor(key).astype(np.float16)
```
3. rewrite the network and pass the fp16 params to initialize each layer, then run the network.
```
    weight_name = scope_name + '/' + get_layer_str() + '/' + 'weight'
    initw = inits[weight_name]
    weight = tf.get_variable('weight', dtype=initw.dtype, initializer=initw)
    out = tf.nn.conv2d(self.get_output(), weight, strides=[1, stride, stride, 1], padding='SAME')
```

I got a approximate accuracy from fp16 reuslts, and its faster than fp32 by 20%-30%. In order to verify the speed of fp16, I construct a simple net with several dense layers, which are initialzed with fp16 random values, and run the forward procedure only. But fp16 is slower than fp32. ...

**Question**
1. Why FP16 is faster on a Pascal GTX GPU, what hardware units were used?
2. Is there any better way to inference with fp16 using a pretrained fp32 model?
3. If the TensorRT is the normal method to inference with fp16?
4. If using RTX 2080 with tensor core, we will get a dramatic performance in fp16?
"
25403,Eager Execution error: Blas GEMM launch failed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: no
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2
- CUDA/cuDNN version: CUDA 9.0, cudnn 7.4.2
- GPU model and memory:  GeForce RTX 2080 Ti

**Describe the current behavior**
Crashes with error ""Blas GEMM launch failed""

**Describe the expected behavior**
Correctly print the matmul result

**Code to reproduce the issue**
I was trying to use eager execution. I tried the following simple code
```python
import tensorflow as tf
tf.enable_eager_execution()
print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))
```
Other eager mode code under at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples fails with the same error.

However, non eager mode code can correctly run.

**Other info / logs**
output
```
2019-01-31 17:00:20.744826: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-01-31 17:00:21.150735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:17:00.0
totalMemory: 10.73GiB freeMemory: 9.36GiB
2019-01-31 17:00:21.399702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:65:00.0
totalMemory: 10.73GiB freeMemory: 10.53GiB
2019-01-31 17:00:21.399746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-01-31 17:00:21.906842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-31 17:00:21.906877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1
2019-01-31 17:00:21.906882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y
2019-01-31 17:00:21.906886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N
2019-01-31 17:00:21.907143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9026 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5)
2019-01-31 17:00:21.907488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10167 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)
2019-01-31 17:00:22.144957: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED
Traceback (most recent call last):
  File ""test.py"", line 5, in <module>
    print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))
  File ""/home/weixu/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 2057, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/weixu/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4586, in mat_mul
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(2, 2), b.shape=(2, 2), m=2, n=2, k=2 [Op:MatMul] name: MatMul/
```
"
25402,Flatten is not constant folded properly by constfold grappler.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master (1/31/19)
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: CUDA 10.0.130
- GPU model and memory: Quadro RTX 6000, 24GiB


**Describe the current behavior**
tf.layers.Flatten uses Shape, StridedSlice, Pack ops to build the new shape for Reshape. These ops are not constant folded.

**Describe the expected behavior**
 Since the shape of `x` is known at compile time, the constant folder should be able to fold away these ops to create a single Const for the new shape for Reshape.

**Code to reproduce the issue**
```python
import tensorflow as tf
import math
from tensorflow.core.protobuf import meta_graph_pb2
from tensorflow.core.protobuf import rewriter_config_pb2
from tensorflow.python.grappler import tf_optimizer

def build_graph(x, output_name='output'):
  # x.shape = (None, 1, 1, 1536)
  x = tf.layers.Flatten()(x)
  # This should perform exactly the same as:
  # x = tf.reshape(x, [-1, 1536])
  return tf.identity(x, name=output_name)

def apply_constfold(frozen_graph, output_nodes):
  graph = tf.Graph()
  with graph.as_default():
    tf.import_graph_def(frozen_graph, name="""")
  grappler_meta_graph_def = tf.train.export_meta_graph(graph_def=graph.as_graph_def(add_shapes=True), graph=graph)

  _to_bytes = lambda s: s.encode(""utf-8"", errors=""surrogateescape"")
  output_collection = meta_graph_pb2.CollectionDef()
  output_list = output_collection.node_list.value
  for i in output_nodes:
    if isinstance(i, tf.Tensor):
      output_list.append(_to_bytes(i.name))
    else:
      output_list.append(_to_bytes(i))
  # TODO(laigd): use another key as the outputs are really not train_op.
  grappler_meta_graph_def.collection_def[""train_op""].CopyFrom(output_collection)
  rewriter_config = rewriter_config_pb2.RewriterConfig()
  rewriter_config.optimizers.extend([""constfold""])

  session_config_with_trt = tf.ConfigProto()
  session_config_with_trt.graph_options.rewrite_options.CopyFrom(
      rewriter_config)
  frozen_graph = tf_optimizer.OptimizeGraph(session_config_with_trt, grappler_meta_graph_def, graph_id=b""tf_graph"")
  return frozen_graph

if __name__ == '__main__':
  with tf.Graph().as_default():
    # Create graph
    x = tf.placeholder(dtype=tf.float32, shape=(None, 1, 1, 1536), name='input')
    y = build_graph(x)
    # Initialize
    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      # Freeze graph
      frozen_graph = tf.graph_util.convert_variables_to_constants(
          sess,
          sess.graph_def,
          output_node_names=['output'])

  print('Nodes before:')
  [print(n.name, n.op) for n in frozen_graph.node]

  # const folding
  frozen_graph = apply_constfold(frozen_graph, output_nodes=['output'])

  print('----------------------------------------')
  print('Nodes after:')
  [print(n.name, n.op) for n in frozen_graph.node]
```

**Output**
```
Nodes before:
input Placeholder
flatten/Shape Shape
flatten/strided_slice/stack Const
flatten/strided_slice/stack_1 Const
flatten/strided_slice/stack_2 Const
flatten/strided_slice StridedSlice
flatten/Reshape/shape/1 Const
flatten/Reshape/shape Pack
flatten/Reshape Reshape
output Identity
-------------------------
Nodes after:
input Placeholder
flatten/strided_slice/stack Const
flatten/strided_slice/stack_1 Const
flatten/strided_slice/stack_2 Const
flatten/Reshape/shape/1 Const
flatten/Shape Shape
flatten/strided_slice StridedSlice
flatten/Reshape/shape Pack
flatten/Reshape Reshape
output Identity
```

** Expected Output **
```
Nodes before:
input Placeholder
flatten/Shape Shape
flatten/strided_slice/stack Const
flatten/strided_slice/stack_1 Const
flatten/strided_slice/stack_2 Const
flatten/strided_slice StridedSlice
flatten/Reshape/shape/1 Const
flatten/Reshape/shape Pack
flatten/Reshape Reshape
output Identity
-------------------------
Nodes after:
input Placeholder
Reshape/shape Const
Reshape Reshape
output Identity
```"
25400,Support SavedModel in WarmStartSettings initialization,"Any reason not to support SavedModel in WarmStartSettings initialization? Currently only checkpoints are supported.

Estimator's `warm_start_from` argument accepts a SavedModel, so it would be great to have parity with that for cases where a user needs finer-grained control via WarmStartSettings (e.g., only initializing a subset of variables for transfer learning/fine tuning)."
25398,Feature Request: tf.diag with a parameter to choose the diagonal ,"
**System information**
- TensorFlow version (you are using):'1.12.0'
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
In the current version tf.diag only allow creating a tensor with the main diagonal filled. 
```python
# 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]

and diagonal.shape = (2, 4)

tf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0]
                                     [0, 2, 0, 0]
                                     [0, 0, 3, 0]
                                     [0, 0, 0, 4]],
                                    [[5, 0, 0, 0]
                                     [0, 6, 0, 0]
                                     [0, 0, 7, 0]
                                     [0, 0, 0, 8]]]

which has shape (2, 4, 4)
```
Numpy has an extra parameter, k. The default value of k is zero. When k=0  np.diag has the same behavior than tf.diag. However, setting k>0 (k<0) the user fill, or select, a diagonal above (below) the main diagonal.

```python
x = np.arange(9).reshape((3,3))
 np.diag(x, k=0)
#array([0, 4, 8])
np.diag(x, k=1)
# array([1, 5])
np.diag([1,5], k=1)
#array([[0, 1, 0],
#      [0, 0, 5],
#       [0, 0, 0]])
```

**Who will benefit with this feature?**
 In general, tridiagonal matrices are quite common in mathematics and physics."
25397,[Docs] Minor nit in the performance overview page,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://www.tensorflow.org/guide/performance/overview


**Describe the documentation issue**
A minor nit, i the performance overview page change /Alexne**x**/Alexne**t**/

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
yes
"
25395,TF debug crashes when nan in loss,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, beyond the trivial fix needed for  https://github.com/tensorflow/models/issues/5857, and wrapping the tf.Session in a debug session
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1080ti / 11GB


**Describe the current behavior**
When run in the debugger, if there is a NaN in the loss, the debugger crashes.

**Describe the expected behavior**

Expected behavior is that the debugger does not crash and I can examine the tensors.
**Code to reproduce the issue**

In models/research/struct2depth, run: 
ckpt_dir=""ckpt""
data_dir=""/opt/baddata/tfrep""

python train.py \
  --logtostderr \
  --checkpoint_dir=$ckpt_dir  \
  --data_dir=$data_dir \
  --architecture=resnet \
  --imagenet_norm=true \
  --joint_encoder=false

**Other info / logs**

I'm attaching a screenshot of the terminal after tfdbg crashes (the terminal gets corrupted so I can't paste this into a text file) :  
<img width=""1422"" alt=""screen shot 2019-01-28 at 12 06 26"" src=""https://user-images.githubusercontent.com/4651708/52070300-8628ac00-254e-11e9-835e-e5ff4ff84282.png"">

A set of input data that triggers this crash will be uploaded soon."
25393,"""reuse"" argument in tf.layers.Dense documentation should be renamed to ""_reuse""","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/layers/Dense


**Describe the documentation issue**
`reuse` is no longer a valid argument, and should be replaced with `_reuse` instead. Passing reuse to Dense will cause the following error:

```python
TypeError: ('Keyword argument not understood:', 'reuse')
```
**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25391,tf.linalg.solve segfaults on invalid matrix dimensions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6 (17G4015)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Calling `tf.linalg.solve(A, b)` with `b.shape == (n,)` causes a segfault.

**Describe the expected behavior**

Invalid shape error expected. This seems to happen on the GPU version I've tested on another system, but crashes on the CPU version on my Mac.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
tf.enable_eager_execution()
# This is ok
tf.linalg.solve([[.1, .0], [.0, .1]], [[.1], [.1]])
# This line causes segfault
tf.linalg.solve([[.1, .0], [.0, .1]], [.1, .1])
```

**Other info / logs**

Same thing happens on the latest nightly build, that is `b'v1.12.0-6726-g5522d670af' 1.13.0-dev20190126`"
25390,Failed in v1.13.0-rc0 docker file build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
Just used the tensorflow/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile in v1.13.0-rc0 tag.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Here is the bug:

https://github.com/tensorflow/tensorflow/blob/a8e5c41c5bbe684a88b9285e07bd9838c089e83b/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L71

**Provide the exact sequence of commands / steps that you executed before running into the problem**

If the condition is false, bash return 1. Thus, it breaks docker build image process.

PLEASE PLEASE PLEASE find a guy who knows dockerfile and bash to write a build script. The quality of build process is absolutely unacceptable.

I tried last stable release v1.12.0 and recent release candidate. They all failed.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25387,TF 2.0 Documents how BatchNormalization update is handle in TF 2.0,"When training model that need specific update such as batch norm mean and variance, it is not clear 
from the current version of documentations how we can handle this. 

This docs https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model suggests that we have to call `model.updates`

While here https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/dcgan.ipynb there's nothing about `model.updates`

**System information**
- TensorFlow version: 2.0-preview
"
25386,Add integer data types to IsTrainable for use with custom gradients,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, there are a limited number of DTypes usable with autograd (see `IsTrainable` and `_IsBackpropagatable` below) https://github.com/tensorflow/tensorflow/blob/6631cd5e0e43e672d2de3cf1f0e29da17b57824e/tensorflow/python/ops/gradients_impl.py#L300

In particular, nodes with integer tensors automatically generate gradients of None, which breaks any call to `tf.gradients`.  This is a reasonable assumption for usual practices in machine learning, however this prevents one from creating their own autograd system using `tf.custom_gradient`.  I'd contend that, as a general purpose automatic differentiation library, TF should enable experimenting with such uses of its autograd system.  This would be particularly useful when intending to perform automatic differentiation over rings or finite fields, which are often represented in computers as sets of consecutive integers (i.e. `Z_p`).

**Will this change the current api? How?**
This change shouldn't affect any standard usage of TensorFlow -- all autograd on floats will be unchanged.  The only change will occur when calling autograd on graphs that compute integer arithmetic and similar.

Currently, performing `tf.gradients` on graphs with integer tensors will raise an unhandled error:
```python
import numpy as np
import tensorflow as tf

x_back = np.ones([2, 2])
y_back = np.ones([2, 2])
x = tf.Variable(x_back, dtype=tf.int32)
y = tf.Variable(y_back, dtype=tf.int32)
z = x + y
vg = tf.gradients([z], [x, y])
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    out = sess.run(vg)
    print(out)
```
Returns:
```
Traceback (most recent call last):
  File ""/Users/jasonmancuso/dropout/research/customgrad/issue_min.py"", line 16, in <module>
    out = sess.run(vg)
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 471, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 261, in for_fetch
    return _ListFetchMapper(fetch)
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 370, in __init__
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 370, in <listcomp>
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 258, in for_fetch
    type(fetch)))
TypeError: Fetch argument None has invalid type <class 'NoneType'>
```
See https://github.com/tensorflow/tensorflow/issues/783#issuecomment-175824168 for other cases in which Ops can generate a `None` gradient.

After implementing this feature, we'd return the following:
```
[array([[1, 1], [1, 1]], dtype=int32),
 array([[1, 1], [1, 1]], dtype=int32)]
```

**Who will benefit with this feature?**
Anyone who wants to perform automatic differentiation on integer data types, as would be the case when operating in integer rings or finite fields.

**Any Other info.**
This request is motivated by the [`tf-encrypted`](https://github.com/mortendahl/tf-encrypted) project."
25385,TF 2.0: Summaries are not written to the disk,"**Describe the current behavior**

The example from [tf2_overview](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/beginner/tf2_overview.ipynb) doesn't write the summaries onto the disk and nothing happen in the tensorboard.

Tensorflow installed with `pip install tf-nightly-2.0-preview`. Same thing happened with the GPU version.

Below is the code we use to reproduce this behaviour

```
import os
import time
import numpy as np

import tensorflow as tf

from tensorflow.python.ops import summary_ops_v2

layers = tf.keras.layers


def create_model():
    max_pool = layers.MaxPooling2D((2, 2), (2, 2), padding='same')
    # The model consists of a sequential chain of layers, so tf.keras.Sequential
    # (a subclass of tf.keras.Model) makes for a compact description.
    return tf.keras.Sequential([
        layers.Reshape(
            target_shape=[28, 28, 1],
            input_shape=(28, 28,)),
        layers.Conv2D(2, 5, padding='same', activation=tf.nn.relu),
        max_pool,
        layers.Conv2D(4, 5, padding='same', activation=tf.nn.relu),
        max_pool,
        layers.Flatten(),
        layers.Dense(32, activation=tf.nn.relu),
        layers.Dropout(0.4),
        layers.Dense(10)])


# Define a loss function and accuracy function
def compute_loss(logits, labels):
    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits))


def compute_accuracy(logits, labels):
    return tf.keras.metrics.categorical_accuracy(labels, logits)


# Set up datasets
def mnist_datasets():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.
    x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)
    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    return train_dataset, test_dataset


@tf.function
def train_step(model, optimizer, images, labels):
    # Record the operations used to compute the loss, so that the gradient
    # of the loss with respect to the variables can be computed.
    with tf.GradientTape() as tape:
        logits = model(images, training=True)
        loss = compute_loss(logits, labels)
        accuracy = compute_accuracy(logits, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss, accuracy


def train(model, optimizer, dataset, log_freq=50):
    """"""Trains model on `dataset` using `optimizer`.""""""
    start = time.time()
    # Metrics are stateful. They accumulate values and return a cumulative
    # result when you call .result(). Clear accumulated values with .reset_states()
    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)
    avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)
    # Datasets can be iterated over like any other Python iterable.
    for images, labels in dataset:
        loss, accuracy = train_step(model, optimizer, images, labels)
        avg_loss(loss)
        avg_accuracy(accuracy)
        if tf.equal(optimizer.iterations % log_freq, 0):
            summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)
            summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=optimizer.iterations)
            avg_loss.reset_states()
            avg_accuracy.reset_states()
            rate = log_freq / (time.time() - start)
            print('Step #%d\tLoss: %.6f (%d steps/sec)' % (optimizer.iterations, loss, rate))
            start = time.time()


def test(model, dataset, step_num):
    """"""Perform an evaluation of `model` on the examples from `dataset`.""""""
    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)
    avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)

    for (images, labels) in dataset:
        logits = model(images, training=False)
        avg_loss(compute_loss(logits, labels))
        avg_accuracy(compute_accuracy(logits, labels))
    print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(
        avg_loss.result(), avg_accuracy.result() * 100))
    summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)
    summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=step_num)


MODEL_DIR = '/tmp/mnist2'

if __name__ == '__main__':
    model = create_model()

    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)

    train_ds, test_ds = mnist_datasets()
    train_ds = train_ds.shuffle(60000).batch(100)
    test_ds = test_ds.batch(100)

    train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')
    test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')

    train_summary_writer = summary_ops_v2.create_file_writer(
        train_dir, flush_millis=10000)
    test_summary_writer = summary_ops_v2.create_file_writer(
        test_dir, flush_millis=10000, name='test')

    for epoch in range(10):
        start = time.time()
        with train_summary_writer.as_default():
            train(model, optimizer, train_ds)
        end = time.time()
        print('\nTrain time for epoch #{} ({} total steps): {}'.format(epoch + 1, optimizer.iterations, end - start))
        with test_summary_writer.as_default():
            test(model, test_ds, optimizer.iterations)
```
"
25382,TF 2.0: eager.function.defun (or tf.function) Cancelled Op Error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): '2.0.0-dev20190126'
- Python version: 3.6.8

**Describe the current behavior**

Assume that there is tf.Assert inside of decorated function, which is independent of input argument (then defun get same graph function from graph cache, right?)

However, after AssertOp invoked once, following function calls are not be called. It directly raise `CancelledError`, which is described as below log information.


**Describe the expected behavior**

Whether previous function call raised assertion error or not,
function result should be independent of previous function call

**Code to reproduce the issue**

I think this is one of the most simplest example to reproduce error.
Since it's stochastic, it might be run without error in single run.
However, you can get `CancelledError` in few trials
```
import tensorflow as tf

@tf.function
def prob_assert():
  x = tf.random.uniform(shape=(1,))
  tf.print('sampled value', x)
  tf.assert_greater(x, 0.5, message='test')
  return x

try:
  prob_assert()
  prob_assert()
  prob_assert()
  prob_assert()
except:
  pass
prob_assert()
```
**Other info / logs**

```
sampled value [0.622857213]
sampled value [0.417618513]  // Assertion Op Invoked
2019-01-31 17:51:14.223845: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: assertion failed: [test] [Condition x > y did not hold element-wise:x (random_uniform:0) = ] [0.417618513] [y (assert_greater/y:0) = ] [0.5]
	 [[{{node assert_greater/Assert/AssertGuard/else/_1/Assert}}]]
2019-01-31 17:51:14.223876: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Invalid argument: assertion failed: [test] [Condition x > y did not hold element-wise:x (random_uniform:0) = ] [0.417618513] [y (assert_greater/y:0) = ] [0.5]
	 [[{{node assert_greater/Assert/AssertGuard/else/_1/Assert}}]]
Traceback (most recent call last):
  File ""test.py"", line 19, in <module>
    (random_assert())
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 376, in __call__
    results = self._stateful_fn(*args, **kwds)
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1072, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 460, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 507, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 312, in call
    ctx=ctx)
  File ""/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.CancelledError:  [Op:__inference_random_assert_38]
```
"
25378,how to export tensorrt int8  model with savedmodel format after calibrationï¼Ÿ,"Is it possible to export a savedModel int8 model after calibration?
If it is possible, is there any demo could be offeredï¼Ÿ
If it is impossibleï¼Œwhen will it maybe possibleï¼Ÿ"
25375,TF 2.0: API symbol renames design review.,"TensorFlow 1.x has over 2000 endpoints total including over 500 endpoints in the root namespace. As number of symbols grows, it is important to maintain a clear structure to aid discoverability.

Certain API symbol placements could be improved:

- Some namespaces were created recently and might not contain all the corresponding symbols. For e.g. `tf.math` namespace was added recently. Symbols such as `tf.round` are not in `tf.math` namespace even though logically they belong in that namespace.
- Some symbols are included in the root namespace even though they are rarely used (for e.g. `tf.zeta`).
- Some symbols currently start with a prefix that could really be replaced by introducing a subnamespace (for e.g. `tf.string_strip` vs `tf.strings.strip`, `tf.sparse_maximum` vs `tf.sparse.maximum`).
- Certain deep hierarchies seem redundant and could be flattened (for e.g. `tf.saved_model.signature_constants.CLASSIFY_INPUTS` could be moved to `tf.saved_model.CLASSIFY_INPUTS`).
- To keep clear structure and reduce duplication, we want to collect all layers, losses and metrics under the `tf.keras` namespace.
- In general, we want to balance flatness and browsability. Flat hierarchies allow for shorter endpoint names that are easy to remember (for e.g. `tf.add` vs `tf.math.add`). At the same time subnamespaces support easier browsability (for e.g. `tf.math` namespace would contain all math functions making it easier to discover available symbols).

Additional information about API symbol renames in TensorFlow 2.0 can be found in the RFC [here](https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md)."
25374,TF 2.0: Remove QueueRunners in favor of tf.data.Dataset.,"**[`tf.train.QueueRunner`](https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner)** is not compatible with eager execution, and will be deprecated in TF 2.0.

As an alternative, **[`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)** can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a ""logical plan"" of transformations that act on those elements.

"
25373,TF 2.0: Move TensorFlow Lite out of tf.contrib.,"**[TensorFlow Lite](https://www.tensorflow.org/lite/)** is the official solution for running machine learning models on mobile and embedded devices. It enables onâ€‘device machine learning inference with low latency and a small binary size on Android, iOS, and other operating systems.

In TensorFlow 1.x, TF Lite was housed in `tf.contrib`, a somewhat volatile location. With TF 2.0, we intend to migrate TF Lite to the main API, under [`tf.lite`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite).

"
25372,ImportError: DLL load failed: The specified procedure could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25371,TF 2.0: Create conversion tool from TensorFlow 1.x.,
25370,TF 2.0: Create nightly CI builds for the TF 2.0 target.,
25369,Feature: End-to-end TensorFlow Extended (TFX) example for TF 2.0.,"**[TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx/)** is an end-to-end, TensorFlow-based, general-purpose machine learning platform implemented at Google. We've already open sourced some TFX libraries with the rest of the system to come. For an overview of TFX, read our [KDDâ€™2017 paper](https://dl.acm.org/citation.cfm?id=3098021) and watch the [Dev Summit talk](https://www.youtube.com/watch?v=vdG7uKQ2eKk).

TFX includes:
- **[TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation)**: a library for exploring and validating machine learning data.
- **[TensorFlow Transform](https://www.tensorflow.org/tfx/transform)**: full-pass analyses over data to create transformation graphs that are consistently applied during training and serving.
- **[TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis)**: libraries and visualization components to compute full-pass and sliced model metrics over large datasets, and analyze them in a notebook.
- **[TensorFlow Serving](https://www.tensorflow.org/serving)**: a flexible, high-performance serving system for machine learning models, designed for production environments.

The purpose of this issue is to track the creation of an end-to-end TFX example using TensorFlow 2.0: a Keras model working with Transform, TFMA, data validation, and Serving."
25367,Testing: TF 2.0 with TensorFlow Datasets (TFDS).,"TFDS is now ready for testing, sans the LMDB dataset!

https://github.com/tensorflow/datasets/issues/34"
25366,Documentation: TF 2.0 docs and tutorials.,"Documentation and tutorials are a critical piece of every open-source project, and a developer's first impression of the product. The purpose of this issue will be to track the creation, migration, and translations for TensorFlow 2.0 docs.

For more information, check out the example notebooks we have available [here](https://github.com/tensorflow/docs/tree/master/site/en/r2), as well as the [TF 2.0 upgrade guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) and the [Effective 2.0 Style Guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md).

"
25365,Feature: Google Colab compatibility with TF 2.0.,"**[Colaboratory](https://colab.research.google.com/)** is a free research tool for machine learning education and research. Itâ€™s a Jupyter notebook environment that requires no setup to use. Colab works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.

The purpose of this issue will be to track TF 2.0 compatibility with Colab."
25364,Documentation: Updating tutorials to use TensorFlow Datasets (TFDS).,"**[TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/api_docs/python/tfds)** is a collection of datasets ready-to-use with TensorFlow. Each dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates the logic to download the dataset and construct an input pipeline, as well as contains the dataset documentation (version, splits, number of examples, etc.).

The purpose of this issue is to migrate all tutorials to use TensorFlow Datasets, and to be compliant with the TF 2.0 API. Each migrated tutorial must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25363,Feature: TF Serving compatibility with TF 2.0.,"**[TensorFlow Serving](https://www.tensorflow.org/serving/)** is a flexible, high-performance serving system for machine learning models, designed for production environments. Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.

The purpose of this issue is to upgrade Serving to be TF 2.0 compliant. The system must be eager and distribution compatible, with tests, and all associated engineering artifacts.
"
25362,Feature: TF Hub compatibility with TF 2.0.,"**[TensorFlow Hub](https://www.tensorflow.org/hub/)** is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. 

Transfer learning can:
- Train a model with a smaller dataset,
- Improve generalization, and
- Speed up training.

The purpose of this issue is to migrate modules in TF Hub into the TF 2.0 Keras API. Each migrated module must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25361,Feature: TensorFlow Lite compatibility with TF 2.0.,"**Problem**
TensorFlow 2.0 introduces features that are not supported by the conversion process used to convert TensorFlow models to TensorFlow Lite models. This issue will track the changes that must be made in order to support the conversion process in TensorFlow 2.0.

**Goals:**

- Add support for converting TensorFlow models generated in TensorFlow 2.0.
- Continue to support conversion for SavedModels generated in TensorFlow 1.X.
- Support conversion for Keras model files generated in TensorFlow 1.X and 2.0.

**Non-goals:**

- Continue to support conversion for frozen GraphDefs generated in TensorFlow 1.X. The related API will be deprecated in 2.0.
"
25360,Feature: TensorFlow.js compatibility with TF 2.0.,"- Function inlining issue.
- [SavedModel breaks in TensorFlow.js](https://github.com/tensorflow/tfjs/issues/1123)."
25359,Feature: Support for FP16 training in Keras.,
25358,"Error occurs when training Mask R-CNN on dataset coco 2017, failed to run optimizer, stage RemoveStackStridedSliceSameAxis","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow_gpu 1.13.0-rc0
- Python version:  Python 3.6.7, pip 18.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.0.130, libcudnn7-dev_7.4.2.24-1, libcudnn7_7.4.2.24-1
- GPU model and memory: Nvidia TITAN Xp 12GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

b'v1.13.0-rc0-0-g6ce86799c8' 1.13.0-rc0

**Describe the current behavior**
Hi, I'm trying to train Mask R-CNN model (https://github.com/matterport/Mask_RCNN) with dataset coco 2017, but it reports error as following, saying failed to run optimizer ArithmeticOptimizer, how to fix it, please help, thanks!

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Code is available at https://github.com/matterport/Mask_RCNN 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

2019-01-31 10:11:33.060384: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
2019-01-31 10:11:33.060528: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
2019-01-31 10:11:43.572615: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
2019-01-31 10:11:43.572742: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
999/1000 [============================>.] - ETA: 0s - loss: 0.3451 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0632 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.16012019-01-31 10:24:40.241827: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
2019-01-31 10:24:40.241952: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
2019-01-31 10:24:41.861193: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]
1000/1000 [==============================] - 880s 880ms/step - loss: 0.3449 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0631 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.1600 - val_loss: 2.0419 - val_rpn_class_loss: 0.1070 - val_rpn_bbox_loss: 0.9081 - val_mrcnn_class_loss: 0.4638 - val_mrcnn_bbox_loss: 0.2041 - val_mrcnn_mask_loss: 0.3590
Epoch 6/12
1000/1000 [==============================] - 799s 799ms/step - loss: 0.3522 - rpn_class_loss: 0.0037 - rpn_bbox_loss: 0.0779 - mrcnn_class_loss: 0.0551 - mrcnn_bbox_loss: 0.0508 - mrcnn_mask_loss: 0.1647 - val_loss: 1.5071 - val_rpn_class_loss: 0.0384 - val_rpn_bbox_loss: 0.8576 - val_mrcnn_class_loss: 0.1833 - val_mrcnn_bbox_loss: 0.1751 - val_mrcnn_mask_loss: 0.2529"
25357,Feature: Unify argument names (~70).,
25356,Feature: TensorBoard compatibility with TF 2.0.,"**Overview**
The `tf.summary` module from TF 1.x will be replaced by a new API that differs as follows:

1. The **data-format-specific parts** will be defined in `tensorboard.summary`.
2. The **generated summary events** will use a more extensible ""wire format"".
3. The **write-side code** will use the V2 summary-writing API (today's `tf.contrib.summary`).

**Background**
Today three sets of summary ops exist: `tf.summary`, `tf.contrib.summary`, and `tb.summary` (aka `tensorboard.summary`).  They differ across two main dimensions: the ""summary-writing API"" which determines how user code calls the ops, and the ""wire format"" of how the summary data gets encoded in event files.  Both dimensions have legacy (V1) and new (V2) styles.  The proposed new summary ops fill in the (V2, V2) quadrant of this grid:

|                                                | V1 (FileWriter, merge_summaries) | V2 (create_file_writer, eager-compatible) |
|------------------------------------------------|----------------------------------|-------------------------------------------|
| V1 (dedicated fields per type in tf.Summary)   | `tf.summary`    | `tf.contrib.summary`                        |
| V2 (extensible tensor format, plugin metadata) | `tb.summary`  | [proposed] `tf.summary`  in TF 2.0, exporting symbols from `tb.summary`  |
"
25355,Library Conversion: Sonnet,"**[Sonnet](https://github.com/deepmind/sonnet/tree/master/docs)** is an open-source library built on top of TensorFlow, created by researchers at **[DeepMind](https://deepmind.com/)**. The main purpose of Sonnet is to first construct Python objects which represent some part of a neural network, and then separately connect these objects into the TensorFlow computation graph.

For more information about Sonnet, check out [DeepMind's announcement](https://deepmind.com/blog/open-sourcing-sonnet/).

The purpose of this ticket will be to migrate the Sonnet library to the TF 2.0 API.

- Convert all existing Sonnet code.
- Run unit tests.
- Ensure processes are well understood."
25353,Library Conversion: OpenSeq2Seq,"**[OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq)** is a toolkit for distributed and mixed-precision training of sequence-to-sequence (seq2seq) models, created by the researchers at NVIDIA.

OpenSeq2Seq is built using TensorFlow and provides all the necessary building blocks for training encoder-decoder models for neural machine translation, automatic speech recognition, speech synthesis, and language modeling. For documentation and TensorFlow 1.x installation instructions, check [here](https://nvidia.github.io/OpenSeq2Seq/).

The purpose of this ticket will be to migrate OpenSeq2Seq to the TF 2.0 API.

- Convert all existing OpenSeq2Seq code.
- Run unit tests.
- Ensure processes are well understood.

"
25352,Library Conversion: Tensor2Tensor,"**Tensor2Tensor**, or [T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library of deep learning models and datasets designed to make deep learning more accessible and [accelerate ML research](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html). T2T is actively used and maintained by researchers and engineers within the [Google Brain team](https://research.google.com/teams/brain/) and a community of users. You can chat with the T2T team on [Gitter](https://gitter.im/tensor2tensor/Lobby) and join the [T2T Google Group](https://groups.google.com/forum/#!forum/tensor2tensor).

The purpose of this ticket will be to migrate Tensor2Tensor to the TF 2.0 API.

- Convert all existing Tensor2Tensor code.
- Run unit tests.
- Ensure processes are well understood."
25350,Library Conversion: TF Probability,"**[TensorFlow Probability](https://www.tensorflow.org/probability/)** (TFP) is a Python library built on TensorFlow that makes it easy to combine probabilistic models and deep learning on modern hardware (TPU, GPU). It's for data scientists, statisticians, ML researchers, and practitioners who want to encode domain knowledge to understand data and make predictions. TFP includes:

- A wide selection of probability distributions and bijectors.
- Tools to build deep probabilistic models, including probabilistic layers and the Edward2 language.
- Variational inference and Markov chain Monte Carlo.
- Optimizers such as Nelder-Mead, BFGS, and SGLD.

The purpose of this ticket will be to migrate TF Probability to the TF 2.0 API.

- Convert TF Probability library.
- Run unit tests.
- Ensure processes are well understood.
"
25349,Library Conversion: Open AI Baselines,"**[OpenAI Baselines](https://github.com/openai/baselines)** is a set of high-quality implementations of reinforcement learning algorithms. These algorithms make it easier for the research community to replicate, refine, and identify new ideas, and creates solid baselines to build research on top of.

The purpose of this ticket will be to migrate the OpenAI Baselines library to the TF 2.0 API.

- Convert OpenAI Baselines library.
- Run unit tests.
- Ensure processes are well understood.

For more information on OpenAI baselines, check out their blog post [here](https://blog.openai.com/openai-baselines-dqn/) - and be sure to amble around their [website](https://openai.com/), as well as the OpenAI [gym](http://gym.openai.com/)!"
25348,Library Conversion: TensorRT,"`tf.contrib` will be deprecated in TF 2.0, which means that TensorRT must migrate to a new home! The purpose of this ticket will be to migrate NVIDIA's TF-TRT library to the TF 2.0 API.

- Convert TF-TRT library.
- Run unit tests.
- Ensure processes are well understood.

TensorFlow integration with TensorRT (TF-TRT) optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Therefore, the workflow includes importing a trained TensorFlow model (graph and weights), freezing the graph, creating an optimized graph with TensorRT, importing it back as the default graph, and running inference. [[0](https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html)]

For the TensorFlow 1.x implementation of TF-TRT, check [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensorrt).

[0] [NVIDIA Deep Learning Frameworks Documentation](https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html) (2019)
"
25346,2.0 Reference Models: NCF Model (TPU with Keras),"This is an implementation of the **Neural Collaborative Filtering** (NCF) framework with Neural Matrix Factorization (NeuMF) model as described in the [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) paper. Current implementation is based on the code from the authors' [NCF code](https://github.com/hexiangnan/neural_collaborative_filtering) and the Stanford implementation in the [MLPerf Repo](https://github.com/mlperf/reference/tree/master/recommendation/pytorch).

**Neural Collaborative Filtering** is a general framework for collaborative filtering of recommendations in which a neural network architecture is used to model user-item interactions. Unlike traditional models, NCF does _not_ resort to Matrix Factorization (MF) with an inner product on latent features of users and items. It replaces the inner product with a multi-layer perceptron that can learn an arbitrary function from data.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/blob/6518c1c7711ef1fdbe925b3c5c71e62910374e3e/official/recommendation/README.md).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25344,"2.0 Reference Models: NCF Model (1 GPU, 8 GPU with dist strat and Keras)","This is an implementation of the **Neural Collaborative Filtering** (NCF) framework with Neural Matrix Factorization (NeuMF) model as described in the [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) paper. Current implementation is based on the code from the authors' [NCF code](https://github.com/hexiangnan/neural_collaborative_filtering) and the Stanford implementation in the [MLPerf Repo](https://github.com/mlperf/reference/tree/master/recommendation/pytorch).

**Neural Collaborative Filtering** is a general framework for collaborative filtering of recommendations in which a neural network architecture is used to model user-item interactions. Unlike traditional models, NCF does _not_ resort to Matrix Factorization (MF) with an inner product on latent features of users and items. It replaces the inner product with a multi-layer perceptron that can learn an arbitrary function from data.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/blob/6518c1c7711ef1fdbe925b3c5c71e62910374e3e/official/recommendation/README.md).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25343,"2.0 Reference Models: NMT Model (1 GPU, 8 GPU with dist strat and Keras)","**Sequence-to-sequence** (seq2seq) models ([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. 

This model will give TF 2.0 end users a full understanding of seq2seq models and show them how to build a competitive seq2seq model from scratch. We focus on the task of **Neural Machine Translation** (NMT), which was the very first [testbed for seq2seq models](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/nmt).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25342,"2.0 Reference Models: Transformer (1 GPU, 8 GPU with dist strat and Keras)","**Transformer** is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.

See the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) for more background.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/transformer).

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts."
25341,2.0 Reference Models: Keras Application Set (1 GPU),"Keras Applications is the `applications` module of the Keras deep learning library. It provides model definitions and pre-trained weights for a number of popular architectures, such as VGG16, ResNet50, Xception, MobileNet, and more.

The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.

For more information on tf.keras.applications, check [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications)."
25340,2.0 Reference Models: ResNet V1.5 (1 GPU and 8 GPU with dist-strat and Keras),"Deep residual networks, or ResNets for short, proposed the breakthrough idea of identity mappings in order to enable training of very deep convolutional neural networks. This issue will track the creation of 1 GPU and 8 GPU TF 2.0-compatible implementations of ResNet for the ImageNet dataset.

See the following papers for more background:

[1] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.

[2] [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.

For the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/resnet)."
25339,[BUG] gradients of `tf.cond` with float64 breaks with ResourceVariable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 3 tried: binary and head (w & w/o xla)
- TensorFlow version (use command below): 1.12 resp. head
- Python version:3.6
- Bazel version (if compiling from source):irrelevant
- GCC/Compiler version (if compiling from source):irrelevant
- CUDA/cuDNN version: irrelevant
- GPU model and memory: irrelevant

**Describe the current behavior**
Using a (resource!) Variable in tf.cond (and everything in tf.float64) breaks the gradient computation with an error like:
```python
TypeError: Tensors in list passed to 'inputs' of 'Merge' Op have types [float64, float32] that don't all match.
```

In gradients_impl.py, _GradientsHelper (line 805) are ZerosLikeOutsideLoop produced with op.This produces in control_flow_ops.py, line 1469, (iff the variable dtype is resource) zeros with 
```python
array_ops.zeros(gen_resource_variable_ops.variable_shape(switch_val))
```
This uses the dtype of the op.outputs. But for a resource dtype, this creates float32 zeros (which then collides with all the other float64 around). Any other type therefore works.

Tried (next to binary 1.12) also against Master/HEAD with and without XLA (on TF compilation). Same error

Solution: no idea how to get the datatype out there or something. Sorry.


**Code to reproduce the issue**
```python
import tensorflow as tf

var1 = tf.get_variable('var1', initializer=tf.constant(42., dtype=tf.float64), use_resource=True)
with tf.Session() as sess:
    sess.run(var1.initializer)
    res = tf.constant(0., dtype=tf.float64)
    cond1 = tf.cond(tf.constant(True), lambda: res, lambda: res + var1)
    grad_cond = tf.gradients(cond1, var1)
```


"
25337,CUDA_ERROR_OUT_OF_MEMORY: out of memory with RTX 2070,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: venv
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 9.0.176 / 7.4.2.24
- GPU model and memory: RTX 2070 8G



**Describe the problem**
Out of memory. The same code run perfectly with the same environment with a GTX 1070 so memory should be no factor. I just swapped a graphics card and the error arose.
The error output is initially CUBLAS_STATUS_ALLOC_FAILED so I looked around a bit and used this chunk of code in #7072:
```
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
set_session(sess)
```
After applying this code the error output is attached below in ""Any other info / logs"" section.
From ""failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory"" I think ""freeMemory: 6.57GiB"" is allocated as usual but somehow not available to cuda.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Try to train a CNN model with
``` 
model.fit_generator(train_datagen, steps_per_epoch=train_datagen.n // batch_size, epochs=epochs, verbose=2, validation_data=dev_datagen, validation_steps=dev_datagen.n // batch_size)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Using TensorFlow backend.
2019-01-30 22:54:18.923758: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-01-30 22:54:19.139480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.57GiB
2019-01-30 22:54:19.139744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-30 22:54:20.029695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 22:54:20.029852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-30 22:54:20.029938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-30 22:54:20.030160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6309 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
Found 209023 images belonging to 2 classes.
Found 11002 images belonging to 2 classes.
Epoch 1/200
2019-01-30 22:54:52.312147: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.312662: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.312846: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.313021: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.480583: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.480762: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.481276: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.481446: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.581230: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.581404: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.581690: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.581856: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.603866: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.604042: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.604320: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.604489: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.609692: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.609966: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.610424: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.610587: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.626099: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.626276: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.626553: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2019-01-30 22:54:52.626718: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-30 22:54:52.665175: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory"
25333,Eager execution - GradientTape.gradient() only returns gradient for last variable w.r.t. y?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
GradientTape.gradient() returns gradient for only the last variable in var_list, which is a strange behavior different from tf.gradient()

**Describe the expected behavior**
GradientTape.gradient() returns gradients for every variable in the var_list, same as tf.gradient()

**Code to reproduce the issue**
With eager execution (wrong output with only gradient for last variable - x2 calculated):
x1 = tf.constant(3.0)
x2 = x1 * x1
with tf.GradientTape() as g:
  g.watch([x1, x2])
  y = x2 + 1
g.gradient([x1, x2]).numpy()
[OUTPUT] 1.0

Without eager execution (the expected output):
x1 = tf.constant(3.0)
x2 = x1 * x1
y = x2 + 1
op = tf.gradient(y, [x1, x2])
with tf.Session() as sess:
  print(sess.run(op))
[OUTPUT] [6.0, 1.0]

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25331,"Unsupported Ops: RandomUniform, SoftmaxCrossEntropyWithLogits.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (or github SHA if from source): 1.13.0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FLOOR, FULLY_CONNECTED, MAX_POOL_2D, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: RandomUniform, SoftmaxCrossEntropyWithLogits.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Traceback (most recent call last):
  File ""/home/smohan/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/home/smohan/.local/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/smohan/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/smohan/.local/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25329,tensorflow-gpu 1.13.0rc0 requires minimum CUDA compute capability 6.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.0rc0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, CUDNN 7.4.1
- GPU model and memory: GeForce GTX 980 Ti



**Describe the problem**

The 1.13 release candidate requires a minimum compute capability of 6.0 (so it ignores lower-level devices).  Is this an intentional change that we can expect to see in the full release? Or just an artifact of the release candidate build?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
2019-01-30 14:29:07.690292: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-01-30 14:29:07.883552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce GTX 980 Ti major: 5 minor: 2 memoryClockRate(GHz): 1.291
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.97GiB
2019-01-30 14:29:07.888807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Ignoring visible gpu device (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 6.0.
2019-01-30 14:29:07.894771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-30 14:29:07.898191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-01-30 14:29:07.901776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 8036038049545966896
]
```"
25327,Error when restoring model using an Embedding layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2070 (8GB)

**Describe the current behavior**
Attempts to restore a frozen Keras model including an ""Embedding"" layer fail. The same problem appears when `tf.graph_util.convert_variables_to_constants()` and `tf.import_graph_def()` are used.
The error is:
```
Traceback (most recent call last):

  File ""<ipython-input-11-95d358f9d586>"", line 3, in <module>
    tf.import_graph_def(sg)

  File ""/home/werner/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)

  File ""/home/werner/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 431, in import_graph_def
    raise ValueError(str(e))

ValueError: Input 0 of node import/state/embedding_lookup was passed float from import/state/embeddings:0 incompatible with expected resource.
```
I tried:
* Saving the model with `tensorflow.saved_model.simple_save()`, then running the `freeze_graph` script to generate a frozen model, then loading the frozen model again in C++ / Python. The same error.

* `convert_variables_to_constants()`, and ` tf.import_graph_def()` the graph -> same error.

**Code to reproduce the issue**

Minimal model:
```
from tensorflow.keras.layers import Embedding, Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
import tensorflow as tf

sinput = Input(shape=(1,),  name='state_input', dtype='int32')
sinput_em = Embedding(output_dim=16, input_dim=100, input_length=1, name=""state"")(sinput)
model = Model(inputs=[sinput], outputs=[sinput_em]) 
model.compile(loss={'state': 'sparse_categorical_crossentropy' },   optimizer='adam')
```

* Steps when `save_model()` is used:

```
### Save model
from tensorflow.saved_model import simple_save
simple_save(K.get_session(),
            ""SavedModel/testembedding"",
            inputs={""state_input"": sinput },
            outputs={'state': sinput_em})
# then run the script:
#  ~/.local/bin/freeze_graph  --input_saved_model_dir=SavedModel/testembedding --output_graph=frozenemb.pb  --output_node_names=state/embedding_lookup/Identity_1 --clear_devices

# try to load again:
frozen_graph=""frozenemb.pb""
with tf.gfile.GFile(frozen_graph, ""rb"") as f:
    restored_graph_def = tf.GraphDef()
    restored_graph_def.ParseFromString(f.read())


with tf.Graph().as_default() as graph:
    tf.import_graph_def(
        restored_graph_def,
        input_map=None,
        return_elements=None,
        name=""""
        )

# -> error in import_graph_def()
```

* OR steps when `convert_variables_to_constants()` is used:

```
def serialize_graph(model):
    g = tf.graph_util.convert_variables_to_constants(
        tf.keras.backend.get_session(),
        tf.keras.backend.get_session().graph.as_graph_def(),
        #[n.name for n in tf.keras.backend.get_session().graph.as_graph_def().node],
        [t.op.name for t in model.outputs]
    )
    return g

sg = serialize_graph(model)

newg = tf.Graph()
with newg.as_default():
   tf.import_graph_def(sg) # -> error
   
```

**Other info / logs**
Seems to be related to #21889 (the error message is different)
"
25326,Binary prediction with LSTM,"I'm trying to implement an LSTM model to make binary (or multiclass) classification from raw log data(Mooc courses log data -> user-level droput/grade prediction ). I have read lots of publication and tutorials which seems to be what I'm looking for, but couldn't find any example on how to use it.

Do you have a link or something about this topic? (RNN, ConvLSTM2D, LSTM, GRU on Keras or TF)"
25325,Custom metric mix train and validation data,"**System information**

- Linux ubuntu, CPU
- installed packages using anaconda with python 3.6
- Tensorflow version v1.12.0-0-ga6d8ffae09 1.12.0
- Keras version  2.2.4

**Describe the current behavior**

When using a custom metric, the metric is computed on training data and validation data together.

**Describe the expected behavior**

The train metric should be on train data only and validation metric (*_val) should be on validation data only.

"
25324,Keras optimizer.adam is not usable with Eager Execution,"**System information**
- See the custom complete snippet below
- Linux Ubuntu 16.04
- TensorFlow installed via pip gpu version 
- TensorFlow version v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2
- CUDA/cuDNN version: 9.0.176 / 7.4.1
- GPU model and memory: GeForce GTX 1080  with 11178MiB

**Describe the current behavior**
If using `tf.enable_eager_execution()` the code expects non Keras optimizer,
if using ""standard"" behavior (compiled graph) the code runs fine.
See the code and logs below.

**Describe the expected behavior**
`tf.enable_eager_execution()` should not affect the behavior and the code should run in both cases.

**Code to reproduce the issue**
Copied from my repository https://github.com/oplatek/tf-eager-playground/blob/master/eagerplayer/test_train.py

```
#!/usr/bin/env python3
import tensorflow as tf
import numpy as np
import sys

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model


def fit_keras_model():
    data = np.random.random((1000, 32))
    labels = np.random.random((1000, 10))
    # source https://keras.io/getting-started/functional-api-guide/
    # This returns a tensor
    inputs = Input(shape=(32,))

    # a layer instance is callable on a tensor, and returns a tensor
    x = Dense(64, activation='relu')(inputs)
    x = Dense(64, activation='relu')(x)
    predictions = Dense(10, activation='softmax')(x)

    # This creates a model that includes
    # the Input layer and three Dense layers
    model = Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    model.fit(data, labels)  # starts training
    return model


if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == 'eager':
        tf.enable_eager_execution()  # fails with eager execution enabled
    fit_keras_model()
    print('success')
```

**Other info / logs**
The logs are copied also from my repository https://github.com/oplatek/tf-eager-playground/blob/master/eagerplayer/test_train.md

Without eager execution the code works fine
-------------------------------------------

	oplatek@gpu:master:tf-eager-playground$ ./eagerplayer/test_train.py
	Epoch 1/1
	2019-01-30 16:49:23.921582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
	2019-01-30 16:49:24.032120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
	2019-01-30 16:49:24.032526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
	name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575
	pciBusID: 0000:01:00.0
	totalMemory: 10.92GiB freeMemory: 10.76GiB
	2019-01-30 16:49:24.032540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
	2019-01-30 16:49:24.230759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
	2019-01-30 16:49:24.230787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
	2019-01-30 16:49:24.230792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
	2019-01-30 16:49:24.230947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
	1000/1000 [==============================] - 1s 745us/step - loss: 11.4860 - acc: 0.0810
	success


With eager execution non Keras optimizer is expected
----------------------------------------------------

	oplatek@gpu:master:tf-eager-playground$ ./eagerplayer/test_train.py eager
	2019-01-30 16:49:33.983911: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
	2019-01-30 16:49:34.069258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
	2019-01-30 16:49:34.069657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
	name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575
	pciBusID: 0000:01:00.0
	totalMemory: 10.92GiB freeMemory: 10.76GiB
	2019-01-30 16:49:34.069671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
	2019-01-30 16:49:34.266286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
	2019-01-30 16:49:34.266311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
	2019-01-30 16:49:34.266316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
	2019-01-30 16:49:34.266465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
	Traceback (most recent call last):
	  File ""./eagerplayer/test_train.py"", line 35, in <module>
		fit_keras_model()
	  File ""./eagerplayer/test_train.py"", line 27, in fit_keras_model
		metrics=['accuracy'])
	  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
		method(self, *args, **kwargs)
	  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 410, in compile
		'a %s' % type(optimizer))
	ValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'str'>"
25323,Fail to build from source with gcc 7.3.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 run as image under Singularity and/or Docker on Scientific Linux CERN SLC release 6.10 (Carbon)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: master, on commit 8eb3cbcb423c4d840e88a910c5db47404207b8a4
- Python version: 2.7
- Installed using virtualenv? pip? conda?: manually compiling using bazel
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): GCC 7.3.1
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the problem**
When compiling with gcc 7.3.1 rev 257125 (from gcc.gnu.org/svn) build chokes like this 
```
ERROR: /tensorflow/tensorflow/core/kernels/BUILD:3206:1: C++ compilation of rule '//tensorflow/core/kernels:reduction_ops' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:124:0,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/kernels/reduction_ops_common.h:27,
                 from tensorflow/core/kernels/reduction_ops_sum.cc:16:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; bool Tileable = false]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 0, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::IndexList<Eigen::type2index<0> >, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 1, 1, long int>, 16, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, false>::StorageIndex)>; _ArgTypes = {long int, long int}]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:801:9: internal compiler error: in emit_move_insn, at expr.c:3698
         values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,
         ^~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 74.923s, Critical Path: 51.44s
INFO: 2018 processes: 2018 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
exactly as described here:
https://github.com/sylabs/singularity/issues/2536
When we changed the 
gcc to 
8.2.0 tag 2d79333765b691fa27d82c1737cb2f00ec6a4499 (from https://github.com/gcc-mirror/gcc)
and to
7.4.0 rev 268351 (from gcc.gnu.org/svn)
the build went fine. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
export CXX_OPT_FLAGS=-std=c++11
BAZEL_OPTS=""--output_user_root ../build build -s --verbose_failures -c opt --cxxopt=${CXX_OPT_FLAGS}""
BAZEL_EXTRA_OPTS=""--action_env PYTHONPATH=${PYTHON27PATH} --distinct_host_configuration=false""
bazel $BAZEL_OPTS $BAZEL_EXTRA_OPTS //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25322,ImportError: No module named 'pandas',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip/binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.6
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

```
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 88, in <module>
    from tensorflow.python import keras
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/__init__.py"", line 29, in <module>
    from tensorflow.python.keras import datasets
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/datasets/__init__.py"", line 25, in <module>
    from tensorflow.python.keras.datasets import imdb
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/datasets/imdb.py"", line 25, in <module>
    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/preprocessing/__init__.py"", line 30, in <module>
    from tensorflow.python.keras.preprocessing import image
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/tensorflow/python/keras/preprocessing/image.py"", line 23, in <module>
    from keras_preprocessing import image
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/keras_preprocessing/image/__init__.py"", line 8, in <module>
    from .dataframe_iterator import DataFrameIterator
  File ""/home/travis/virtualenv/python3.5.6/lib/python3.5/site-packages/keras_preprocessing/image/dataframe_iterator.py"", line 11, in <module>
    from pandas.api.types import is_numeric_dtype
ImportError: No module named 'pandas'
```

**Describe the expected behavior**

`import tensorflow` should work.

**Code to reproduce the issue**

```
pip install tensorflow
```

**Other info / logs**

This is via Travis.
See [here](https://travis-ci.org/rwth-i6/returnn/jobs/486014809) for the full Travis log.

The TF package it installs is this:
```
$ pip install tensorflow
Collecting tensorflow
  Downloading https://files.pythonhosted.org/packages/b1/ad/48395de38c1e07bab85fc3bbec045e11ae49c02a4db0100463dd96031947/tensorflow-1.12.0-cp35-cp35m-manylinux1_x86_64.whl (83.1MB)
...
Installing collected packages: gast, protobuf, termcolor, astor, grpcio, werkzeug, markdown, tensorboard, keras-preprocessing, absl-py, keras-applications, tensorflow
Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.18.0 keras-applications-1.0.7 keras-preprocessing-1.0.6 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1
```

I guess the `pandas` dependency is just missing?
"
25320,tf.data.Dataset error in documentation,"**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip


**Describe the documentation issue**
I have tested the code provided in the documentation. Here the code : 

```
from tensorflow.data import Dataset # i have added my own


# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { 1, 2, 3 }
b = { 4, 5, 6 }
c = { (7, 8), (9, 10), (11, 12) }
d = { 13, 14 }

# The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
Dataset.zip((b, a)) == { (4, 1), (5, 2), (6, 3) }

# The `datasets` argument may contain an arbitrary number of
# datasets.
Dataset.zip((a, b, c)) == { (1, 4, (7, 8)),
                            (2, 5, (9, 10)),
                            (3, 6, (11, 12)) }

# The number of elements in the resulting dataset is the same as
# the size of the smallest dataset in `datasets`.
Dataset.zip((a, d)) == { (1, 13), (2, 14) }
```

Here the error provided by the system:
```
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    Dataset.zip((a, b)) == { (1, 4), (2, 5), (3, 6) }
  File ""/home/idolon/.virtualenvs/research/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 612, in zip
    return ZipDataset(datasets)
  File ""/home/idolon/.virtualenvs/research/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2037, in __init__
    raise TypeError(message)
TypeError: The argument to `Dataset.zip()` must be a nested structure of `Dataset` objects.
```

I don't really understand what is Dataset and how it works, there's no explicit examples of its functionalities. What can we do with it ? How to build a dataset, how it's used, and how to show data. We have only examples on official datasets but what about making our own datasets ? Maybe i've not found the correct documentation page because of my bad english mastering. 

So, is it an error of the documentation or just me that have not followed correctly the doc ?
Best regards. 
"
25319,CollectiveAllReduceStrategy,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
25317,Can't launch TF Detect Android sample with Yolov3-tiny model,"- OS Platform - Linux Ubuntu 16.04;
- Tools: Android Studio 3.3 (Android Studio 3.3 Build #AI-182.5107.16.33.5199772)
- Mobile device Huawei P20 lite, Android Emulator (Nexus 5X, API 28)
- use the TensorFlow from Jcenter
- TensorFlow android version 1.12.0
- Python version: Python 3.6.8

I converted Yolov3-tiny model (downloaded from https://pjreddie.com/darknet/yolo/) with [DW2TF](https://github.com/jinyu121/DW2TF). Then I put .pb file to assets and changed MODE == DetectorMode.YOLO in DetectorActivity.java.

When I start TF Detect app I have a crash:
```E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 4643
    java.lang.IllegalArgumentException: No Operation named [input] in the Graph
        at org.tensorflow.Session$Runner.operationByName(Session.java:372)
        at org.tensorflow.Session$Runner.feed(Session.java:142)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.addFeed(TensorFlowInferenceInterface.java:577)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.feed(TensorFlowInferenceInterface.java:318)
        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:159)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```
I added logs and i have params `inputName` and `inputSize`. Can you help me with this issue?
"
25316,Tensorflow lite logs do not get enabled until any LOG(FATAL) command is not encountered,"Tensorflow version 1.13.0 rc0 and 1.12.0 cpu version(cuda disable) for Linux ubuntu 16.0

I am unable to generate any logs on console using toco tool of tensorflow.
_ use export TF_CPP_MIN_LOG_LEVEL=1  , the info logs do not start to appear on console.

Until I any error case while using toco is not hit in the source code i,e when LOG(FATAL)  is encountered then all debug logs appear on console.

Let me know how to enable all the logs"
25315,Error importing tensorflow.,"I installed tensorflow using `python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl`, but importing tensorflow gives me:
```
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ayush99/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ayush99/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ayush99/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ayush99/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/ayush99/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Python: 3.7
Linux(Ubuntu), trying to install tensorflow for CPU."
25314,label_image segmentation issue for operator code 25 which is softmax,"When using the standard label_image to test the mobilenet tflite file in the default P OS android NN.
we are facing the segmentation issue for the softmax operation with the default command as specified in the readme.md.

We are using latest Android NN on P Os, for Arm64-v8a series soc.

The following is the crash snippet:
```
01-24 15:28:12.701 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)
01-24 15:28:12.701 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:12.702 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0
01-24 15:28:12.702 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)
01-24 15:28:12.702 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)
01-24 15:28:12.707 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})
01-24 15:28:12.707 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo
01-24 15:28:12.763 D/SDHMS:ComponentLevelSetter(22281): mWifiTotalUsage = 0, throughput = 0.0
01-24 15:28:12.928 I/CpuExecutor( 4787): Completed run normally
01-24 15:28:12.932 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:12.932 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1
01-24 15:28:12.934 I/ExecutionBuilder( 4787): ExecutionBuilder::ExecutionBuilder
01-24 15:28:12.936 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)
01-24 15:28:12.936 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:12.936 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0
01-24 15:28:12.936 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)
01-24 15:28:12.936 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)
01-24 15:28:12.940 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})
01-24 15:28:12.941 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo
01-24 15:28:13.295 I/UiThreadMonitor(20720): setAwake 103 5001
01-24 15:28:13.301 I/CpuExecutor( 4787): Completed run normally
01-24 15:28:13.305 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:13.305 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1
01-24 15:28:13.306 I/ExecutionBuilder( 4787): ExecutionBuilder::ExecutionBuilder
01-24 15:28:13.307 I/ExecutionBuilder( 4787): ExecutionBuilder::startCompute (from plan, iteratively)
01-24 15:28:13.307 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:13.307 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 0
01-24 15:28:13.307 I/ExecutionBuilder( 4787): input[0] = POINTER(0x7bfcf4ac00)
01-24 15:28:13.307 I/ExecutionBuilder( 4787): output[0] = POINTER(0x7bfce00fc0)
01-24 15:28:13.308 I/CpuExecutor( 4787): CpuExecutor::run() with request({.inputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 0, .offset = 0, .length = 602112}, .dimensions = [4]{1, 224, 224, 3}}}, .outputs = [1]{{.hasNoValue = 0, .location = {.poolIndex = 1, .offset = 0, .length = 4004}, .dimensions = [2]{1, 1001}}}, .pools = [0]{}})
01-24 15:28:13.308 I/CpuExecutor( 4787): CpuExecutor::initializeRunTimeInfo
01-24 15:28:13.547 I/CpuExecutor( 4787): Completed run normally
01-24 15:28:13.551 I/ExecutionBuilder( 4787): looking for next StepExecutor
01-24 15:28:13.551 I/ExecutionPlan( 4787): ExecutionPlan::next(0x7c00830460, 0x7bfe80c308): mNextStepIndex = 1
--------- beginning of crash
01-24 15:28:13.554 F/libc    ( 4787): Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x3d38 in tid 4787 (label_image), pid 4787 (label_image)
01-24 15:28:13.636 I/crash_dump64( 5086): obtaining output fd from tombstoned, type: kDebuggerdTombstone
01-24 15:28:13.639 I//system/bin/tombstoned( 5616): received crash request for pid 4787
01-24 15:28:13.641 I/crash_dump64( 5086): performing dump of process 4787 (target tid = 4787)
01-24 15:28:13.644 F/DEBUG   ( 5086): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-24 15:28:13.645 F/DEBUG   ( 5086): Build fingerprint: 'samsung/beyond2ltexx/beyond2:9/PPR1.180610.011/G975FXXE1ASAM:eng/test-keys'
01-24 15:28:13.645 F/DEBUG   ( 5086): Revision: '20'
01-24 15:28:13.645 F/DEBUG   ( 5086): ABI: 'arm64'
01-24 15:28:13.645 F/DEBUG   ( 5086): pid: 4787, tid: 4787, name: label_image  >>> ./label_image <<<
01-24 15:28:13.645 F/DEBUG   ( 5086): signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x3d38
01-24 15:28:13.646 F/DEBUG   ( 5086):     x0  0000005dc122ebf8  x1  0000000000000000  x2  0000000000000000  x3  0000007c00ef33c8
01-24 15:28:13.646 F/DEBUG   ( 5086):     x4  0000007fd73faad8  x5  0000000000000020  x6  0000007fd73faaa8  x7  0000000000000010
01-24 15:28:13.646 F/DEBUG   ( 5086):     x8  0000000000003d38  x9  385838b172084ce5  x10 00000000000012b3  x11 00000000000012b3
01-24 15:28:13.646 F/DEBUG   ( 5086):     x12 0000000000000003  x13 00000000ffffffff  x14 0000007fd73fa6a4  x15 0000007fd73fa23c
01-24 15:28:13.646 F/DEBUG   ( 5086):     x16 0000007c00ef11b0  x17 0000007c00e87120  x18 0000007fd73fa23a  x19 0000007c0082d2c0
01-24 15:28:13.646 F/DEBUG   ( 5086):     x20 0000005dc122ebf8  x21 0000007c0082b0b0  x22 0000007c0082b0a0  x23 0000005dc11cf6ff
01-24 15:28:13.647 F/DEBUG   ( 5086):     x24 0000005dc11dbf2a  x25 0000005dc11cf78e  x26 000000000000028d  x27 0000007c00831480
01-24 15:28:13.647 F/DEBUG   ( 5086):     x28 0000007c008314a8  x29 0000000000000018
01-24 15:28:13.647 F/DEBUG   ( 5086):     sp  0000007fd73fab90  lr  0000005dc105c084  pc  0000005dc105c090
01-24 15:28:13.648 I/unwind  ( 5086): Malformed section header found, ignoring...
01-24 15:28:13.651 F/DEBUG   ( 5086): 
01-24 15:28:13.651 F/DEBUG   ( 5086): backtrace:
01-24 15:28:13.651 F/DEBUG   ( 5086):     #00 pc 0000000000017090  /data/local/tmp/androidnn/label_image
01-24 15:28:13.651 F/DEBUG   ( 5086):     #01 pc 00000000000179e0  /data/local/tmp/androidnn/label_image
01-24 15:28:13.651 F/DEBUG   ( 5086):     #02 pc 0000000000017a88  /data/local/tmp/androidnn/label_image
01-24 15:28:13.652 F/DEBUG   ( 5086):     #03 pc 00000000000aeeb4  /system/lib64/libc.so (__libc_init+88)
01-24 15:28:14.039 I/crash_dump64( 5086): Start dumpstate
01-24 15:28:14.054 W/NativeCrashListener( 5488): Couldn't find ProcessRecord for pid 4787
01-24 15:28:14.057 E//system/bin/tombstoned( 5616): Tombstone written to: /data/tombstones/tombstone_01
01-24 15:28:14.062 E/audit   ( 5261): type=1701 audit(1548323894.051:192): auid=4294967295 uid=0 gid=0 ses=4294967295 subj=u:r:su:s0 pid=4787 comm=""label_image"" exe=""/data/local/tmp/androidnn/label_image"" sig=11 res=1
01-24 15:28:14.071 I/BootReceiver( 5488): Copying /data/tombstones/tombstone_01 to DropBox (SYSTEM_TOMBSTONE)
01-24 15:28:14.117 I/HqmInfo::LogAnalyzer( 5488): ContextBroadcastReceiver : received DropBoxManager ""SYSTEM_TOMBSTONE"" event
01-24 15:28:14.118 I/HqmInfo::LogAnalyzer( 5488): MSG_TYPE:MSG_APP_CRASH_CHECK_REQ
01-24 15:28:14.120 W/BroadcastQueue( 5488): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.stats.service.DropBoxEntryAddedReceiver
01-24 15:28:14.121 I/HqmInfo::AppCrashAnalyzer( 5488): checkAppError: list is null
01-24 15:28:14.123 D/dumpstate( 5093): Loading stats from /data/log/dumpstate-stats.txt
01-24 15:28:14.124 I/dumpstate( 5093): Average max progress: 1492 in 1 runs; estimated max: 1492
01-24 15:28:14.132 I/dumpstate( 5093): begin
01-24 15:28:14.133 I/dumpstate( 5093): dumpstate info: id=2, args='/system/bin/dumpstate -k -z', extra_options= app_native)
01-24 15:28:14.133 I/dumpstate( 5093): bugreport format version: 2.0
01-24 15:28:14.134 D/dumpstate( 5093): Bugreport dir: /data/log
01-24 15:28:14.134 D/dumpstate( 5093): Base name: dumpstate_app_native
01-24 15:28:14.134 D/dumpstate( 5093): Suffix: 2019-01-24-15-28-14
01-24 15:28:14.134 D/dumpstate( 5093): Log path: /data/log/dumpstate_app_native-2019-01-24-15-28-14-dumpstate_log-5093.txt
01-24 15:28:14.134 D/dumpstate( 5093): Temporary path: /data/log/dumpstate_app_native-2019-01-24-15-28-14.tmp
01-24 15:28:14.134 D/dumpstate( 5093): Screenshot path: 
01-24 15:28:14.134 D/dumpstate( 5093): Creating initial .zip file (/data/log/dumpstate_app_native-2019-01-24-15-28-14.zip)
01-24 15:28:14.135 D/dumpstate( 5093): Adding zip text entry version.txt
01-24 15:28:14.136 I/dumpstate( 5093): Vibrate: 'cmd vibrator vibrate 150 dumpstate'
01-24 15:28:14.143 I/ActivityManager( 5488): Killing 3567:com.samsung.android.bixby.agent/5018 (adj 906): empty #31
01-24 15:28:14.146 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.154 W/BroadcastQueue( 5488): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.chimera.GmsIntentOperationService$PersistentTrustedReceiver
01-24 15:28:14.196 I/chatty  ( 5488): uid=1000(system) ActivityManager identical 5 lines
01-24 15:28:14.202 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.206 V/VibratorService( 5488): vibrate - package: dumpstate, token: com.android.server.VibratorService@62e6ba4, usage: 0, effect: OneShot{mDuration=150, mAmplitude=-1, mMagnitude=-1, mMagnitudeType=TYPE_EXTRA}, mag: 10000, TYPE_EXTRA
01-24 15:28:14.206 D/VibratorService( 5488): Turning vibrator off
01-24 15:28:14.207 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:0
01-24 15:28:14.211 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.213 D/VibratorService( 5488): vibratorOn() : 150ms, amplitude :-1, mag :10000, f : 0
01-24 15:28:14.215 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/multi_freq val:0
01-24 15:28:14.217 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/intensity val:10000
01-24 15:28:14.217 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:150
01-24 15:28:14.219 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.293 I/chatty  ( 5488): uid=1000(system) ActivityManager identical 9 lines
01-24 15:28:14.301 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.305 V/[DMS]SemDesktopModeStateNotifier( 5488): binderDied(): DesktopModeListenerInfo(name=com.samsung.android.bixby.agent.app.BixbyApplication$$Lambda$1@bfd57fd, pid=3567, uid=5018)
01-24 15:28:14.307 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.309 D/ForegroundUtils(21076): Process died; UID 5018 PID 3567
01-24 15:28:14.309 D/ForegroundUtils(21076): could not check pending caller
01-24 15:28:14.309 D/ForegroundUtils(21076): Foreground changed, PID: 3567 UID: 5018 foreground: false
01-24 15:28:14.309 D/ForegroundUtils(21076): Foreground UID/PID combinations:
01-24 15:28:14.310 D/ForegroundUtils(21076): UID: 10110 PID: 23150
01-24 15:28:14.323 W/libprocessgroup( 5488): kill(-3567, 9) failed: No such process
01-24 15:28:14.323 I/libprocessgroup( 5488): Successfully killed process cgroup uid 5018 pid 3567 in 177ms
01-24 15:28:14.328 I/Zygote  ( 5267): Process 3567 exited due to signal (9)
01-24 15:28:14.369 D/VibratorService( 5488): Turning vibrator off
01-24 15:28:14.370 D/SecVibrator-HAL( 5296): writeNode node:/sys/class/timed_output/vibrator/enable val:0
01-24 15:28:14.376 D/[a11y]  ( 5488): getUserAccounts0
01-24 15:28:14.379 I/chatty  ( 5488): uid=1000(system) Binder:5488_11 identical 1 line
01-24 15:28:14.381 D/[a11y]  ( 5488): getUserAccounts0
01-24 15:28:14.401 E/dumpstate( 5093): can't find the pid
01-24 15:28:14.402 E/dumpstate( 5093): Failed to find: /data/misc/anrd/
01-24 15:28:14.402 D/dumpstate( 5093): Skipping systrace because '/sys/kernel/debug/tracing/tracing_on' content is '0'
01-24 15:28:14.402 D/dumpstate( 5093): /data/misc/raft does not exist or is not a directory
01-24 15:28:14.403 D/dumpstate( 5093): Adding dir /cache/recovery (recursive: 1)
01-24 15:28:14.406 D/[a11y]  ( 5488): getUserAccounts0
01-24 15:28:14.649 D/dumpstate( 5093): Duration of '/cache/recovery': 0.246s
01-24 15:28:14.649 D/dumpstate( 5093): Adding dir /data/misc/recovery (recursive: 1)
01-24 15:28:14.650 D/dumpstate( 5093): Duration of '/data/misc/recovery': 0.001s
01-24 15:28:14.650 D/dumpstate( 5093): Adding dir /data/misc/update_engine_log (recursive: 1)
01-24 15:28:14.650 D/dumpstate( 5093): Duration of '/data/misc/update_engine_log': 0.000s
01-24 15:28:14.650 D/dumpstate( 5093): Adding dir /data/misc/logd (recursive: 0)
01-24 15:28:14.651 D/dumpstate( 5093): Duration of '/data/misc/logd': 0.000s
01-24 15:28:14.651 D/dumpstate( 5093): Adding dir /data/misc/profiles/cur (recursive: 1)
01-24 15:28:14.765 D/dumpstate( 5093): Duration of '/data/misc/profiles/cur': 0.114s
01-24 15:28:14.765 D/dumpstate( 5093): Adding dir /data/misc/profiles/ref (recursive: 1)
01-24 15:28:14.815 D/dumpstate( 5093): Duration of '/data/misc/profiles/ref': 0.050s
01-24 15:28:14.858 D/dumpstate( 5093): Adding zip text entry systemserver_fileinfo.txt
01-24 15:28:14.858 D/dumpstate( 5093): Duration of 'FILE INFO': 0.043s  ```

The default AI Benchmark successfully runs all the models which definitely has softmax operation also.
Is there any known reason for softmax failure via lable_image.

"
25313,FastParseExample hash collision,"**System information**
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): GCC4.8.5

**Describe the current behavior**
build config index retry always failed.
code in `tensorflow/core/util/example_proto_fast_parsing.cc` function `FastParseExample`
[https://github.com/tensorflow/tensorflow/blob/a93f06d160955c99bc279a419902ac40824a2cab/tensorflow/core/util/example_proto_fast_parsing.cc#L988](url)
ok flag is not reset to true. And this will fail for 1000 times, if hash collision is found at first time.
**Describe the expected behavior**
 reset ok flag and rehash with new seed, so that retry logic will work

"
25312,Tensorflow ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.6, but also tested on Centos 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 2.6
- Installed using virtualenv? pip? conda?: manually compiling using bazel
- Bazel version (if compiling from source): 1.16.1, but also tried 1.15.0 and 1.18.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: Cuda 9 CuDNN 7
- GPU model and memory: Compute Cluster: multpile Tesla K80 (24GB) or Tesla P100 (16GB).

**Describe the problem**
During compilation I'm encountering the following ERROR messages:
[205 / 208] Compiling tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.cc; 10s local
ERROR: /tmp/Tensorflow/PACKAGES/tensorflow/tensorflow/contrib/image/BUILD:115:1: undeclared inclusion(s) in rule '//tensorflow/contrib/image:python/ops/_distort_image_ops_gpu':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.cc':
  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/Core'
  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h'
  '/tmp/bazel/userid_output/external/eigen_archive/Eigen/src/Core/util/ReenableStupidWarnings.h
However, when I add ---verbose_failures to bazel and execute the failing compilation job manually to debug the issue, the compilation is executed correctly. So I'm assuming only some check of bazel is failing, although the compilation itself could be executed correctly.

Restarting the build (after my manually compilation) will also continue, until the next job where Eigen is used again. The build is failing with a similar error. Previous versions, including 1.11.0, are not encountering this issue. I could not find any noticeable changes in the Eigen build files for bazel or related to Eigen. Therefore I'm currently out of ideas.....

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel --output_base=/tmp/bazel/userid_output build --jobs 12 -c opt --copt=-mavx2 --copt=-O --copt=-msse4.2 --copt=-mfma -config=cuda //tensorflow/tools/pip_package:build_pip_package
(I'm using bazel output_base because my home directory is on NFS)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25311,tflite_convert makes no difference to converted model after changing detection_postprocess.cc,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (or github SHA if from source): **r1.12**


**Describe the issue**
I am converting SSD object detection model trained through object detection api. The only difference in my custom model and other pre-trained model is that the **detection_boxes** have 6 output coords instead of 4 box coords, I am also predicting the center coordinates of the bounding box to make the model more robust. It is performing great in frozen_graph.pb but as I convert it into .tflite format by using tflite_convert, it only gives 4 box coords whereas I need 6 box coords. But, eventually I figured out that I need to change the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc), but changes made in this file are not effecting the model, it seems like tflite_convert is ignoring this file while converting the model.

**Steps to reproduce the issue**
- tensorflow build from source and installed in a virtualenv
- make some changes in the [detection_postprocess.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc)
- uninstall tensorflow from virtualenv
- again build tensorflow using bazel and install it using pip in same virtualenv
- run following command to convert model:
`tflite_convert \
  --output_file=detect.tflite \
  --graph_def_file=tflite_graph.pb \
  --input_shapes=1,300,300,3 \
  --input_arrays=normalized_input_image_tensor \
  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
  --mean_values=128 \
  --std_dev_values=128 \
  --change_concat_input_ranges=false \
  --allow_custom_ops` 

**More information**
- tflite_graph.pb is obtained by using [export_tflite_ssd_graph.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py)


How to make sure that changes made in detection_postprocess.cc will effect the converted model using tflite_convert?"
25310,"i use tf.data.TFRecordDataset read tfcord , why the data i read is not right","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
tensorrflow 1.12
tensorboard 1.12
windows7 cpu

**Describe the current **
i use tf.data.TFRecordDataset read tfcord , why the data i read is not right

**Describe the expected behavior**
The label and image is Corresponding

**Code to reproduce the issue**
https://stackoverflow.com/questions/54424335/i-use-tf-data-tfrecorddataset-read-tfcord-why-the-data-i-read-is-not-right

"
25308,[tf.keras 1.8.0] the size of loaded model increases after calling model.fit(),"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.2
- **GPU model and memory**: GTX 1080 Ti

### Describe the problem
I'm using tf.keras. I'm working with Densenet169 from tensorflow.python.keras.applications.densenet. I checked the model size (almost 53 MB) on Keras website, and obviously the size could be verified by saving it directly (using save_model()) on my PC before training. When calling mode.fit(), I found the size of model increased up to 153 MB without any changes of the number of parameter, data type and model structure (even for 1 epoch). Do you have any idea?

### The list of what I tested
- Comparison model.get_config() before and after training -> no difference
- The possibility that a list variable for validation set was not set free in model.fit() function -> no difference whether validation set exists or not
- Data type for the weights -> fixed as float32
- Callback functions (checkpoint and earlystopping) -> no effect
- model.compile() was called for several times to drop learning rate after a few epochs -> no relationship between model.compile() and model size  "
25306,[Docs] Do not link to symbols within code blocks,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12, 1.13, 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/TFRecordDataset#map


**Describe the documentation issue**
The [documentation's symbol resolver](https://github.com/tensorflow/docs/blob/58e99a76ebd4b533fed5591147441ee3f6db3dae/tools/tensorflow_docs/api_generator/parser.py#L119) will automatically replace all appearances of \`tf.symbol\` with markdown links to the symbol's definition, which is generally great, however when these symbols appear 
within code blocks (e.g. in [usage examples](https://github.com/tensorflow/tensorflow/blob/09696450acc1a69dcac4717de87ee4f30544865a/tensorflow/python/data/ops/dataset_ops.py#L912)) the inserted link cannot be rendered correctly, and is interpreted as a plain string like `<a href=""../../tf/Tensor""><code>tf.Tensor</code></a>`. Thus, it would be best if symbols within code blocks are not replaced by links, or by links that do render correctly (AFAIK links cannot be placed within code blocks). 

Note: I have only observed this behavior in comments within code blocks.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes, I could submit a PR. However I'm unsure as to the best strategy to efficiently determine if the symbol lies within a code block. I imagine that one could split the strings by re matching ` ``` ` and then deciding whether or not to `re.sub` the current section of the docstring, but this much string manipulation might be somewhat costly. "
25305,how to enable xla with estimator,"cuda:9.0
tensorflow:1.12.0

i want to enable xla, it is my code:
```
session_config = tf.ConfigProto()
session_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
distribution_strategy = distribution_utils.get_distribution_strategy(flags_core.get_num_gpus(flags_obj), flags_obj.all_reduce_alg)
run_config = tf.estimator.RunConfig(
      train_distribute=distribution_strategy,
      session_config=session_config,
      save_checkpoints_secs=60*60*24)
classifier = tf.estimator.Estimator(
      model_fn=model_function, model_dir=flags_obj.model_dir, config=run_config,
      warm_start_from=warm_start_settings, params={
          'resnet_size': int(flags_obj.resnet_size),
          'data_format': flags_obj.data_format,
          'batch_size': flags_obj.batch_size,
          'resnet_version': int(flags_obj.resnet_version),
          'loss_scale': flags_core.get_loss_scale(flags_obj),
          'dtype': flags_core.get_tf_dtype(flags_obj),
          'fine_tune': flags_obj.fine_tune
      })
......
classifier.train(input_fn=lambda: input_fn_train(num_train_epochs),
                       hooks=train_hooks, max_steps=flags_obj.max_train_steps)
```
and get_distribution_strategy() is 
```
def get_distribution_strategy(num_gpus,all_reduce_alg=None,turn_off_distribution_strategy=False):
  if num_gpus == 1:
      return tf.contrib.distribute.OneDeviceStrategy(""device:GPU:0"")
  else:  # num_gpus > 1 and not turn_off_distribution_strategy
      return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus,
          cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(
              all_reduce_alg, num_packs=2))
```

when i train by one gpu, and the distribute strategy is OneDeviceStrategy(""device:GPU:0""), xla is ok,
but when i train by two gpus, and the distribute strategy is MirroredStrategy, xla has error:
```
 *** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
I0130 10:55:37.183267 140192737777408 tf_logging.py:115] Running local_init_op.
I0130 10:55:37.853480 140192737777408 tf_logging.py:115] Done running local_init_op.
I0130 10:56:25.948692 140192737777408 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.
2019-01-30 10:57:17.807557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-01-30 10:57:17.807765: E tensorflow/compiler/xla/service/gpu/cudnn_convolution_algorithm_picker.cc:338] Internal: All algorithms tried for convolution %custom-call = (f32[64,256,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,56,56]{3,2,1,0} %reduce-window.13518.10568, f32[1,1,64,256]{1,0,2,3} %copy.56), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target=""__cudnn$convForward"", backend_config=""{\""convResultScale\"":1}"" failed.  Falling back to default algorithm.
```

or

```
*** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.
I0130 11:05:42.691364 140448661317376 tf_logging.py:115] Running local_init_op.
I0130 11:05:43.474775 140448661317376 tf_logging.py:115] Done running local_init_op.
I0130 11:06:26.442031 140448661317376 tf_logging.py:115] Saving checkpoints for 20020 into /tmp/model.ckpt.
I0130 11:07:53.810114 140448661317376 tf_logging.py:115] cross_entropy = 2.8381975, learning_rate = 0.02560256, train_accuracy = 0.390625
I0130 11:07:53.812137 140448661317376 tf_logging.py:115] loss = 6.150483, step = 20020

E0130 11:09:08.093458 140448661317376 tf_logging.py:105] Model diverged with loss = NaN.

```



"
25304,Failed to build tensorflow pip package from source in dockerfile provided under tools folder,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Docker image nvidia/cuda:9.0-base-ubuntu16.04 provided by tensorflow/tensorflow/tools/dockerfiles/dockerfiles/nvidia-devel.Dockerfile

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
NA

- TensorFlow version:
v1.12.0

- Python version:
3.5

- Installed using virtualenv? pip? conda?:
NONE

- Bazel version (if compiling from source):
20.0

- GCC/Compiler version (if compiling from source):
gcc 5.4

- CUDA/cuDNN version:
CUDA 9

- GPU model and memory:
1050ti



**Describe the problem**
Build failed in Bazel.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Checkout tensorflow release tag v.1.12 commit a6d8ffae097d0132989ae4688d224121ec6d8f35.
1. Build image `docker build -f ./dockerfiles/nvidia-devel.Dockerfile --build-arg USE_PYTHON_3_NOT_2=True -t tf-nvidia-devel-python3 .`
1. Run container `docker run --runtime=nvidia  -v /home/Ricky/repo/github/tensorflow:/tensorflow -it tf-nvidia-devel-python3`
1. Run `\.configure` as a sane person, eg enable CUDA and etc.
1. Run ` bazel build -k //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
root@252b84b2aa65:/tensorflow# bazel build -k //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
/tensorflow/tools/bazel.rc
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=130
INFO: Reading rc options for 'build' from /tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages --python_path=/usr/bin/python3 --define with_ignite_support=true --define with_xla_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_VERSION=9.0 --action_env CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env TF_CUDNN_VERSION=7 --action_env TENSORRT_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env TF_TENSORRT_VERSION=4.1.2 --action_env NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu --action_env NCCL_HDR_PATH=/usr/include --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env TF_CUDA_CLANG=0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda
ERROR: Config value cuda is not defined in any .rc file
INFO: Invocation ID: ef2903c5-eec1-4d50-8f50-cb88cfc320e1
```

Please don't take it personally. But you guys write a very very very bad dockerfile which doesn't handle the build tools dependency carefully. I found that bazel is 0.22!

```
root@effd8d74a22c:/# bazel version
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: f740cfea-bba1-4a08-b04c-c41c4cd39a8b
Build label: 0.22.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Mon Jan 28 12:58:08 2019 (1548680288)
Build timestamp: 1548680288
Build timestamp as int: 1548680288
```"
25303,How can I pass a parameter for ever examples in a tensor?,"I have a many sequence sentence with different lengthï¼Œand I padded all sentence with same length 100 ï¼ŒI  got a input tensor with shape:  (1000, 100, 10) ï¼Œbut I want pass the real valid length of every length information together with input tensor, and use the parameter to do some calculates, how can I suppose to do?

I have a one way that is create a new input with shape(1000,100,1)ï¼Œpadded with all valid length of every examples. But when I want use the parameter as a indice to do the slice of the input tensor, how can I convert a tensor to a number to do,say : tensor[: ,slice,: ]?"
25301,TensorFLow v1.13.0rc0 tflite_convert fails to convert TensorFlow model with FusedBatchNormalization layers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 26
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0rc0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia Tesla P100


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
tflite_convert fails to convert simple conv2d model with batch norm file to .tflite. The trained model was frozen to a .pb file using freeze_graph. It fails with the following errors:

tflite_convert --graph_def_file=frozen_saved_model.pb --output_file=frozen_saved_model.tflite --input_shapes=""1,28,28,1"" --input_arrays=conv2d_input --output_arrays=dense_1/Softmax

Traceback (most recent call last):
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 426, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/virtualenv/tf-1.13.0-gpu/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 122, in _convert_model
    converter = _get_toco_converter(flags)
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 109, in _get_toco_converter
    return converter_fn(**converter_kwargs)
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 274, in from_frozen_graph
    _import_graph_def(graph_def, name="""")
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 501, in new_func
    return func(*args, **kwargs)
  File ""/virtualenv/tf-1.13.0-gpu/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_v1/gamma:0 incompatible with expected resource.


**Describe the expected behavior**
A TensorFlow Lite version of the frozen model file should have been generated by the tool.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
import tensorflow.keras.optimizers as optimizers
import tensorflow.keras.losses as losses
from tensorflow.keras.datasets import mnist

(train_x, train_y), _ = mnist.load_data()

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(28,28,1),
                 kernel_regularizer=regularizers.l1(reg_weight)))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(reg_weight)))
model.add(Dense(10, activation='softmax', kernel_regularizer=regularizers.l1(reg_weight)))

model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(train_x, train_y, batch_size=64, epochs=2, verbose=1)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25299,Dictionary as model targets for subclassed Keras models,"It is currently possible to build subclassed Keras models with a dict of tensors as inputs, but not as outputs, e.g.,
```python
import tensorflow as tf

tf.enable_eager_execution()

class MyModel(tf.keras.Model):
  def call(self, inputs):
    return inputs
  
model = MyModel()
model.compile(tf.keras.optimizers.Adam(), 'mean_squared_error')
inputs = labels = {'a': tf.range(5.0)}
model.fit(x=inputs, y=labels)
```
This results in the error:
```python-stacktrace
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-21d4cae99b87> in <module>()
     10 model.compile(tf.keras.optimizers.Adam(), 'mean_squared_error')
     11 inputs = labels = {'a': tf.range(5.0)}
---> 12 model.fit(x=inputs, y=labels)

/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    900         steps=steps_per_epoch,
    901         validation_split=validation_split,
--> 902         shuffle=shuffle)
    903 
    904     # Prepare validation data.

/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2458           all_inputs += list(y_input)
   2459         elif isinstance(y_input, dict):
-> 2460           raise ValueError('You cannot pass a dictionary as model targets.')
   2461         else:
   2462           if (not isinstance(y_input, np.ndarray) and

ValueError: You cannot pass a dictionary as model targets.
```

I'd like to build Keras models that receive and output tensors as dictionaries -- would it be reasonable to add support for this? It currently works to specify inputs as a dict, but not labels for training moels.

**System information**
- TensorFlow version (you are using): development version
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**
No, this currently results in an error.

**Who will benefit with this feature?**
Users building complex models that want to keep track of multiple tensors by name instead of by position.

**Any Other info.**
"
25296,"TensorFlow build for raspberry pi, Python 3.6","Hello,

**System information**
- TensorFlow version (you are using): HEAD
- Are you willing to contribute it? Yes

**Describe the feature and the current behavior/state.**
Currently, [this page](https://www.tensorflow.org/install/source_rpi) provides instructions on how to build a TF pip package for raspberry pi. However, it limits Python 3 versions to `3.4`, which is not officially supported by some popular packages (including `pandas`), which makes using `2.7` the only reasonable solution.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Raspberry Pi developers

"
25295,Data augmentation on 4-D or 5-D data,"Hi, 

I would like to request a new feature.

**System information**
- TensorFlow version (you are using): 1.12, compiled with CUDA 10, NCCL, CuDNN7.
- Are you willing to contribute it (Yes/No): Maybe, but as a student, time is a real constraint.


**Describe the feature and the current behavior/state.**
I would like to have data augmentation functions that accept 4-D arrays with shape [width, height, depth, channels] or 5-D Tensor of shape [batch, width, height, depth, channels]. Current supported format is only 4-D [batch, height, width, channels] or 3-D Tensor of shape [height, width, channels]. 
Having functions like random rotation, random shear, and/or random flip for this data shape would be useful.

**Will this change the current api? How?** 
No.

**Who will benefit with this feature?** 
Any people that works like me in medical imaging and have to deal with 3D data.
"
25294,Image Retraining: maybe reference retrain.py?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
1.12
- Doc Link:
https://www.tensorflow.org/hub/tutorials/image_retraining

The Image Retraining document is nice because it talks about the behaviors you need around the retraining code. However, it doesn't go into much detail of how someone would produce that code to retrain the model. There's a 1.3K line `retain.py` linked from the tutorial takes a while to dig through. It might make things clearer if the tutorial referenced or displayed parts of `retrain.py` or something similar. 

For instance, other venues talk about ""removing the output layer"" in these models and it was unclear what that meant in the tensorflow context. The bottleneck discussion also mentioned ""before the output layer"". I was unsure if I'd have to take a Module's Graph and strip something off the end. I think showing the code involved might clear that up? 

Similarly, I think the bottleneck discussion might benefit from referencing the code that actually writes data down because, while it's probably obvious to others, I wasn't sure what all was going into those files. (I was also curious if there was some special ordering or collation being done.)"
25293,error: futures 3.0.5 is installed but futures>=3.1.1 is required by set(['tensorboard']),"Hello, I am using UBuntu 16.04 LTS. I have use this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L45

but when i run under:
/tensorflow/tensorflow/tools/pip_package$

sudo python setup.py install

its return error: futures 3.0.5 is installed but futures>=3.1.1 is required by set(['tensorboard'])
![screenshot from 2019-01-30 04-25-56](https://user-images.githubusercontent.com/1058663/51944781-272c3f80-2447-11e9-9be1-b50723d5a8b0.png)

pls help "
25292,"Error in configure.py, MPI_HOME","**System information**
- OS Platform and Distribution: Linux Ubuntu (4.15.0-44-generic)
- TensorFlow installed from: Source
- TensorFlow version: r1.13 (e7f2979fc7bbbd491a5c1db2268d4ee67cc46f88)
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source) : gcc-7 (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.4.2.24-1+cuda10.0
- GPU model and memory: K1100M



**Describe the problem**
If you specify a location for the MPI Toolkit, or use the default location the `configure.py` script will throw and error instead of printing a help message.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
#!/bin/bash
git clone git@github.com:tensorflow/tensorflow.git
cd tensorflow
git checkout r1.13
PYTHON_BIN=""${HOME}/anaconda3/envs/tf_dev_3.5/bin/python""
export PYTHON_BIN_PATH=""${PYTHON_BIN}""
export PYTHON_LIB_PATH=""${HOME}/anaconda3/envs/tf_dev_3.5/lib/python3.5/site-packages""
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_ROCM=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""10.0""
export CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.0""
export TF_CUDNN_VERSION=""7""
export CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0""
export TF_NEED_TENSORRT=1
export TENSORRT_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
export TF_NCCL_VERSION=""2.3"" # 2.3.7
export TF_CUDA_COMPUTE_CAPABILITIES=""3.0""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-7"" #""/usr/bin/gcc"" #""$(which gcc)""

export TF_NEED_MPI=1
./configure
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: c5371cf2-ec36-4f56-b7f9-c2355305c07e
You have bazel 0.21.0 installed.
NCCL libraries found in /usr/lib/x86_64-linux-gnu/libnccl.so
This looks like a system path.
Assuming NCCL header path is /usr/include

Please specify the MPI toolkit folder. [Default is /usr]: 


Traceback (most recent call last):
  File ""./configure.py"", line 1701, in <module>
    main()
  File ""./configure.py"", line 1663, in main
    set_other_mpi_vars(environ_cp)
  File ""./configure.py"", line 1483, in set_other_mpi_vars
    mpi_home, mpi_home, mpi_home)
TypeError: not enough arguments for format string
```
"
25289,tf-nightly-2.0-preview installation issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Professinal 64-bit (10.0, Build 17763) (17763.rs5_release.180914-1434)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary (via Pypi, both origin host and mirror)
- TensorFlow version: 2.0-preview (CPU)
- Python version: 3.5&&3.7(could not find any available distribution) || 3.6(can't find `TensorContractionThreadPool.h`)
- Installed using virtualenv? pip? conda?:  pip 19.0.1 & conda 4.5.12
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: CUDA 9 && cudnn 7.3.1
- GPU model and memory: NVIDIA GeForce 940MX 2010 MB



**Describe the problem**
I'm a GDE in ML and taking part in TF 2.0 test plan. however, I could not install tf-nightly-2.0-preview on my laptop, even I installed eigen_archive separately or reinstall my OS.
```bash
C:\Users\kurileo
(tf2) Î» pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  Using cached https://files.pythonhosted.org/packages/e1/c6/1221e21c46031f2b71b688e575263c287e76284a0214b2395bc9498910ec/tf_nightly_2.0_preview-1.13.0.dev20190115-cp36-cp36m-win_amd64.whl
Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\kurileo\\AppData\\Local\\Temp\\pip-install-4nwb1mwi\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-1.13.0.dev20190115.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h'
```
in Python 3.5/3.7, pip could not find any available distribution:
```bash
C:\Users\kurileo
(tf2) Î» pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: )
No matching distribution found for tf-nightly-2.0-preview
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
In build 1213(or 1203... I've forgotten) tf-2.0-nightly-preview worked fine, but when I updated it, it crashed with log `Could not install packages due to an EnvironmentError` and pointed that `TensorContractionThreadPool.h` is missing.
I've once set a [pypi mirror](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/), but when I switched back

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

For the first issue, it may be caused by Win10 path limitation as #24835 && #24886 pointed.
But for the second one, I'm not sure if it's an issue."
25288,Models produced by SavedModel.simple_save can't be loaded with saved_model.loader,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows
- TensorFlow installed from (source or binary):
PIP
- TensorFlow version (use command below):
1.12.0/'v1.12.0-rc2-3-ga6d8ffae09'
- Python version:
3.6.2

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
TF.saved_model.simple_save can't be loaded by tf.saved_model_loader  - it states
```
RuntimeError: MetaGraphDef associated with tags serve could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`
```
**Describe the expected behavior**
a model created with tf.saved_model_simple_save can be loaded by using 
tf.saved_model.loader.load(sess, tf.saved_model.tag_constants.SERVING, ""./model"")

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy

learning_rate = 0.01
training_epochs = 2

# Training Data
train_X = numpy.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,
						 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])
train_Y = numpy.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,
						 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])
n_samples = train_X.shape[0]

with tf.Session() as sess:
	X = tf.placeholder(""float"")
	Y = tf.placeholder(""float"")

	W = tf.Variable(1.0, name=""weight"")
	b = tf.Variable(2.0, name=""bias"")

	pred = tf.add(tf.multiply(X, W), b)
	cost = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples)
	sess.run(tf.global_variables_initializer())

	optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
	for epoch in range(training_epochs):
		for (x, y) in zip(train_X, train_Y):
			sess.run(optimizer, feed_dict={X: x, Y: y})

	tf.saved_model.simple_save(sess, ""./model"", inputs={""x"": X, ""y"": Y},
							   outputs={""c"": pred})
							  
with tf.Session() as sess:
	tf.saved_model.loader.load(sess, tf.saved_model.tag_constants.SERVING, ""./model"")
```

**Other info / logs**
When I go to check the model with 'saved_model_cli' - show lists the tag 'serve':
```
saved_model_cli.py show --dir . --all

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['x'] tensor_info:
        dtype: DT_FLOAT
        shape: unknown_rank
        name: Placeholder:0
    inputs['y'] tensor_info:
        dtype: DT_FLOAT
        shape: unknown_rank
        name: Placeholder_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['c'] tensor_info:
        dtype: DT_FLOAT
        shape: unknown_rank
        name: Add:0
  Method name is: tensorflow/serving/predict
```

but when I try to run it, I encounter the same issue:
```
saved_model_cli.py run --dir . --tag_set 'serve' --signature_def 'serving_default' --input_exprs x=[1];y=[2]
Traceback (most recent call last):
  File ""C:\Users\coverste\AppData\Local\Continuum\anaconda3\Lib\site-packages\tensorflow\python\tools\saved_model_cli.py"", line 827, in <module>
    sys.exit(main())
  File ""C:\Users\coverste\AppData\Local\Continuum\anaconda3\Lib\site-packages\tensorflow\python\tools\saved_model_cli.py"", line 823, in main
    args.func(args)
  File ""C:\Users\coverste\AppData\Local\Continuum\anaconda3\Lib\site-packages\tensorflow\python\tools\saved_model_cli.py"", line 644, in run
    init_tpu=args.init_tpu, tf_debug=args.tf_debug)
  File ""C:\Users\coverste\AppData\Local\Continuum\anaconda3\Lib\site-packages\tensorflow\python\tools\saved_model_cli.py"", line 304, in run_saved_model_with_feed_dict
    tag_set)
  File ""C:\Users\coverste\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\tools\saved_model_utils.py"", line 49, in get_meta_graph_def
    ' could not be found in SavedModel')
RuntimeError: MetaGraphDef associated with tag-set 'serve' could not be found in SavedModel
```"
25285,Training with multiple datasets using the estimator,"
**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Not in the near future



**Describe the feature and the current behavior/state.**

I'll start with what i'm trying to achieve.
I want to train a model (a language model) with Multitask Learning and Transfer Learning.
I want to train with such techniques while using 3 different tasks.
Each task has it's own loss function, optimizer method training hooks, evaluation hooks and different data input.
I'm using the tf.data TFRecord dataset.
I've read a lot about the head API which seems to solve my problem just as described in this image:

![1_jcoaugzhqn3_7oyvu58t4q](https://user-images.githubusercontent.com/28672915/51889143-512b2680-23a1-11e9-8756-f03c3522adbd.png)

but my problem is the features - the input data, which changes according to what task i'm training on.
I want to be able to control the data input with a single call to estimator.train or estimator.eval

**Current Behavior**

Suppose the following:
| Tasks  | Task's Dataset |
|--------|----------------|
| Task A | Database A     |
| Task B | Database B     |
| Task C | Database C     |

and i want to train one task for x steps and then perform a ""context switch"" - switch to a different task and train it for `x` steps as well.
So I train Task A using `estimator.train(input_fn=Dataset_A, steps=x)` and then i train Task B using `estimator.train(input_fn=Dataset_B, steps=x)`

The problem starts when i want to train Task A again for x steps. Task A will be trained be first x records from it's input_fn (Dataset_A), this occurs since that estimator recreates the input_fn (and model_fn). I know the estimator does it this by design but this causes issues when performing Multitask learning while training with different datasets.

The tf.data api supports this kind of functionality. Using the [feedable](https://www.tensorflow.org/guide/datasets#creating_an_iterator) iterator, i can switch between datasets by invoking a different string handles of each dataset i got. **But how can i make the estimator to manage different string handles of multiple datasets?**
I would like to invoke estimator.train once and have a control over the flow of the data during the training (maybe be using session run hooks?)

**Will this change the current api? How?**

In order to support feedable iterators the estimator should be able to expected an input_fn that returns multiple tf.data.Dataset objects or iterators of those datasets.

**Who will benefit with this feature?**

Anyone that will want to train and experiment models using multiple datasets.
"
25284,TensorFlow1.11 estimator train_and_evaluate distributedï¼ˆMirroredStrategyï¼‰ï¼Œ the sum eval metric ï¼ˆfalse_negativesï¼Œfalse_positivesï¼Œtrue_negativesï¼Œ true_positives  ï¼‰at different GPUs is wrong,"The test set is 1000
when I use one GPUï¼Œthe sum of test set is true, 1000
INFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.762, eval_loss = 0.5891396, **false_negatives = 27.0, false_positives = 211.0**, global_step = 102, loss = 0.5891396, negative_f1_score = 0.715311, negative_precision = 0.9171779, negative_recall = 0.5862745, positive_f1_score = 0.7955326, positive_precision = 0.6869436, positive_recall = 0.94489795, **true_negatives = 299.0, true_positives = 463.0**

when I use two GPUï¼Œthe sum of test set is wrong, 992
INFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.68245965, eval_loss = 0.6352085, **false_negatives = 134.0, false_positives = 181.0**, global_step = 102, loss = 0.6352085, negative_f1_score = 0.6764509, negative_precision = 0.70869565, negative_recall = 0.64299804, positive_f1_score = 0.6902654, positive_precision = 0.6597744, positive_recall = 0.7237113, **true_negatives = 326.0, true_positives = 351.0**

when I use three GPUï¼Œthe sum of test set is wrong, 984
INFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.8231707, eval_loss = 0.40057757, **false_negatives = 99.0, false_positives = 75.0**, global_step = 102, loss = 0.40057757, negative_f1_score = 0.83139527, negative_precision = 0.8125, negative_recall = 0.85119045, positive_f1_score = 0.81410253, positive_precision = 0.8355263, positive_recall = 0.79375, **true_negatives = 429.0, true_positives = 381.0**"
25283,Tensorflow wheel install fails on s390x ,"**System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04 s390x
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): source
TensorFlow version: v1.12.0
Python version: 2.7.x
Installed using virtualenv? pip? conda?: Building from source
Bazel version (if compiling from source): v0.15.0
GCC/Compiler version (if compiling from source): 7.3.0 (Ubuntu 18.04), 5.4.0 (Ubuntu 16.04)
CUDA/cuDNN version: NA
GPU model and memory: NA



**Describe the problem**
We have build TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform. Observed below failure on installing wheel ( pip install tensorflow_wheel/tensorflow-1.12.0-cp27-cp27mu-linux_s390x.whl )
```
third_party/boringssl/include/openssl/base.h:118:2: error: #error ""Unknown target CPU""
     #error ""Unknown target CPU""
 CompileError: command 's390x-linux-gnu-gcc' failed with exit status 1
```

**Any other info / logs**
Looks like it initiates grpcio installation which is throwing an error on s390x.

"
25282,ImportError: cannot import name 'mock' durring import tensorflow,"**System information**
- CentOS Linux release 7.4.1708
- Source, hash: bf4767c6bafb077fc591107691199a6981c29304
- TensorFlow version: 1.12
- Python version: Python 3.4.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.19.2 (bazel from centos repos)
- GCC/Compiler version (if compiling from source): 4.8.5 (gcc from centos repos)
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the problem**

Cannot  import tensorflow due to  import 'mock' library from tensorflow.python.platform.googletest  failling.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Successfully built and installed TensorFlow with the following commands:

`bazel build --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false --config=opt //tensorflow/tools/pip_package:build_pip_package`

`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

`pip3.4 install --user /tmp/tensorflow_pkg/tensorflow-1.12.0-cp34-cp34m-linux_x86_64.whl`

> See pip log below for found dependencies

Attempting to import tensorflow i get this:

`Python 3.4.9 (default, Aug 14 2018, 21:28:57) `
`[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux`
`Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.`
`>>> import tensorflow`
`/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as 'np.float64 == np.dtype(float).type'.`
`  return f(*args, **kwds)`
`Traceback (most recent call last):`
`  File ""<stdin>"", line 1, in <module>`
`  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/__init__.py"", line 34, in <module>`
`    from tensorflow._api.v1 import compat`
`  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/__init__.py"", line 21, in <module>`
`    from tensorflow._api.v1.compat import v1`
`  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/v1/__init__.py"", line 69, in <module>`
`    from tensorflow._api.v1.compat.v1 import test`
`  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/v1/test/__init__.py"", line 24, in <module>`
`    from tensorflow.python.platform.googletest import mock`
`ImportError: cannot import name 'mock'`


**Any other info / logs**

pip install output:

`Processing /tmp/tensorflow_pkg/tensorflow-1.12.0-cp34-cp34m-linux_x86_64.whl`
`Requirement already satisfied: grpcio>=1.8.6 in /usr/lib64/python3.4/site-packages (from tensorflow==1.12.0) (1.10.0)`
`Requirement already satisfied: keras-preprocessing>=1.0.5 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.0.5)`
`Requirement already satisfied: six>=1.10.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.12.0)`
`Requirement already satisfied: absl-py>=0.1.6 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.1.11)`
`Requirement already satisfied: google-pasta>=0.1.1 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.1.1)`
`Requirement already satisfied: numpy>=1.13.3 in /usr/lib64/python3.4/site-packages (from tensorflow==1.12.0) (1.14.1)`
`Requirement already satisfied: protobuf>=3.6.1 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (3.6.1)`
`Requirement already satisfied: wheel>=0.26 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.32.3)`
`Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.12.2)`
`Requirement already satisfied: astor>=0.6.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.6.2)`
`Requirement already satisfied: keras-applications>=1.0.6 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.0.6)`
`Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.13.0rc0)`
`Requirement already satisfied: gast>=0.2.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.2.0)`
`Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.1.0)`
`Requirement already satisfied: setuptools in /home1/private/mavridis/.local/lib/python3.4/site-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.6.3)`
`Requirement already satisfied: werkzeug>=0.11.10 in /usr/lib64/python3.4/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (0.14.1)`
`Requirement already satisfied: markdown>=2.6.8 in /usr/lib64/python3.4/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (2.6.11)`
`Requirement already satisfied: h5py in /usr/lib64/python3.4/site-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.7.1)`
`Requirement already satisfied: mock>=2.0.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.12.0) (2.0.0)`
`Requirement already satisfied: pbr>=0.11 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.12.0) (5.1.1)`
`Installing collected packages: tensorflow`
`Successfully installed tensorflow-1.12.0`

"
25281,TensorFlow 2.0 Preview - TypeError: 'Attribute' object is not iterable when using tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, the code is attached below
- OS Platform and Distribution: Arch Linux
- TensorFlow installed from (source or binary): PyPI
- TensorFlow version (use command below): `tf-nightly-gpu-2.0-preview==2.0.0.dev20190129` AKA the latest nightly
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**
Running the `train()` procedure provided below breaks while using the `@tf.function` decorator.

**Describe the expected behavior**
Not encountering any errors as per the [""Effective TensorFlow 2.0 Guide""](https://github.com/tensorflow/docs/blob/7c9d49ee188c67a315deaf92ebd41fd0f3b15c4a/site/en/r2/guide/effective_tf2.md#combine-tfdatadatasets-and-tffunction)

**Code to reproduce the issue**
```python
""""""
Implement DCGAN using the new TF 2.0 API.

Also test tensorflow-datasets.

Celeb-A dataset.
""""""

from typing import Dict
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras as k


def bce(x: tf.Tensor, label: tf.Tensor, label_smoothing: float = 0.0) -> tf.Tensor:
    """"""Returns the discrete binary cross entropy between x and the discrete label
    Args:
        x: a 2D tensor
        label: the discrite label, aka, the distribution to match
        label_smoothing: if greater than zero, smooth the labels

    Returns:
        The binary cros entropy
    """"""
    # FIXME: Fix the warning
    # assert len(x.shape) == 2 and len(label.shape) == 0

    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)


def min_max(
    positive: tf.Tensor, negative: tf.Tensor, label_smoothing: float = 0.0
) -> tf.Tensor:
    """"""Returns the discriminator (min max) loss
    Args:
        positive: the discriminator output for the positive class: 2D tensor
        negative: the discriminator output for the negative class: 2D tensor
        smooth: if greater than zero, appiles one-sided label smoothing
    Returns:
        The sum of 2 BCE
    """"""

    one = tf.constant(1.0)
    zero = tf.constant(0.0)
    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)
    return d_loss


class Generator(k.Model):
    def __init__(self) -> None:
        super(Generator, self).__init__()
        self.fc1 = k.layers.Dense(4 * 4 * 1024)
        self.batchnorm1 = k.layers.BatchNormalization()

        self.conv2 = k.layers.Conv2DTranspose(
            filters=512,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization()

        self.conv3 = k.layers.Conv2DTranspose(
            filters=256,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm3 = k.layers.BatchNormalization()

        self.conv4 = k.layers.Conv2DTranspose(
            filters=128,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm4 = k.layers.BatchNormalization()

        self.conv5 = k.layers.Conv2DTranspose(
            filters=3,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding=""same"",
            use_bias=False,
        )
        self.batchnorm5 = k.layers.BatchNormalization()

    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:
        x = self.fc1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.relu(x)
        x = tf.reshape(x, shape=(-1, 4, 4, 1024))

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv5(x)
        x = self.batchnorm5(x, training=training)

        x = tf.nn.tanh(x)
        return x


class Discriminator(k.Model):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=""same"")
        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm2 = k.layers.BatchNormalization()
        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm3 = k.layers.BatchNormalization()
        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding=""same"")
        self.batchnorm4 = k.layers.BatchNormalization()
        self.flatten = k.layers.Flatten()
        self.fc5 = k.layers.Dense(1)

    def call(self, x, training=True):
        x = self.conv1(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x)
        x = tf.nn.leaky_relu(x)

        x = self.flatten(x)
        x = self.fc5(x)
        return x


class GAN:
    def __init__(self, generator, discriminator, encoder=None):
        """"""
        GAN initializer.

        Args:
            generator: A ``tensorflow.keras.Model`` to use as Generator.
            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.
            encoder: A ``tensorflow.keras.Model`` to use as Encoder.

        Returns:
            Trained GAN model (?).

        """"""
        self.G = generator()
        self.D = discriminator()
        self.E = encoder() if encoder is not None else None
        self.latent_vector_dims = 100

        self.G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)
        self.D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)

    @tf.function()
    def train(self, dataset: tf.data.Dataset):
        """"""
        Train.
        """"""
        for step, features in enumerate(dataset, start=1):
            x = features[""image""]
            z = tf.random.normal((x.shape[0], self.latent_vector_dims))

            # We record all the operations in the tape
            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                G_z = self.G(z, training=True)

                D_x = self.D(x, training=True)
                D_Gz = self.D(G_z, training=True)

                g_loss = bce(D_Gz, tf.constant(1.0))
                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)

            # We retrieve the gradients from our records
            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)
            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)

            # Optimize and apply the gradients
            self.G_opt.apply_gradients(zip(G_grads, self.G.trainable_variables))
            self.D_opt.apply_gradients(zip(D_grads, self.D.trainable_variables))

            if step % 10 == 0:
                print(f""--------------------------"")
                print(f""STEP: {step}"")
                print(f""D_LOSS: {d_loss}"")
                print(f""G_LOSS: {g_loss}"")


class InputPipeline:
    def __init__(
        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size
    ):
        self.batch_size = batch_size
        self.dataset_name = dataset
        self.epochs = epochs
        self.prefetched_items = prefetched_items
        self.shuffle_buffer = shuffle_buffer
        self.size = size

    def get_input_fn(self) -> tf.data.Dataset:
        """"""Input fn.""""""
        return self.input_fn

    def load_public_dataset(self):
        """"""
        Load one of the publicly available datasets, will merge together all the splits.

        Args:
            chosen_dataset: dataset to use.

        Return:
            The chosen dataset as a ``tf.data.Dataset``

        """"""
        # Construct a tf.data.Dataset
        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)
        return datasets

    def resize_images(self, features: Dict) -> Dict:
        """"""
        Overwrite the \""image\"" feature in order to resize them.

        Args:
            features: features dictionary.
            size: desired target size.

        Returns:
            Features with \""image\"" resized to the correct shape.

        """"""
        features[""image""] = tf.image.resize(features[""image""], self.size)
        return features

    def input_fn(self):
        dataset = self.load_public_dataset()
        dataset = (
            dataset.map(self.resize_images)
            .shuffle(self.shuffle_buffer)
            .batch(self.batch_size)
            .prefetch(self.prefetched_items)
            .repeat(self.epochs)
        )
        return dataset


def main():

    # TODO: replace with CLI
    CHOICE = ""celeb_a""
    EPOCHS = 10
    BATCH_SIZE = 64
    PREFETCH = 10
    SHUFFLE_BUFFER = 10000

    # See available datasets
    public_datasets = tfds.list_builders()

    gan = GAN(Generator, Discriminator)
    input_pipeline = InputPipeline(
        dataset=CHOICE,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        prefetched_items=PREFETCH,
        shuffle_buffer=SHUFFLE_BUFFER,
        size=(64, 64),
    )
    dataset = input_pipeline.input_fn()
    gan.train(dataset=dataset)


if __name__ == ""__main__"":
    main()
```

**Other info / logs**

**Full Traceback**
```
Traceback (most recent call last):
  File ""dcgan-tf2.py"", line 289, in <module>
    main()
  File ""dcgan-tf2.py"", line 285, in main
    gan.train(dataset=dataset)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 383, in __call__
    self._initialize(args, kwds)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 355, in _initialize
    *args, **kwds))
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1097, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1322, in _maybe_define_function
    arg_names=arg_names), self._function_attributes)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 540, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 298, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1803, in bound_method_wrapper
    return wrapped_fn(weak_instance(), *args, **kwargs)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 533, in wrapper
    ), *args, **kwargs)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 293, in converted_call
    experimental_partial_types=partial_types)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 415, in to_graph
    arg_values, arg_types)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 222, in entity_to_graph
    entity_to_graph(candidate, program_ctx, {}, {})
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 175, in entity_to_graph
    node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 376, in function_to_graph
    node = node_to_graph(node, context)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 435, in node_to_graph
    node = converter.apply_(node, context, call_trees)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py"", line 507, in apply_
    node = converter_module.transform(node, context)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/converters/call_trees.py"", line 350, in transform
    return CallTreeTransformer(ctx).visit(node)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py"", line 440, in visit
    return super(Base, self).visit(node)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 484, in visit
    result = super(Base, self).visit(node)
  File ""/usr/lib64/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/converters/call_trees.py"", line 282, in visit_FunctionDef
    node.returns = self.visit_block(node.returns)
  File ""/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 368, in visit_block
    for node in nodes:
TypeError: 'Attribute' object is not iterable
```

I had previously opened a [question on SO](https://stackoverflow.com/questions/54369539/tensorflow-2-0-preview-typeerror-attribute-object-is-not-iterable-when-usin).

EDIT: even writing a simpler input pipeline, dropping `tensorflow-datasets` and using the builtin Keras datasets the error persists.

CC @galeone"
25279,tf.contrib.opt.ScipyOptimizerInterface error,"Hello.

I tried to use tf.contrib.opt.ScipyOptimizerInterface with an example code from your documentation.
`vector = tf.Variable([7., 7.], 'vector')`
`loss = tf.reduce_sum(tf.square(vector))`
`optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': 100})`
`with tf.Session() as session:`
`---->optimizer.minimize(session)`.

I am getting the following error: `Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, float32, float32, float32, float32, int32, int32, float32, float32, float32, float32] that don't all match.`

OS Platform and Distribution: Ubuntu 18.04.1
TensorFlow installed from: source
TensorFlow version: 1.12.0
Python version: 3.6
Bazel version: 0.21.0
GCC version: 7.3.0
CUDA and GPU: no running on GPU, because I don't have GPU.

Thank you!"
25278,Reducing the binary size of tflite library for android,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: all
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.10
- Python version:.3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):1.15
- GCC/Compiler version (if compiling from source):5.4
- CUDA/cuDNN version:8.0
- GPU model and memory:16GB

I build the libtensorflowLite.so following the https://github.com/tensorflow/tensorflow/issues/19642#issuecomment-422697028, and i can use it in the android project . But the .so of the  libtensorflowLite.so is about 3.7M. From the document ,it should be hundreds of KB.
Here is my build commit:
```
bazel build -c opt //tensorflow/contrib/lite:libtensorflowLite.so 
 --crosstool_top=//external:android/crosstool 
 --cpu=arm64-v8a
 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
 --cxxopt=""-std=c++11"" 
 --verbose_failures

```
So, could someone helps me, how can I reduce the size of tflite library?"
25277,build tensorflow-r1.13 fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Red Hat 4.8.2-16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source 
- TensorFlow version:r1.13
- Python version:python2.7
- Installed using virtualenv? pip? conda?:bazel
- Bazel version (if compiling from source):0.22.0, 0.19.1, 0.18.0
- GCC/Compiler version (if compiling from source):4.8.2
- CUDA/cuDNN version:
- GPU model and memory:CPU



**Describe the problem**
cannot fetch @bazel_toolchains when building tensorflow-r1.13

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Starting local Bazel server and connecting to it...
INFO: Invocation ID: 8c0b66e8-db86-4ac7-a5db-052c331926db
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@bazel_toolchains//repositories': java.io.IOException: thread interrupted
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@bazel_toolchains//repositories': java.io.IOException: thread interrupted
INFO: Elapsed time: 142.844s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    Fetching @bazel_toolchains; fetching 140s
"
25276,build failstensorflow-r1.13,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25275,bazel build aot armeabi-v7a so failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version:
1.12.0
- Python version:
3.6.2
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
0.21.0
- ndk version
14b
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
no
- GPU model and memory:



**Describe the problem**

I have tried to perform a simple demo in //tensorflow/compiler/aot/test/test_graph_tfmatmul.pb by using this command:
bazel build //tensorflow/compiler/aot/tests:my_library
and have succeeded in the building procedure.

However, when I tried to generate a .so file for an android project I got failed. The command I used is as below:
```text
bazel build --crosstool_top=@androidndk//:default_crosstool  --cpu=armeabi-v7a  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11"" //tensorflow/compiler/aot/tests:my_library
```
I don't know how to fix the problem. Please help.  The error message is as below:


```text
/private/var/tmp/_bazel_sunday/7a01d1437f77ea8b21ffc614971e2517/external/mkl_dnn/BUILD.bazel:71:1: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1)
In file included from external/mkl_dnn/src/common/utils.cpp:22:
In file included from external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/xmmintrin.h:27:
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:5: error: use of undeclared identifier '__builtin_ia32_emms'; did you mean '__builtin_isless'?
    __builtin_ia32_emms();
    ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:5: note: '__builtin_isless' declared here
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:25: error: too few arguments to function call, expected 2, have 0
    __builtin_ia32_emms();
                        ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:64:19: error: use of undeclared identifier '__builtin_ia32_vec_init_v2si'
    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:81:12: error: use of undeclared identifier '__builtin_ia32_vec_ext_v2si'
    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
           ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:143:19: error: use of undeclared identifier '__builtin_ia32_packsswb'
    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:173:19: error: use of undeclared identifier '__builtin_ia32_packssdw'
    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:203:19: error: use of undeclared identifier '__builtin_ia32_packuswb'
    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:230:19: error: use of undeclared identifier '__builtin_ia32_punpckhbw'
    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:253:19: error: use of undeclared identifier '__builtin_ia32_punpckhwd'
    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:274:19: error: use of undeclared identifier '__builtin_ia32_punpckhdq'
    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:301:19: error: use of undeclared identifier '__builtin_ia32_punpcklbw'
    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:324:19: error: use of undeclared identifier '__builtin_ia32_punpcklwd'
    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:345:19: error: use of undeclared identifier '__builtin_ia32_punpckldq'
    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: error: use of undeclared identifier '__builtin_ia32_paddb'; did you mean '__builtin_arm_qadd'?
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v8qi' (vector of 8 'char' values)
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
                                       ^~~~~~~~~~~~
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:387:19: error: use of undeclared identifier '__builtin_ia32_paddw'; did you mean '__builtin_arm_qadd'?
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:387:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v4hi' (vector of 4 'short' values)
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
                                       ^~~~~~~~~~~~
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:408:19: error: use of undeclared identifier '__builtin_ia32_paddd'; did you mean '__builtin_arm_qadd'?
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
                  ^
external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:408:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v2si' (vector of 2 'int' values)
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
                                       ^~~~~~~~~~~~
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Target //tensorflow/compiler/aot/tests:my_library failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.407s, Critical Path: 0.28s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
the content of the bazel BUILD file:
```text
load(""//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(
    name = ""test_graph_tfmatmul"",
    cpp_class = ""foo::bar::MatMulComp"",
    graph = ""test_graph_tfmatmul.pb"",
    config = ""test_graph_tfmatmul.config.pbtxt"",
)

cc_library(
    name = ""my_library"",
    srcs = [
        ""my_code.cc"",  # include test_graph_tfmatmul.h to access the generated header
    ],
    hdrs = [""my_code.h""],
    deps = [
        "":test_graph_tfmatmul"",  # link in the generated object file
        ""//third_party/eigen3"",
    ],
    linkopts = [
        ""-lpthread"",
    ]
)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25274,tensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13,"Hello ,I met a mistake when using tensorflow 1.10.0 as follows. I install tensorflow 1.10.0 with command `pip install tensorflow-gpu ==1.10.0` 
More information:
system:ubuntu 16.04
GPUï¼šRTX2080
CUDA: 9.0
cudnn:7.4.1
python: 2.7/3.6
When run the code ,we get a mistake about cuBlas and the status is 13. We know little about  this and we try using docker to run this but failed.Can you help me? Thanks!  
Code:
```
def input_fn_train():
        return input_function(True, flags.data_dir, flags.batch_size,
                              flags.epochs_per_eval, flags.num_parallel_calls,
                              flags.multi_gpu)
    if flags.mode == tf.estimator.ModeKeys.EVAL:
        eval_results = estimator.evaluate(input_fn=input_fn_eval,steps=flags.max_train_steps)
        print(eval_results)

    if flags.mode == tf.estimator.ModeKeys.TRAIN:
        for _ in range(flags.train_epochs // flags.epochs_per_eval):
            train_hooks = hooks_helper.get_train_hooks([""LoggingTensorHook""], batch_size=flags.batch_size)

            print('Starting a training cycle.')
            estimator.train(input_fn=input_fn_train,
                            max_steps=flags.max_train_steps)

            print('Starting to evaluate.')
            eval_results = estimator.evaluate(input_fn=input_fn_eval,
                                                steps=flags.max_train_steps)
            print(eval_results)
```

Mistake:
```
2019-01-29 12:12:50.442014: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55bd7c128050
2019-01-29 12:12:53.677220: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at matrix_inverse_op.cc:191 : Internal: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13
Traceback (most recent call last):
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13
	 [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](s2/Reshape_2)]]
	 [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1510_s2/transform/ImageProjectiveTransform"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""DAN_V2.py"", line 141, in <module>
    tf.app.run(argv=sys.argv)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""DAN_V2.py"", line 137, in main
    dan_run_loop.dan_main(flags,vgg16_model_fn,input_function)
  File ""/home/qiujia/DAN/DAN_V2/dan_run_loop.py"", line 203, in dan_main
    max_steps=flags.max_train_steps)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1173, in _train_model_default
    saving_listeners)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1451, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 583, in run
    run_metadata=run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1059, in run
    run_metadata=run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1150, in run
    raise six.reraise(*original_exc_info)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/six.py"", line 686, in reraise
    raise value
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1135, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1207, in run
    run_metadata=run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 987, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13
	 [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](s2/Reshape_2)]]
	 [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1510_s2/transform/ImageProjectiveTransform"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 's2/MatrixInverse', defined at:
  File ""DAN_V2.py"", line 141, in <module>
    tf.app.run(argv=sys.argv)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""DAN_V2.py"", line 137, in main
    dan_run_loop.dan_main(flags,vgg16_model_fn,input_function)
  File ""/home/qiujia/DAN/DAN_V2/dan_run_loop.py"", line 203, in dan_main
    max_steps=flags.max_train_steps)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1170, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""DAN_V2.py"", line 131, in vgg16_model_fn
    multi_gpu=params['multi_gpu'])
  File ""/home/qiujia/DAN/DAN_V2/dan_run_loop.py"", line 102, in dan_model_fn
    mean_shape,imgs_mean,imgs_std)
  File ""/home/qiujia/DAN/DAN_V2/dan_model.py"", line 152, in __call__
    inputs = self.__affine_image(inputs_imgs,r,t)
  File ""/home/qiujia/DAN/DAN_V2/dan_model.py"", line 78, in __affine_image
    r = tf.matrix_inverse(r)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py"", line 1049, in matrix_inverse
    ""MatrixInverse"", input=input, adjoint=adjoint, name=name)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13
	 [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](s2/Reshape_2)]]
	 [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1510_s2/transform/ImageProjectiveTransform"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```


"
25272,estimator saving eval metrics to /eval_0 not /eval,"I decided to give early stopping a shot with [stop_if_no_decrease_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_decrease_hook) but after tweaking the params and making sure the hook was actually being triggered, discovered an oddity with my estimator. 

The hook reads eval data from the directory[ specified by estimator.eval_dir()](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/early_stopping.py#L391) which in this case pops out ""trained_models/rcnn_inception_v2/eval"". This looks all good until I check my actual directory structure and find that /eval doesn't exist. Instead, I have eval_0/ (containing the proper logs) with no other directories in the rcnn_inception_v2/ folder.

I'm curious why eval_0/ is being created instead of eval/ when I've done nothing special to specify this directory. And importantly on top of that, why does estimator.eval_dir() return the wrong directory?"
25271,How Java calls the estimator model?,"Hello, I would like to ask how Java calls the estimator model. Is there any sample code to see?"
25270,"Training accuracy of TF official tutorial is very low with Ubuntu 18.04, CUDA 10.0 and cuDNN 7.4","**System information**
- Ubuntu 18.04
- Python 3.7.1 (Anaconda 3)
- CUDA 10.0
- cuDNN 7.4.2
- GTX 1070 mobile
- Core i7 8950H, 6C12T @2.2G Hz

**Describe the current behavior**
    I run the code from the TensorFlow tutorial from https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_classification.ipynb, but the training accuracy is very low (~0.1). 

No matter I installed the TensorFlow from 1) compiling source or from 2) `pip install tf-nightly-gpu`, the accuracies are both around 0.1. The stable release of tensorflow is not available for Python 3.7. Besides, I have tested running the code on `GPU` and `CPU`, the results are same.

**Describe the expected behavior**
In the tutorial, the expected accuracy is around 0.8.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras
import os

USE_CPU = 1
if USE_CPU == 1:
    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""  
    os.environ[""CUDA_VISIBLE_DEVICES""] = """"

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']
              )
model.fit(train_images, train_labels, epochs=5)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)
```

The output of the above code:
```
WARNING: Logging before flag parsing goes to stderr.
W0128 17:35:44.705833 140133976348480 deprecation.py:506] From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0128 17:35:44.735007 140133976348480 deprecation.py:506] From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-01-28 17:35:44.862411: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-01-28 17:35:44.866879: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-01-28 17:35:44.867464: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5574ea7aa690 executing computations on platform Host. Devices:
2019-01-28 17:35:44.867481: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-01-28 17:35:44.868907: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcuda.so.1 locally
2019-01-28 17:35:44.871695: E tensorflow/stream_executor/cuda/cuda_driver.cc:303] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-01-28 17:35:44.871714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: toughlife-AW17R5
2019-01-28 17:35:44.871719: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: toughlife-AW17R5
2019-01-28 17:35:44.871794: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 410.78.0
2019-01-28 17:35:44.871831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 410.78.0
2019-01-28 17:35:44.871836: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version seems to match DSO: 410.78.0
Epoch 1/5
60000/60000==============================] - 1s 22us/sample - loss: 0.5021 - acc: 0.1024
Epoch 2/5
60000/60000==============================] - 1s 21us/sample - loss: 0.3748 - acc: 0.1025
Epoch 3/5
60000/60000==============================] - 1s 21us/sample - loss: 0.3365 - acc: 0.1025
Epoch 4/5
60000/60000==============================] - 1s 21us/sample - loss: 0.3132 - acc: 0.1031
Epoch 5/5
60000/60000==============================] - 1s 21us/sample - loss: 0.2931 - acc: 0.1023
60000/60000==============================] - 1s 9us/sample - loss: 0.2689 - acc: 0.1015
```

**Other info / logs**
"
25263,tf.data performance slow when returning many tensors from a dataset op,"
**System information**
- Linux Ubuntu 16.04
- i7-7800X CPU @ 3.50GHz x 8
- Tensorflow binary, r1.12
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version: Python 2.7.12

**Describe the current behavior**

We load tabular data from a database to a training script, and this tabular data contains quite a many columns (30 - 40), most of them scalar. We observe performance that seems slow, given the simplicity of the data. To demonstrate and isolate the performance bottleneck, I created a simple C++ dataset op that returns just a specified number of int scalar tensors, and we get only about 1000 - 1500 entries / second  (calls of get_next) with this fake data.  These overheads lead to high CPU utilization and limit the throughput of our training.

My code with more results can be seen here:
https://github.com/akyrola/tf_data_repro

The key code is simply:
```
        for (int i = 0; i < dataset()->n_; ++i) {
                    tensorflow::Tensor tensor(ctx->allocator({}), tensorflow::DT_INT32, shape);
                    tensor.scalar<tensorflow::int32>()() = row;
                    out_tensors->emplace_back(std::move(tensor));
                }
```

The C++ op is compiled with -O2 optimizations.

I am suspecting the overhead is due to the tensor allocations, because the throughput changes almost linearly with the number of tensors (when n sufficiently large). I am looking for advice if using different allocator would help, for example.

**Describe the expected behavior**

I would expect at least 10x better performance on fake data generation.

**Code to reproduce the issue**

See code checked here for a benchmark:
https://github.com/akyrola/tf_data_repro

**Other info / logs**
Benchmark results (approximate), where n = number of scalar tensors:
- n=1:   8300 rows/sec
- n=2:   6800 rows/sec
- n=5:   4800 rows/sec
- n=10:  3200 rows/sec
- n=20:  2000 rows/sec
- n=40:  1100 rows/sec

"
25262,Usage of tf_stack.extract_stack in registry.py breaks TensorFlow R client,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 29
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

Hi,

this usage of `tf_stack` in registry.py

```
stack = tf_stack.extract_stack()
user_function = stack[2]
```

breaks the TensorFlow for R client, because at that point, when called from R `stack` is of length 2, with both elements being of length 6.

This is analogous to the recently fixed https://github.com/tensorflow/tensorflow/issues/25067
(thank you @jtkeeling)

It would be awesome if this could still be fixed for the 1.13 release, as I'm aware of no workaround and we have users that want to register a custom gradient.

Many thanks!



"
25260,TypeError: Failed to convert object of type <class 'tensorlayer.layers.PReluLayer'> to Tensor. Contents:   Last layer is: PReluLayer. Consider casting elements to a supported type.,"I am using tensorflow to construct CNN model. However, I got two errors when using tf.image.resize_images function:

1st error: TypeError: Expected binary or unicode string, got <tensorlayer.layers.PReluLayer object at 0x0000000031962D68>

2nd error: TypeError: Failed to convert object of type <class 'tensorlayer.layers.PReluLayer'> to Tensor. Contents:   Last layer is: PReluLayer. Consider casting elements to a supported type.



- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow version (use command below): 1.8.0
- Python version: 3.5
- CUDA/cuDNN version: 9.0.167 (CUDA)
- GPU model and memory: P6000

**Describe the current behavior**
The part of the code is:

'conv4 = Conv2d(conv3_bn_relu_pool, 256, (3, 3), (1, 1), act=None, padding='SAME', W_init=w_init, name='conv4')
        conv4_bn = BatchNormLayer(conv4, act=tf.identity, is_train=is_train, gamma_init=g_init, name='conv4_bn')
        conv4_bn_relu = PReluLayer(conv4_bn, name='conv4_bn_relu')
    
    ################################Deconvolution (up-sample + Conv)#########################
        
        deconv1_upsample = tf.image.resize_images(conv4_bn_relu, size=[64, 64], method=1)'
"
25259,Keras & Estimator: 'MeanMetricWrapper' object has no attribute '__name__',"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary, using pip
- TensorFlow version (use command below): b'v1.12.0-4728-ga8e5c41c5b' 1.13.0-rc0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Buid a simple Sequencial model using Keras. It works well with Keras `model.fit()`. Now converting this model to be used with estimator` tf.keras.estimator.model_to_estimator(keras_model=model)`. Then using `train()` then the code crashed with:
 'MeanMetricWrapper' object has no attribute '__name__'

**Describe the expected behavior**
The same code was runing with tf 1.12 and I could train the model using estimator

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The code can be find in the following notebook:
https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/08_Example_mnist_keras_estimator_tf_1_13.ipynb

```
def baseline_model():
    # create model
    model = tf.keras.Sequential()
 
    # hidden layer
    model.add(tf.keras.layers.Dense(dim_input, 
                    input_dim=dim_input, 
                    kernel_initializer=tf.keras.initializers.he_normal(),
                    bias_initializer=tf.keras.initializers.Zeros(),
                    activation='relu'))
    # last layer
    model.add(tf.keras.layers.Dense(num_classes, 
                    kernel_initializer=tf.keras.initializers.he_normal(),
                    bias_initializer=tf.keras.initializers.Zeros(),
                    activation='softmax'))

    optimiser=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9)
    # Compile model
    model.compile(loss='categorical_crossentropy', 
                  optimizer=optimiser, 
                  metrics=['accuracy'])
    return model
```

`model = baseline_model()`

```
def input_dataset_fn(x_data, y_data, batch_size=128, mode=tf.estimator.ModeKeys.TRAIN):
    
    if mode == tf.estimator.ModeKeys.PREDICT:
        tf.logging.info(""input_dataset_fn: PREDICT, {}"".format(mode))
    elif mode == tf.estimator.ModeKeys.EVAL:
        tf.logging.info(""input_dataset_fn: EVAL, {}"".format(mode))
    elif mode == tf.estimator.ModeKeys.TRAIN:
        tf.logging.info(""input_dataset_fn: TRAIN, {}"".format(mode))
    
    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))
   
    if mode == tf.estimator.ModeKeys.TRAIN:
        num_epochs = FLAGS.epoch
        dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size, seed=2)# depends on sample size
    else:
        num_epochs = FLAGS.epoch # to use validation data with keras
    dataset = dataset.repeat(num_epochs)
    dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)
    dataset = dataset.prefetch(FLAGS.prefetch_buffer_size)

    return dataset
```

`estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model)`

```
estimator_train_model.train(input_fn=lambda: input_dataset_fn(x_train, y_train, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),
                            steps=10)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
INFO:tensorflow:Calling model_fn.

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-123-a7db13f8b1dd> in <module>
      1 # Fit the model (using estimator.train and data.Dataset)
      2 estimator_train_model.train(input_fn=lambda: input_dataset_fn(x_train, y_train, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),
----> 3                             steps=10)
      4 
      5 #estimator_train_model.train(input_fn=get_train_input_fn,

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    352 
    353       saving_listeners = _check_listeners_type(saving_listeners)
--> 354       loss = self._train_model(input_fn, hooks, saving_listeners)
    355       logging.info('Loss for final step: %s.', loss)
    356       return self

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1181       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1182     else:
-> 1183       return self._train_model_default(input_fn, hooks, saving_listeners)
   1184 
   1185   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1211       worker_hooks.extend(input_hooks)
   1212       estimator_spec = self._call_model_fn(
-> 1213           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   1214       global_step_tensor = training_util.get_global_step(g)
   1215       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1169 
   1170     logging.info('Calling model_fn.')
-> 1171     model_fn_results = self._model_fn(features=features, **kwargs)
   1172     logging.info('Done calling model_fn.')
   1173 

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)
    286       loss = model.total_loss
    287 
--> 288       eval_metric_ops = _convert_keras_metrics_to_estimator(model)
    289 
    290     # Set train_op only during train.

~/anaconda3/envs/env_gcp_dl_1_13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in _convert_keras_metrics_to_estimator(model)
    231     for i, metric_name in enumerate(model.metrics):
    232       if callable(metric_name):
--> 233         metric_name = metric_name.__name__
    234       eval_metric_ops[metric_name] = metrics_module.mean(
    235           model.metrics_tensors[i])

AttributeError: 'MeanMetricWrapper' object has no attribute '__name__'

"
25258,tf.nn.bias_add and tf.add as checkpointable,"Please consider adding operation to checkpointable such as add, multiply, etc. Checkpointable does not work with any operands tf.keras.layers.add, tf.nn.bias_add, tf.add, +"
25257,TF 2.0 Conversion Script does not have a replacement for tf.contrib.eager.defun(train_step).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Submitting on behalf of @victordibia. Thanks, Victor! ðŸ˜Š

**Describe the expected behavior**

I used the TF 2.0 conversion script to convert my DCGAN sample from tf1.x to tf2.0.

It suggested use of tf.compat.v1.losses.sigmoid_cross_entropy  (previously in contrib), but am unclear if this has same behaviour as tf.nn.softmax_cross_entropy_with_logits. The script did not recommend a replacement for tf.contrib.eager.defun(train_step).

**Code to reproduce the issue**
https://github.com/victordibia/tf2/blob/master/dcgan/dcgan.py

**Other info / logs**
Friction log: https://docs.google.com/document/d/1Sv6INZzqLUUChy46jeawX1FdO1rG5aAjjGde7wOSBb8/edit?usp=sharing"
25254,"Keras & data.Dataset : ""Your dataset iterator ran out of data""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 1.12
- Python version: 3.6.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Keras model.fit() does not reset validation dataset iterator between epochs. Thus, when specifying `validation_steps` < `validation_dataset_size / batch_size`, then every evaluation will be performed on a different set of examples.

**Describe the expected behavior**
I would expect that `model.fit()` restarts from the beginning in the validation dataset after every epoch of training. This way the validation dataset could be used without `.repeat()` and the evaluation would be performed on the same set of examples.

**Code to reproduce the issue**
https://colab.research.google.com/drive/1UjKNbX38UC4EG6EPm6xLzQ1AmFV8HWe5

**Other info / logs**
```
WARNING:tensorflow:Your dataset iterator ran out of data interrupting testing. Make sure that your dataset can generate at least `steps` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.
```"
25252,windows c_api error with tensorflow.dll,"I tried to compile on windows c program with tenserflow c api and tenserflow.dll from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.12.0.zip founded on https://www.tensorflow.org/install/lang_c. 
This example:
#include <stdio.h>
#include <tensorflow/c/c_api.h>

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}

Compiling is success, but when i have run it, i recieved a mistake that libtenserflow.so not found. Its look like that tensorfow,dll from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.12.0.zip was builded with some mistakes for windows sistem, becaurse libtensorflow.so is a target for Linux.

Can you explain or fix this?  
"
25247,[Docker] include jupyter_http_over_ws in Jupyter Docker images,"**System information**
- TensorFlow version (you are using): `tensorflow/tensorflow:nightly-gpu-py3-jupyter`
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

Following [these instructions](https://research.google.com/colaboratory/local-runtimes.html), it requires to have `jupyter_http_over_ws` Jupyter extension to work. The current Docker image doesn't seem to have it (or I'm mistaken, please correct if so).

Having the Docker Jupyter images have this preinstalled would allow for this integration to be made more straightforward.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Any Docker Jupyter users using Google's Colaboratory (or other frontend using the interface).

**Any Other info.**

N/A."
25246,Go Binding: Use runtime.KeepAlive,"Please ensure to call runtime.KeepAlive, because the go bindings use finalizers which they shouldn't...
Reference: https://github.com/golang/go/issues/13347

Also ensure to call runtime.KeepAlive on every:

```go
var buf []byte
//...
unsafe.Pointer(&buf[0])
// C call here
runtime.KeepAlive(buf)
```"
25245,Error while training the 1D CNN in keras with tensorflow backend ,"Hello Everyone!!
I want to make a 1D CNN model using keras.
Consider that I've got a dataset X which is (2015930,13) where 13 is the number of features of audio where I want to do the 1d convolution on My Y is (2015930, 7) and I'm working with keras with tensorflow backend.

While training I am getting the following error. I am very new in this area. So I am unable to find the exact solution. The code of model is :
model = Sequential()
model.add(layers.Conv1D(40,2, activation='relu', input_shape=(13,1)))
model.add(layers.Conv1D(26,1, activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='relu'))
model.add(Dropout(0.2))
model.add(layers.Dense(y_train.shape[1],activation='softmax'))

UnknownError : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
[[{{node conv1d_12/convolution/Conv2D}} = Conv2D[T=DT_FLOAT, _class=[""loc:@train...propFilter""], data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](training_3/Adam/gradients/conv1d_12/convolution/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv1d_12/convolution/ExpandDims_1)]]
[[{{node loss_6/mul/_525}} = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_770_loss_6/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
My system specification are:
GPU system with NVIDIA graphics QuadroK2200.
OS: Ubuntu 16.04.1
print(tf.version) : 1.12.0(tensor flow)

Actually all the software were already installed by someone else so I am not sure CUDA and CUDNN are present or not.
So I used these command to check it.
cat /usr/local/cuda/version.txt : CUDA Version 9.0.176
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 :
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 0
#define CUDNN_PATCHLEVEL 5

#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 +CUDNN_PATCHLEVEL)
#include ""driver_types.h""

Can anyone help me why this error is coming ?
Please reply I am struggling since one week.
If any additional information require I can share it.

Thanks in advance"
25243,A question about tflite inference,"Hello, i use aware training method get a quantize model ,so in the tflite inference implementation, which part processes ""zero_point""(Such as input and filter data should be combined with offset)  in the convolution code of tflite? Could you please give me a linkï¼Ÿ thanks "
25242,how to test data stream in speech_commands project?,"Dear,
I have a question,in the [project](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands)
when I test the wave data decode by [librosa.load](https://github.com/librosa/librosa/blob/master/librosa/core/audio.py) not read by open in the row of 96 from the [script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/label_wav.py)
it ups bug,
`Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/python/speechtf/label_wav4_6.py"", line 41, in <module>
    y_pred,_=run_graph(graph,y)
  File ""D:/python/speechtf/label_wav4_6.py"", line 14, in run_graph
    predictions, = sess.run(softmax_tensor, {'wav_data:0': wav_data})
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.`

So how do I test data stream after decode by librosa.load but not wav file read by open ?
Could U help me,Please?
Any advice or suggestion will be great!
Thx
"
25240,Tensor vs _EagerTensorBase in 2.0 preview with eager execution,"Playing around with tensorlfow 2.0 preview (`tf_nightly_2.0_preview-2.0.0.dev20190126`) I noticed something that I found weird

Tensorflow version : tf_nightly_2.0_preview-2.0.0.dev20190126
OS : Mac OS 10.14.1
Installed via `pip install tf-nightly-2.0-preview`

using a simple model 

    from tensorflow.keras.layers import *

    class CNN(tf.keras.model.Model):
    
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        self.conv1 = Conv2D(32, (3, 3), padding='same',input_shape=valid_features.shape[1:])
        
        self.softmax = Activation('softmax')
        
    def call(self, inputs):
        
        x = self.conv1(inputs)
        ipdb.set_trace()
        return self.softmax(x)


evaluating `type(inputs)` in the debugger yields `<class 'tensorflow.python.framework.ops.Tensor'>` which has no way ( that I know) to actually get the contents of.

However, after searching through tensorflow code I found `class _EagerTensorBase(Tensor):`
which has a `numpy()` property the gives you the actual content.

``` python 
x = np.array([1,2,3])
y = tf.nn.relu(x)
type(y)
tensorflow.python.framework.ops.EagerTensor
y.numpy() works!
```

 I couldn't find a place online that actually tells the difference between these two. Also shouldn't `_EagerTensorBase` be the default when using eager execution with keras?

EDIT : 

If i ran `tf.executing_eagerly()` in the debugger it returns `False` however running it in a cell returns `True`"
25239,"Build issue of ""cannot find symbol class Fill where T is a type-variable: T extends Object declared in class Zeros"" still persists, when building Bazel. ","**System information**
- Ubuntu 16.04 LTS:
- TensorFlow installed from source:
- TensorFlow version 1.12:
- Python version 2.7:
- Bazel version 0.21.0:

**Describe the problem**
I am going to build Tensorflow Object Detection API Android demo using Bazel which is **def nativeBuildSystem = 'bazel'** But got the following error _**error: cannot find symbol class Fill where T is a type-variable: T extends Object declared in class Zeros **_
I followed the instruction given [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md](url). All steps were completed, except this [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md#android-studio-with-bazel](url), which is building the demo using Android Studio and Bazel. This step is important for my project since Object tracking option requires Bazel. 

**Any other info / logs**
 _**Android Studio Version: 3.3**_
_**Gradle version: 3.3.0**_
Any of the instructions and suggestions in either [https://github.com/tensorflow/tensorflow/issues/21431](url), or [https://github.com/tensorflow/tensorflow/issues/23457](url) **DID NOT HELP**, however issues were closed. 

"
25238,deeplab with tensorflow lite + gpudelegate issue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I tried to build deeplab (input_image + float32) and succeeded

# modified export_model.py # line:77

```
//input_image = tf.placeholder(tf.uint8, [1, None, None, 3], name=_INPUT_NAME)
input_image = tf.placeholder(tf.float32, [1, None, None, 3], name=_INPUT_NAME)
```

S. I converted below command and succeeded

tflite_convert \
  --output_file=deeplab_257_float.tflite \
  --graph_def_file=$PB_FILE \
  --output_format=TFLITE \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,257,257,3 \
  --inference_input_type=FLOAT \
  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=127 \

But. when i  try to run on android, I saw below issue.

` java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #38 cannot be handled by this delegate.  Only the first 32 ops will run on the GPU, and the remaining 88 on the CPU.GpuDelegate Prepare: First dimension is supposed to be BATCH and always equal to 1.Node number 120 (GpuDelegate) failed to prepare.`

what's wrong with my model?

deeplabv3_257_mv_gpu.tflite is working.

# Can you tell me about how can I do that?
# Can you tell me about your tflite_convert command?

**Describe the expected behavior**

working.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
25237,Error building tensorflow pip package,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): cl 19.16.27025.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
Get error when building the pip package from build_pip_package:

> error in tensorflow setup command:
> 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at ""'< 1.14.0'""

This error only appears after a recent pull request (2019/01/26)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

_Get clean copy of tensorflow_ 
`git pull`

 _Build CPU version_
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

 _Build  pip package:_
`bazel-bin\tensorflow\tools\pip_package\build_pip_package.exe tmp/tensorflow_pkg`

> Sun Jan 27 14:02:41 PST 2019 : === Preparing sources in dir: /tmp/tmp.YaQ7qcBkdZ
> Unzipping simple_console_for_windows.zip to create runfiles tree...
> Unzip finished.
> /e/src/gitClones/tensorflow /e/src/gitClones/tensorflow
> /e/src/gitClones/tensorflow
> Sun Jan 27 14:05:55 PST 2019 : === Building wheel
> error in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at ""'< 1.14.0'""



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Digging into this I believe the issue is at line 61 in [setup.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L61) 
which was introduced by c4388465d0d04ed90c814a11216217d19a8267e8

I made this change locally and the problem goes away

> @@ -58,7 +58,7 @@ REQUIRED_PACKAGES = [
>      'six >= 1.10.0',
>      'protobuf >= 3.6.1',
>      'tensorboard >= 1.12.0, < 1.13.0',
> **-    'tensorflow_estimator >= 1.13.0 < 1.14.0',**
> **+    'tensorflow_estimator >= 1.13.0, < 1.14.0',**
>      'termcolor >= 1.1.0',
>  ]

cc @case540 "
25235,Code example to save a custom model with a serve() method fails,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes and No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION='2.0.0-preview'
GIT_VERSION=""b'v1.12.0-6502-g76aa6cf917'""
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
The [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/save) of the `tf.saved_model.save()` function shows a code example to save a custom model with a `serve()` method, but it fails:
1. when subclassing the `keras.models.Model` class, it is compulsory to define a `call()` method (or else you get `NotImplementedError: When subclassing the Model class, you should implement a call method.`.
2. when saving the model as shown in the code example, I get `ValueError: Exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: ['_default_save_signature', 'serve']. The signature keys for these functions are ambiguous. Specify signature functions explicitly.`

However it works if I define a `call()` method and I specify the signature explicitly.

**Describe the expected behavior**
Perhaps the documentation just needs to be fixed to add a `call()` method and an explicit `signature` argument, but it would be nicer if it could work without having to do that.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.probas = keras.layers.Dense(10, activation=""softmax"")
    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string)])
    def serve(self, serialized):
        expected_features = {
            ""image"": tf.io.FixedLenFeature([28 * 28], dtype=tf.float32)
        }
        examples = tf.io.parse_example(serialized, expected_features)
        return self.probas(examples[""image""])
    def call(self, inputs):
        return self.probas(inputs)

m = MyModel()
m.compile(loss=""sparse_categorical_crossentropy"", optimizer=""sgd"")
m.fit(X_train.reshape(-1, 28*28), y_train)
tf.saved_model.save(m, ""my_model_test"",
    signatures=m.serve.get_concrete_function(
        tf.TensorSpec(shape=[None], dtype=tf.string, name=""serialized_inputs"")))
```

You can try removing the `call()` method, it will fail. You can also try removing the `signatures` argument in the `tf.saved_model.save()` call, it will fail.

**Other info / logs**

Here is the output I get when I don't specify the signature:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-424-4d06f50f5010> in <module>
      2 m.compile(loss=""sparse_categorical_crossentropy"", optimizer=""sgd"")
      3 m.fit(X_train.reshape(-1, 28*28), y_train)
----> 4 tf.saved_model.save(m, ""my_model_test"")#,
      5 #    signatures=m.serve.get_concrete_function(
      6 #        tf.TensorSpec(shape=[None], dtype=tf.string, name=""serialized_inputs"")))

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)
    820 
    821   if signatures is None:
--> 822     signatures = _find_function_to_export(saveable_view)
    823   signatures = _canonicalize_signatures(signatures)
    824 

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _find_function_to_export(saveable_view)
    144            ""keys for these functions are ambiguous. Specify signature ""
    145            ""functions explicitly."").format(
--> 146                [previous_attribute_name, name]))
    147     exported_function = value
    148     previous_attribute_name = name

ValueError: Exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: ['_default_save_signature', 'serve']. The signature keys for these functions are ambiguous. Specify signature functions explicitly.
```"
25234,tf.saved_model.save() exports a model with an empty method_name,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION='2.0.0-preview'
GIT_VERSION=""b'v1.12.0-6502-g76aa6cf917'""
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Saving a Keras model with `tf.saved_model.save()` works, but the `method_name` is empty, so deploying the model to TensorFlow Serving fails.

**Describe the expected behavior**
I expect the `method_name` to be equal to `""tensorflow/serving/predict""`.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import os

X_train = np.random.rand(100, 3)
y_train = np.random.rand(100, 1)
model = keras.models.Sequential([keras.layers.Dense(1)])
model.compile(loss=""mse"", optimizer=""sgd"")
model.fit(X_train, y_train)

model_version = 1
model_path = os.path.join(""my_model"", str(model_version))
os.makedirs(model_path)
tf.saved_model.save(model, model_path)
```

Next inspect the saved model:

```bash
$ saved_model_cli show --dir my_model/1 --all
```

Notice that the `method_name` is empty.

**Other info / logs**

Here is the output I get (I added comments marked with `# <=`):

```
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID   # <= what is this?
        shape: unknown_rank # <= what is this?
        name: NoOp
  Method name is:   # <= empty method name

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: serving_default_input_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:0
  Method name is:  # <= empty, should be tensorflow/serving/predict
```"
25233,"import tensorflow.keras,code can not work.import keras,the same code run perfectly.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:4g


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
import tensorflow.keras,code can not work.
import keras,the same code run perfectly.
**Describe the expected behavior**
import tensorflow.keras,code can work.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import numpy as np
train_x = np.random.random((1000, 32))
train_y = np.random.random((1000, 10))

import tensorflow as tf
from tensorflow.keras import layers
inputs = tf.keras.Input(shape=(32,)) 
x = layers.Dense(60, activation='relu')(inputs)
x = layers.Dense(30, activation='relu')(x)
predictions = layers.Dense(10)(x)
model = tf.keras.Model(inputs=inputs, outputs=predictions)

model.compile(optimizer='adam',
              loss='mse',
              metrics=['accuracy'])

import keras.backend as K
def scheduler(epoch):
    lr = K.get_value(model.optimizer.lr)
    print(""lr:{}"".format(lr * 1))
    return K.get_value(model.optimizer.lr)
 
reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])
b = history.history['lr']
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```python
FailedPreconditionError (see above for traceback): Error while reading resource variable Adam/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/Adam/lr)
	 [[node Adam/lr/Read/ReadVariableOp (defined at <ipython-input-2-48494f67c56a>:15)  = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Adam/lr)]]
	 [[{{node Adam/lr/Read/ReadVariableOp/_1}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_5_Adam/lr/Read/ReadVariableOp"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

**Code to reproduce the Success**
```python
import numpy as np
train_x = np.random.random((1000, 32))
train_y = np.random.random((1000, 10))

from keras.models import Model
from keras.layers import Input, Dense
inputs = Input(shape=(32,)) 
x = Dense(60, activation='relu')(inputs)
x = Dense(30, activation='relu')(x)
predictions = Dense(10)(x)

model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='adam',
              loss='mse',
              metrics=['acc'])

import keras.backend as K
from keras.callbacks import LearningRateScheduler
def scheduler(epoch):
    lr = K.get_value(model.optimizer.lr)
    print(""lr:{}"".format(lr * 1))
    return K.get_value(model.optimizer.lr)
 
reduce_lr = LearningRateScheduler(scheduler)
history = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])
b = history.history['lr']
```

"
25232, failed to process â€œapi_def_Conv2D.pbtxtâ€: Attribute explicit_paddings not defined in base api for Conv2D,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  -
`Mac OS X 10.13.6`
- TensorFlow version (use command below): - 
```
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-rc2-3-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-rc2-3-ga6d8ffae09
Sanity check: array([1], dtype=int32)
```
- Python version:
`Python 3.6.5`


**Describe the current behavior**
Hi, I try to run tensorflow using golang, </br>
When I execute the following command: </br>`go generate github.com/tensorflow/tensorflow/tensorflow/go/op` </br>
I get this error: </br>
```
2019/01/27 18:21:15 failed to process â€œapi_def_Conv2D.pbtxtâ€: Attribute explicit_paddings not defined in base api for Conv2D
exit status 1
op/generate.go:18: running â€œgoâ€: exit status 1
```
 
"
25231,tensorflow.contrib.lite.Interpreter.invoke() crashes ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os - Mojave
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below):  from pip -'v1.12.0-rc2-3-ga6d8ffae09', '1.12.0'
- Python version: 3.6

**Describe the current behavior**
Hey. 
-im building a test model from a net I designed for OCR. 
-The model is running fine on Tensorflow mobile and I get good results (the model is running fine and I have NO errors).
-All the work is in python.
-Part of the work is to convert the model to 'lite' and running it on mobile, I do it with this code : 

 ```
  input_arrays = [""input""]
  output_arrays = [""output""]
  converter = lite.TFLiteConverter.from_frozen_graph(
  forzen_file_path, input_arrays, output_arrays, input_shapes=shape)
  converter.allow_custom_ops = True
  tflite_model = converter.convert()
  open(output_path, ""wb"").write(tflite_model)
```

This part is working well. 

- Using the following code (obtained from the docs ) Im checking the conversion:


   ```
    interpreter = lite.Interpreter(model_path=lite_model)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test model on random input data.
    input_shape = input_details[0]['shape']
    input_type = input_details[0]['dtype']
    random_sample = np.random.random_sample(input_shape)
    input_data = np.array(random_sample, dtype=input_type)
    interpreter.set_tensor(input_details[0]['index'], input_data)

    interpreter.invoke() <---- crash! SIGSEGV 11 
    output_data = interpreter.get_tensor(output_details[0]['index'])



And I get a crash with no details on what happened:

```
2019-01-27 17:45:08.423844: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)

```

I've tried to compile tensorflow and debug it myself but google uses bazel and I cant get it to import on CLion (with the bazel plugin). 

Does anybody knows what is going on? 

**Describe the expected behavior**
Not crashing :)

**Code to reproduce the issue**
I'm not allowed to share the model but i've shared the conversion and the interpreter code above. 

**Other info / logs**
I want to state again these few things:
- The model runs well on Tensorflow (not lite)
- The conversion runs well with no errors. 
- The crash occurs when calling interpreter.invoke() 

**Thank you!**
"
25229,Native TF methods not found Exception (Installed as System Apps),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_templateem>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Galaxy S6 Edge

**Describe the problem**

<em>I am making an extension of the demo app which requires the app to be installed as a system app in the ""priv-app"" of the device. I have not changed anything in the demo app itself but I still get the following error whenever I open the TF detect app.</em>

```
2019-01-27 20:29:19.235 8201-8201/? E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
2019-01-27 20:29:19.236 8201-8201/? I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
2019-01-27 20:29:19.238 8201-8201/? D/AndroidRuntime: Shutting down VM
2019-01-27 20:29:19.239 8201-8201/? E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.tensorflow.demo, PID: 8201
    java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.prepareNativeRuntime(TensorFlowInferenceInterface.java:544)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:60)
        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:97)
        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:167)
        at org.tensorflow.demo.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:429)
        at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:454)
        at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:462)
        at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:66)
        at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:101)
        at android.view.TextureView.getHardwareLayer(TextureView.java:389)
        at android.view.TextureView.draw(TextureView.java:338)
        at android.view.View.updateDisplayListIfDirty(View.java:17296)
        at android.view.View.draw(View.java:18080)
        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)
        at android.view.View.updateDisplayListIfDirty(View.java:17291)
        at android.view.View.draw(View.java:18080)
        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)
        at android.view.View.draw(View.java:18321)
        at android.view.View.updateDisplayListIfDirty(View.java:17296)
        at android.view.View.draw(View.java:18080)
        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)
        at android.view.View.updateDisplayListIfDirty(View.java:17291)
        at android.view.View.draw(View.java:18080)
        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)
        at android.view.View.updateDisplayListIfDirty(View.java:17291)
        at android.view.View.draw(View.java:18080)
        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)
        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)
        at android.view.View.draw(View.java:18321)
        at com.android.internal.policy.DecorView.draw(DecorView.java:919)
        at android.view.View.updateDisplayListIfDirty(View.java:17296)
        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:692)
        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:698)
        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:806)
        at android.view.ViewRootImpl.draw(ViewRootImpl.java:3128)
        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2924)
        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2516)
        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1515)
        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:7091)
        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:927)
        at android.view.Choreographer.doCallbacks(Choreographer.java:702)
        at android.view.Choreographer.doFrame(Choreographer.java:638)
        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:913)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:154)
        at android.app.ActivityThread.main(ActivityThread.java:6682)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1520)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1410)
2019-01-27 20:29:24.764 8201-8210/org.tensorflow.demo E/System: Uncaught exception thrown by finalizer
2019-01-27 20:29:24.768 8201-8210/org.tensorflow.demo E/System: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.Session.close()' on a null object reference
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.close(TensorFlowInferenceInterface.java:276)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.finalize(TensorFlowInferenceInterface.java:287)
        at java.lang.Daemons$FinalizerDaemon.doFinalize(Daemons.java:222)
        at java.lang.Daemons$FinalizerDaemon.run(Daemons.java:209)
        at java.lang.Thread.run(Thread.java:762)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Steps :
1. Build Apk(s)
2. Copy the apk to the priv-app of the device
3. Restart the device
4. Error Occurs on opening of TF Detect app

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25228,TF Lite works wrong on quantized TF Hub retrained Inception V3 mnist model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android Emulator
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly 1.13.0.dev20190123
- Python version:3.7.1
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

**Describe the current behavior**
- Using TF Hub to retrain Inception V3 for mnist images
- Converting the retrained model to quantized model
- Deploy the converted quantized model to Android App
- The quantized TF Lite works wrong like below:
![image](https://user-images.githubusercontent.com/2778945/51802538-1385ab00-2286-11e9-8297-cd9c7c3c5350.png)


**Describe the expected behavior**
- The mnist images are well classified.

**Code to reproduce the issue**
https://github.com/dpinthinker/TFGrocery/tree/master/MnistClassifier
"
25227,Porting codebase utilizing tf.slim to TF-2.0,"A lot of my TensorFlow  codebase depends upon the use of pretrained models as a part of [tensorflow/models/research/slim/nets](https://github.com/tensorflow/models/tree/master/research/slim/nets) repository.

I routinely utilize the dictionary of `end points` provided by `tf.slim` models in my codebase. Furthermore, the model definitions in the aforementioned repository is impressive and general in the sense that for models like `resnet_v2`, it allows me to directly specify the `output_stride` which leads to an automatic specification of dilation rates. 

As I aim to write my new codes in TF-2.0 ( as well as thinking of porting my existing codebase to TF-2.0), I realize (based on documentation and my knowledge of Keras !!), that there is no concept of `end point` dictionaries and `output_stride` (wherever applicable). 

Hence, I am confused about how to handle these situations. 

I believe that it could be very reasonable for the TensorFlow team to provide documentation on this issue for it could really make transition to TF-2.0,  very simple and practical. 
"
25226,"""ModuleNotFoundError : No module named 'keras' "" Error while 'import keras' in Spyder(Python 3.5) launched through a environment with python 3.5 .","
**System information**
- Windows 8.1
- TensorFlow version: 1.10.0
- Python version:3.5
- Installed using  conda
- GCC/Compiler version




I had created a new conda environment named py35 for using spyder. py35 with Python3.5. I had also installed theano,tensorflow and keras using conda install commands with env activated . In admin cmd import command works fine for all three libraries. But in Spyder it return the error ""ModuleNotFoundError: No module named  'keras'"" 


"
25224,ModuleNotFoundError: No module named 'tensorflow.core.framework',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25222,Duplicated Java outer classname,"**Describe the current behavior**
Option values of `java_outer_classname` in `tensorflow/compiler/tf2xla/host_compute_metadata.proto` and `tensorflow/compiler/tf2xla/tf2xla.proto` are the same, result in protoc failure for Java:

~~~
> protoc: stdout: . stderr: tensorflow/compiler/xla/rpc/xla_service.proto: warning: Import tensorflow/compiler/xla/xla_data.proto but not used.
  tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/framework/graph.proto but not used.
  tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/protobuf/cluster.proto but not used.
  org/tensorflow/tf2xla/Tf2XlaProtos.java: Tried to write the same file twice.
~~~

**Describe the expected behavior**
protoc succeeds without error.
`java_outer_classname` in `tensorflow/compiler/tf2xla/host_compute_metadata.proto` may be renamed to `HostComputeMetadataProtos`

**Code to reproduce the issue**

Just protoc

**Other info / logs**
"
25220,Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
No, compiled from Source 
- TensorFlow version (use command below):
1.12
- Python version:
2.7
- Bazel version (if compiling from source):
Bazel release 0.17.2
- GCC/Compiler version (if compiling from source):
gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version:
Cuda 10.0/ cuDNN 7.3.1
- GPU model and memory:
Nvidia GeForce RTX 2080 (Two gpus) each with 8 GM memory

**Describe the current behavior**

I've compiled tensorflow using the following command to get libtensorflow_cc.so. (Non Monolithic build)
`bazel build --config=cuda //tensorflow:libtensorflow_cc.so`

I would like to use multiple gpus at inference time in C++. Currently I have two GeForce RTX 2080 GPUs. So I'm running two threads (with thread id 0 and 1) in a c++ standalone example. Currently I'm using label_image ([main.cc)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc). 

I want to have two tensorflow session, one session for each thread using each gpu. Thread with thread_id 0 uses gpu:0 and thread with thread_id 1 uses gpu:1

My load_graph method looks as follows:

```
Status LoadGraph(const string& graph_file_name,
                 std::shared_ptr<tensorflow::Session>* session, int i_threadid) {
  tensorflow::GraphDef graph_def;
  printf("".. IN load graph %d\n"", i_threadid);
  Status load_graph_status =
      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);
  if (!load_graph_status.ok()) {
    return tensorflow::errors::NotFound(""Failed to load compute graph at '"",
                                        graph_file_name, ""'"");
  }
  tensorflow::SessionOptions session_options;
  if(i_threadid == 0) {
    session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");
    tensorflow::graph::SetDefaultDevice(""/device:GPU:0"", &graph_def);
  } else if(i_threadid == 1) {
    session_options.config.mutable_gpu_options()->set_visible_device_list(""0,1"");
    tensorflow::graph::SetDefaultDevice(""/device:GPU:1"", &graph_def);
  }
  session_options.config.mutable_gpu_options()->set_allow_growth(true);
  session_options.config.set_allow_soft_placement(true);
  session->reset(tensorflow::NewSession(session_options));

  //tensorflow::graph::SetDefaultDevice(""/cpu:0"", &graph_def);
  Status session_create_status = (*session)->Create(graph_def);
  if (!session_create_status.ok()) {
    return session_create_status;
  }
  return Status::OK();
}
```

In the else part: if I use the following line:    
`session_options.config.mutable_gpu_options()->set_visible_device_list(""1"");`, I'm getting the following error:
```
2019-01-26 21:47:00.872434: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-01-26 21:47:01.130371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:17:00.0
totalMemory: 7.77GiB freeMemory: 7.62GiB
2019-01-26 21:47:01.130396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-26 21:47:01.388128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-26 21:47:01.388163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-26 21:47:01.388170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-26 21:47:01.388331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)
2019-01-26 21:47:01.632637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:65:00.0
totalMemory: 7.76GiB freeMemory: 7.47GiB
2019-01-26 21:47:01.632673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1
2019-01-26 21:47:01.898614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-26 21:47:01.898649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 
2019-01-26 21:47:01.898655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N 
2019-01-26 21:47:01.898776: E tensorflow/core/common_runtime/session.cc:64] Failed to create session: Already exists: TensorFlow device (GPU:0) is being mapped to multiple CUDA devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
Segmentation fault (core dumped)

```

So I added    `session_options.config.mutable_gpu_options()->set_visible_device_list(""0,1"");` so the second thread with thread_1 is creating session that uses both GPUs (or atleast initializing to utilize both gpus) but the graph runs on the second gpu.
Now I'm getting the output as follows without any error:
```
2019-01-26 21:51:20.126025: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-01-26 21:51:20.380183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:17:00.0
totalMemory: 7.77GiB freeMemory: 7.62GiB
2019-01-26 21:51:20.380209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-26 21:51:20.637473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-26 21:51:20.637508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-26 21:51:20.637513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-26 21:51:20.637668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)
2019-01-26 21:51:20.892308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:65:00.0
totalMemory: 7.76GiB freeMemory: 7.47GiB
2019-01-26 21:51:20.892414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-01-26 21:51:21.142666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-26 21:51:21.142699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-01-26 21:51:21.142705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N 
2019-01-26 21:51:21.142709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N 
2019-01-26 21:51:21.142932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)
2019-01-26 21:51:21.143217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7187 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:65:00.0, compute capability: 7.5)

```

And the resizing image is done using the following code as I want to run this resizing computation on CPU to get better efficiency
```
Status LoadImageResizeGraph(std::shared_ptr<tensorflow::Session>* session, const int input_height,
                               const int input_width, const float input_mean,
                               const float input_std) {
  string output_name = ""normalized"";
  auto root = tensorflow::Scope::NewRootScope();
  using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)

  // use a placeholder to read input data
  auto file_reader =
      Placeholder(root.WithOpName(""input""), tensorflow::DataType::DT_STRING);

  // Now try to figure out what kind of file it is and decode it.
  const int wanted_channels = 3;
  tensorflow::Output image_reader;
  
    image_reader = DecodeJpeg(root.WithOpName(""jpeg_reader""), file_reader,
                              DecodeJpeg::Channels(wanted_channels));
  // Now cast the image data to float so we can do normal math on it.
  auto float_caster =
      Cast(root.WithOpName(""float_caster""), image_reader, tensorflow::DT_FLOAT);
  // The convention for image ops in TensorFlow is that all images are expected
  // to be in batches, so that they're four-dimensional arrays with indices of
  // [batch, height, width, channel]. Because we only have a single image, we
  // have to add a batch dimension of 1 to the start with ExpandDims().
  auto dims_expander = ExpandDims(root, float_caster, 0);
  // Bilinearly resize the image to fit the required dimensions.
  auto resized = ResizeBilinear(
      root, dims_expander,
      Const(root.WithOpName(""size""), {input_height, input_width}));
  // Subtract the mean and divide by the scale.
  Div(root.WithOpName(output_name), Sub(root, resized, {input_mean}),
      {input_std});

  // This runs the GraphDef network definition that we've just constructed, and
  // returns the results in the output tensor.
  tensorflow::GraphDef graph_def;
  TF_RETURN_IF_ERROR(root.ToGraphDef(&graph_def));

  tensorflow::SessionOptions session_options;
  session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");
  session_options.config.mutable_gpu_options()->set_allow_growth(true);
  session_options.config.set_allow_soft_placement(true);
  session->reset(tensorflow::NewSession(session_options));
  tensorflow::graph::SetDefaultDevice(""/cpu:0"", &graph_def);
  TF_RETURN_IF_ERROR((*session)->Create(graph_def));
  return Status::OK();
}

```

I'm using Inception_v3 as specified in the [main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) and want to run inference on 1000 images. Each thread runs 500 images each in parallel for efficient runtime.

But at the end of the program, I'm getting the following error

```
[8.853 s] Finished running 1000 images successfully
2019-01-26 21:59:10.712562: F tensorflow/stream_executor/cuda/cuda_driver.cc:206] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)
Aborted (core dumped)
```

I searched the issues history and seems other people are facing the similar problem but no successful resolution has been metioned

https://github.com/tensorflow/tensorflow/issues/3509
https://github.com/tensorflow/tensorflow/issues/10961
https://github.com/tensorflow/tensorflow/issues/526

In the issue #526, one of the owners of the StreamExecutor has mentioned some resolution 
https://github.com/tensorflow/tensorflow/issues/526#issuecomment-236259722 
But it is not working for me.

Can someone please help me to fix this error and use multiple gpus with multiple threads in a single program.

Thank you

**Code to reproduce the issue**
My sample code:
```

int Initialize(const string& graph_file_name, int i_nthreads) {
  int32 input_width = 299;
  int32 input_height = 299;
  float input_mean = 0;
  float input_std = 255;
  nthreads = i_nthreads;

  // Initialize all sessions;
  for(int i = 0; i < nthreads; i++) {

    int threadid = i;
    if(threadid == 0) {
      LoadGraph(graph_file_name, &global_session_00, 0);
    } else if (threadid == 1) {
      LoadGraph(graph_file_name, &global_session_10, 1);
    }
  }

  LoadImageResizeGraph(&global_session_01, input_height, input_width, input_mean, input_std);
}

void *Perform_Computation_Parallel(void *arg1)
{

  
  string input_layer = ""input"";
  string output_layer = ""InceptionV3/Predictions/Reshape_1"";


  int threadid = (int)(((size_t)(arg1)));
  printf(""threadid %d\n"", threadid);

  std::shared_ptr<tensorflow::Session> session;
  if(threadid == 0) {
    session = global_session_00;
  } else if(threadid == 1) {
    session = global_session_10;
  }

  char *image_path = (char *)malloc(1000000);
  for(int r = threadid; r < global_number_of_images; r += nthreads)
  {
    sprintf(image_path, ""%s"", global_images[r]);
    printf(""%s --------> %d \n"", image_path, threadid);

    std::string image_file(image_path);
    std::vector<Tensor> resized_tensors;

    // read file_name into a tensor named input
    Tensor input_image(tensorflow::DT_STRING, tensorflow::TensorShape());
    
    ReadEntireFile(tensorflow::Env::Default(), image_file, &input_image);

    Status run_resize_status = global_session_01->Run({{""input"", input_image}},
                                      {""normalized""}, {}, &resized_tensors);

    if (!run_resize_status.ok()) {
      LOG(ERROR) << ""Running resize graph failed: "" << run_resize_status;
    } 
    // else if(run_resize_status.ok()){
    //   std::cout << ""successfully resized image ""<< image_file << ""\n"";
    // }

    // Status read_tensor_status =
    //         ReadTensorFromImageFile(image_file, input_height, input_width, input_mean,
    //                                 input_std, &resized_tensors);
    // if (!read_tensor_status.ok()) {
    //   LOG(ERROR) << read_tensor_status;
    //   //return -1;
    // }
    const Tensor& resized_tensor = resized_tensors[0];


    // Actually run the image through the model.
    std::vector<Tensor> outputs;
    Status run_status = session->Run({{input_layer, resized_tensor}},
                                     {output_layer}, {}, &outputs);
    if (!run_status.ok()) {
      LOG(ERROR) << ""Running model failed: "" << run_status;
      //return -1;
    }
  }

    
    
}

void Perform_Computation(void)
{
    for(long long int i = 1; i < nthreads; i++) pthread_create(&threads[i], NULL, Perform_Computation_Parallel, (void *)(i));
    Perform_Computation_Parallel(0);
    for(long long int i = 1; i < nthreads; i++) pthread_join(threads[i], NULL);
}

int main(int argc, char* argv[]) {
  //printf(""Hello World!!!\n"");

  // These are the command-line flags the program can understand.
  // They define where the graph and input data is located, and what kind of
  // input the model expects. If you train your own model, or use something
  // other than inception_v3, then you'll need to update these.
  string image = ""./data/grace_hopper.jpg"";
  string graph =
      ""./data/inception_v3_2016_08_28_frozen.pb"";
  string labels =
      ""./data/imagenet_slim_labels.txt"";
  int32 input_width = 299;
  int32 input_height = 299;
  float input_mean = 0;
  float input_std = 255;
  string input_layer = ""input"";
  string output_layer = ""InceptionV3/Predictions/Reshape_1"";
  bool self_test = false;
  string root_dir = """";
  string test_file = ""./data/images/test_images_1000.txt"";
  string out_file = ""./data/results/test_results_1000.txt"";

  //nthreads = 2;
  int ret_init_val = Initialize(graph, nthreads);
  std::cout << "" Initialization done --------------------------\n"";
  FILE *fp = fopen(test_file.c_str(), ""r"");
  int nol = 0;


  char *line = (char *)malloc(2048);

  while (!feof(fp))
  {
    line[0] = '\0';
    fgets(line, 2048, fp);
    if (line[0] == '\0')  break;
    nol++;
  }
  fclose(fp);

  global_number_of_images = nol;

  global_images = (char **)malloc(global_number_of_images * sizeof(char *));
  

  fp = fopen(test_file.c_str(), ""r"");
  for(int q = 0; q < global_number_of_images; q++)
  {
    line[0] = '\0';
    fgets(line, 2048, fp);
    line[strlen(line)-1] = '\0';
    global_images[q] = (char *)malloc(strlen(line)+10);
    sprintf(global_images[q], ""%s"", line);
  }
  fclose(fp);
  double t0 = elapsed();
  Perform_Computation();
  printf (""[%.3f s] Finished runnning %d images successfully\n"", elapsed() - t0, nol);
  return 0;
}
```
"
25218,Complex step derivatives,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Is it possible to implement the complex-step derivative method as a way to compute gradients? This would presumably require a ""fairly simple"" change of traversing the Ops of the graph, copying each one, and modifying its args from Real to Complex numbers and doing a 1e-16 delta in imaginary space. It may allow for easier computation of full Jacobians and Hessians. 

**Will this change the current api? How?**
I'm not sure.

**Who will benefit with this feature?**
People who need full rank Jacobian/Hessians.

**Any Other info.**
Reference paper:

http://mdolab.engin.umich.edu/sites/default/files/Martins2003CSD.pdf

The trick is basically to convert real numbers into complex numbers, and doing a small delta in imaginary space. Since many of tensorflow's ops already naturally support complex numbers, this seems like something that's doable.
"
25217,Merge #24666 into 1.13 release,"In #24666 support for OpenSSL 1.1.0 was added, which fixes a major issue that caused TF to be unbuildable in a number of environments. Unfortunately that PR was merged too late to make it into the 1.13 release candidate, and yet without it another version of TF will go by unbuildable for me.

Is there any chance this patch can be merged into the next RC and make it onto the final 1.13 release? I think it's a small change, overall, that ends up being the difference between being able to use TF, or not at all for me and other running more up-to-date environments.

I wasn't sure what the most appropriate way to communicate this request was, hopefully posting this issue is ok.

Thank you!"
25216,tf.contrib.nn.alpha_dropout returns tensor with no shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7

**Describe the current behavior**
I tried out selu activation and also used alpha_dropout for that. When wrapping tf.contrib.nn.alpha_dropout() around my dense layer, a Tensor with no shape will be returned and therefore I can't feed the next layer with this tensor.

A solution for this can be to assign the input shape to the returned tensor, like in the tf.nn.dropout() Layer."
25215,lite: non-standard variable-length arrays,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit [01dec19](https://github.com/tensorflow/tensorflow/tree/01dec19e827ab46ff7ff5dbd6142ba5a26efc4da)
- GCC/Compiler version (if compiling from source): gcc 8.2.1 20181127

**Describe the problem**

Structures `TfLiteIntArray` and `TfLiteFloatArray` have flexible array member `data`, which is not supported by the C++ standard. The header file name ""c_api_internal.h"" suggests that it's ""C API"" and ""internal API"", but it gets indirectly included by user C++ code via ""interpreter.h"". Although in practice it may not cause problems other than compiler warnings, it may still worth rewriting in standard C++ if the fix is easy and doesn't interfere with other code.

I propose using arrays of length 1:

    typedef struct {
      int size;
      int data[1];
    } TfLiteIntArray;

    typedef struct {
      int size;
      float data[1];
    } TfLiteFloatArray;

The dynamic size then can be calculated as follows:

    offsetof(TfLiteIntArray, data) + sizeof(int) * size

and

    offsetof(TfLiteFloatArray, data) + sizeof(float) * size

I checked that the dynamic size is always obtained via the functions `TfLiteIntArrayGetSizeInBytes` and `TfLiteFloatArrayGetSizeInBytes`. So only these functions would need to be changed, and no other code is affected.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Compile tensorflow/tensorflow/lite/examples/minimal/minimal.cc with -Wpedantic."
25214,lite: incorrectly written comparison in slice.cc,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): commit [01dec19](https://github.com/tensorflow/tensorflow/tree/01dec19e827ab46ff7ff5dbd6142ba5a26efc4da)
- GCC/Compiler version (if compiling from source): gcc 8.2.1 20181127

**Describe the current behavior**

An assertion in [tensorflow/lite/kernels/slice.cc](https://github.com/tensorflow/tensorflow/blob/01dec19e827ab46ff7ff5dbd6142ba5a26efc4da/tensorflow/lite/kernels/slice.cc#L120):

    TF_LITE_ENSURE(context, NumDimensions(begin) == NumDimensions(size) == 1);

It evaluates the first `==` and then compares the result (true or false) with 1. It's probably not what's intended.

**Describe the expected behavior**

Based on the description of Slice, I believe, it should be:

    TF_LITE_ENSURE(context, NumDimensions(begin) == 1 && NumDimensions(size) == 1);"
25213,Build tensorflow on windows with mkl fails,"**System information**
- OS Platform: windows server 2012 R2 standard
- TensorFlow build from source:
- TensorFlow version: r1.13
- Python version: 3.6.7
- Bazel version: 0.21
- Compiler version (if compiling from source): 15.4


I'm trying build tf on windows with mkl running this command:
```
bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package
```

build failed with this error:
```
INFO: From Executing genrule //tensorflow/python:pywrap_tensorflow_filtered_def_file:
symbols=108196, taken=36183, dupes=0
ERROR: C:/tfb/tensorflow/tensorflow/lite/toco/python/BUILD:44:1: Linking of rule '//tensorflow/lite/toco/python:_tensorflow_wrap_toco.so' failed (Exit 1000): link.exe failed: error executing command
  cd C:/users/awaizman101364/_bazel_awaizman101364/5eebsbmp/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.16299.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.16299.0\um\x
64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Windows\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\
10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/AWaizman101364/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/AWaizman101364/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\AWAIZM~1\AppData\Local\Temp\2
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\AWAIZM~1\AppData\Local\Temp\2
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/python/_tensorflow_wrap_toc
o.so-2.params /OPT:ICF /OPT:REF
Execution platform: @bazel_tools//platforms:host_platform
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored

bazel-out/x64_windows-opt/bin/external/com_google_absl/absl/numeric/int128.lib : fatal error LNK1000: Internal error during CImplib::EmitThunk

  Version 14.00.24210.0

  ExceptionCode            = C0000005
  ExceptionFlags           = 00000000
  ExceptionAddress         = 00007FF787836896 (00007FF787820000) ""C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\link.exe""
  NumberParameters         = 00000002
  ExceptionInformation[ 0] = 0000000000000000
  ExceptionInformation[ 1] = 0000000000000008

CONTEXT:
  Rax    = 0000000000000000  R8     = 00007FF78791FBE0
  Rbx    = 0000000000000000  R9     = 00007FF78791E9F0
  Rcx    = 0000000000000000  R10    = 0000000000000000
  Rdx    = 00007FF78791FBD8  R11    = 0000000000000000
  Rsp    = 00000084E2C2E1B8  R12    = 00007FF7878ED950
  Rbp    = 00000084E4E5D5A0  E13    = 0000000000000000
  Rsi    = 0000000000008000  R14    = 0000000000000000
  Rdi    = 00000084E57B2100  R15    = 0000000000000000
  Rip    = 00007FF787836896  EFlags = 0000000000010246
  SegCs  = 0000000000000033  SegDs  = 000000000000002B
  SegSs  = 000000000000002B  SegEs  = 000000000000002B
  SegFs  = 0000000000000053  SegGs  = 000000000000002B
  Dr0    = 0000000000000000  Dr3    = 0000000000000000
  Dr1    = 0000000000000000  Dr6    = 0000000000000000
  Dr2    = 0000000000000000  Dr7    = 0000000000000000
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 606.229s, Critical Path: 369.31s
INFO: 4122 processes: 4122 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

Is there a way to build tf with mkl on windows?

Thanks."
25212,Keras API reproducibility,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic (Google Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): 2.0.0-dev20190125 (module 'tensorflow' has no attribute 'GIT_VERSION', module 'tensorflow' has no attribute 'VERSION')
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Everytime I build a keras model, the model is initialized differently. I have Googled and applied all methods I have found, including seeding seeds for environment variable, random package, np.random package, and tf.random. (I used Google Colab.)

**Describe the expected behavior**
Since I have set all seeds, I should get the same initialization every time I run the same code.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Run the following code in Google Colab:
```
!pip install tf-nightly-2.0-preview
import os
os.environ['PYTHONHASHSEED'] = '0'
import random
random.seed(0)
import numpy as np
np.random.seed(0)
import tensorflow as tf
tf.random.set_seed(0)
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
dense = layers.Dense(1, input_shape=(1,),
        kernel_initializer=keras.initializers.GlorotUniform(seed=0),
        bias_initializer=keras.initializers.Constant(value=0.))
print(hash(dense))
```
Sample output:
```
2.0.0-dev20190125
-9223363310562421858
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
25211,"Using transform_graph to quantize yolo-tiny-v2, performance & mAP drops. ","I am using tranform_graph to quantize darkflow's tiny-yolo-v2 *.pb
[https://github.com/thtrieu/darkflow](https://github.com/thtrieu/darkflow)

found several issues
1. when i use video to do inference (or images) the performance drops to very low with gpu enable
from **5.xx fps to 1.xx fps**, and i can feel the images inference speed drops, by input with lots of pictures.

2. after use tranform_graph's quantization, the mAP drops from **68% to 13%** (with my 500 pics selcted from pascal_voc2012)
the function i used list below
```
../tensorflow-src/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=_151875_12_28_02_36_51_tiny-yolo-v2.pb  --inputs='input' --outputs='output' --out_graph=_151875_12_28_02_36_51_tiny-yolo-v2.quantize_node.pb --transforms='remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  merge_duplicate_nodes
  sort_by_execution_order'
```

3. after quantize mAP drops, i use ""insert_logging"" & ""freeze_requantization_ranges""
tried to bring back mAP with that pascal_voc selected 500 pictures
but the mAP drops to **0%.**

4. i wrote a script try to fix the logging file
I go thorough logging with single image, one by one 
if the mAP goes up, i merge with the last highest mAP ...
until the last image
so comes out with a valid logging file, which bring back the mAP to **62%** 


now the questions are
1. is that possible to quantize yolo-tiny-v2?
because the mAP and performance drops so much (even thou the mAP can be adjust back)
2. are there any other neural network models that you suggest me to use? 
my target is to create some quantized small models for running on ic chips
3. does that means logging and freeze_requantization_ranges might not suitable for tiny-yolo-v2?

Thanks for your time for reading this


== cat /etc/issue ===============================================
Linux 8fbb96ca119d 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 8fbb96ca119d 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                 1.15.4
protobuf              3.6.1
tensorflow-gpu        1.12.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sat Jan 26 05:56:00 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro GV100        Off  | 00000000:18:00.0 Off |                  Off |
| 40%   54C    P0    50W / 250W |      1MiB / 32508MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro GV100        Off  | 00000000:3B:00.0 Off |                  Off |
| 40%   54C    P0    50W / 250W |      0MiB / 32508MiB |      5%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176



"
25210,"tf-gpu's conda Installation works with python 3.6, but not with python 2.7","
Hi, I installed tensorflow-gpu on the same machine using Conda.

Python 3.6 version works:

$ python
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
2019-01-25 22:38:46.581588: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
2019-01-25 22:38:46.788700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.29GiB
2019-01-25 22:38:46.788736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-25 22:38:54.632438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-25 22:38:54.632511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2019-01-25 22:38:54.632550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2019-01-25 22:38:54.633135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10927 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1
2019-01-25 22:38:55.230804: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1

But Python 2.7 version doesn't work (I tried with versions 1.12.0 + 1.10.0):

$ python
Python 2.7.15 |Anaconda, Inc.| (default, Dec 14 2018, 19:04:19) 
[GCC 7.3.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
2019-01-25 22:41:40.507815: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
2019-01-25 22:41:41.043892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 415.94MiB
2019-01-25 22:41:41.043925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2019-01-25 22:41:41.044186: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/vraunak/anaconda2/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/vraunak/anaconda2/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.

How could I make tf work with python 2.7? Is any specific version required?
"
25209,Error on using feature_columns with tf.keras,"


**System information**
- TensorFlow version: 1.12
- Doc Link: https://github.com/tensorflow/docs/blob/b4d8d7096099c2b0a7df6a0564bf6eca8c96c4a0/site/en/tutorials/structured_data/feature_cols_keras.ipynb

**Describe the documentation issue**
when comment `tf.enable_eager_execution()`, the demo code fails. 

`ValueError: logits and labels must have the same shape ((32, 1) vs (32,))`



"
25208,ImportError: DLL load failed: The specified module could not be found.,"ImportError                               Traceback (most recent call last)
~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\envs\tensorflow\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\envs\tensorflow\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-4-cefece728b07> in <module>
----> 1 import tensorflow as tf
      2 import numpy as np
      3 
      4 matrix1 = np.array([(2,2,2),(2,2,2),(2,2,2)],dtype = 'int32')
      5 matrix2 = np.array([(1,1,1),(1,1,1),(1,1,1)],dtype = 'int32')

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\david\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\david\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\david\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\david\Anaconda3\envs\tensorflow\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\david\Anaconda3\envs\tensorflow\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
25207,ImportError: cannot import name abs,"Hi, I am getting the error while importing tensorflow:

$ python
Python 2.7.15 |Anaconda custom (64-bit)| (default, Dec 14 2018, 19:04:19) 
[GCC 7.3.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 88, in <module>
    from tensorflow.python import keras
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 36, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/engine/__init__.py"", line 23, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 35, in <module>
    from tensorflow.python.keras import backend
  File ""/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name abs

Removing protobuf and reinstalling still gives the same error.

**System information**
- Linux CentOS
- Python version: 2.7 tensorflow 1.12.0 
- Installed using conda"
25200,tf.keras load_model fails to load a model created with Keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Win7 same issue on Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):   b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
- Python version:  Python 3.6.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: CUDA Toolkit 9.0 (Sept 2017) cuDNN v7.0.5 (Dec 5, 2017)
- GPU model and memory: device: 0, name: Quadro M4000, compute capability: 5.2 memory_limit: 7801630311
                                            device: 1, name: Quadro M4000, compute capability: 5.2 memory_limit: 7801630311
- Keras version: 2.2.4

I am trying to load a model saved with Keras (not tf.keras) to create an Estimator.    The Keras model is a slightly modified version of Inception_V3 from the Keras model library.  (The modification is that the final layer was replaced with a dense layer and sigmoid to use the model for regression.)  

A co-worker was able to load a standard (unmodified) Keras model with tf.keras.  So I believe the intended behavior is that all Keras created models are loadable.  Please let me know if I have a misunderstanding about the intended functionality.

**Describe the current behavior**
Notebook1: Save the Keras model like this (abbreviated steps):

from keras.applications.inception_v3 import InceptionV3
base_model =InceptionV3(include_top=True, weights=viewClassWeightsFile, input_shape=input_shape, classes=nb_classes)
base_model.layers.pop() 
base_model.outputs = [base_model.layers[-1].output]
base_model.layers[-1].outbound_nodes=[]
output = base_model.get_layer('avg_pool').output
output = Dense(activation=""sigmoid"", units=num_regression_targets)(output)
new_model = Model(base_model.input, output)

new_model.save(modelSaveFile) 

Notebook2: Load the Keras model with tf.keras
new_model = tf.keras.models.load_model(modelSaveFile)

AttributeError: module 'tensorflow.python.keras.backend' has no attribute 'slice'

**Describe the expected behavior**

I expected that tf.keras.models.load_model() should be able to load a model created witih Keras.  

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is full traceback:

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-0e6b1a78cac7> in <module>
      6 modelSaveFile = modelDir+'\\model_singlePt_mix6_mlx100_set2.h5'
      7 
----> 8 base_model = keras.models.load_model(modelSaveFile)
      9 
     10 

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\engine\saving.py in load_model(filepath, custom_objects, compile)
    228       raise ValueError('No model found in config file.')
    229     model_config = json.loads(model_config.decode('utf-8'))
--> 230     model = model_from_config(model_config, custom_objects=custom_objects)
    231 
    232     # set weights

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\engine\saving.py in model_from_config(config, custom_objects)
    308                     '`Sequential.from_config(config)`?')
    309   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
--> 310   return deserialize(config, custom_objects=custom_objects)
    311 
    312 

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\layers\serialization.py in deserialize(config, custom_objects)
     62       module_objects=globs,
     63       custom_objects=custom_objects,
---> 64       printable_module_name='layer')

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    171             custom_objects=dict(
    172                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 173                 list(custom_objects.items())))
    174       with CustomObjectScope(custom_objects):
    175         return cls.from_config(config['config'])

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\engine\network.py in from_config(cls, config, custom_objects)
   1300         if layer in unprocessed_nodes:
   1301           for node_data in unprocessed_nodes.pop(layer):
-> 1302             process_node(layer, node_data)
   1303 
   1304     name = config.get('name')

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\engine\network.py in process_node(layer, node_data)
   1258       if input_tensors:
   1259         if len(input_tensors) == 1:
-> 1260           layer(input_tensors[0], **kwargs)
   1261         else:
   1262           layer(input_tensors, **kwargs)

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    767 
    768       if not in_deferred_mode:
--> 769         outputs = self.call(inputs, *args, **kwargs)
    770         if outputs is None:
    771           raise ValueError('A layer\'s `call` method should return a Tensor '

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\layers\core.py in call(self, inputs, mask)
    715       arguments['mask'] = mask
    716     print(self.function.__name__) # added by Cindy
--> 717     return self.function(inputs, **arguments)
    718 
    719   def compute_mask(self, inputs, mask=None):

c:\users\wolvci10\appdata\local\continuum\anaconda3\envs\tf_1p9\lib\site-packages\tensorflow\python\keras\layers\core.py in get_slice(data, i, parts)
    193         stride = K.concatenate([step, input_shape * 0], axis=0)
    194         start = stride * i
--> 195         return K.slice(data, start, size)
    196 
    197     # Relocate the model definition under CPU device scope if needed

AttributeError: module 'tensorflow.python.keras.backend' has no attribute 'slice'

"
25198,"Build from source in RelWithDebInfo build type, get a error : LNK 1248 image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF)","**System information**
- OS:  Windows10 64bit pro  32g RAM
- TensorFlow version:  r1.10
- Python version:  3.5.2
- cmake version : 3.6.3
- GCC/Compiler version : VS2015 update3
- CUDA/cuDNN version: CUDA9.0 Cudnn7.0
- GPU model and memory:GTX1080 8G
- sigwin version :3.0.10

**Describe the problem**
I have built from source in Release build type successfuly.
But when I try to build it in RelWithDebInfo , many errors occurred :
Fatal error LNK1248: image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF) 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I used cmake-gui 3.6.3 to generate the project with tensorflow_ENABLE_GPU is ON and tensorflow_BUILD_SHARED_LIB is ON

2. I used the VS2015 x64 x86 cross tools Command Prompt to open the tensorflow.sln

3. Change build type from Debug to RelWithDebInfo  , Choose Build->ALL_BUILD.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Fatal error LNK1248: image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF) 
tf_core_kernels	D:\tensorflow-r1.10\tensorflow-r1.10\tensorflow\contrib\cmake\build\tf_core_kernels.dir\RelWithDebInfo\tf_core_kernels.lib

Fatal error LNK1248: image size(100206D1B) exceeds maximum allowable size (FFFFFFFF) tensorflow_static	D:\tensorflow-r1.10\tensorflow-r1.10\tensorflow\contrib\cmake\build\RelWithDebInfo\tensorflow_static.lib

Fatal error LNK1248: image size(100062160)exceeds maximum allowable size(FFFFFFFF)	pywrap_tensorflow_internal_static	D:\tensorflow-r1.10\tensorflow-r1.10\tensorflow\contrib\cmake\build\RelWithDebInfo\pywrap_tensorflow_internal_static.lib
"
25197,tensor_shape.cc : Check failed: 0 <= new_num_elements (0 vs. -1),"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: raspberry-pi 3B
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.11.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.2
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
First you should know that this piece of code is compiled with the GCC flag : `add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)` whenever I am compiling for the raspberry-pi target because I was not able to build tensorflow for ARM32bit with the new ABI version.

I'm having troubles running a piece of code on my raspberry-pi that actually works just fine on my local computer. The error comes from the declaration of a tensor::TensorShape supposed to contain the dimension of my input tensor, prior to filling a tensorflow::Tensor with my data contained in a cv::Mat.
It crashes on the declaration line with the error
```
F tensorflow/core/framework/tensor_shape.cc:249] Check failed: 0 <= new_num_elements (0 vs. -1)
```
It seems I am giving a wrong input dimension. Which is weird as the exact same line works juste fine on my local machine : 
```
auto shape = tensorflow::TensorShape({1,64,64,1});
```
(sparing you the variable names). The 64 are uint16_t while the 1s are int on 4 bytes.
Could the error come from the 32bit architecture of my raspberry-pi ? Looking at tensor_shape.cc
```
template <class Shape>
void TensorShapeBase<Shape>::AddDim(int64 size) {
  if (!kIsPartial) CHECK_GE(size, 0);
  if (unknown_rank()) return;
  CHECK_LT(ndims_byte(), MaxDimensions()) << ""Too many dimensions in tensor"";
  int64 new_num_elements;
  if (kIsPartial && (num_elements() < 0 || size < 0)) {
    new_num_elements = -1;
  } else {
    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);
    CHECK_LE(0, new_num_elements);
  }
  UnsafeAddDim(size, new_num_elements);
}
```
The (crashing) 249th line is the check : `CHECK_LE(0, new_num_elements);` so it looks that something doesn't pass....

**Code to reproduce the issue**
Compile tensorflow v1.11.0 from source for raspberry-pi target architecture with gcc 6.3.0. Then simply try to declare a tensorshape.

**Other info / logs**
The only error line was already reported.
"
25196,How to do distributed training with multi-gpus,"Hi, I got a problem of doing synchronous training on distributed training. I want to use multi-machines with multi-GPUs on distributed training. But I can't find any material about it. Is here any one giving me some help? Thanks a lot.

Here is part of my code.

    with tf.device(""/job:worker/task:%d"" % FLAGS.task_index, cluster=cluster):
            global_step = tf.Variable(0, name=""global_step"", trainable=False)
            alpha = dynamic_lrate(hparams, global_step)
            optimizer = tf.contrib.opt.LazyAdamOptimizer(alpha, beta1=0.9, beta2=0.997, epsilon=1e-9)
            rep_op = tf.train.SyncReplicasOptimizer(optimizer,
                                                    replicas_to_aggregate=len(worker_hosts),
                                                    total_num_replicas=len(worker_hosts),
                                                    use_locking=True)

            """"""in RNNModel contains multi-gpus""""""
            model = RNNModel(None, hparam_list, train_ds, tf.estimator.ModeKeys.TRAIN, rep_op, global_step)
            valid_model = RNNModel(model.graph, hparam_list, valid_ds, tf.estimator.ModeKeys.PREDICT, rep_op, global_step)

            train_op = model.train_op

            init_token_op = rep_op.get_init_tokens_op()
            chief_queue_runner = rep_op.get_chief_queue_runner()

            saver = saver_mod.Saver(max_to_keep=hparams.max_save,
                            var_list=model.all_vars)

            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index==0),
                                     logdir=log_dir,
                                     saver=saver,
                                     global_step=global_step,
                                     save_summaries_secs=3600,
                                     save_model_secs=hparams.sfreq)

            with sv.prepare_or_wait_for_session(server.target, config=config) as sess:
                if FLAGS.task_index == 0 and issync == 1:
                    sv.start_queue_runners(sess, [chief_queue_runner])
                    sess.run(init_token_op)

                #tf.global_variables_initializer().run()


                idx = 0
                max_bleu_score = -10.0
                best_path = os.path.join(log_dir, ""model_best"")
                total_accu = 0.0
                total_size = 0
                for epoch in range(FLAGS.maxsteps):
                    training_step = sess.run([global_step])
                    step_num = int(training_step[0])

                    res = sess.run([model.tower_print_loss,
                                    model.tower_accu,
                                    #model.tower_print_loss_2,
                                    #model.tower_accu_2,
                                    model.batch_size,
                                    #model.fsrc,
                                    #model.ftgt,
                                    train_op
                                    ])"
25195,Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: CUDA 10, cudnn 7.4.2
- GPU model and memory: 2 x RTX 2080 Ti

**Describe the problem**

During the build process described on the tensorflow website for r1.13, the command bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... fails due to not finding the package '@com_google_protobuf//'. The full error message is:

```
ERROR: /media/titan/resources/oss/machine-learning/Google/tensorflow/tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /media/titan/resources/oss/machine-learning/Google/tensorflow/tensorflow/tensorflow/compiler/tests/build_defs.bzl:94:20
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Create new conda environment
2. Install Bazel using binary installer
3. Install dependencies for the build
4. Run `./configure` in the cloned repo with `r1.13` checked-out
5. Run `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`

Then I encounter the mentioned error.
"
25193,TensorflowLite GPU + DeepLab porting,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12
- Python version:2.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0
- GPU model and memory:k80 + 12GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I tried to work Tensorflowlite +gpu + deeplabv3_257_mv_gpu.tflite, but not working
runInference was success.
(1) how can use output (pixelClasses)? Can u give sample code? I attached my code.
(2) performance was very slow 600ms~900ms

[ImageSegmentation.zip](https://github.com/tensorflow/tensorflow/files/2795483/ImageSegmentation.zip)

```
  protected float[][][][] pixelClasses;
  pixelClasses = new float[1][getImageSizeX()][getImageSizeY()][21];
 Bitmap classifyFrame(Bitmap bitmap, SpannableStringBuilder builder) {
        if (tflite == null) {
            Log.e(TAG, ""Image classifier has not been initialized; Skipped."");
            builder.append(new SpannableString(""Uninitialized Classifier.""));
            return bmp;
        }
        convertBitmapToByteBuffer(bitmap);
        long startTime = SystemClock.uptimeMillis();
        runInference();
        long endTime = SystemClock.uptimeMillis();
        Log.d(TAG, ""Timecost to run model inference: "" + Long.toString(endTime - startTime));
        int batchNum = 0;
        float[][][][] output = pixelClasses;
         for(int x = 0; x < getImageSizeX(); x++) {
            for (int y = 0; y < getImageSizeY(); y++) {
                pixels[x * bmp.getHeight() + y] = argb(100, (int)(output[batchNum][y][x][2]), (int)(output[batchNum][y][x][1]), (int)(output[batchNum][y][x][0]));
            }
        }
        bmp.setPixels(pixels, 0, bmp.getWidth(), 0, 0, bmp.getWidth(), bmp.getHeight());
        return bmp;
    }
```

**Describe the expected behavior**
overlay color.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
25191,"convert densenet169 model to .tflite model fails, report lack min/max data","### System information
- TensorFlow-gpu 1.12
-OS Platform e.g., Linux Ubuntu 18.
-Python version3.5
- gtx1070
-bash scripts:
echo 'starting convert fake model into tflite model:'${model_name}.....

    tflite_path=""./models/${model_name}/fake/model.tflite""

    test -f ${tflite_path} || tflite_convert --output_file=./models/${model_name}/fake/model.tflite \
                    --graph_def_file=./models/${model_name}/fake/model.pb \
                        --inference_type=QUANTIZED_UINT8 \
                            --input_arrays=input \
                            --output_arrays=${model_output} \
                            --mean_values=${mean_values} \
                            --std_dev_values=${std_dev_values}

-report nessage:
Traceback (most recent call last):
  File ""/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b""2019-01-24 11:24:42.409109: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2871 operators, 4306 arrays (0 quantized)\n2019-01-24 11:24:42.496442: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2871 operators, 4306 arrays (0 quantized)\n2019-01-24 11:24:43.560215: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 514 operators, 1027 arrays (1 quantized)\n2019-01-24 11:24:43.575464: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 514 operators, 1027 arrays (1 quantized)\n2019-01-24 11:24:43.584665: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 430 operators, 943 arrays (1 quantized)\n2019-01-24 11:24:43.594373: F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array densenet169/dense_block1/conv_block1/x1/BatchNorm/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array densenet169/dense_block1/conv_block1/x1/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\nAborted (core dumped)\n""
None


when i convert the densenet .pb file to .tflite and i failed ,maybe that because lack of min/max value but i not sure how to fix this ,i there anyone who have get this problem or have any solution for this problem ?please let me know and i will appriciate it."
25190,How to obtain testing_list.txt ?,"Dear,
In the [project,](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands)
the testing_list.txt in data_dir file is not written in training data,it existed before,
so how to write the testing_list.txt with my data?
Thx"
25187,IExecutionContext in TRTEngineOp is non thread-safe. ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home: CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11
- Python version: 2.7
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): gcc 5.4
- CUDA/cuDNN version: cuda-10.0, cudnn 7.3, tensorrt 5.0
- GPU model and memory: Nvidia P40


**Describe the Problem**
We are using DirectSession to do inference with c++ code in gpu environment. The model using TRTEngineOp to improve performance. But TRTEngineOp maintain a engine_map to process input.  This map used to keep tensorrt engines and their tensorrt execution context for given batch size. 
The tensorrt engine(nvinfer1::ICudaEngine) is thread safety. But the tensorrt execution context(nvinfer1::IExecutionContext) is non thread-safe. So it will crashed when session processing two same batch_size task in the same time.

**Describe the expected behavior**
We hope the engine_map in TRTEngineOp can maintain a pair, which the first value can be tensorrt engine.  the sescond value can be a tensorrt context pool.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Crashed when session process two task in the same time.
<pre><code>
2019-01-25 11:57:59.952217: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-01-25 11:57:59.952296: F external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
2019-01-25 11:57:59.952430: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***











	clone
*** End stack trace ***
<code><pre>"
25185,"tf 2.0 preview installed successfully, unable to import tensorflow, ImportError: DLL load failed","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- TensorFlow version: tf 2.0 preview gpu
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10
- GPU model and memory: Nvidia GTX 1050

**Describe the problem**
Windows 10 Home, Nvidia 1050 gpu, tensorflow installed successfully using 
`pip install tf-nightly-gpu-2.0-preview`
However, I am unable to import tensorflow, please see log below.

**Any other info / logs**

```
(tf2wpy36) C:\Users\prach>python
Python 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 11:48:23) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import autograph
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\_api\v2\autograph\__init__.py"", line 20, in <module>
    from tensorflow._api.v2.autograph import experimental
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\_api\v2\autograph\experimental\__init__.py"", line 8, in <module>
    from tensorflow.python.autograph import Feature
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\prach\Anaconda3\envs\tf2wpy36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
```



"
25181,Accept h5py.Group in tf.keras save/load functions,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**
The save_model function in tf.keras.models currently accepts a filepath or a h5py.File model.
It would be nice if this was generalized to accepting h5py.Group objects. This makes it possible to store more than one model in one h5py file, or combine it with other objects.
This change has been implemented in keras.io as well (I'm not the OP), and it would be very nice to see this in TF.keras as well: 
* https://github.com/keras-team/keras/issues/10905 
* https://github.com/keras-team/keras/pull/10912

**Will this change the current api? How?**
save_model and load_model would accept filepaths and h5py.Group, which is more general.

**Who will benefit with this feature?**
Everybody using the keras h5py saving/loading options who wants to store complexer models such as ensembles.

Happy to provide a PR if this has a chance to be merged.

Best, Boris"
25179,how can I solve this error InvalidArgumentError during visualizing the output?,"I had the following code for implementing an autoencoder. I implement it and after learning phase, I want to visualize the output of each layer, but when I want to show the output of layer 5 and more it produces this error and I can not see the output of layers. I'm a little confused. if I do not feed a value, how does the network finish training? I also can show the output of layers before 5th layer. please help me with this problem.
  the error 

> InvalidArgumentError: You must feed a value for placeholder tensor
> 'input_78' with dtype float and shape [?,28,28,1] 	 [[{{node
> input_78}} = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1],
> _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]] 	 [[{{node lambda_35/add/_2359}} = _Recv[client_terminated=false,
> recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"",
> send_device=""/job:localhost/replica:0/task:0/device:GPU:0"",
> send_device_incarnation=1, tensor_name=""edge_50_lambda_35/add"",
> tensor_type=DT_FLOAT,
> _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

     

   ```
```
```
[ from keras.layers import Input, Concatenate, GaussianNoise,Dropout
    from keras.layers import Conv2D
    from keras.models import Model
    from keras.datasets import mnist
    from keras.callbacks import TensorBoard
    from keras import backend as K
    from keras import layers
    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras as Kr
    import numpy as np
    import pylab as pl
    import matplotlib.cm as cm
```
```
    
    #-----------------building w train---------------------------------------------
    w_main = np.random.randint(2,size=(1,4,4,1))
    w_main=w_main.astype(np.float32)
    w_expand=np.zeros((1,28,28,1),dtype='float32')
    w_expand[:,0:4,0:4]=w_main
    w_expand.reshape(1,28,28,1)
    w_expand=np.repeat(w_expand,49999,0)
    
    #-----------------building w test---------------------------------------------
    w_test = np.random.randint(2,size=(1,4,4,1))
    w_test=w_test.astype(np.float32)
    wt_expand=np.zeros((1,28,28,1),dtype='float32')
    wt_expand[:,0:4,0:4]=w_test
    wt_expand.reshape(1,28,28,1)
    wt_expand=np.repeat(wt_expand,9999,0)
    #-----------------------encoder------------------------------------------------
    #------------------------------------------------------------------------------
    wtm=Input((28,28,1))
    image = Input((28, 28, 1))
    conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='convl1e')(image)
    conv2 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl2e')(conv1)
    conv3 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl3e')(conv2)
    DrO=Dropout(0.25)(conv3)
    encoded =  Conv2D(1, (3, 3), activation='relu', padding='same',name='reconstructed_I')(conv3)
    
    
    #-----------------------adding w---------------------------------------
    #add_const = Kr.layers.Lambda(lambda x: x + Kr.backend.constant(w_expand))
    add_const = Kr.layers.Lambda(lambda x: x + wtm)
    encoded_merged = add_const(encoded)
    
    encoder=Model(inputs=image, outputs=encoded_merged)
    encoder.summary()
    
    #-----------------------decoder------------------------------------------------
    #------------------------------------------------------------------------------
    
    #encoded_merged = Input((28, 28, 2))
    deconv1 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl1d')(encoded_merged)
    deconv2 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl2d')(deconv1)
    deconv3 = Conv2D(16, (3, 3), activation='relu',padding='same', name='convl3d')(deconv2)
    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='decoder_output')(deconv3) 
    
    #decoder=Model(inputs=encoded_merged, outputs=decoded)
    #decoder.summary()
    model=Model(inputs=image,outputs=decoded)
    #----------------------w extraction------------------------------------
    convw1 = Conv2D(8, (3,3), activation='relu', padding='same', name='conl1w')(decoded)
    convw2 = Conv2D(4, (3, 3), activation='relu', padding='same', name='convl2w')(convw1)
    convw3 = Conv2D(2, (3, 3), activation='relu', padding='same', name='conl3w')(convw2)
    pred_w = Conv2D(1, (3, 3), activation='relu', padding='same', name='reconstructed_W')(convw3)  
    # reconsider activation (is W positive?)
    # should be filter=1 to match W
    watermark_extraction=Model(inputs=[image,wtm],outputs=[decoded,pred_w])
    
    
    #----------------------training the model--------------------------------------
    #------------------------------------------------------------------------------
    #----------------------Data preparesion----------------------------------------
    
    (x_train, _), (x_test, _) = mnist.load_data()
    x_validation=x_train[1:10000,:,:]
    x_train=x_train[10001:60000,:,:]
    #
    x_train = x_train.astype('float32') / 255.
    x_test = x_test.astype('float32') / 255.
    x_validation = x_validation.astype('float32') / 255.
    x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format
    x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format
    x_validation = np.reshape(x_validation, (len(x_validation), 28, 28, 1))
    
    #---------------------compile and train the model------------------------------
    # is accuracy sensible metric for this model?
    watermark_extraction.compile(optimizer='adadelta', loss={'decoder_output':'mse','reconstructed_W':'mse'}, metrics=['mae'])
    watermark_extraction.fit([x_train,w_expand], [x_train,w_expand],
              epochs=4,
              batch_size=128, 
              validation_data=([x_validation,wt_expand], [x_validation,wt_expand]),
              callbacks=[TensorBoard(log_dir='C:/tmp/autoencoder', histogram_freq=0, write_graph=False)])
    model.summary()
    #model.fit([images, w], [images, w], batch_size=64, epochs=5)
    
    #--------------------visuallize the output layers------------------------------
    inputs = [K.learning_phase()] + watermark_extraction.inputs
    
    _convout1_f = K.function(inputs, [watermark_extraction.layers[5].output])
    def convout1_f(X):
        # The [0] is to disable the training phase flag
        return _convout1_f([0] + [X])
    
    # utility functions 
    i = 4600
    
    # Visualize the first layer of convolutions on an input image
    X = x_test[i:i+1]    
    # Visualize weights
    W = model.layers[1].get_weights()[0][:,:,0,:]
    w1=W.reshape(16,3,3)
    W = np.squeeze(w1)
    print(""W shape : "", W.shape)
    
    for i in range(0,16):
        plt.subplot(4,4,i+1)
        plt.imshow(w1[i,:,:], interpolation='nearest',cmap='gray')
    plt.show()
    W = model.layers[2].get_weights()[0][:,:,0,:]
    w2=W.reshape(8,3,3)
    W = np.squeeze(w2)
    print(""W shape : "", W.shape)
    
    for i in range(0,8):
        plt.subplot(4,4,i+1)
        plt.imshow(w2[i,:,:], interpolation='nearest',cmap='gray')
    plt.show()
    
    
    # Visualize convolution result (after activation)
    C1 = convout1_f(X)
    C1 = np.squeeze(C1)
    print(""C1 shape : "", C1.shape)
    for i in range(0,C1.shape[2]):
        plt.subplot(4,4,i+1)
        plt.imshow(C1[:,:,i], interpolation='nearest',cmap='gray')
    plt.show()
```"
25178,Inconsistent behavior of sample weight for Keras,"TF: home compiled master from a few days ago
Keras: 2.2.4 (pypi)

Hi

Consider the following snippet
```python
import numpy as np
import tensorflow as tf
import keras
from tensorflow import keras as tf_keras

x = np.array([[1.], [2], [3]])
y = np.array([[1], [4], [5]])
w = np.array([0.1, 0., 0.1])

i_x = keras.layers.Input(shape=(1,))
model = keras.Model(inputs=i_x, outputs=i_x)

model.compile('sgd', 'mse')
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---> 0.2 = sum(w * squared_diff) / count(w not null)

model.compile('sgd', keras.losses.mean_squared_error)
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---> 0.2

i_x = tf_keras.layers.Input(shape=(1,))
model = tf_keras.Model(inputs=i_x, outputs=i_x)

model.compile('sgd', 'mse')
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---> 0.13333 = mean(w * squared_diff)

model.compile('sgd', keras.losses.mean_squared_error)
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---> 2.0 = mean(w * squared_diff) / mean(w)
```

This seems a bit all over the place. Keras behavior is cringeworthy as the loss is discontinuous in `w` . For w[1] = 1e-6, one gets 0.13333... but at least its consistent.

I don't know what should be done but I'm sure others have wasted half a day of work because of this ;)

"
25177,tf.function decorated training loop not working with tf-nightly-gpu-2.0-preview==2.0.0rc0,"Using the autograph example, as is, from https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/autograph.ipynb

When trying to run the 'Define the Training Loop' cell, I get this error
![image](https://user-images.githubusercontent.com/93366/51704290-80bae700-1fcd-11e9-9a99-8bfcc9da4e1f.png)

```
FailedPreconditionError: Error while reading resource variable _AnonymousVar3 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar3 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[{{node sequential/dense_1/MatMul/ReadVariableOp}}]]
	 [[ReduceDataset]]
	 [[ReduceDataset/_48]] [Op:__inference_train_915]
```

Tensorflow version: tf-nightly-gpu-2.0-preview==2.0.0rc0

System information:

== cat /etc/issue ===============================================
Linux 797af6742322 4.4.0-141-generic #167-Ubuntu SMP Wed Dec 5 10:40:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04.1 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 797af6742322 4.4.0-141-generic #167-Ubuntu SMP Wed Dec 5 10:40:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                            1.16.0
protobuf                         3.6.1
tensorflow-estimator-2.0-preview 1.13.0.dev2019012200

== check for virtualenv =========================================
False

== tensorflow import ============================================

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Jan 24 19:41:50 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN V             Off  | 00000000:01:00.0  On |                  N/A |
| 28%   38C    P8    27W / 250W |  11614MiB / 12033MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a
"
25176,Cannot build Tensorflow 1.12 On Windows 10 and CUDA 9.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): MSVC2015 
- CUDA/cuDNN version: 9.0.176 / 7.4.2
- GPU model and memory: Nvidia Geforce GTX 750 Ti/ 2GB DDR5
Cpu : AMD Athlon II x4 631


**Describe the problem**
So I've been using Anaconda to build Tensorflow 1.12, since my cpu is old and doesn't support AVX (i want tensorflow to run on gpu) i cant use the pre-built pip packages. 
I've installed bazel and msys64 correctly and added them to path directory. After configuring the configure.py and when trying to build, i get errors posted below.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


> WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
f:\tensorflow\tensorflow/.bazelrc
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: ce75c25f-4ec0-436f-91bb-da2ed1092a50
INFO: Build option --define has changed, discarding analysis cache.
WARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:3099:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:100:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: F:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 14335 targets configured).
INFO: Found 1 target...
ERROR: F:/tensorflow/tensorflow/tensorflow/core/BUILD:2497:1: ProtoCompile tensorflow/core/example/example_pb2.py failed (Exit -1073741795): protoc failed: error executing command
  cd C:/users/albis/_bazel_albis/a6so2axr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin
    SET PYTHON_BIN_PATH=F:/Anaconda3/envs/Tensorflow/python.exe
    SET PYTHON_LIB_PATH=F:/Anaconda3/envs/Tensorflow/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  bazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc --python_out=bazel-out/x64_windows-opt/genfiles/ -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python tensorflow/core/example/example.proto tensorflow/core/example/feature.proto tensorflow/core/framework/allocation_description.proto tensorflow/core/framework/api_def.proto tensorflow/core/framework/attr_value.proto tensorflow/core/framework/cost_graph.proto tensorflow/core/framework/device_attributes.proto tensorflow/core/framework/function.proto tensorflow/core/framework/graph.proto tensorflow/core/framework/graph_transfer_info.proto tensorflow/core/framework/kernel_def.proto tensorflow/core/framework/log_memory.proto tensorflow/core/framework/node_def.proto tensorflow/core/framework/op_def.proto tensorflow/core/framework/reader_base.proto tensorflow/core/framework/remote_fused_graph_execute_info.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/step_stats.proto tensorflow/core/framework/summary.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_description.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor_slice.proto tensorflow/core/framework/types.proto tensorflow/core/framework/variable.proto tensorflow/core/framework/versions.proto tensorflow/core/protobuf/config.proto tensorflow/core/protobuf/cluster.proto tensorflow/core/protobuf/debug.proto tensorflow/core/protobuf/device_properties.proto tensorflow/core/protobuf/queue_runner.proto tensorflow/core/protobuf/rewriter_config.proto tensorflow/core/protobuf/tensor_bundle.proto tensorflow/core/protobuf/saver.proto tensorflow/core/util/event.proto tensorflow/core/util/memmapped_file_system.proto tensorflow/core/util/saved_tensor_slice.proto tensorflow/core/example/example_parser_configuration.proto tensorflow/core/protobuf/checkpointable_object_graph.proto tensorflow/core/protobuf/control_flow.proto tensorflow/core/protobuf/meta_graph.proto tensorflow/core/protobuf/named_tensor.proto tensorflow/core/protobuf/saved_model.proto tensorflow/core/protobuf/tensorflow_server.proto tensorflow/core/protobuf/transport_options.proto tensorflow/core/util/test_log.proto
Execution platform: @bazel_tools//platforms:host_platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 18.245s, Critical Path: 4.43s
INFO: 2 processes: 2 local.
FAILED: Build did NOT complete successfully"
25175,Dropout layer is broken,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
- TensorFlow installed from (source or binary): conda default channel
- TensorFlow version (use command below): 1.12 (conda version mkl_py36h69b6ba0_0)
- Python version: 3.6.8

**Describe the current behavior**
The Dropout layer always acts as if it is in testing mode.

**Describe the expected behavior**
The Dropout layer switches between training and testing.

**Code to reproduce the issue**
The tutorial code available from the [hompage](https://www.tensorflow.org/tutorials/):

    import tensorflow as tf
    mnist = tf.keras.datasets.mnist
    
    (x_train, y_train),(x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    
    model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(512, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    
    model.fit(x_train, y_train, epochs=5)
    model.evaluate(x_test, y_test)

Setting the droprate to 0.999 yields the same performance as 0.2 (well above 95% acc). This is not possible, it means that the training/training switch inside the Dropout layer is broken, it is always returning the `inputs`, even during the training phase. 

With tensorflow 1.10, the tutorial script gives  the ""correct"" result which is ~11% accuracy with 99.9% droprate.


**Other info / logs**
It seems that either `tensorflow.keras.backend.learning_phase` is at the root of the problem, or `model.fit` doesn't correctly sets the training flag.
"
25174,Why does tensorflow element not forget origin after using .eval(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I am using tensorflow_probability (0.5.0) to sample points (random walk metropolis algorithm) that I want to use as inputs for my neural network. The sample points are Tensor(""mcmc_sample_chain/current_state:0"", shape=(), dtype=float32) so I use tf.eval() to get the actual float number. After multiplying the input with my first layer I get as an object Tensor(""mcmc_sample_chain/mh_bootstrap_results/rwm_bootstrap_results/cond/Add:0"", shape=(1, 16), dtype=float32).


**Describe the expected behavior**
I would expect that tf.eval() gives me a numpy float/array and therefore the ouput after multiplying with my first layer should result in something like Tensor(""Add_12:0"", shape=(1, 16), dtype=float32). Why does the numpy array/float ""remember its origin""?
"
25173,ImportError: DLL load failed: NÃ£o foi possÃ­vel encontrar o mÃ³dulo especificado.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10 64bits

- TensorFlow installed from (source or binary): anaconda
- TensorFlow version: 1.12.0
- Python version: 3.6.5 | Acaconda INC
- Installed using virtualenv? pip? conda?:conda

- CUDA/cuDNN version: none, there is no card board
- GPU model and memory: none



**Describe the problem**
I canÂ´t run keras installed by anaconda in a pc without car board.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
runfile('C:/Users/CAST/Desktop/train/IA_2.py', wdir='C:/Users/CAST/Desktop/train')
C:\ProgramData\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):

  File ""<ipython-input-1-1706b5d23745>"", line 1, in <module>
    runfile('C:/Users/CAST/Desktop/train/IA_2.py', wdir='C:/Users/CAST/Desktop/train')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/CAST/Desktop/train/IA_2.py"", line 16, in <module>
    import keras

  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: NÃ£o foi possÃ­vel encontrar o mÃ³dulo especificado.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
25172,Reason for big performance difference between TFLite and TF Mobile app? Protobuf and .tflite file as reason?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I used the DetectorActivity of android mobile demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and the TensorFlow Lite demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 LTS and LineageOS Android 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5
- TensorFlow installed from (source or binary): compiled from source with cuda 10 and bazel
- TensorFlow version ( use command below): in Python 2.7 (command python): (tf.GIT_VERSION, tf.VERSION) = ('v1.12.0-0-ga6d8ffae09', '1.12.0') in Python 3.6.7 (command python3): b'v1.9.0-rc2-5108-g4e06be5f8f' 1.12.0-rc0 
- Python version: Python 2.7.15rc1 and Python 3.6.7
- Bazel version (if compiling from source): either 0.20.0 or 0.21.0 (I don't know with which version I compiled the tensorflow installed in python but I used 0.20.0 in order to execute the bazel commands for the transform of the protobuf to the tflite file - I think it wasn't possible to use bazel 0.21 therefore)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: Cuda 10.0 und cuDNN 7.3.1
- GPU model and memory:  GeForce GTX 970, RAM total: 16345820 and RAM swap: 2097148

**Describe the current behavior**

Because the calculation of  the classification and the bounding boxes in the DetectorActivity (from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) with the model ssd_mobilenet_v1_android_export.pb (https://www.dropbox.com/home/Modelfile?preview=ssd_mobilenet_v1_android_export.pb) took 200ms - I rely to the runtime of the native method run in the TensorFlowInferenceInterface.class - I compared the runtime of the native method run() in the Interpreter.class of the classifier (no bounding boxes) from the tensorflow lite example app under https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo with the models ""mobilenet_v1_1.0_224.tflite"" and ""mobilenet_v1_1.0_224_quant.tflite"" that were autodonwloaded into asserts. So I assumed that that huge performance is because of the fact that the TFLite example app uses a model ending with .tflite and the mobile app a protobuf file. 
Is this a possible reason or is bounding box calculation such a expensive operation? On model zoo the ssd_mobilenet_v1 model should be provide results within ~30 ms. 

On https://www.tensorflow.org/lite/apis#loading_a_model_2 under the heading Running Model Inference I found a hint to https://www.tensorflow.org/lite/convert/cmdline_examples. This points out that you have to optimize you model. Can the fact that the .pb files aren't optimezed be another reason for the bad performance? 

**Describe the expected behavior**
I expected that the runtime for classification (and bounding box calculation) in Tensorflow lite and Tensorflow mobile demo app doesn't differ so much!
**Code to reproduce the issue**
1) download the two demo apps and import them to Android Studio
2) print you a timestamp before and after the execution of the following methods:
- Tensorflow Lite demo app: In the ImageClassifier.java in the method classifyFrame 

` long startTime = SystemClock.uptimeMillis();
    runInference();
    long endTime = SystemClock.uptimeMillis();
    Log.d(TAG, ""Timecost to run model inference: "" + Long.toString(endTime - startTime));`

- Tensorflow Mobile demo app: in DetectorActivity in the method processImage() runInBackground {} put the code 
`                        final long startTime = SystemClock.uptimeMillis();
                        final List<Classifier.Recognition> results = detector.recognizeImage(croppedBitmap);
                        lastProcessingTimeMs = SystemClock.uptimeMillis() - startTime;` 
around the recognizeImage() method.

**Other info / logs**

"
25171,Runtime error: failed to find input Node 'image_tensor' after conversion from protobuf to tflite model file,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I used the DetectorActivity of android mobile demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and the TensorFlow Lite demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 LTS and LineageOS Android 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5
- TensorFlow installed from (source or binary): compiled from source with cuda 10 and bazel
- TensorFlow version ( use command below): in Python 2.7 (command python): (tf.GIT_VERSION, tf.VERSION) = ('v1.12.0-0-ga6d8ffae09', '1.12.0') in Python 3.6.7 (command python3): b'v1.9.0-rc2-5108-g4e06be5f8f' 1.12.0-rc0 
- Python version: Python 2.7.15rc1 and Python 3.6.7
- Bazel version (if compiling from source): either 0.20.0 or 0.21.0 (I don't know with which version I compiled the tensorflow installed in python but I used 0.20.0 in order to execute the bazel commands for the transform of the protobuf to the tflite file - I think it wasn't possible to use bazel 0.21 therefore)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: Cuda 10.0 und cuDNN 7.3.1
- GPU model and memory:  GeForce GTX 970, RAM total: 16345820 and RAM swap: 2097148


**Describe the current behavior**

Because I assumed that protobuf files were much slower than .tflite files I tried to converted a .pb to a .tflite:
Thus I downloaded the r1.95 branch of Tensorflow and converted the frozen_inference_graph.pb from (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) to a .tflite using https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630 and https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-456853685. This worked well!

The .pb file worked well with my Android App, but after copying the .tflite model to the app/assets directory of the TensorFlow Mobile demo App (from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and replacing the .pb file in the code with
`   private static final DetectorMode MODE = DetectorMode.TF_OD_API;
   
    private static final int TF_OD_API_INPUT_SIZE = 300;
    private static final String TF_OD_API_MODEL_FILE =
            ""file:///android_asset/frozen_inference_graph.pb"";
    private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/coco_labels_list.txt"";`

the following runtime error appears:

`
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: myPackage.myProcess, PID: 15491
    **java.lang.RuntimeException: Failed to find input Node 'image_tensor'**
        at myPackage.myProcess.myClass.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:106)
        at myPackage.myProcess.myClass.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:146)
        at myPackage.myProcess.myClass.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:370)
        at myPackage.myProcess.myClass.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:412)
        at myPackage.myProcess.myClass.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:419)
        at myPackage.myProcess.myClass.CameraConnectionFragment.access$000(CameraConnectionFragment.java:66)
        at myPackage.myProcess.myClass.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:97)
        at android.view.TextureView.getHardwareLayer(TextureView.java:390)
`
I think there is a similar issue #22565 .

**Describe the expected behavior**
I would have expected that the .tflite version works because the .pb version of the same ssd_model works well!
**Code to reproduce the issue**
1) Download the Tensorflow Mobile Demo App from the Link from above
2) Compile the tensorflow 1.12 version from source using the mentioned cuda settings
3) Download the model from model zoo
4) Execute the bazel commands from https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630 and https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-456853685


# Download and extract SSD MobileNet model
wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz
tar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz 
DETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb
STRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb
DETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite

# Strip out problematic nodes before even letting TOCO see the graphdef
bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \
--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \
--input_names=Preprocessor/sub --output_names=concat,concat_1 \
--alsologtostderr

# Run TOCO conversion.
bazel run tensorflow/lite/toco:toco -- \
--input_file=$STRIPPED_PB --output_file=$DETECT_FB \
--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \
--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr

# Build and install the demo
bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo
adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk


5) Then you have to copy the .tflite model to you apps app/assert directory and refer to it in you code like mentioned above
6) After the gradle build and running the app on you phone you might probably get the runtime error I did. 

**Other info / logs**

[bug_tracker_bazel_run_warnings.txt](https://github.com/tensorflow/tensorflow/files/2792167/bug_tracker_bazel_run_warnings.txt)
[bug_tracker_runtime_error.txt](https://github.com/tensorflow/tensorflow/files/2792169/bug_tracker_runtime_error.txt)

"
25170,TFLite model returns completely wrong results.,"**System information**
- Have I written custom code: YES
- OS Platform and Distribution : Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip (`pip install tf-gpu-nightly`)
- TensorFlow version (use command below): b'v1.12.0-6574-gee5c4383f9' 1.13.0-dev20190124
(Installed `tf-gpu-nigtly` version about two weeks ago, so decided to test everything again in current nigly version and problem remains).
- Python version: 3.6.7
- CUDA/cuDNN version:
   CUDA: 10.0.130  (but have also 9.0.176 installed)
   cuDNN: 7.4.2
- GPU model and memory:  GeForce GTX 1070 

**Intro**

Hi , 

I try to train resnet using `tf.keras` API, convert it to `Lite` version and then later use it on embedded system. 
I have struggled to convert model to Lite version, but finally when I managed to do it, the model converted seems to return completely different results. Below I provide simple example to reproduce the problem. I also highlited one particular line `tf.keras.backend.set_learning_phase(1)` and described there why it is important

**Toy example to reproduce the problem**

```
import numpy as np
import tensorflow as tf
from scipy import signal


DATA_SHAPE = (50, 100, 1)  # single channel image


def prepare_simple_resnet(n_filters=10):

    parameters_input = tf.keras.layers.Input(shape=DATA_SHAPE)

    x = parameters_input
    res_node = tf.keras.layers.Conv2D(n_filters, kernel_size=(3, 3), padding='same', use_bias=False)(x)
    res_node = tf.keras.layers.ReLU()(res_node)
    res_node = tf.keras.layers.BatchNormalization()(res_node)
    res_node = tf.keras.layers.Conv2D(n_filters, kernel_size=(3, 3), padding='same', use_bias=False)(res_node)
    res_node = tf.keras.layers.ReLU()(res_node)
    res_node = tf.keras.layers.BatchNormalization()(res_node)

    x = tf.keras.layers.add([res_node, x])

    x = tf.keras.layers.Lambda(lambda z: tf.math.reduce_mean(z, axis=[1, 2]))(x)   # mean for each filter
    x = tf.keras.layers.Dense(2)(x)  # output two classes

    model = tf.keras.models.Model(inputs=parameters_input, outputs=x)

    return model


class DataGenerator(tf.keras.utils.Sequence):
    """""" Generates random images, which may or may not be blurred""""""

    def __init__(self, batch_size=32, random_state=None):
        self.batch_size = batch_size
        if random_state is None:
            self.random_state = np.random.RandomState()
        else:
            self.random_state = random_state

        self.averaging_filter = np.ones((3, 3))

    def __len__(self):  # returns number of batches per epoch, not dataset size
        return 50

    def __getitem__(self, index):  # returns batch of data
        batch = []
        labels = []
        for i in range(self.batch_size):
            single_data, label = self.generate_single_data_sample()
            batch.append(single_data)
            labels.append(label)
        return np.array(batch), np.array(labels)

    def generate_single_data_sample(self):
        is_blurred = self.random_state.randint(0, 2)
        data_sample = self.random_state.rand(*DATA_SHAPE[0:2])
        if is_blurred:
            data_sample = signal.convolve2d(data_sample, self.averaging_filter, mode='same')
            label = np.array([0, 1])
        else:
            label = np.array([1, 0])
        data_sample = np.expand_dims(data_sample, -1).astype(np.float32)

        return data_sample, label


if __name__ == '__main__':
    checkpoint_path = '/tmp/test_checkpoint.ckpt'
    tflite_file_path = '/tmp/tflite_model.tflite'
    random_state = np.random.RandomState(seed=0)

    with tf.device(""/gpu:0""):

        ############################################################################################
        # LINE BELOW IS IMPORTANT (FOR SOME REASON)
        #  - if set to:
        #        tf.keras.backend.set_learning_phase(0)
        #    model of course is not training, but Lite model returns almost same results as tf.keras model
        #  - if set to:
        #        tf.keras.backend.set_learning_phase(1)
        #    model is training, good acc, but lite model returns completely different results
        #  - if commented/removed
        #    model is training, but there is error in converting model to Lite version
        tf.keras.backend.set_learning_phase(1)
        ############################################################################################

        # train tf.keras model
        data_generator = DataGenerator(random_state=random_state)
        tfkeras_resnet = prepare_simple_resnet()

        tfkeras_resnet.compile(
            optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.00001),
            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
            metrics=['accuracy'],
        )
        cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=False,
                                                         save_best_only=True, verbose=1)
        tfkeras_resnet.fit_generator(generator=data_generator, validation_data=data_generator,
                                     use_multiprocessing=False, epochs=5, callbacks=[cp_callback])

        # test some random data before and after changing learning phase
        random_data = random_state.rand(1, *DATA_SHAPE).astype(np.float32)
        result_tfkeras_train = tfkeras_resnet.predict(random_data)
        tf.keras.backend.set_learning_phase(0)
        result_tfkeras_test = tfkeras_resnet.predict(random_data)

        # Convert to tensorflow lite
        session = tf.keras.backend.get_session()
        converter = tf.lite.TFLiteConverter.from_session(session, [tfkeras_resnet.input],
                                                         [tfkeras_resnet.outputs[0]])
        tflite_model = converter.convert()
        with open(tflite_file_path, 'wb') as f:
            f.write(tflite_model)

        interpreter = tf.lite.Interpreter(model_path=tflite_file_path)
        interpreter.allocate_tensors()

        # Get input and output tensors.
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        # Test model on random input data.
        input_shape = input_details[0]['shape']
        input_data = np.array(random_data, dtype=np.float32)
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        result_tflite = interpreter.get_tensor(output_details[0]['index'])

        print(""TFKeras Train: {}"".format(result_tfkeras_train))
        print(""TFKeras Test: {}"".format(result_tfkeras_test))
        print(""TFLite: {}"".format(result_tflite))

```"
25168,Error while calling TFLite interpreter - cannot import name cloud,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Applicable
- TensorFlow installed from (source or binary):whl file
- TensorFlow version: 1.12.0
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Pip3
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: Not applicable
- GPU model and memory: Not applicable



**Describe the problem**
I'm using a Raspberry Pi 3B+, with Raspbian Stretch as my OS.

I installed Tensorflow 1.12 using [this guide](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-on-the-Raspberry-Pi), I downloaded the latest versions of TensorFlow and Protobuf, but as I was trying out [this guide](https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection.py), I hit an error:

`File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 39, in <module>
    from tensorflow.contrib import cloud
ImportError: cannot import name 'cloud'` 


**Provide the exact sequence of commands / steps that you executed before running into the problem**
The steps that I had followed are based off of the guide linked above, I had used the updated compiled TensorFlow wheels and the updated Protobuf as well.

Here are the exact steps I took:
TensorFlow Lite Guide

You may do the following in the RPi itself, or connect via SSH


If you encounter the â€œRemote Host Identification has changedâ€ message, you may have to edit the ~/.ssh/known_hosts file.
sudo nano ~/.ssh/known_hosts

remove the line for the IP Address of the RPi

1. Delete bloatware
$ sudo apt-get purge wolfram-engine
$ sudo apt-get purge libreoffice*
$ sudo apt-get clean
$ sudo apt-get autoremove

2. Install Dependencies
$ sudo apt-get update
$ sudo apt-get dist-upgrade

3. Install Tensorflow
$ mkdir tf && cd tf

check: https://github.com/lhelontra/tensorflow-on-arm/releases 
for latest tensorflow releases

$ wget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.12.0/tensorflow-1.12.0-cp35-none-linux_armv7l.whl

$ sudo pip3 install /home/pi/tf/tensorflow-1.12.0-cp35-none-linux_armv7l.whl

$ sudo apt-get install libatlas-base-dev
$ sudo pip3 install pillow lxml jupyter matplotlib cython
$ sudo apt-get install python-tk

3. Install OpenCV
$ sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev
$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
$ sudo apt-get install libxvidcore-dev libx264-dev
$ sudo apt-get install qt4-dev-tools
$ pip3 install opencv-python

4. Install Protobuf
$ sudo apt-get install autoconf automake libtool curl
check https://github.com/protocolbuffers/protobuf/releases/ 
for latest releases
$ wget https://github.com/google/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz
$ tar -zxvf protobuf-all-3.6.1.tar.gz
$ cd protobuf-3.6.1
$ ./configure
$ make
$ make check
$ sudo make install
$ cd python
$ export LD_LIBRARY_PATH=../src/.libs
$ python3 setup.py build --cpp_implementation 
$ python3 setup.py test --cpp_implementation
$ sudo python3 setup.py install --cpp_implementation
$ export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp
$ export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION=3
$ sudo ldconfig
$ protoc
$ sudo reboot now

$ cd ~
$ mkdir tensorflow1 && cd tensorflow1
$ git clone --recurse-submodules https://github.com/tensorflow/models.git

$ sudo nano ~/.bashrc
Add: export PYTHONPATH=$PYTHONPATH:/home/pi/tensorflow1/models/research:/home/pi/tensorflow1/models/research/slim

$ cd /home/pi/tensorflow1/models/research
$ protoc object_detection/protos/*.proto --python_out=.

$ cd /home/pi/tensorflow1/models/research/object_detection
$ wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz
$ tar -xzvf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz

Then I ran:
$ python3
>> from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
 But I hit an error, as shown below:

**Any other info / logs**
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 47, in <module>
    import numpy as np
  File ""/home/pi/.local/lib/python3.5/site-packages/numpy/__init__.py"", line 187, in <module>
    from .testing import Tester
  File ""/home/pi/.local/lib/python3.5/site-packages/numpy/testing/__init__.py"", line 10, in <module>
    from unittest import TestCase
  File ""/usr/lib/python3.5/unittest/__init__.py"", line 64, in <module>
    from .main import TestProgram, main
  File ""/usr/lib/python3.5/unittest/main.py"", line 7, in <module>
    from . import loader, runner
  File ""/usr/lib/python3.5/unittest/runner.py"", line 8, in <module>
    from .signals import registerResult
  File ""/usr/lib/python3.5/unittest/signals.py"", line 1, in <module>
    import signal
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/signal/__init__.py"", line 42, in <module>
    from tensorflow.contrib.signal.python.ops.mel_ops import linear_to_mel_weight_matrix
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 39, in <module>
    from tensorflow.contrib import cloud
ImportError: cannot import name 'cloud'

I've checked the __init__.py code, and I'm kind of clueless as to what or how to approach this. I'm not sure if it's safe to just comment the line out. 

Has anyone solved this issue? 

"
25167,what is the FFT size of the tensorflow.audio_contrib.audio_spectrogram in input_audio.py?,"in input_audio.py, is the FFT size equal to window_size?
spectrogram = contrib_audio.audio_spectrogram(
          background_clamp,
          window_size=model_settings['window_size_samples'],
          stride=model_settings['window_stride_samples'],
          magnitude_squared=True)

if it is, should the window size be 2's power? because most of time the FFT size is equal to 2's power.
"
25166, Text generation tutorial issue,"**My issues relate to the following tutorial ""Text generation using a RNN with eager execution""**: 
- https://www.tensorflow.org/tutorials/sequences/text_generation

**System information**
- TensorFlow version: 1.12

(disclaimer: I am fairly new to using tensorflow, so please let me know if I am misunderstanding something!)

**1) Questions about the 'Try the Model' section**
`sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()`

When I run this code above, my program would produce an error message:
`AttributeError: module 'tensorflow._api.v1.random' has no attribute 'categorical'`

I checked the Tensorflow API for 1.12, and indeed random does not have a 'categorical' attribute. So as per suggestion by a Stack Overflow comment, I am using the 'multinomial' attribute (namely  tf.random.multinomial instead of  tf.random.categorical) : https://www.tensorflow.org/api_docs/python/tf/random/multinomial

**My questions:** Is this an adequate solution? Am I missing something, or does random indeed not have the categorical attribute? If so, would it be possible to update the tensorflow tutorial to prevent future users from running into this issue?

**2) Questions about the 'Train the Model' section**
`def loss(labels, logits):`
                   `return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)`

`example_batch_loss  = loss(target_example_batch, example_batch_predictions)`
`print(""Prediction shape: "", example_batch_predictions.shape, "" # (batch_size, sequence_length, vocab_size)"") `
`print(""scalar_loss:      "", example_batch_loss.numpy().mean())`

When I run this above code, my program would produce an error message:
`sparse_categorical_crossentropy() got an unexpected keyword argument 'from_logits'`

I checked the Tensorflow API for 1.12, and indeed tf.keras.LOSSES.sparse_categorical_crossentropy() (alias for metrics which I linked to) only has two parameters (neither of which are 'from_logits'):
https://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy

Whereas tf.keras.BACKEND.sparse_categorical_crossentropy() has four paramaters, one of which is
'from_logits`: https://www.tensorflow.org/api_docs/python/tf/keras/backend/sparse_categorical_crossentropy

So I changed my program to use tf.keras.backend.sparse_categorical_crossentropy() instead of  tf.keras.losses.sparse_categorical_crossentropy().

**My questions:** Is this an adequate solution? If the tutorial is indeed mistaken for using losses instead of backend, would it be possible to update it with this as well?

Thank you!




"
25165,Contrib package initialization is broken in 1.13.0.rc0,"When I try to use any functionality from `contrib` package I get an exception.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 17.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **b'v1.13.0-rc0-0-g6ce86799c8' 1.13.0-rc0**
- Python version: **3.6.3**
- Bazel version (if compiling from source): **-**
- GCC/Compiler version (if compiling from source): **-**
- CUDA/cuDNN version: **-**
- GPU model and memory: **-**

**Describe the current behavior**

When I try to run `tensorflow/contrib/ignite/python/tests/ignite_dataset_test.py` (and any other code that imports from `contrib`) I get the following exception:
```
Traceback (most recent call last):
  File ""ignite_dataset_test.py"", line 23, in <module>
    from tensorflow.contrib.ignite import IgniteDataset
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 40, in <module>
    from tensorflow.contrib import distribute
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py"", line 33, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py"", line 73, in <module>
    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 62, in <module>
    from tensorflow.contrib.tpu.python.tpu import tpu
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 24, in <module>
    from tensorflow.contrib.compiler import xla
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/compiler/xla.py"", line 28, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/__init__.py"", line 26, in <module>
    from tensorflow_estimator.python import estimator
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 54, in <module>
    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2
  File ""/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/mode_keys.py"", line 22, in <module>
    from tensorflow.python.training.mode_keys import ModeKeys
ModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'
```

**Describe the expected behavior**
No exception

**Code to reproduce the issue**
Install last released version of TensorFlow, `pip3 install tensorflow=1.13.0rc0`. Then run `tensorflow/contrib/ignite/python/tests/start_ignite.sh` to start server. Then run `python3 tensorflow/contrib/ignite/python/tests/ignite_dataset_test.py` and get an exception.

**Other info / logs**
See above.
"
25164,"os.environ[""CUDA_DEVICE_ORDER""] ='0' not work in tensorflow ","I have two GPU, I want to create 2 graph in 2 GPU, first graph in first GPU, second graph in second GPU.

when I create first graph in first GPU, I use os.environ[""CUDA_DEVICE_ORDER""] ='0'
2.when I create second graph in second GPU, I use os.environ[""CUDA_DEVICE_ORDER""] = '1', but second graph still create on first GPU, I try many different ways, but it still not work.
It's a bug?  Please help me. 
@ry @jmhodges @eggie5 @bmabey @djones "
25162,Cant convert my model to tflite. Show the following error. (included code).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):anaconda
- TensorFlow version (use command below):latest
- Python version:3.6.5
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:No
- GPU model and memory:920m

code used to convert the file:
from tensorflow.contrib import lite
keras_file = ""movie_classifier.h5""
keras.models.save_model(model, keras_file)
converter = lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""linear.h5"" , wb).write(tflite_model)


Error is as follows:
RuntimeError: TOCO failed see console for info.
b'C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File ""C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module(\'_tensorflow_wrap_toco\', [dirname(__file__)])\r\n  File ""C:\\ProgramData\\Anaconda3\\lib\\imp.py"", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named \'_tensorflow_wrap_toco\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\ProgramData\\Anaconda3\\Scripts\\toco_from_protos-script.py"", line 6, in <module>\r\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\r\n  File ""C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py"", line 22, in <module>\r\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\r\n  File ""C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 28, in <module>\r\n    _tensorflow_wrap_toco = swig_import_helper()\r\n  File ""C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py"", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_toco\r\nModuleNotFoundError: No module named \'_tensorflow_wrap_toco\'\r\n'
None"
25160,"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12, 1.11
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:7.4.2, 7.3.1
- GPU model and memory: CUDA 9.0

**Describe the current behavior**
I have installed TF using pip, I have tested and it was able to detect the GPU, but when start to train, it throws the error below:

> UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
>          [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at C:\Users\bahra\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py:2777)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/read/_165)]]
>          [[{{node BatchMultiClassNonMaxSuppression/map/while/Exit_6/_76}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1252_BatchMultiClassNonMaxSuppression/map/while/Exit_6"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

**Note**
I have tried TF 1.12, 1.11, and 1.8.0 all have the same problem.
Why it throwing this error and how to solve?

Before this error, I was able to train, and it was successfully worked, but when to start the second time training then this error happens.
"
25159,eager.function.defun bypasses Autograph even when autograph=True,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly (1.13.0-dev20190118)
- Python version: 3.5.2
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA TITAN Xp (12196MB)

**Describe the current behavior**

From tensorflow 1.13, there is integration between autograph and tf.eager.function.defun (alias of tf.function in tf2.0 maybe?) 
The logic flow is if programmer enables `autograph` option as `True` in `defun` decorator,
`func_graph_from_py_func` function in `tf.python.framework.func_graph.py` will be called with `autograph=True`.
Inside that function, it will call `converted_call` which is defined in `autograph.impl.api.py` and convert py_function to graph_funcion via autograph lib

However, inside `converted_call`, it should shows some log information about conversion in current version, but in my simplest example, it does not in actual execution.
Moreover, if I add some redundant print statement inside converted_call, it prints nothing.
I think rewrapping autograph wrapper in line 445-446 inside of `tf.python.framework.func_graph.py` does not wrap autograph's `converted_call`

**Describe the expected behavior**

The program should print some logging information in autograph conversion
e.g. autograph converted code information

**Code to reproduce the issue**

```
import tensorflow as tf

from tensorflow.python import autograph
from tensorflow.python.eager.function import defun

tf.enable_eager_execution()

@defun(autograph=True)
def add(x, y):
    return x + y

print(add(tf.constant(4), tf.constant(3))
```

**Other info / logs**

I think this issue occurs because
Inside `unwrap` function in `tf.python.util.tf_decorator.py`, it does nothing if `innermost_decorator` is `None`
Thus, if I change line 445-446 in `tf.python.framework.func_graph.py` as 
```
converted_func = tf_decorator.make_decorator(original_func, wrapper)
if not hasattr(python_func, '_tf_decorator'):
    python_func = converted_func
else:
    tf_decorator.rewrap(python_func, original_func, converted_func)
```
it works.
"
25155,How to measure every node flops form tflite modelï¼Ÿ,benchmark_model  or profile only have run time and memoryï¼Œi  also want to get flops from node
25154,Is there tf_cuda_cc_binary rule in TF?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  RHEL 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.4
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.9.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 9.0
- GPU model and memory: k40m



**Describe the problem**

I want to put a simple cuda cupti program(Eg, the official CUPTI samples) in TF for some tests. But I can't find any `tf_cuda_cc_binary` rule in TF. So, my question is: how could I put such raw cuda souce in TF, and compile them by bazel?
"
25152,"os.environ[""CUDA_DEVICE_ORDER""]  not work","I have two GPU, I want to create 2 graph in 2 GPU, first graph in first GPU, second graph in second GPU.
1. when I create first graph in first GPU, I use os.environ[""CUDA_DEVICE_ORDER""] ='0'
2.when I create second graph in second GPU, I use os.environ[""CUDA_DEVICE_ORDER""] = '1', but second graph still create on first GPU, I try many different ways, but it still not work.

It's a bug?
"
25151,version 1.13.0rc0 build failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.0rc0
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?:  no
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):  7.3.0
- CUDA/cuDNN version: 10.0/ 7.4
- GPU model and memory:  GTX1080Ti GDDR5X 11GB X 7



**Describe the problem**

bazel build failed

ERROR: /home/wmind/repo/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 72, in <module>
    from tensorflow.python.ops.standard_ops import *
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py"", line 25, in <module>
    from tensorflow.python import autograph
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py"", line 37, in <module>
    from tensorflow.python.autograph.core.converter import ConversionOptions
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/core/converter.py"", line 74, in <module>
    from tensorflow.python.autograph.pyct import cfg
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/cfg.py"", line 41, in <module>
    from tensorflow.python.autograph.pyct import compiler
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/compiler.py"", line 30, in <module>
    import astor
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/__init__.py"", line 14, in <module>
    from .code_gen import to_source  # NOQA
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/code_gen.py"", line 311
    def visit_FunctionDef(self, node, async=False):
                                          ^
SyntaxError: invalid syntax
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2248.641s, Critical Path: 409.07s
INFO: 15004 processes: 15004 local.
FAILED: Build did NOT complete successfully
"
25149,could you teach me how to config the openssl and icu libï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ,"tensorflow 1.12
win10
python 3.5
vs 2015 update3
compile tensorflow for tensorflow.dll and tensorflow.lib
could you teach me how to config the openssl and icu libï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
how to package openssl and icu lib to tensorflow.dll and tensorflow.lib"
25148,TypeError when using PReLU activations with TPUs,"I'm updating a Keras Functional API SRGAN model, so that it will run on a TPU instead of GPU (i'm using colaboratory).
Everything is now OK, with the exception of being able to use PReLU Activation.  When I Fit a model that has a PReLU layer:

`x = tf.keras.layers.PReLU(alpha_initializer='zeros',alpha_regularizer=None,alpha_constraint=None,shared_axes=[1,2])(x)`

I get the following error:

`TypeError: bad operand type for unary -: 'ReplicatedVariable'`

If I swap the PReLU with:

`x = tf.keras.layers.Activation('relu')(x)`

The model runs with no error on the TPU.  Has anyone seen this problem?
"
25147,Profiler hook causes exception,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No. I am using 1.12.0

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14. Docker 16.
- TensorFlow installed from (source or binary): binary whl
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: Repros on both local 1080ti + TitanV and 8xV100 on gcloud


```
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.12.0-0-ga6d8ffae09', '1.12.0')
```

**Describe the current behavior**

```
INFO:tensorflow:loss = 0.28926048, step = 0
2019-01-23 22:36:02.026095: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally
pure virtual method called
terminate called without an active exception
Aborted (core dumped)
```

**Describe the expected behavior**

**Code to reproduce the issue**

```python
        hooks = []
        if self.params.profile:
            profile_destination = '/'.join([self.params.job_dir, 'profiler'])
            tf.logging.info(""Profiling session to: {}"".format(profile_destination))
            profiler_hook = tf.train.ProfilerHook(save_steps=30,
                                                    output_dir=profile_destination,
                                                    show_memory=True)
            hooks.append(profiler_hook)

        # Build train and eval specs
        train_spec = tf.estimator.TrainSpec(
            input_fn=lambda: self.model.input_tf_dataset(DataSplitKey.TRAIN),
            max_steps=self.params.train_steps,
            hooks=hooks,
        )
```
"
25144,Build of 1.13 //tensorflow/python/eager:pywrap_tfe_lib fails with internal compiler error: unexpected expression â€˜Iâ€™ of kind template_parm_index,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Fedora 28
cuda-cusparse-10.0.130-1.fc28.x86_64
cuda-cufft-devel-10.0.130-1.fc28.x86_64
cuda-nvjpeg-devel-10.0.130-1.fc28.x86_64
cuda-cublas-10.0.130-1.fc28.x86_64
cuda-nvrtc-devel-10.0.130-1.fc28.x86_64
cuda-gcc-c++-7.3.1-2.fc28.x86_64
nvidia-driver-cuda-libs-415.27-1.fc28.x86_64
cuda-cublas-devel-10.0.130-1.fc28.x86_64
cuda-extra-libs-10.0.130-1.fc28.x86_64
cuda-nvtx-10.0.130-1.fc28.x86_64
cuda-cusparse-devel-10.0.130-1.fc28.x86_64
cuda-cupti-10.0.130-1.fc28.x86_64
cuda-gcc-7.3.1-2.fc28.x86_64
cuda-cudart-devel-10.0.130-1.fc28.x86_64
cuda-nvjpeg-10.0.130-1.fc28.x86_64
cuda-curand-10.0.130-1.fc28.x86_64
cuda-npp-devel-10.0.130-1.fc28.x86_64
cuda-cudnn-devel-7.4.2.24-1.fc28.x86_64
bazel-0.21.0-1.fc28.x86_64
cuda-nvgraph-devel-10.0.130-1.fc28.x86_64
cuda-nvgraph-10.0.130-1.fc28.x86_64
cuda-cusolver-devel-10.0.130-1.fc28.x86_64
cuda-npp-10.0.130-1.fc28.x86_64
cuda-cudart-10.0.130-1.fc28.x86_64
cuda-nvml-devel-10.0.130-1.fc28.x86_64
nvidia-driver-cuda-415.27-1.fc28.x86_64
cuda-cusolver-10.0.130-1.fc28.x86_64
cuda-cupti-devel-10.0.130-1.fc28.x86_64
cuda-devel-10.0.130-1.fc28.x86_64
cuda-libs-10.0.130-1.fc28.x86_64
cuda-nvtx-devel-10.0.130-1.fc28.x86_64
cuda-10.0.130-1.fc28.x86_64
cuda-curand-devel-10.0.130-1.fc28.x86_64
cuda-nvrtc-10.0.130-1.fc28.x86_64
cuda-cufft-10.0.130-1.fc28.x86_64
cuda-cudnn-7.4.2.24-1.fc28.x86_64

- **TensorFlow version (use command below)**: 1.13
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: bazel-0.21.0-1.fc28.x86_64
- **GCC/Compiler version (if compiling from source)**: cuda-gcc-7.3.1-2.fc28.x86_64
- **CUDA/cuDNN version**: cuda-cudnn-7.4.2.24-1.fc28.x86_64
- **GPU model and memory**: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] 11GB RAM
- **Exact command to reproduce**: 
- # ./configure                                                                                           
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: fdbe3c4d-e72c-4726-a665-e2bc04669f74
You have bazel 0.21.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/local/lib/python3.6/site-packages
  /usr/lib64/python3.6/site-packages
  /usr/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/cuda-gcc


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

- # bazel build --local_resources 2048,6,1.0 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
...
...
ERROR: /home/spryor/git/tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)
tensorflow/python/eager/pywrap_tfe_src.cc: In function â€˜void TFE_Py_Execute(TFE_Context*, const char*, const char*, TFE_InputTensorHandles*, PyObject*, TFE_OutputTensorHandles*, TF_Status*)â€™:
tensorflow/python/eager/pywrap_tfe_src.cc:685:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < inputs->size() && TF_GetCode(out_status) == TF_OK;
                     ~~^~~~~~~~~~~~~~~~
In file included from tensorflow/python/eager/pywrap_tfe_src.cc:23:0:
external/com_google_absl/absl/types/variant.h: In substitution of â€˜template<long unsigned int I, class T> using variant_alternative_t = typename absl::variant_alternative::type [with long unsigned int I = I; T = absl::variant<tensorflow::TensorShape, _object*>]â€™:
external/com_google_absl/absl/types/variant.h:581:7:   required by substitution of â€˜template<class T, long unsigned int I, class Tj, typename std::enable_if<(std::is_assignable<Tj&, T>::value && std::is_constructible<Tj, T>::value), void>::type* <anonymous> > absl::variant<tensorflow::TensorShape, _object*>& absl::variant<tensorflow::TensorShape, _object*>::operator=<T, I, Tj, <enumerator> >(T&&) [with T = const absl::variant<tensorflow::TensorShape, _object*>&; long unsigned int I = <missing>; Tj = <missing>; typename std::enable_if<(std::is_assignable<Tj&, T>::value && std::is_constructible<Tj, T>::value), void>::type* <anonymous> = <missing>]â€™
tensorflow/python/eager/pywrap_tfe_src.cc:914:20:   required from here
external/com_google_absl/absl/types/variant.h:581:7: internal compiler error: unexpected expression â€˜Iâ€™ of kind template_parm_index
       class Tj = absl::variant_alternative_t<I, variant>,
       ^~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <https://gcc.gnu.org/bugs/> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 600.890s, Critical Path: 104.25s
INFO: 3259 processes: 3259 local.
FAILED: Build did NOT complete successfully


### Describe the problem
Attempting to build r1.13 on Fedora 28 with the given packages/etc results in a build error


[tensorflow_build.log](https://github.com/tensorflow/tensorflow/files/2789013/tensorflow_build.log)
"
25143,[XLA] Ambiguous/dynamic shape during `xla.compile` when using `tf.custom_gradient` and `tf.while_loop`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): public Colab runtime
- TensorFlow version (use command below): 1.12
- Python version: 3

**Describe the current behavior**
Using `xla.compile` on a gradient coming from a while loop within `tf.custom_gradient` seems to generate an inverse permutation op with undefined shape. While all intermediate tensors appear to have fully defined shapes, `xla.compile` will still raise an exception:

```
InvalidArgumentError: Input 0 to InvertPermutation operator must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.
	 [[{{node gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/InvertPermutation}} = InvertPermutation[T=DT_INT32](_arg3, ^_arg0)]]
	 [[{{node gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/LoopCond}} = While[T=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], body=_functionalize_body_4[], cond=_functionalize_cond_4[]](gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/iteration_counter, gradients/pow_grad/Reshape, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/Fill, ConstantFolding/gradients/pow_grad/BroadcastGradientArgs-folded-1, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/maximum_iterations, gradients/IdentityN_grad/backwards_fixed_point/Tensordot/Reshape_1, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/Tensordot/Reshape, gradients/pow_grad/Reshape)]]
	 [[{{node cluster}} = XlaLaunch[Nresources=0, Targs=[DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[], Tresults=[DT_FLOAT, DT_FLOAT], function=cluster_55287096143340976_f15n_0[], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Const, Const_1, zeros)]]
```

**Code to reproduce the issue**
I've been able to distill the issue into ~150 lines of code and package it in a colab. I'd rather not post this code publicly, any way for me to share it with the dev's without pasting it here?


"
25139,Basic Regression Docs Code typo,"https://www.tensorflow.org/tutorials/keras/basic_regression#split_features_from_labels

```
def plot_history(history):
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mean_absolute_error'],
           label='Train Error')
```

This function references `hist` instead of `history` so it doesn't parse."
25138,Tensorflow v2 Limit GPU Memory usage,"Need a way to prevent TF from consuming all GPU memory, on v1, this was done by using something like:
```
opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
sess = tf.Session(config=tf.ConfigProto(gpu_options=opts))

```

On v2 there is no Session and GPUConfig on tf namespace.


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 (AWS EC2 P2)
- TensorFlow installed from (source or binary): pip install tf-nightly-2.0-preview
- TensorFlow version (use command below):'1.13.0-dev20190117'
- Python version:Python 3.6.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80 12GB


"
25137,Unable to build TF 1.12 with CUDA support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016 Datacenter, x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-----
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: Python 3.6.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): Build label: 0.19.2
- GCC/Compiler version (if compiling from source): C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC
- CUDA/cuDNN version: Cuda v10, cudnn v7.4.2.24
- GPU model and memory: Tesla K80 (0MiB / 11445MiB memory usage)



**Describe the problem**
I try to compile TF from source according to this guide: https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8

The bazel build fails with error: 

    ERROR: 
    C:/users/administrator/bin/tensorflow_bazel_src/tensorflow/tensorflow/core/kernels/BUILD:207:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:extract_image_patches_op_gpu':
    this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/extract_image_patches_op_gpu.cu.cc':
     'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/extract_image_patches_op_gpu.cu.cudafe1.stub.c'
     'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/extract_image_patches_op_gpu.cu.fatbin.c'

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Exactly as described here: https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[log.txt](https://github.com/tensorflow/tensorflow/files/2787302/log.txt)
"
25136,[Feature Request] [python API] make tf.RefVariable._strided_slice_assign public,"**System information**
- TensorFlow version (you are using): 1.12 .0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently slice assignment is (publicly) accessible only indirectly through `strided_slice()` using the `var` kwarg, in a rather convoluted way creating a closure and adding it as an attribute to the generated op.
As a side effect this also adds an unnecessary (and mostly inaccessible) `strided_slice` op, bloating the graph needlessly.

**Will this change the current api? How?**
It will add a public `tf.Variable.strided_slice_assign()` with exactly the same interface as the existing protected version.
The protected function is already de-facto part of the API when called using the supported slicing + assign() syntax, so there is no functional change. It will also not interfere with any existing code using that syntax.

**Who will benefit with this feature?**
People with use cases similar to mine:
I have a rather elaborate slice assign operation than needs to be repeated on several variables, using the same slice. Naively one would just repeat 
`V[...elaborate slicing here...].assign(something)` for all variables V. This creates lots and lots of ops in the graph as the entire index expression gets replicated for each variable, plus the unnecessary and unused `strided_slice` read operation.
Instead one could create the tensors for the indexing (begin, end, stride, masks) once, and then repeatedly call `.strided_slice_assign` for all the variables, resulting in a concise and manageable subgraph.

 I realize that the vast majority of people never call `strided_slice()` or `strided_slice_assign()` directly. However, for the same few for which `strided_slice()` is publicly available, so should `strided_slice_assign()` be.

**Any Other info.**
None
"
25133,DNNClassifier estimator train shows unsupported feed type error occasionally ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  9.0
- GPU model and memory: GeForce GTX 1070 with Max-Q Design


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have a dataset contains labels with true and false, and a column of English test.
Labels are converted to 1 and 0 as below:
```
      review                                          opinion
0              1  cls late nite music club with shoji tabuchi wh...
1              0  new south valley subdivision taking shape the ...
2              0  celtics visit lakers aim to avoid fourth strai...
3              1  russia signs ceasefire agreement with reservat...
4              1  new j c penney ceo to have limited role report...

```
The data are split into train_df and test_df then created an input_fn for training.

```
 train_input_fn = tf.estimator.inputs.pandas_input_fn(
    train_df, train_df['review'],batch_size=40, num_epochs=5, shuffle=True)
```
When I reach 
```estimator.train(input_fn=train_input_fn, steps=1000)```

It shows 

`tensorflow.python.framework.errors_impl.InternalError: Unsupported feed type`

But after I change the label to floats by df['review'] = df['review']*1.0 
```
      review                                          opinion
0              1.0  cls late nite music club with shoji tabuchi wh...
1              0.0  new south valley subdivision taking shape the ...
2              0.0  celtics visit lakers aim to avoid fourth strai...
3              1.0  russia signs ceasefire agreement with reservat...
4              1.0  new j c penney ceo to have limited role report...

```
The train function works again. 

But this problem occurs occasionally, estimator.train works well if I have review labels in integer with 0 and 1 as the data in the first table.

**Describe the expected behavior**
```estimator.train(input_fn=train_input_fn, steps=1000)``` 
should works without showing unsupported type
"
25132,Failed to apply delegate: WARNING: op code #43 cannot be handled by this delegate.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 - Android 5.1
- Mobile device : Nexus  10 - Nexus 7 2012 Android 5.1
- TensorFlow installed from: Binary
- TensorFlow version : b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0 (0.0.0-gpu-experimental for mobile device)
- Python version: 3.6

**Describe the current behavior**
The tflite demo app crashes on Nexus 7 2012 and Nexus 10 when GPU is selected from device list. There was no such problem on the other devices I tested.

Logs:
```
01-01 05:23:50.161 6539-6558/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 6539
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #43 cannot be handled by this delegate.  Only the first 29 ops will run on the GPU, and the remaining 2 on the CPU.GpuDelegate Prepare: No EGL error, but eglChooseConfig failed.Node number 31 (GpuDelegate) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)
        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.java:379)
        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source)
        at android.os.Handler.handleCallback(Handler.java:739)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:135)
        at android.os.HandlerThread.run(HandlerThread.java:61)
01-01 05:23:50.242 6539-6556/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] queueBuffer: BufferQueue has been abandoned
01-01 05:23:50.243 124-6580/? E/Surface: queueBuffer: error queuing buffer to SurfaceTexture, -19
01-01 05:23:50.243 124-6580/? E/NvOmxCamera: Queue Buffer Failed. Skipping buffer.
01-01 05:23:50.243 6539-6619/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] dequeueBuffer: BufferQueue has been abandoned
01-01 05:23:50.244 124-6580/? E/NvOmxCamera: Dequeue Buffer Failed
01-01 05:23:50.275 6539-6555/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] queueBuffer: BufferQueue has been abandoned
01-01 05:23:50.277 124-6580/? E/Surface: queueBuffer: error queuing buffer to SurfaceTexture, -19
01-01 05:23:50.277 124-6580/? E/NvOmxCamera: Queue Buffer Failed. Skipping buffer.
01-01 05:23:50.277 6539-6556/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] dequeueBuffer: BufferQueue has been abandoned
01-01 05:23:50.277 124-6580/? E/NvOmxCamera: Dequeue Buffer Failed
01-01 05:23:50.278 6539-6619/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] cancelBuffer: BufferQueue has been abandoned
01-01 05:23:50.279 6539-6555/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] cancelBuffer: BufferQueue has been abandoned
01-01 05:23:50.380 124-565/? E/NvOmxCamera: Already called release()
01-01 05:25:48.050 473-554/? E/WifiStateMachine: cancelDelayedScan -> 6
```"
25131,TFLite GPU Delegate has problem with MobileNetV2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : Linux Ubuntu 16.04
- Mobile device : Samsung Galaxy S9 Android 8.0 - Nexus 10 Android 5.1
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0 (0.0.0-gpu-experimental for mobile device)
- Python version: 3.6

**Describe the current behavior**
GPU delegate has problem with MobileNetV2. When I select GPU from device list at tflite demo project (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo) the app crashes.
The only things I've changed on this project was changing MobileNet V1 float model to MobileNet v2. The MobileNetV2 model is taken from ""https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/classification/2"" , retrained by ""https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py"" script and converted to tflite format using the following command:

```
tflite_convert \
  --output_file=graph.tflite \
  --graph_def_file=retrained_graph.pb \
  --input_arrays=Placeholder \
  --output_arrays=final_result
 --input_shapes=1,160,160,3
```
All the necessary changes (such as changing the graph name, input size, etc.) in the ImageClassifierFloatMobileNet class is made.

Logs:
```
2019-01-23 13:51:12.091 22222-22294/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 22222
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: Dimension is empty.Node number 68 (GpuDelegate) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)
        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.java:379)
        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source:8)
        at android.os.Handler.handleCallback(Handler.java:789)
        at android.os.Handler.dispatchMessage(Handler.java:98)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
2019-01-23 13:51:12.102 4411-8924/? E/CameraDeviceClient: Disconnect from CameraDeviceClient
```
"
25127,//tensorflow/python/kernel_tests:init_ops_test fails on AVX512 builds,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): ('v1.8.0-14337-g18b1875', '1.13.0-rc0')
- Python version: Python 2.7.12
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
$  bazel test --config=opt  -- //tensorflow/python/kernel_tests:init_ops_test

//tensorflow/python/kernel_tests:init_ops_test                           FAILED in 1 out of 4 in 22.9s

**Describe the expected behavior**

The unit test should pass on AVX512 builds as it does on AVX2 builds.

**Code to reproduce the issue**

$ bazel test --config=opt  -- //tensorflow/python/kernel_tests:init_ops_test

**Other info / logs**
```
======================================================================
FAIL: testShapesValues (__main__.ConvolutionOrthogonal3dInitializerTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1103, in decorated
    return f(self, *args, **kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py"", line 1142, in testShapesValues
    self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1013, in decorated
    return f(*args, **kwds)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2121, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2090, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2025, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 1452, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 789, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.001, atol=0.001
Mismatched value: a is different from b. 
not close lhs = 3.15808677673
not close rhs = 3.14
not close dif = 0.0180867767334
not close tol = 0.00414
dtype = float32, shape = ()
(mismatch 100.0%)
 x: array(3.158087, dtype=float32)
 y: array(3.14)
```
"
25123,ScheduleWithHintâ€: is not â€œtensorflow::thread::ThreadPool::Impl â€˜s member ,"ScheduleWithHintâ€: is not â€œtensorflow::thread::ThreadPool::Impl â€˜s member

D:\tensorflow-1.13.0\tensorflow\core\lib\core\threadpool.cc(100): error C2661: â€œEigen::ThreadPoolDevice::ThreadPoolDeviceâ€: æ²¡æœ‰é‡è½½å‡½æ•°æŽ¥å— 3 ä¸ªå‚æ•°
D:\tensorflow-1.13.0\tensorflow\core\lib\core\threadpool.cc(208): error C2039: â€œScheduleWithHintâ€: ä¸æ˜¯â€œtensorflow::thread::ThreadPool::Implâ€çš„æˆå‘˜
  D:\tensorflow-1.13.0\tensorflow\core\lib\core\threadpool.cc(88): note: å‚è§â€œtensorflow::thread::ThreadPool::Implâ€çš„å£°æ˜Ž
D:\tensorflow-1.13.0\tensorflow\core\lib\core\threadpool.cc(213): error C2039: â€œSetStealPartitionsâ€: ä¸æ˜¯â€œtensorflow::thread::ThreadPool::Implâ€çš„æˆå‘˜
  D:\tensorflow-1.13.0\tensorflow\core\lib\core\threadpool.cc(88): note: å‚è§â€œtensorflow::thread::ThreadPool::Implâ€çš„å£°æ˜Ž"
25122,ScheduleWithHint,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25120,TF lite has link error when I try to build with build_rpi_lib.sh,"When I try to build for Tensorflow lite(in latest version) occur link error like this.

/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::~NNAPIAllocation()':
nnapi_delegate.cc:(.text+0x2c): undefined reference to `NnApiImplementation()'
/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::NNAPIAllocation(char const*, tflite::ErrorReporter*)':
nnapi_delegate.cc:(.text+0x110): undefined reference to `NnApiImplementation()'
/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIDelegate::~NNAPIDelegate()':
nnapi_delegate.cc:(.text+0x190): undefined reference to `NnApiImplementation()'
nnapi_delegate.cc:(.text+0x1b4): undefined reference to `NnApiImplementation()'
/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::addTensorOperands(tflite::Subgraph*, ANeuralNetworksModel*, unsigned int*, std::vector<long long, std::allocator<long long> >*)':
nnapi_delegate.cc:(.text+0x214): undefined reference to `NnApiImplementation()'
/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o):nnapi_delegate.cc:(.text+0x504): more undefined references to `NnApiImplementation()' follow
collect2: error: ld returned 1 exit status

how can I fix it? "
25119,Can't import tensorflow in Anaconda,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10

- TensorFlow version: 
tensorboard                        1.12.2
tensorflow                         1.12.0
tensorflow-gpu                     1.12.0

- Python version: Python 3.6.8 | Anaconda Python 3.5.2
- Installed using virtualenv? pip? conda?: pip & conda

- CUDA/cuDNN version: cuDNN v7.4.2 for CUDA 9.0
- GPU model and memory: NVIDIA GeForce 920mx | 8gb

### Anaconda Prompt
**import tensorflow as tf**

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\New User\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\New User\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\New User\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\New User\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 102, in <module>
    _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant(_pywrap_tensorflow_internal)
AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant'"
25118,Any project about the evaluation of music or piano,"Dear,
Is there any project about evaluating the music or piano music,
When we get many datasets of the same music from many people,
How could we evaluate one of them,that is ,the label is the grad of the music is A ,B,C or D, 
or just score between 0~100, Could we do something about this?
Any advice or suggestion will be good.
If U have any question,Please do not hesitate to tell me.
Thx"
25117,Can't import tensorflow-gpu or tensorflow (CPU only) 1.12.0,"Ubuntu: 16.04
Tensorflow installed from Anaconda
Tensorflow version: 1.12.0
Python: Python2.7
CUDA: 9.0
Cudnn: 7.2.1
GPU: Nvidia Quadro P5000 16GB
Command: import tensorflow

[ysaputra@vulcan2 ~]$ python
Python 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>
    import numpy as np
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>
    from . import core
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/__init__.py"", line 57, in <module>
    from . import numerictypes as nt
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/numerictypes.py"", line 111, in <module>
    from ._type_aliases import (
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/_type_aliases.py"", line 63, in <module>
    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
  File ""/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/_type_aliases.py"", line 63, in <setcomp>
    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
AttributeError: 'tuple' object has no attribute 'type'
>>> 

I installed new tensorflow-gpu 1.12.0 or tensorflow (CPU only) 1.12.0. However, I have the same problems using import tensorflow after successful installations. It seems the tensorflow is not imported properly. Do you know how to solve it?"
25116,XLA JIT compiler fails on MLPerf Transformer reference model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, just enabled XLA JIT compilation.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
NVIDIA container based on TF 1.12

- TensorFlow version (use command below):
1.12

- Python version:
3.5.2

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.0.130

- GPU model and memory:
V100 32GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Program exits with the following error message:
2019-01-22 19:29:40.093032: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at retval_op.cc:70 : Internal: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.
Error: Unsupported type for iota
2019-01-22 19:29:40.094015: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at xla_ops.cc:408 : Internal: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.
Error: Unsupported type for iota
	 [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.
Error: Unsupported type for iota
	 [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]
	 [[{{node cluster_6_1/xla_compile}} = _XlaCompile[Nresources=0, Targs=[DT_INT64, DT_INT32, DT_FLOAT, DT_INT64, DT_INT64, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], function=cluster_6[_XlaCompiledKernel=true, _XlaNumConstantArgs=14, _XlaNumResourceArgs=0], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](model/get_train_op/Equal/y, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/Fill, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/mod, model/get_train_op/gradients/model/Transformer/encode/embedding_shared_weights/embedding_1/mul_grad/Shape_1, model/loss/smoothing_cross_entropy/softmax_cross_entropy_with_logits/concat/values_0, model/Transformer/encode/embedding_shared_weights/embedding_1/ExpandDims/dim, model/Transformer/decode/decoder_self_attention_bias/Reshape/shape/0, model/Transformer/decode/shift_targets/Pad/paddings, model/Transformer/decode/shift_targets/strided_slice/stack, model/Transformer/decode/shift_targets/strided_slice/stack_1, model/Transformer/decode/shift_targets/strided_slice/stack_2, model/get_train_op/gradients/model/Transformer/encode/dropout/div_grad/Shape_1, model/get_train_op/gradients/model/Transformer/encode/encoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, global_step/read, model/get_train_op/AssignAdd/_4383, model/Transformer/embedding_shared_weights/embedding_and_softmax/weights/read, IteratorGetNext/_4385, IteratorGetNext/_4387, model/loss/pad_to_same_length/Shape_1/_4389, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read)]]
	 [[{{node model/Transformer/decode/decoder_stack/layer_4/ffn/feed_foward_network/filter_layer/Tensordot/Shape/declustered/_4241}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3582_...eclustered"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""transformer/transformer_main.py"", line 446, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""transformer/transformer_main.py"", line 356, in main
    FLAGS.bleu_threshold)
  File ""transformer/transformer_main.py"", line 274, in train_schedule
    estimator.train(dataset.train_input_fn, steps=single_iteration_train_steps)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.
Error: Unsupported type for iota
	 [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]
	 [[{{node cluster_6_1/xla_compile}} = _XlaCompile[Nresources=0, Targs=[DT_INT64, DT_INT32, DT_FLOAT, DT_INT64, DT_INT64, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], function=cluster_6[_XlaCompiledKernel=true, _XlaNumConstantArgs=14, _XlaNumResourceArgs=0], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](model/get_train_op/Equal/y, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/Fill, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/mod, model/get_train_op/gradients/model/Transformer/encode/embedding_shared_weights/embedding_1/mul_grad/Shape_1, model/loss/smoothing_cross_entropy/softmax_cross_entropy_with_logits/concat/values_0, model/Transformer/encode/embedding_shared_weights/embedding_1/ExpandDims/dim, model/Transformer/decode/decoder_self_attention_bias/Reshape/shape/0, model/Transformer/decode/shift_targets/Pad/paddings, model/Transformer/decode/shift_targets/strided_slice/stack, model/Transformer/decode/shift_targets/strided_slice/stack_1, model/Transformer/decode/shift_targets/strided_slice/stack_2, model/get_train_op/gradients/model/Transformer/encode/dropout/div_grad/Shape_1, model/get_train_op/gradients/model/Transformer/encode/encoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, global_step/read, model/get_train_op/AssignAdd/_4383, model/Transformer/embedding_shared_weights/embedding_and_softmax/weights/read, IteratorGetNext/_4385, IteratorGetNext/_4387, model/loss/pad_to_same_length/Shape_1/_4389, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read)]]
	 [[{{node model/Transformer/decode/decoder_stack/layer_4/ffn/feed_foward_network/filter_layer/Tensordot/Shape/declustered/_4241}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3582_...eclustered"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


**Describe the expected behavior**
Program runs normally.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
1. Clone the mlperf reference from https://github.com/mlperf/training
2. Pass config object to estimator.train that turns global jit compilation on. This is how I did it:
Insert following at line 345 in transformer/transformer_main.py
  config = tf.ConfigProto()
  config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
  run_config = tf.estimator.RunConfig(
      model_dir=FLAGS.model_dir,
      session_config=config)
After inserting the above, change line 350 to include run_config:
  estimator = tf.estimator.Estimator(
      model_fn=model_fn, model_dir=FLAGS.model_dir, config=run_config,
      params=params)
3. Change the path /research/transformer to wherever you cloned transformer. This needs to be done in scripts run_preprocessing.sh and run_training.sh
4. Start ./run_and_time.sh. This will download and prepare the training data, which takes a long time. The second time, you can launch ./run_training.sh 1 25 directly in order to avoid downloading the training data 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25113,"Can't access resource variable using GetResourceFromContext in a custom op, probably because of binary incompatibility","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- GCC/Compiler version (for custom op): g++ 4.2.1 Apple LLVM version 10.0.0 (clang-1000.11.45.5)

**Describe the problem**

When I try to use `GetResourceFromContext` with `T` being `Var` from a custom op, I get the following error ```InvalidArgumentError (see above for traceback): Trying to access resource using the wrong type. Expected N10tensorflow3VarE got N10tensorflow3VarE```

So it seems like the types match up, but for whatever reason the `std::type_index`s generated from them don't? My hypothesis is it's because using `std::type_index` across a shared library is implementation defined behavior and the compiler does not produce the same `std::type_index` for the same type in both the custom op library and `libtensorflow_framework`.

For context, I'm trying to get access to the resource's underlying tensor so that I can modify it.
"
25112,[Tensorflow GitHub] Do Inception-v3 Top1 Results match with the paper?,"This issue is to inquire about the top-1 rates of the inception-v3 results in the paper and available on the Tensorflow Github.

Currently, I am using the inception-v3 model [[link](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz) - inception-2015-12-05.tgz] from the official Tensorflow Github: [https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py](https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py)

When I calculate the top-1 accuracy rate for the 50K validation images in imageNet, I get 77.6%. However, I cannot find any reference to this result neither in the paper nor somewhere else.

Therefore, can you please help me get answers to the following question:
**What are the training specs of inception-2015-12-05.tgz? Single crop or basic or what? Are there references to its top1 rate results?** 

Please advise on my question as I believe this is a documentation issue that should be addressed. 

Please note that I am aware of this link:
[https://github.com/tensorflow/models/tree/master/research/slim]( https://github.com/tensorflow/models/tree/master/research/slim)
However, I am not also able to match the results from the updated link (inception_v3_2016_08_28.tar.gz - 78%) with the paper

I have seen a lot of links report different accuracies for the inception-v3 model - Please see some below:
[https://ai.googleblog.com/2016/08/improving-inception-and-image.html](https://ai.googleblog.com/2016/08/improving-inception-and-image.html)  [78% top1]
[https://github.com/tensorflow/models/tree/master/research/inception](https://github.com/tensorflow/models/tree/master/research/inception)  [78.8% top1]
[https://arxiv.org/pdf/1512.00567v1.pdf](https://arxiv.org/pdf/1512.00567v1.pdf) [76.6% top1]
https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c [76.6% top1]
[](https://github.com/tensorflow/models/tree/master/research/slim#downloading-and-converting-to-tfrecord-format
) [78% top1]"
25111,Trying to build tensorflow from source and fails,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): downloaded from github
- Python version: 3.x
- Bazel version (if compiling from source): .18
- GCC/Compiler version (if compiling from source): 6
- CUDA/cuDNN version: 9/7
- GPU model and memory: Quadro K5000

This is my config and error message below, please advise what is a possible fix?
--
./configure
WARNING: Processed legacy workspace file /home/robotlab/Desktop/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/robotlab/.cache/bazel/_bazel_robotlab/install/855b86426ac3a2fcf0dcf5a6fb99b79a/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.18.0- (@non-git) installed.
Please specify the location of python. [Default is /home/robotlab/.conda/envs/keras_gpu/bin/python]: 


Found possible Python library paths:
  /home/robotlab/.conda/envs/keras_gpu/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/home/robotlab/.conda/envs/keras_gpu/lib/python3.5/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 2.2.12


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-6]: 


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: n


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
(keras_gpu) robotlab@robotlab:~/Desktop/tensorflow$ bazel build --config=opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package
WARNING: Processed legacy workspace file /home/robotlab/Desktop/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
ERROR: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/lite/BUILD:62:1: C++ compilation of rule '//tensorflow/contrib/lite:context' failed (Exit 1)
x86_64-linux-gnu-gcc-6: error: n: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 141.101s, Critical Path: 58.27s
INFO: 243 processes: 243 local.
FAILED: Build did NOT complete successfully
(keras_gpu) robotlab@robotlab:~/Desktop/tensorflow$ bazel build --config=opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package

--
Cheers. 

"
25110,[TF Lite] Add support for dilated convolution with valid padding.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
uname -a
Linux archlinux 4.19.2-arch1-1-ARCH #1 SMP PREEMPT Tue Nov 13 21:16:19 UTC 2018 x86_64 GNU/Linux
```
- TensorFlow installed from (source or binary): 
```
pip install tensorflow
```
- TensorFlow version (or github SHA if from source): 
```
1.12.0
```

**Provide the text output from tflite_convert**

```
RuntimeError: TOCO failed see console for info.
2019-01-22 11:50:43.257264: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)
2019-01-22 11:50:43.257374: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)
2019-01-22 11:50:43.257409: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:217] Replaced sub-network with Dilated Conv2D op outputting ""conv2d/Conv2D"".
2019-01-22 11:50:43.257458: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] Check failed: height_with_paddings % block_height == 0 (12 vs. 0)
```

Also, please include a link to a GraphDef or the model if possible: [keras_model.zip](https://github.com/tensorflow/tensorflow/files/2783805/keras_model.zip)

**Any other info / logs**

Running this in jupyter notebook:
```python
import numpy as np
import tensorflow as tf
tf.keras.backend.clear_session()
print(tf.__version__)
print(tf.keras.__version__)
keras_model_savepath = '/tmp/keras_model.h5'

x_input = x = tf.keras.layers.Input(shape=(725, 725, 1))
x = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=25)(x)
x = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=19)(x)
keras_model = tf.keras.models.Model(x_input, x)
print(keras_model.summary())
pred = keras_model.predict(np.ones([1,725,725,1]))
print(pred[0,0:5,0,0])
keras_model.save(keras_model_savepath)
toco = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)
model_tflite = toco.convert()
```
gives:
```python
1.12.0
2.1.6-tf
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 725, 725, 1)       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 675, 675, 6)       60        
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 637, 637, 6)       330       
=================================================================
Total params: 390
Trainable params: 390
Non-trainable params: 0
_________________________________________________________________
None
[-1.9924886 -1.9924886 -1.9924886 -1.9924886 -1.9924886]
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
INFO:tensorflow:Froze 4 variables.
INFO:tensorflow:Converted 4 variables to const ops.

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-48-0a572848505b> in <module>
     15 keras_model.save(keras_model_savepath)
     16 toco = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)
---> 17 model_tflite = toco.convert()

~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py in convert(self)
    451           input_tensors=self._input_tensors,
    452           output_tensors=self._output_tensors,
--> 453           **converter_kwargs)
    454     else:
    455       # Graphs without valid tensors cannot be loaded into tf.Session since they

~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    340   data = toco_convert_protos(model_flags.SerializeToString(),
    341                              toco_flags.SerializeToString(),
--> 342                              input_data.SerializeToString())
    343   return data
    344 

~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    133     else:
    134       raise RuntimeError(""TOCO failed see console for info.\n%s\n%s\n"" %
--> 135                          (stdout, stderr))
    136 
    137 

RuntimeError: TOCO failed see console for info.
b'2019-01-22 11:50:43.257264: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)\n2019-01-22 11:50:43.257374: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)\n2019-01-22 11:50:43.257409: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:217] Replaced sub-network with Dilated Conv2D op outputting ""conv2d/Conv2D"".\n2019-01-22 11:50:43.257458: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] Check failed: height_with_paddings % block_height == 0 (12 vs. 0)\n'
None
```"
25109,Repetitive build issues inside bazel project!,"tensorflow v.1.12.0
bazel 0.18.0

repetitive compilation issue:
```sh
export PYTHON_BIN_PATH=/usr/bin/python                                                                                                                                                                                                                                                                                                                                                                                                  
export USE_DEFAULT_PYTHON_LIB_PATH=1                                                                                                                                                                                                                                                                                                                                                                                                    
export TF_NEED_JEMALLOC=1                                                                                                                                                                                                                                                                                                                                                                                                               
export TF_NEED_KAFKA=0                                                                                                                                                                                                                                                                                                                                                                                                                  
export TF_NEED_OPENCL_SYCL=0                                                                                                                                                                                                                                                                                                                                                                                                            
export TF_NEED_AWS=0                                                                                                                                                                                                                                                                                                                                                                                                                    
export TF_NEED_GCP=0                                                                                                                                                                                                                                                                                                                                                                                                                    
export TF_NEED_HDFS=0                                                                                                                                                                                                                                                                                                                                                                                                                   
export TF_NEED_S3=0                                                                                                                                                                                                                                                                                                                                                                                                                     
export TF_ENABLE_XLA=1                                                                                                                                                                                                                                                                                                                                                                                                                  
export TF_NEED_GDR=0                                                                                                                                                                                                                                                                                                                                                                                                                    
export TF_NEED_VERBS=0                                                                                                                                                                                                                                                                                                                                                                                                                  
export TF_NEED_OPENCL=0                                                                                                                                                                                                                                                                                                                                                                                                                 
export TF_NEED_MPI=0                                                                                                                                                                                                                                                                                                                                                                                                                    
export TF_NEED_TENSORRT=0                                                                                                                                                                                                                                                                                                                                                                                                               
export TF_NEED_NGRAPH=0                                                                                                                                                                                                                                                                                                                                                                                                                 
export TF_NEED_IGNITE=0                                                                                                                                                                                                                                                                                                                                                                                                                 
export TF_NEED_ROCM=0                                                                                                                                                                                                                                                                                                                                                                                                                   
export TF_SET_ANDROID_WORKSPACE=0                                                                                                                                                                                                                                                                                                                                                                                                       
export TF_DOWNLOAD_CLANG=0                                                                                                                                                                                                                                                                                                                                                                                                              
export TF_NCCL_VERSION=2.3                                                                                                                                                                                                                                                                                                                                                                                                              
export NCCL_INSTALL_PATH=/usr                                                                                                                                                                                                                                                                                                                                                                                                           
export CC_OPT_FLAGS=""-march=x86-64""                                                                                                                                                                                                                                                                                                                                                                                                     
export TF_NEED_CUDA=0                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                        
echo \                                                                                                                                                                                                                                                                                                                                                                                                                                  
    //external:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                                       
    @protobuf_archive//:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                              
    //external:protobuf_clib \                                                                                                                                                                                                                                                                                                                                                                                                          
    @protobuf_archive//:protoc_lib                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                        
bazel build \                                                                                                                                                                                                                                                                                                                                                                                                                           
    //external:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                                       
    @protobuf_archive//:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                              
    //external:protobuf_clib \                                                                                                                                                                                                                                                                                                                                                                                                          
    @protobuf_archive//:protoc_lib                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                        
echo '@snappy//:snappy'                                                                                                                                                                                                                                                                                                                                                                                                                 
bazel build '@snappy//:snappy'                                                                                                                                                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                                                                                                                                        
echo \                                                                                                                                                                                                                                                                                                                                                                                                                                  
    @grpc://atomic \                                                                                                                                                                                                                                                                                                                                                                                                                    
    @grpc://grpc \                                                                                                                                                                                                                                                                                                                                                                                                                      
    @grpc://gpr_base \                                                                                                                                                                                                                                                                                                                                                                                                                  
    //external:grpc_cpp_plugin                                                                                                                                                                                                                                                                                                                                                                                                          
bazel build \                                                                                                                                                                                                                                                                                                                                                                                                                           
    @grpc//:atomic \                                                                                                                                                                                                                                                                                                                                                                                                                    
    @grpc//:grpc \                                                                                                                                                                                                                                                                                                                                                                                                                      
    @grpc//:gpr_base \                                                                                                                                                                                                                                                                                                                                                                                                                  
    //external:grpc_cpp_plugin                                                                                                                                                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                                                                                                                                        
echo \                                                                                                                                                                                                                                                                                                                                                                                                                                  
    //tensorflow:libtensorflow_cc.so                                                                                                                                                                                                                                                                                                                                                                                                
bazel build \                                                                                                                                                                                                                                                                                                                                                                                                                           
    //tensorflow:libtensorflow_cc.so         

                                                                                                                                                                                                                                                                                                                                                                                                                                        
echo \                                                                                                                                                                                                                                                                                                                                                                                                                                               
    //tensorflow:install_headers                                                                                                                                                                                                                                                                                                                                                                                                        
bazel build \                                                                                                                                                                                                                                                                                                                                                                                                                                      
    //tensorflow:install_headers                                                                                                                                                                                                                                                                                                                                                                                                                
```

1. With the above script //external:protobuf_headers target is being built multiple times.
2. What is it the point of such a non-constructive attitude towards builds?
3. How can I make sure that ```bazel build //tensorflow:install_headers``` won't recomile 5K object files after calling ```bazel build //tensorflow:libtensor_cc.so```?
4. Do you recompile 5K object files each time you want to compile some 5 lines example? I'm talking about C++ API of tensorflow and libtensorflow_cc.so shared library. Which is not present on almost of all of the normal linux distributions.
5. Does bazel support verbose printing of checked targets?
```sh
bazel build \                                                                                                                                                                                                                                                                                                                                                                                                                           
    //external:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                                       
    @protobuf_archive//:protobuf_headers \                                                                                                                                                                                                                                                                                                                                                                                              
    //external:protobuf_clib \                                                                                                                                                                                                                                                                                                                                                                                                          
    @protobuf_archive//:protoc_lib  
```
If the command above will be called second or third time. The output says built is successful, yet no target lists are being printed. Although, ninja won't say anythin as well in this case, probably."
25108,"Function plot_history in fuel efficiency, the regression example is not synchronized with the notebook","<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_regression

**Describe the documentation issue**
The function plot_history(history) is using pandas DataFrame ""hist"" from outside of the function scope, making it draw the same figure even when the EarlyStopping callbacks are used the second time. This error is corrected in the notebook (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_regression.ipynb), but for some reason, it is not reflected on the website.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I will look into it later."
25107,"Building error on debian, too many commands in cuda-include genrule","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.17.2 (same observed with 0.15.2 and 0.19.1)
- GCC/Compiler version (if compiling from source): gcc-6
- CUDA/cuDNN version: 9.2.148/7.0.5
- GPU model and memory: Nvidia Ti 1080 8GB



**Describe the problem**
While trying to build tensorflow from source encounter segmentation fault from bash executable which I suspect is caused by a line in an automatically generated script that is too long.

The automatically generated script is `bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda-include.genrule_script.sh` which is generated from the genrule that appears in the automatically generated build file at `/home/myusername/.cache/bazel/_bazel_myusername/01d241adc9daa256c4cb8e009a69aaab/external/local_config_cuda/cuda/BUILD`. This genrule lists approximately 35 thousand as outputs which translates into 35 thousand chained copy statements (which succeed if placed on different lines). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Clone github repository
2. Checkout r1.12 branch
3. Run `configure` script which results in following in `.tf_configure.bazelrc`
```
build --action_env PYTHON_BIN_PATH=""/home/nmoran/anaconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/nmoran/anaconda3/lib/python3.7/site-packages""
build --python_path=""/home/nmoran/anaconda3/bin/python""
build --define with_ignite_support=true
build --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr""
build --action_env TF_CUDA_VERSION=""9.2""
build --action_env CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env NCCL_INSTALL_PATH=""/usr/local/nccl_2.3.7-1+cuda9.2_x86_64/lib""
build --action_env NCCL_HDR_PATH=""/usr/local/nccl_2.3.7-1+cuda9.2_x86_64/lib/../include""
build --action_env TF_NCCL_VERSION=""2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-6""
build --config=cuda
test --config=cuda
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
```
4, Attempt to build using
`bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Error message reported is:
```
ERROR: /home/myusername/.cache/bazel/_bazel_myusername/01d241adc9daa256c4cb8e009a69aaab/external/local_config_cuda/cuda/BUILD:206:1: Executing genrule @local_config_cuda//cuda:cuda-include failed (Segmentation fault): bash failed: error executing command /bin/bash bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda-include.genrule_script.sh                                                                        
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.546s, Critical Path: 0.69s
INFO: 9 processes: 9 local.
FAILED: Build did NOT complete successfully
```

First portion of the genrule that is causing the issues:
```
genrule(
    name = ""cuda-include"",
    outs = [
        ""cuda/include/CL/cl.h"",
        ""cuda/include/CL/cl_d3d10.h"",
        ""cuda/include/CL/cl_d3d11.h"",
        ""cuda/include/CL/cl_dx9_media_sharing.h"",
        ""cuda/include/CL/cl_dx9_media_sharing_intel.h"",
        ""cuda/include/CL/cl_egl.h"",
        ""cuda/include/CL/cl_ext.h"",
        ""cuda/include/CL/cl_ext_intel.h"",
        ""cuda/include/CL/cl_gl.h"",
        ""cuda/include/CL/cl_gl_ext.h"",
        ""cuda/include/CL/cl_platform.h"",
        ""cuda/include/CL/cl_va_api_media_sharing_intel.h"",
        ""cuda/include/CL/cl_version.h"",
        ""cuda/include/CL/opencl.h"",
        ""cuda/include/CharLS/charls.h"",
        ""cuda/include/CharLS/publictypes.h"",
        ""cuda/include/EGL/egl.h"",
        ""cuda/include/EGL/eglext.h"",
        ""cuda/include/EGL/eglextchromium.h"",
        ""cuda/include/EGL/eglmesaext.h"",
        ""cuda/include/EGL/eglplatform.h"",
        ""cuda/include/GL/gl.h"",
        ""cuda/include/GL/gl_mangle.h"",
        ""cuda/include/GL/glcorearb.h"",
        ""cuda/include/GL/glext.h"",
        ""cuda/include/GL/glu.h"",
        ""cuda/include/GL/glu_mangle.h"",
        ""cuda/include/GL/glx.h"",
        ""cuda/include/GL/glx_mangle.h"",
        ""cuda/include/GL/glxext.h"",
        ""cuda/include/GL/glxint.h"",
        ""cuda/include/GL/glxmd.h"",
        ""cuda/include/GL/glxproto.h"",
        ""cuda/include/GL/glxtokens.h"",
        ""cuda/include/GL/internal/dri_interface.h"",
        ""cuda/include/GL/internal/glcore.h"",
        ""cuda/include/KHR/khrplatform.h"",
        ""cuda/include/OpenEXR/Iex.h"",
        ""cuda/include/OpenEXR/IexBaseExc.h"",
        ""cuda/include/OpenEXR/IexErrnoExc.h"",
        ""cuda/include/OpenEXR/IexExport.h"",
        ""cuda/include/OpenEXR/IexForward.h"",
...
``` "
25105,Export Control Classification Number (ECCN) for Tensorflow,"**System information**
- TensorFlow version: 1.8.0
- Doc Link:

I would like to use Tensorflow in commercial software which will be sold in the U.S. For this reason, the legal department asks me about Export Control Classification Number (ECCN) for Tensorflow library. From my understanding, the open sources software is not subject to [Encryption and Export Administration Regulations (EAR)](https://www.bis.doc.gov/index.php/policy-guidance/encryption/1-encryption-items-not-subject-to-the-ear). 
Can anyone confirm that Tensorflow is not a subject to EAR or point a ECCN class for Tensorflow? 

Does Tensorflow use any encryption functionality, which should be mention when applying for ECCN for software which uses Tensorflow?
"
25104,Xcode: How to use an static library (a.framework) which force_load  libtensorflow-core.a?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iphone 7
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.6
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

I'm trying to provide an IOS static library as an SDK (named demo.framework) using tensorflow. By tensorflow's instruction, I must link libTensorflow.a with force_load flag in my project. So I link libTensorflow.a with force_load flag in demo.framework's project, and build a library(demo.framework).

In a test, I create a sample app project (appXXX), and link my demo.framework to use the SDK. Compiling was success, but when running it gives the error:

Not found: No session factory registered for the given session options: {target: """" config: } Registered factories are {}.

I try to solve this by link libTensorflow.a with force_load flag for ""other link flags"" in my appXXX's project, it works fine but that is not what I want (In my option, appXXX needn't be care of tensorflow, it just care of demo.framework). How can I provide a ""clean"" demo.framework library without force_load any library in appXXX? 

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25103,facing this error while generating tfrecords in LabelImg,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 4.5.12
- Python version: 3.5.2
- GPU model and memory: GPU 0 - Intel(R) HD Graphics 620 |  GPU 1 - NVIDIA GeForce 920MX | 8gb RAM

**(tensorflow1) C:\tensorflow1\models\research\object_detection>python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record**

Traceback (most recent call last):
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 17, in <module>
    import tensorflow as tf
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\New User\Anaconda3\envs\tensorflow1\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
25102,tutorial example for eager evaluation not working,"I am using anaconda with python 3 and have tensorflow 1.12 
I am trying to run [this](https://www.tensorflow.org/guide/eager) tutorial example 

Using with code: 
```
from __future__ import absolute_import, division, print_function
import tensorflow as tf
tf.enable_eager_execution()
```

and getting this error 

> raise RuntimeError(""tf.placeholder() is not compatible with ""
> RuntimeError: tf.placeholder() is not compatible with eager execution.

What am i missing here? "
25101, tensorflow/contrib/gan/README.md link compromised,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:
https://github.com/tensorflow/tensorflow/commit/3ae375aa92fbb6155f82393735d0b98d8fb9c1b2?diff=split#diff-4ffc4dce469256b24264cb6c7db54363

**Describe the documentation issue**
The link to the TF-GAN tutorial is in tensorflow/contrib/gan/README.md L:59
points to http://https://github.com/tensorflow/models/tree/master/research/gan/tutorial.ipynb but it should point to https://github.com/tensorflow/models/tree/master/research/gan/tutorial.ipynb
The wrong link takes one to some sales page.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25100,xavier_initializer documentations claims use of normal distribution but truncated normal is actually being used,"**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer

**Describe the documentation issue**

According to the documentation the Xavier initializer allows use of both the uniform distribution and the normal distribution.  However, when passing `uniform=False` to `xavier_initializer`, the code in `variance_scaling_initializer` actually uses a truncated normal distribution instead, [code link](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/layers/python/layers/initializers.py#L146).

The fix would consist of adjusting the documentation to provide details about the truncated Normal sampling."
25098,WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS,"WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS
tf.get_collectionï¼ˆGraphKeys.UPDATE_OPSï¼‰is tf.operation,but i use a simple graph is tf.tensor,how to solve it

import tensorflow as tf
 bn = tf.layers.batch_normalization(tf.constant([0.0]), training=True)
 print(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
will output:
[< tf.Tensor 'batch_normalization/AssignMovingAvg:0' shape=(1,) dtype=float32_ref>, 
 < tf.Tensor 'batch_normalization/AssignMovingAvg_1:0' shape=(1,) dtype=float32_ref>]

print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))
[<tf.Variable 'batch_normalization/gamma:0' shape=(1,) dtype=float32_ref>, 
 <tf.Variable 'batch_normalization/beta:0' shape=(1,) dtype=float32_ref>]
"
25096,TF2.0-preview: cannot use layer as activation function anymore,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20190117"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-6228-g69b9e5358b'""
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When I try to use a layer as an activation function in a `Dense` layer, I get an `AttributeError: 'Tensor' object has no attribute 'numpy'`. This did not happen a few days ago in TF 2.0-preview.

**Describe the expected behavior**
I expect no error.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras

#### BAD (raise AttributeError):
my_softplus = keras.layers.Activation(""softplus"")
#my_softplus = keras.layers.Lambda(tf.nn.softplus)
#my_softplus = keras.layers.Lambda(lambda X: tf.nn.softplus(X))
#my_softplus = keras.layers.Lambda(tf.function(lambda X: tf.nn.softplus(X)))

#### GOOD:
#my_softplus = ""softplus""
#my_softplus = tf.nn.softplus
#my_softplus = lambda X: tf.nn.softplus(X)
#my_softplus = tf.function(lambda X: tf.nn.softplus(X))

model = keras.models.Sequential([
    keras.layers.Dense(1, activation=my_softplus, input_shape=[5])
])
```

There is no problem when the `my_softplus` layer is used as a separate layer, but the Keras API specifies that layers can be used like any function, so I expect to be able to use them as activation functions (and it was possible before).

**Other info / logs**

Here is the stacktrace:

```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 455, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py"", line 112, in __init__
    self.add(layer)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 455, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py"", line 167, in add
    layer(x)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 564, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 979, in call
    return self.activation(outputs)  # pylint: disable=not-callable
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 531, in __call__
    base_layer_utils.create_keras_history(inputs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 251, in create_keras_history
    _create_keras_history_helper(tensors, set())
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 288, in _create_keras_history_helper
    constants[i] = backend.function([], [op_input])([])
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3174, in __call__
    [x.numpy() for x in outputs])
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3174, in <listcomp>
    [x.numpy() for x in outputs])
AttributeError: 'Tensor' object has no attribute 'numpy'
```"
25094,ImportError: DLL load failed with error code -1073741795,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 7 Home Premium 64-bit SP1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12.0
- Python version:3.6.8
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:ATI Mobility Radeon HD 6370


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
It failed on importing tensorflow
**Describe the expected behavior**
It should be able to import tensorflow. I don't have problem in Anaconda.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
#install Python3.6.8
#install vc_redist.x64
pip install -U pip virtualenv
virtualenv --system-site-packages -p python ./keras
.\keras\Scripts\activate keras
(keras) pip install --upgrade pip
(keras) pip list
(keras) pip install --upgrade tensorflow
(keras) python
import tensorflow as tf
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license()"" for more information.
import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Stlee\keras\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Stlee\keras\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Stlee\keras\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Stlee\keras\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Stlee\keras\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
25093,can't rollback to python 3.6.5 due to recursive dependency between sphinx-doc and python,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?:Homebrew


**Describe the problem**
I just start learning python programming and wants to use Tensorflow package which requires an order version of Python to be installed. I've searched the web and many recommends using Homebrew for installation. So I tried the following commands but got the errors which I've struggled with for several hours already ...


**Provide the exact sequence of commands / steps that you executed before running into the problem**

`brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here's the error message i got:
Error: python contains a recursive dependency on itself:
  python depends on sphinx-doc
  sphinx-doc depends on python
"
25092,Data Augmentation and pre-processing in tf.data.Dataset,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-2.0-preview
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 10
- GPU model and memory: K80 



**Describe the current behavior**
Suppose I want to do `transfer learning`. I use the `base model` from `tf.keras.appliactions`, for example, `VGG16`. During transfer learning/fine-tuning, there are two steps that we almost always follow:
1) `Pre-process` each batch. e.g  subtract imagenet mean, converting values in range [0,1] or [-1,1]
2) Data Augmentation

The actual pipeline looks like this:
```python

def get _model()
  base_model = tf.keras.applications.vgg16.VGG16(...)
  base_model_output = base_model.output

  # add new layers
  x = tf.keras.layers.Flatten()(base_model_output)(x)
  x = tf.keras.layers.Dense(...)(x)
  ...
  model = Model(base_model.input, output)
  model.compile(..)
  return model

model = get_model()
dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.batch(32).repeat()
```
In order to `pre-process` the images and perform `data augmentation`, I can write a function and map the dataset elements to it. For example,

```python
def _parse_function(filename, label):
  image_string = tf.read_file(filename)
  image_decoded = tf.image.decode_jpeg(image_string)
  image_resized = tf.image.resize_images(image_decoded, [28, 28])
 
  # preprocess
  image_resized = tf.expand_dims(image_resized, axis=0)
  image_resized = vgg.preprocess_input(image_resized)
  image_resized =  tf.squeeze(image_resized)
  
  # perform augmentation using tf or some other library like Augmenter
  image_resized = ....

  return image_resized, label
``` 
This works fine but I see two problems with this approach:
1) We are not utilising `vectorization`. In order to preprocess, we expand the dimensions to reshape the image in `[1, H, W, C]` form and then we apply `vgg.preprocess_input()` function. The problem is that we are doing too many extra operations `expand_dims` and `squeeze` whereas `preprocess_input()` can operate on a batch which is the ideal case.

2) Data Augmentation libraries like [Augmentor](https://github.com/mdbloice/Augmentor) or [imgaug](https://github.com/aleju/imgaug) works with batches and performing augmentation on random samples of a batch makes more sense than for each image but there is no way to achieve this if we are using `map` with `datasets`. 


This is a performance bottleneck IMO.  It might be the case that I am missing something here. Please correct me if that's the case. "
25089,Estimator 1.13 doesn't build against Tensorflow 1.13,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL ppc64le
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13 (preRC)
- Python version: 3.6
- Installed using virtualenv? pip? conda?:  pip install from .whl
- Bazel version (if compiling from source): 19.2
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: P100/16GB



**Describe the problem**

Tensorflow-Estimator built from the 1.13 branch fails to compile against fully built Tensorflow 1.13

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I used this bazel rc file to build Tensorflow (*not 2.0*)
```
build --action_env PYTHON_BIN_PATH=""/home/furmanek/anaconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/furmanek/anaconda3/lib/python3.6/site-packages""
build --python_path=""/home/furmanek/anaconda3/bin/python""
build --action_env OMP_NUM_THREADS=""1""
build:xla --define with_xla_support=true
build --config=xla
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""10.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.5,7.0""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build:opt --copt=-mcpu=native
build:opt --define with_default_optimizations=true
```

`bazel build tensorflow/tools/pip_package/build_pip_package`
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./`
pip install..
`bazel build //tensorflow_estimator/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

TFE Build fail:
```
File ""/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/estimator_lib.py"", line 54, in <module>
    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2
  File ""/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/mode_keys.py"", line 22, in <module>
    from tensorflow.python.training.mode_keys import ModeKeys
ModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'
Target //tensorflow_estimator/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.044s, Critical Path: 1.96s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

The problem is this commit (https://github.com/tensorflow/tensorflow/commit/8bcc801a7cd748e7d9d47f0f5d7ebd84a2f2eaea) was done in Tensorflow master, and this commit (https://github.com/tensorflow/estimator/commit/f7640c1aa61e798fdde5001ed42237ab7f941667) is in Estimator 1.13.
It's probably best to pull that Tensorflow commit into Tensorflow 1.13.

Found on ppc64le, but is not platform specific."
25088,InceptionResnetV2 quantization: block35_1/Relu is lacking min/max data,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- TensorFlow installed from (source or binary): tensorflow-gpu==1.12.0


**Command as follows**
    tflite_model = tf.contrib.lite.toco_convert(
        frozen_graphdef, [images], [logits], inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8,
        quantized_input_stats=[(127.5, 127.5)])

**Graph as follows**
![graph_run 1](https://user-images.githubusercontent.com/22855898/51511778-926d8480-1e3d-11e9-9f24-29d09fb5c7d9.png)

**Provide the text output from toco_convert**
```
F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array InceptionResnetV2/InceptionResnetV2/Repeat/block35_1/Relu, which is an input to the MaxPool operator producing the output array InceptionResnetV2/InceptionResnetV2/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
```

"
25087,Two roundings in MultiplyByQuantizedMultiplier(TFLite) leads to inconsistent result with tensorflow,"https://github.com/tensorflow/tensorflow/blob/ff91cd691027076e6128afbd902d3d38e3672787/tensorflow/lite/kernels/internal/common.h#L105
Per my understanding, `MultiplyByQuantizedMultiplier` is used to simulate floating point multiplication.
But in some cases, the two roundings in it will cause different result from floating point arithmetic, which will further lead to unpredictable tflite accuracy compared with tensorflow training pipeline.

`inline int32 MultiplyByQuantizedMultiplier(int32 x, int32 quantized_multiplier, int shift)`
Assume x = 7984, quantized_multiplier = 1583594044, shift = -9 (the simulated floating point arithmetic is `std::round(7984 * 0.0014402703931141053)`). The multiplication of x and quantized_multiplier will be 12643414847296(0xB7FC6402F40), it will undergo two rounding shift(one in `SaturatingRoundingDoublingHighMul` and the other one in `RoundingDivideByPOT`), and results in number 12. But the floating point result is 11. We can find that if we combine these two rounding shifts into one, i.e. do only one rounding shift, the results will be 12, which matches with floating point result."
25086,tf.keras.layers.UpSampling2D - static shapes for ResizeNearestNeighbor are NOT inferred in GraphDef (with `add_shapes=True`),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **1.12**
- Python version: **3.6**
- Bazel version (if compiling from source): **n/a**
- GCC/Compiler version (if compiling from source): **n/a**
- CUDA/cuDNN version: **CUDA 9 / cuDNN 7**
- GPU model and memory:  **n/a**

### Premise:
Assume we're given a `GraphDef` that was serialized as `graph.as_graph_def()`. To infer static shapes of every node in this graph and store them as `_output_shapes` attr, TensorFlow [provides](https://www.tensorflow.org/api_docs/python/tf/Graph#as_graph_def) the option`add_shapes=True`.

### Issue:
Let's consider the two APIs that internally use `ResizeNearestNeighbor` kernel:
- `tf.image.resize_nearest_neighbor`
- `tf.keras.layers.UpSampling2D`

The static shape attr (`_output_shapes`) for `ResizeNearestNeighbor` node is correctly inferred when created with `tf.image.resize_nearest_neighbor` API. Yet using `tf.keras.layers.UpSampling2D` results in incomplete shape information for height and width dims.

### Minimal examples:

**Using `tf.image.resize_nearest_neighbor` static shapes are inferred correctly:**
```python3
import tensorflow as tf

# Prepare `GraphDef`
tf.reset_default_graph()
input = tf.placeholder(tf.float32, shape=[None, 26, 128, 256])
output = tf.image.resize_nearest_neighbor(input, (52, 256))
graph_def = tf.get_default_graph().as_graph_def()

# Import `GraphDef` and restore static shapes
tf.reset_default_graph()
with tf.Session(graph=tf.Graph()) as sess:
    tf.import_graph_def(graph_def, name='')
    output_graph_def = tf.get_default_graph().as_graph_def(add_shapes=True)

# Check shapes
for node in output_graph_def.node:
    if node.op == 'ResizeNearestNeighbor':
        print(node.attr[""_output_shapes""])
```
**Output**
```
list {
  shape {
    dim {
      size: -1
    }
    dim {
      size: 52
    }
    dim {
      size: 256
    }
    dim {
      size: 256
    }
  }
}
```

**Using `tf.keras.layers.UpSampling2D` static shapes are NOT inferred for H, W:**
```python3
import tensorflow as tf

# Prepare `GraphDef`
tf.reset_default_graph()
input = tf.placeholder(tf.float32, shape=[None, 26, 128, 256])
output = tf.keras.layers.UpSampling2D((2, 2))(input)
graph_def = tf.get_default_graph().as_graph_def()

# Import `GraphDef` and restore static shapes
tf.reset_default_graph()
with tf.Session(graph=tf.Graph()) as sess:
    tf.import_graph_def(graph_def, name='')
    output_graph_def = tf.get_default_graph().as_graph_def(add_shapes=True)

# Check shapes
for node in output_graph_def.node:
    if node.op == 'ResizeNearestNeighbor':
        print(node.attr[""_output_shapes""])
```
**Output**
```
list {
  shape {
    dim {
      size: -1
    }
    dim {
      size: -1
    }
    dim {
      size: -1
    }
    dim {
      size: 256
    }
  }
}
```"
25085,How I can convert .tflite model to a .pb frozen graph or a keras model? ,"I have a post-training quantized .tflite model and would like to convert it back to either a .pb frozen graph or keras model. How can I achieve this?

I have searched a lot but didn't find any solutions or any hints. Anyone has ideas about this?
"
25083,Object Detection API Android demo performance issue,"**System information**
- Windows 10:
- Android app testing on LG Nexus 5:
- TensorFlow installed from source:
- TensorFlow version: 1.12
**Describe the problem**
I have built Object Detection API Android demo and testing on LG Nexus 5. But it has following performance issue:
After some object is detected e.g. ""person"", it is keeping to show bounding box and score for last detected object even if the camera is capturing scene with no objects e.g. black screen.  Screenshots are given below.
**Any other info / logs**
Project has been built using following configuration: 
def nativeBuildSystem = 'none'
Trained Model: SSD MobileNet
<b>Screenshots</b>
With object:
![screenshot_20190122-094725](https://user-images.githubusercontent.com/27653152/51506149-1cadec80-1e2e-11e9-9cf6-6582e5448cf3.png)
Without object:
![screenshot_20190122-100945](https://user-images.githubusercontent.com/27653152/51506151-1fa8dd00-1e2e-11e9-9539-a6f5ddd699ce.png)


"
25082,Wish help: Does the tf.Session.run() function process each tensor one-by-one in a batch??,"Hi,

Currently, I would like to use the tf.Session.run() function to train a CNN model. For this mission, the input data to the model is a batch (4D, [batch_size, height of image, width of image, depths]) contains 16 image blocks. My understanding is that the 16 blocks should be processed by the CNN model one-by-one, and then calculate the mean loss based on the 16 outputs to update the model parameters (one batch data has been processed).  

May I ask if the tf.session.run() will process the image block (a tensor) one-by-one?

Thank you in advance."
25081,checkpoint.restore not working for tf.keras custom model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Mac OS High Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): use default tensorflow on Colab ( 1.12.0 ) I installed 1.12.0 binary for my Mac with pip install tensorflow
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Using CPU on Mac
- GPU model and memory: GPU on Colab. Using CPU on Mac. 


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I followed [nmt_with_attention.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) on Tensorflow Tutorial.
Saved the weight by 
```
# download fines
from google.colab import files

files.download( ""./training_checkpoints/checkpoint"" ) 
files.download( ""./training_checkpoints/ckpt-5.index"" ) 
files.download( ""./training_checkpoints/ckpt-5.data-00000-of-00001"" ) 
```
Then I load the weight on my Mac. Source code are same. I only use 
```
checkpoint_dir = '<my local directory>'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

# restoring the latest checkpoint in checkpoint_dir
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
```

**Describe the expected behavior**
translate test function return same results but 

On training model, I got 
```
Input: <start> hace mucho frio aqui . <end>
Predicted translation: it s very cold here . <end>
```

On restored model, I got
```
Input: <start> hace mucho frio aqui . <end>
Predicted translation: shame back look right work to get right work to get 
```

Looks almost random.
How can I save and load weight of tf.keras custom model properly?
Or does nightly solve the problem?

**Code to reproduce the issue**
I use Tensorflow Tutorial's code and just restore weight on different session.

**Other info / logs**

"
25080,MirroredStrategy is not supported by ExponentialUpdateLossScaleManager and LossScaleOptimizer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf-nightly-gpu==1.13.0.dev20181022
- Python version:
3.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
Cuda 9.2
- GPU model and memory:
1x V100

----------
* TLDR: MirroredStrategy is not supported by [ExponentialUpdateLossScaleManager](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L104) and [LossScaleOptimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L28)

* When attempting to use the LossScaleOptimizer with a MirroredStrategy, we get the following error: 

    * ```ValueError: Outputs of true_fn and false_fn must have the same type: int64, bool``` emanating from [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L158).
    We are able to get around this line by changing:
     ```is_overall_finite, true_apply_gradients_fn, gen_control_flow_ops.no_op)```
    To:
     ```is_overall_finite, true_apply_gradients_fn, lambda: tf.zeros([1], tf.int64))```

* Note, this error does not occur if MirroredStrategy is not enabled, and training proceeds as normal.
If we make the above change, then we can run the LossScaleOptimizer +  FixedLossScaleManager with MirroredStrategy enabled.

* However, if we attempt to use the ExponentialLossScaleManager  with MirroredStrategyEnabled instead of the FixedLossScaleManager we get the following:
```ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context"".``` emanating from this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L170

* This suggests that variables introduced and used by ExponentialLossScaleManager need to be aware of MirroredStrategy.  What exactly do we need to do to make sure ExponentialLossScaleManager variables can be handled by MirroredStrategy?  

* Our suspicion is that we need to handle variables from ExponentialLossScaleManager in _create_slots(), _prepare(), _apply_dense(), and _apply_sparse() as suggested by this part of the code in Optimizer https://github.com/tensorflow/tensorflow/blob/f9b9cf52eb36a5d5e8bbf96fc1f2b6f584ce7867/tensorflow/python/training/optimizer.py#L552
In addition, [this comment](https://github.com/tensorflow/tensorflow/issues/23986#issuecomment-444391385) (which has the same error as we do) suggested that reimplementing the optimizer is the right solution."
25078,ERROR: /tmp/tensorflow-1.9.0/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1) ,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Alpine 3.8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source (https://github.com/tensorflow/tensorflow/archive/v${TENSORFLOW_VERSION}.tar.gz)
- TensorFlow version: 1.9.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?:  Bazel
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA. CPU only
- GPU model and memory: NA. CPU only



**Describe the problem**
Build fails with error
ERROR: /tmp/tensorflow-1.9.0/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1) 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
PFA the docker
[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/2780053/Dockerfile.txt)
[build.log](https://github.com/tensorflow/tensorflow/files/2780059/build.log)



**Any other info / logs**
Traceback (most recent call last): File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/usr/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec) ImportError: Error relocating /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so: je_tsd_tls: initial-exec TLS resolves to dynamic definition in /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py"", line 27, in <module> from tensorflow.python.util import tf_decorator File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module> from tensorflow.python import pywrap_tensorflow File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module> raise ImportError(msg) ImportError: Traceback (most recent call last): File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/usr/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec) ImportError: Error relocating /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so: je_tsd_tls: initial-exec TLS resolves to dynamic definition in /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/install_sources#common_installation_problems for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25077,No code examples for rewriting existing graph using tf.quantization.fake_quant_with_min_max_vars for quantization-aware training,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link:


**Describe the documentation issue**
It would be great if code examples or a tutorial can be written up to help with manually inserting fake quant nodes into an existing TensorFlow graph. The current rewriter provided in contrib/quantize doesn't seem to be able to handle complex graphs well.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25076,tf.Estimator as checkpointable,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (or github SHA if from source): 1.12


**Provide the text output from tflite_convert**

```
ValueError: `Checkpoint` was expecting a checkpointable object (an object derived from `CheckpointableBase`), got <tensorflow.python.estimator.canned.dnn.DNNClassifier object at 0x11b370e48>. If you believe this object should be checkpointable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.

```

Model:
```
self.classifier = tf.estimator.DNNClassifier(
            feature_columns=my_feature_columns,
            # Two hidden layers of 10 nodes each.
            hidden_units=[10, 10],
            # The model must choose between 3 classes.
            n_classes=3)
```
I want to save checkpoints on demand in different directories. I use external optimizer for hyperparameter search ray-tune, which requires implementation of checkpointing which is control by scheduler from ray-tune optimizations and does not interfere into tf.Estimator internal saving procedure.  tf.train.Checkpoint looks like the perfect choice to save checkpoints on demand, yet does not work with tf.estimotor."
25073,fit gets slower on consecutive calls when model is recompiled,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Home

- TensorFlow installed from (source or binary):
using pip3
- TensorFlow version (use command below):
reproduced on both
b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
b'v1.11.0-rc2-4-gc19e29306c' 1.11.0

- Python version:
Python 3.6.7 :: Anaconda, Inc.

**Describe the current behavior**
Over multiple iterations with a re-compile of model, fit() time continuously increases.  If the re-compile is commented out, fit() time remains constant.  I found a similar bug in the js project but not sure if it's related: https://github.com/tensorflow/tfjs/issues/448

**Describe the expected behavior**
Expect fit() time to remain constant even with multiple re-compiles

**Code to reproduce the issue**
`
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten
import numpy as np

input1 = np.array([[0, 1, 1, 1, 0, 0, 0, 1, 1, 1]])
dOutput1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])

model = Sequential()
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=1)
model.add(Dense(25, input_dim=10, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('relu'))
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)
model.add(Dense(25, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('relu'))
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=3)
model.add(Dense(10, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('softmax'))

#loss='categorical_crossentropy'
sgd = tf.keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

for i in range(100):
    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    model.fit(input1, dOutput1, epochs=1, batch_size=1)
    output1 = model.predict(input1)
    output1
`

**Output**
1/1 [==============================] - 0s 155ms/step - loss: 2.2998 - acc: 0.0000e+00
Epoch 1/1
1/1 [==============================] - 0s 131ms/step - loss: 2.2906 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 183ms/step - loss: 2.2815 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 211ms/step - loss: 2.2724 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 235ms/step - loss: 2.2633 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 221ms/step - loss: 2.2543 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 367ms/step - loss: 2.2452 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 324ms/step - loss: 2.2362 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 269ms/step - loss: 2.2271 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 307ms/step - loss: 2.2181 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 316ms/step - loss: 2.2091 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 315ms/step - loss: 2.2002 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 346ms/step - loss: 2.1912 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 404ms/step - loss: 2.1822 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 380ms/step - loss: 2.1732 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 417ms/step - loss: 2.1643 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 421ms/step - loss: 2.1553 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 442ms/step - loss: 2.1464 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 465ms/step - loss: 2.1375 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 486ms/step - loss: 2.1286 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 468ms/step - loss: 2.1197 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 509ms/step - loss: 2.1108 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 544ms/step - loss: 2.1020 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 676ms/step - loss: 2.0932 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 543ms/step - loss: 2.0844 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 584ms/step - loss: 2.0756 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 608ms/step - loss: 2.0668 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 643ms/step - loss: 2.0580 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 651ms/step - loss: 2.0493 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 660ms/step - loss: 2.0406 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 664ms/step - loss: 2.0319 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 702ms/step - loss: 2.0232 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 735ms/step - loss: 2.0145 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 709ms/step - loss: 2.0059 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 773ms/step - loss: 1.9973 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 901ms/step - loss: 1.9887 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 902ms/step - loss: 1.9801 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 948ms/step - loss: 1.9715 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 865ms/step - loss: 1.9629 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 839ms/step - loss: 1.9544 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 987ms/step - loss: 1.9459 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 1s/step - loss: 1.9374 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 1s/step - loss: 1.9289 - acc: 1.0000
Epoch 1/1"
25072,ImportError: DLL load failed: The specified procedure could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro (version: 1803, build: 17134.523)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A (using a Dell G5 5587 laptop)
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 1.12.0 (tensorflow-gpu)
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip (via PyCharm installer)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce 1060 with Max-Q Design (version: 24.21.14.1131)



**Describe the problem**
Getting an error regarding the not finding the dll file. The file I presume it's referring to is `â€ªC:\tools\cuda\bin\cudnn64_7.dll`, which has been added to the path (well, the folder containing it has).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Downloaded/installed prerequisites according to TensorFlow site
Created virtual environment (within PyCharm)
Installed TensorFlow (using the automatic pip options within PyCharm)
Entered python from the terminal
`import tensorflow`
Error/traceback below produced
Trawled online to try to find solutions.... ending with writing this


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\b5027438\PycharmProjects\DAFNI-UO\venv2\lib\site-packages\tensorflow\__init__.py"", l
ine 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\b5027438\PycharmProjects\DAFNI-UO\venv2\lib\site-packages\tensorflow\python\__init__
.py"", line 59, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\b5027438\PycharmProjects\DAFNI-UO\venv2\lib\site-packages\tensorflow\core\framework\
graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\b5027438\PycharmProjects\DAFNI-UO\venv2\lib\site-packages\google\protobuf\descriptor
.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.
```
"
25071,Error loading transfer learning resnet model from frozen graph ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25070,Use my data up bug?,"In the project,
[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands](url)

My code is down:

`if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--data_dir',
      type=str,
      default='D:/python/speechtf/data/',
      help=""""""\
      Where to download the speech training data to.
      """""")
  parser.add_argument(
      '--background_volume',
      type=float,
      default=0.1,
      help=""""""\
      How loud the background noise should be, between 0 and 1.
      """""")
  parser.add_argument(
      '--background_frequency',
      type=float,
      default=0.8,
      help=""""""\
      How many of the training samples have background noise mixed in.
      """""")
  parser.add_argument(
      '--silence_percentage',
      type=float,
      default=10.0,
      help=""""""\
      How much of the training data should be silence.
      """""")
  parser.add_argument(
      '--unknown_percentage',
      type=float,
      default=10.0,
      help=""""""\
      How much of the training data should be unknown words.
      """""")
  parser.add_argument(
      '--time_shift_ms',
      type=float,
      default=0.0,
      help=""""""\
      Range to randomly shift the training audio by in time.
      """""")
  parser.add_argument(
      '--testing_percentage',
      type=int,
      default=10,
      help='What percentage of wavs to use as a test set.')
  parser.add_argument(
      '--validation_percentage',
      type=int,
      default=10,
      help='What percentage of wavs to use as a validation set.')
  parser.add_argument(
      '--sample_rate',
      type=int,
      default=16000,
      help='Expected sample rate of the wavs',)
  parser.add_argument(
      '--clip_duration_ms',
      type=int,
      default=100,
      help='Expected duration in milliseconds of the wavs',)
  parser.add_argument(
      '--window_size_ms',
      type=float,
      default=30.0,
      help='How long each spectrogram timeslice is.',)
  parser.add_argument(
      '--window_stride_ms',
      type=float,
      default=10.0,
      help='How far to move in time between spectogram timeslices.',)
  parser.add_argument(
      '--feature_bin_count',
      type=int,
      default=40,
      help='How many bins to use for the MFCC fingerprint',
  )
  parser.add_argument(
      '--how_many_training_steps',
      type=str,
      default='25000,5000',
      help='How many training loops to run',)
  parser.add_argument(
      '--eval_step_interval',
      type=int,
      default=400,
      help='How often to evaluate the training results.')
  parser.add_argument(
      '--learning_rate',
      type=str,
      default='0.001,0.0001',
      help='How large a learning rate to use when training.')
  parser.add_argument(
      '--batch_size',
      type=int,
      default=100,
      help='How many items to train with at once',)
  parser.add_argument(
      '--wanted_words',
      type=str,
      default='1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88',
      help='Words to use (others will be added to an unknown label)',)
  parser.add_argument(
      '--train_dir',
      type=str,
      default='D:/python/speechtf/train_model/',
      help='Directory to write event logs and checkpoint.')
  parser.add_argument(
      '--save_step_interval',
      type=int,
      default=100,
      help='Save model checkpoint every save_steps.')
  parser.add_argument(
      '--start_checkpoint',
      type=str,
      default='',
      help='If specified, restore this pretrained model before any training.')
  parser.add_argument(
      '--model_architecture',
      type=str,
      default='conv',
      help='What model architecture to use')
  parser.add_argument(
      '--check_nans',
      type=bool,
      default=False,
      help='Whether to check for invalid numbers during processing')
  parser.add_argument(
      '--quantize',
      type=bool,
      default=False,
      help='Whether to train the model for eight-bit deployment')
  parser.add_argument(
      '--preprocess',
      type=str,
      default='mfcc',
      help='Spectrogram processing mode. Can be ""mfcc"" or ""average""')

  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)`



`Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Data too short when trying to read string
	 [[{{node DecodeWav}} = DecodeWav[desired_channels=1, desired_samples=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/python/speechtf/train4.py"", line 404, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""D:\python\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/python/speechtf/train4.py"", line 70, in main
    FLAGS.testing_percentage, model_settings)
  File ""D:/python/speechtf\input_data4.py"", line 176, in __init__
    self.prepare_background_data()
  File ""D:/python/speechtf\input_data4.py"", line 334, in prepare_background_data
    feed_dict={wav_filename_placeholder: wav_path}).audio.flatten()
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Data too short when trying to read string
	 [[node DecodeWav (defined at D:/python/speechtf\input_data4.py:328)  = DecodeWav[desired_channels=1, desired_samples=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]

Caused by op 'DecodeWav', defined at:
  File ""<string>"", line 1, in <module>
  File ""D:\python\lib\idlelib\run.py"", line 144, in main
    ret = method(*args, **kwargs)
  File ""D:\python\lib\idlelib\run.py"", line 474, in runcode
    exec(code, self.locals)
  File ""D:/python/speechtf/train4.py"", line 404, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""D:\python\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/python/speechtf/train4.py"", line 70, in main
    FLAGS.testing_percentage, model_settings)
  File ""D:/python/speechtf\input_data4.py"", line 176, in __init__
    self.prepare_background_data()
  File ""D:/python/speechtf\input_data4.py"", line 328, in prepare_background_data
    wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_audio_ops.py"", line 222, in decode_wav
    desired_samples=desired_samples, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Data too short when trying to read string
	 [[node DecodeWav (defined at D:/python/speechtf\input_data4.py:328)  = DecodeWav[desired_channels=1, desired_samples=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]`
"
25069,Floating point exception when trying to build and compile my keras model  with xla support. ,"SYSTEM INFO:

UBUNTU 16.04
TENSORFLOW 1.12.0 (compiled from source with XLA support)
KERAS 2.2.4
GPU Nvidia Geforce RTX 2080 Ti

I am trying to build and compile a simple CNN with XLA but I get a floating point exception.

# Here's the block of code

`
```
import keras.backend.tensorflow_backend as bck
config = bck.tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = bck.tf.OptimizerOptions.ON_1
bck.set_session(bck.tf.Session(config=config))


model = Sequential()

model.add(Conv2D(32, (3,3),padding=""same"",input_shape=input_shape))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(256))#64
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(34))#num_classes
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='SGD',metrics=[""accuracy""])


model.summary()
model.get_config()


from keras import callbacks

filename='model_train_new.csv'
csv_log=callbacks.CSVLogger(filename, separator=',', append=False)
filepath=""save_xla/Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5""


checkpoint = callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=False, mode='auto',period=1)

callbacks_list = [csv_log,checkpoint]



hist = model.fit(x, y, batch_size=32,epochs=num_epoch, verbose=1,callbacks=callbacks_list)
````


I am getting the following Output for this

2019-01-21 16:33:21.512092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-21 16:33:21.512662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:02:00.0
totalMemory: 10.73GiB freeMemory: 10.33GiB
2019-01-21 16:33:21.512678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-21 16:33:24.538253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-21 16:33:24.538298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-21 16:33:24.538312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-21 16:33:24.538695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9966 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      320       
_________________________________________________________________
activation_1 (Activation)    (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 126, 126, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 63, 63, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 61, 61, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 61, 61, 64)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 57600)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               14745856  
_________________________________________________________________
activation_4 (Activation)    (None, 256)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 34)                8738      
_________________________________________________________________
activation_5 (Activation)    (None, 34)                0         
=================================================================
Total params: 14,782,658
Trainable params: 14,782,658
Non-trainable params: 0
_________________________________________________________________
2019-01-21 16:33:25.223019: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f6f20001130 executing computations on platform CUDA. Devices:
**2019-01-21 16:33:25.223063: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
Floating point exception (core dumped)**


when I remove this part `import keras.backend.tensorflow_backend as bck
config = bck.tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = bck.tf.OptimizerOptions.ON_1
bck.set_session(bck.tf.Session(config=config))`


everything works fine.



 "
25068,[XLA] Support unknown dimension.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source r1.13 w/ XLA
- **TensorFlow version (use command below)**: r1.13
- **Python version**: 3.6.7
- **Bazel version (if compiling from source)**: 0.18.1
- **GCC/Compiler version (if compiling from source)**: gcc 7.3.0
- **CUDA/cuDNN version**: 10.0
- **GPU model and memory**: GTX 1080Ti
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have a graph that selects certain ""records"" from some Tensors, based on an optimization strategy for relevance of the records in the current context. Details don't really matter here. The essence is that the `tf.boolean_mask()` returns a Tensor with a shape containing one unknown dimension. The result is that XLA is compiling the graph for every resulting length of that unknown dimension again and again. I can see that this is the issue, as multiple invocations of this graph lead to timings of being either very fast (3 ms) or very slow (350ms). The longer you wait, the more subgraphs XLA has compiled for a specific length and the more I see the faster invocations (where it does not need to compile anything).

Is there anything available or planned to tell XLA to not fix the unknown length for every invocation, such that the compilation happens only once, for a general length in that specific dimension?
Are there other tricks I can try to circumvent this issue?

### Source code / logs
```python
center = tf.placeholder(...)
diff = tf.subtract(data, center)
mask = tf.less(tf.abs(diff), 1.0) # select near data first
near_data = tf.boolean_mask(diff, mask)
# Now, perform actual computations on near_data
# because we know we can skip doing the computations on the other data, as the result
# will be 0, due to the large distance.
# However, now, near_data has a shape like [None, 5, 5] for example
# and XLA recompiles the graph for every length of the first dimension
```
"
25067,Latest commit in deprecation.py breaks R TensorFlow client ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 28
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly (r1.13)
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


Hi,

the commit 

https://github.com/tensorflow/tensorflow/commit/b97727bc3c7a9216670f361b639a60ed516917e0#diff-fa5fb3b8d9f512ad269f7eb67903ec2b

breaks the R TensorFlow client which uses embedded Python from R (see https://github.com/rstudio/tfdatasets/issues/17)

Specifically, the line

```
frame = stack[-4 if outer else -3]
```

triggers the error

```
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  IndexError: list index out of range

Detailed traceback: 
  File ""/home/key/anaconda3/envs/tf-master-1215/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 314, in new_func
    _call_location(), decorator_utils.get_qualified_name(func),
  File ""/home/key/anaconda3/envs/tf-master-1215/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 103, in _call_location
    frame = stack[-4 if outer else -3]
```

on our side. For us, in one specific example, instead of a list with at least 4 items, `stack` contains a list of 2 tuples, each of which are of length 6.

Unfortunately, this will error and stop execution every time a new deprecation warning is issued.

Current workaround on our side is https://github.com/rstudio/tensorflow/pull/287, however this comes at the cost of totally disabling deprecation warnings.

Is there anything you could do to make this change compatible?

Thanks,
Sigrid

"
25066,AdamWOptimizer doesn't work with MirroredStrategy and tf.get_variable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2
- CUDA/cuDNN version: 9.0.176 / 7.4.1
- GPU model and memory: TITAN X, 12189MiB

**Describe the current behavior**
`tf.contrib.opt.AdamWOptimizer` does not do weight decay when using `MirroredStrategy` along with `tf.get_variable`
**Describe the expected behavior**
`tf.contrib.opt.AdamWOptimizer` should always do weight decay, regardless of how variables are created.

**Code to reproduce the issue**
```
import tensorflow as tf

flags = tf.flags
FLAGS = flags.FLAGS
flags.DEFINE_bool('use_get_variable', False, 'Whether to use tf.get_variable to create variables')


def model_fn(features, labels, mode, params):
    if FLAGS.use_get_variable:
        x = tf.get_variable('x', initializer=[1.])
    else:
        x = tf.Variable([1.], name='x')
    loss = tf.reduce_sum(x * x)  # the original loss is 14.

    # here we let learning rate be 0 so that the updates come only from weight decay.
    # after 1 step of weight decay, the final loss should be 0.81.
    optimizer = tf.contrib.opt.AdamWOptimizer(learning_rate=0.0, weight_decay=0.1)

    tvars = tf.trainable_variables()
    tf.logging.info('All variables: {}'.format(tvars))
    grads = tf.gradients(loss, tvars)
    global_step = tf.train.get_or_create_global_step()
    train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step, decay_var_list=tvars)
    output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)
    return output_spec


def input_fn(params):
    return tf.data.Dataset.from_tensors([0.]).repeat()


def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)

    distribution = tf.contrib.distribute.MirroredStrategy()
    run_config = tf.contrib.tpu.RunConfig(train_distribute=distribution)
    estimator = tf.contrib.tpu.TPUEstimator(use_tpu=False, model_fn=model_fn, config=run_config)
    # here max_steps=2 so that the final loss printed is after 1 step update.
    estimator.train(input_fn=input_fn, max_steps=2)


if __name__ == ""__main__"":
    tf.app.run()
```

**Other info / logs**
I ran the above script with 0 or 1 GPU, both producing:
`python test.py --use_get_variable=False`
![image](https://user-images.githubusercontent.com/7380587/51468725-a1efbd80-1daa-11e9-9328-367769a8c19f.png)

`python test.py --use_get_variable=True`
![image](https://user-images.githubusercontent.com/7380587/51468786-be8bf580-1daa-11e9-8958-d30f1c3dcf19.png)
"
25064,Example missing for using the field `tensor_content` with Golang grpc client,"**Describe the documentation issue**

With my python client i can call my model server properly with the code like this:
```
request.inputs[""input""].CopyFrom(
    tf.contrib.util.make_tensor_proto([165, 60, 35],dtype=dtypes.int32)
)
response = stub.Predict(request,30.0)
```

And in python terminal i checked the output of the `make_tensor_proto`, i got something like this:
```
>>> tf.contrib.util.make_tensor_proto([165, 60, 35],dtype=dtypes.int32)
dtype: DT_INT32
tensor_shape {
  dim {
    size: 3
  }
}
tensor_content: ""\245\000\000\000<\000\000\000#\000\000\000""
```
**But I cannot make this work with Golang, tried to search with different terms cannot find one example how to fill the `tensor_content` field.**

According to the documentations i got:
```
// Serialized raw tensor content from either Tensor::AsProtoTensorContent or
// memcpy in tensorflow::grpc::EncodeTensorToByteBuffer. This representation
// can be used for all tensor types. The purpose of this representation is to
// reduce serialization overhead during RPC call by avoiding serialization of
// many repeated small items.
TensorContent []byte `protobuf:""bytes,4,opt,name=tensor_content,json=tensorContent,proto3"" json:""tensor_content,omitempty""`
```
(do i need to implement the above mentioned `Tensor::AsProtoTensorContent` and `tensorflow::grpc::EncodeTensorToByteBuffer` by myself?)

But tried different ways like:
```
pr          tf.PredictRequest
// Construct the `pr` structure properly.
// ...
pr.Inputs[""input""].TensorContent = []byte{165, 60, 35}
```
etc cannot make it work, always resulted in error message:
```
panic: rpc error: code = Internal desc = transport: received the unexpected content-type ""text/plain; charset=utf-8""
```

**Also if i want to send such a request in JSON, what should i put in the `tensor_content` field:**
```
{
  ""model_spec"": {
    ""name"": ""gender"",
    ""version"": ""1""
  },
  ""inputs"": {
    ""input"": {
      ""dtype"": 3,
      ""tensor_shape"": {
        ""dim"": [
          {
            ""size"": 3
          }
        ]
      },
      ""tensor_content"": ""WzEsMl0=""
    }
  }
}
```

Pls help to provide a proper example in the corresponding docs, thanks."
25063,Problem in installing tensorflow on window 8.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : **window 8.1, 64bit** ,,,intel Core i3-5015U Processor,,,,2.1 GHz Processor,,,,,,4 GB DDR3
- TensorFlow installed from: **conda** 
- Python version: 3.5
- Installed using virtualenv: conda
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: NVIDIA geforce 820M



**Describe the problem** 

(py35) C:\Users\AAMIR SIDDIQUI>conda install tensorflow
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - tensorflow

Current channels:

  - https://conda.anaconda.org/conda-forge/win-32
  - https://conda.anaconda.org/conda-forge/noarch
  - https://repo.anaconda.com/pkgs/main/win-32
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/win-32
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/win-32
  - https://repo.anaconda.com/pkgs/r/noarch
  - https://repo.anaconda.com/pkgs/pro/win-32
  - https://repo.anaconda.com/pkgs/pro/noarch
  - https://repo.anaconda.com/pkgs/msys2/win-32
  - https://repo.anaconda.com/pkgs/msys2/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.

**i tried this also but nothing worked**

(py35) C:\Users\AAMIR SIDDIQUI>conda search tensorflow --channel conda-forge
Loading channels: done

PackagesNotFoundError: The following packages are not available from current cha
nnels:

  - tensorflow

Current channels:
"
25062,AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules',"### System information
- **Linux Ubuntu 16.04**
- **Firefly RK3399**:
- **TensorFlow version: 1.12.0**:
- **Python version: 3.5.2**:

### Describe the problem
I'm trying the objection_detection with opencv and tensorflow. The source code has been given at the last. When I python3 11.py, it comes error. 
Though it says ""AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'"", I find    'set_keras_submodules'exits in init.py of keras_applications. So it confuses me quite a long time. I try to update .pyc in pycache but it didn't work.
```
root@firefly:/usr/local/lib/python3.5/dist-packages/keras_applications# ls -l
total 200
-rw-r--r-- 1 root staff  3116 Jan 18 07:29 __init__.py
drwxr-sr-x 2 root staff  4096 Jan 21 02:53 __pycache__
-rw-r--r-- 1 root staff 13450 Jan 18 07:29 densenet.py
-rw-r--r-- 1 root staff 12632 Jan 18 07:29 imagenet_utils.py
-rw-r--r-- 1 root staff 14725 Jan 18 07:29 inception_resnet_v2.py
-rw-r--r-- 1 root staff 14586 Jan 18 07:29 inception_v3.py
-rw-r--r-- 1 root staff 20306 Jan 18 07:29 mobilenet.py
-rw-r--r-- 1 root staff 21720 Jan 18 07:29 mobilenet_v2.py
-rw-r--r-- 1 root staff 29931 Jan 18 07:29 nasnet.py
-rw-r--r-- 1 root staff 11860 Jan 18 07:29 resnet50.py
-rw-r--r-- 1 root staff  8504 Jan 18 07:29 vgg16.py
-rw-r--r-- 1 root staff  8985 Jan 18 07:29 vgg19.py
-rw-r--r-- 1 root staff 14008 Jan 18 07:29 xception.py
root@firefly:/usr/local/lib/python3.5/dist-packages/keras_applications# sudo vim __init__.py
```
'set_keras_submodules' exits
```
def set_keras_submodules(backend=None,
                         layers=None,
                         models=None,
                         utils=None,
                         engine=None):
    # Deprecated, will be removed in the future.
    global _KERAS_BACKEND
    global _KERAS_LAYERS
    global _KERAS_MODELS
    global _KERAS_UTILS
    _KERAS_BACKEND = backend
    _KERAS_LAYERS = layers
    _KERAS_MODELS = models
    _KERAS_UTILS = utils
```
```
root@firefly:/home/firefly/models/research/object_detection/models# python3 11.py
Traceback (most recent call last):
  File ""11.py"", line 5, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 88, in <module>
    from tensorflow.python import keras
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/__init__.py"", line 25, in <module>
    from tensorflow.python.keras import applications
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/applications/__init__.py"", line 37, in <module>
    keras_applications.set_keras_submodules)[0]:
AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'
```
### Source code / logs
11.py
```
import numpy as np
import os
import sys
import tarfile
import tensorflow as tf
import cv2
import time
from collections import defaultdict

sys.path.append(""../.."")

from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util


MODEL_NAME = 'ssd_mobilenet_v1_coco_2018_01_28'

PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'

PATH_TO_LABELS = os.path.join('/home/firefly/models/research/object_detection/data', 'mscoco_label_map.pbtxt')

model_path = ""/home/firefly/models/research/object_detection/models/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt""

start = time.clock()
NUM_CLASSES = 90

end= time.clock()
print('load the model' ,(end -start))

## Load a (frozen) Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

## Loading label map
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

cap = cv2.VideoCapture(1)
cap.set(3,640)
cap.set(4,480)
with detection_graph.as_default():
    with tf.Session(graph=detection_graph) as sess:
        writer = tf.summary.FileWriter(""logs/"", sess.graph)
        sess.run(tf.global_variables_initializer())

        loader = tf.train.import_meta_graph(model_path + '.meta')
        loader.restore(sess, model_path)
        while(1):
            start = time.clock()
            ret, frame = cap.read()
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            image_np =frame

            image_np_expanded = np.expand_dims(image_np, axis=0)
            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            scores = detection_graph.get_tensor_by_name('detection_scores:0')
            classes = detection_graph.get_tensor_by_name('detection_classes:0')
            num_detections = detection_graph.get_tensor_by_name('num_detections:0')

            (boxes, scores, classes, num_detections) = sess.run(
                [boxes, scores, classes, num_detections],
                feed_dict={image_tensor: image_np_expanded})

            vis_util.visualize_boxes_and_labels_on_image_array(
                image_np, np.squeeze(boxes),
                np.squeeze(classes).astype(np.int32),
                np.squeeze(scores),
                category_index,
                use_normalized_coordinates=True,
                line_thickness=6)
            end = time.clock()

            print ('One frame detect take time:' ,end - start)

            cv2.imshow(""capture"", image_np)
            print('after cv2 show')
            cv2.waitKey(1)
cap.release()
cv2.destroyAllWindows()
```
"
25058,Failed to build for iOS project using Xcode 10.1: Undefined symbols for architecture x86_64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 2.7.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.20.0-homebrew
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
Target: x86_64-apple-darwin18.2.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a
- Xcode: Version 10.1 (10B61)

**Describe the problem**

Error during compiling iOS app with TF static library. The steps are described below.
After compiling TF library from source, importing it in a blank Xcode project I get the following error:

`ld: symbol(s) not found for architecture x86_64`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Cloned the TF repo with: `git clone https://github.com/tensorflow/tensorflow -b r1.12`
2. Configuring:
```
You have bazel 0.20.0-homebrew installed.
Please specify the location of python. [Default is /usr/bin/python]:


Found possible Python library paths:
  /Library/Python/2.7/site-packages
Please input the desired Python library path to use.  Default is [/Library/Python/2.7/site-packages]

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n
No Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: N
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: N
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
Configuration finished 
```
3. Updated makefile for iOS as mentioned in https://github.com/tensorflow/tensorflow/issues/18356 and modified `tensorflow/tensorflow/core/util/work_sharder.cc` file as mentioned by https://github.com/tensorflow/tensorflow/issues/18356#issuecomment-422373566

4. After compiling (successfully) I obtain:

- `tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a`
- `tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a`
- `tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a`

5. I created a new blank Xcode project and following https://stackoverflow.com/questions/37769904/how-to-compile-ios-example-in-tensorflow?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa I linked all the libraries

6. The compile phase ends with errors show below

**Any other info / logs**

`Undefined symbols for architecture x86_64:
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::at(unsigned long) const"", referenced from:
      google::protobuf::io::Tokenizer::IsIdentifier(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libprotobuf.a(tokenizer.o)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__grow_by(unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >& std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__append_forward_unsafe<std::__1::__wrap_iter<char const*> >(std::__1::__wrap_iter<char const*>, std::__1::__wrap_iter<char const*>) in libprotobuf-lite.a(strutil.o)
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >& std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__append_forward_unsafe<char const*>(char const*, char const*) in libprotobuf.a(strtod.o)
  ""std::__1::condition_variable::__do_timed_wait(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l> > >)"", referenced from:
      std::__1::cv_status std::__1::condition_variable::wait_until<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > > const&) in nsync.a(nsync_semaphore_mutex.o)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::replace(unsigned long, unsigned long, char const*)"", referenced from:
      google::protobuf::EnumDescriptor::DebugString(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, google::protobuf::DebugStringOptions const&) const in libprotobuf.a(descriptor.o)
      google::protobuf::Descriptor::DebugString(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, google::protobuf::DebugStringOptions const&, bool) const in libprotobuf.a(descriptor.o)
  ""std::__1::this_thread::sleep_for(std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l> > const&)"", referenced from:
      nsync::nsync_time_sleep(timespec) in nsync.a(time_rep_timespec.o)
  ""std::__1::basic_ostream<char, std::__1::char_traits<char> >::put(char)"", referenced from:
      tensorflow::StatsCalculator::GetShortSummary() const in libtensorflow-core.a(stats_calculator.o)
      tensorflow::StatsCalculator::HeaderString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in libtensorflow-core.a(stats_calculator.o)
      tensorflow::StatsCalculator::GetStatsByNodeType() const in libtensorflow-core.a(stats_calculator.o)
      tensorflow::StatsCalculator::GetStatsByMetric(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::StatsCalculator::SortingMetric, int) const in libtensorflow-core.a(stats_calculator.o)
      tensorflow::StatsCalculator::GetOutputString() const in libtensorflow-core.a(stats_calculator.o)
      tensorflow::PrintOp::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(logging_ops.o)
      tensorflow::PrintV2Op::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(logging_ops.o)`

      ...

`  ""std::__1::basic_ostream<char, std::__1::char_traits<char> >::operator<<(long)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(ctc_loss_calculator.o)
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(ctc_decoder_ops.o)
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(pad_op.o)
""operator new(unsigned long)"", referenced from:
 protobuf_tensorflow_2fcontrib_2fboosted_5ftrees_2fproto_2flearner_2eproto::protobuf_AssignDescriptors() in libtensorflow-core.a(learner.pb.o)    tensorflow::boosted_trees::learner::LearnerConfig::LearnerConfig(tensorflow::boosted_trees::learner::LearnerConfig const&) in libtensorflow-core.a(learner.pb.o)
      tensorflow::boosted_trees::learner::TreeRegularizationConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::TreeRegularizationConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)
      tensorflow::boosted_trees::learner::TreeConstraintsConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::TreeConstraintsConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)
      tensorflow::boosted_trees::learner::LearningRateConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)
      tensorflow::boosted_trees::learner::LearningRateFixedConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateFixedConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)
      tensorflow::boosted_trees::learner::LearningRateLineSearchConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateLineSearchConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)
      ...
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)`
"
25057,tensorflow mirroredstrategy takes forever to start training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- CUDA/cuDNN version: 9.0/7.3
- GPU model and memory: 1080ti

**Describe the current behavior** 
tf.Estimator using mirrored strategy takes forever to start training. Details are provided here https://stackoverflow.com/questions/54125722/tensorflow-mirroredstrategy-takes-forever-to-initialize
After asking the question I additionaly tried different versions of tf/cuda, no changes. Also I was finally able to see the start of the training, It took about 40 minutes! 


**Describe the expected behavior**
The same model starts training in a couple of minutes without distribute strategy

When using much simpler model, like ResNet50 mirrored strategy also lags at startup compared to single GPU, but nevertheless training starts in a couple of minutes. What may be the problem with mirrored strategy? "
25054,How install tensorflow by python 3.7.2 on windows 10?,"**System information**
-- Windows version: Windows 10 64 bit
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: virtualenv 16.2; pip 18.1

**Describe the problem**
I follow the tensorflow installation guide to install tensorflow by cmd methods.
When I input ""pip3 install --user --upgrade tensorflow"" on cmd, the cmd shows the error messages
that is:
""Collecting tensorflow
Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow""

Q1: Is this problem about the python version problem?
Q2: Does the Tensorflow support the Python 3.7.2 version?

Tensorflow Link: [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)
**Provide the exact sequence of commands / steps that you executed before running into the problem**
![image](https://user-images.githubusercontent.com/25798404/51440345-25ed6b00-1d01-11e9-8b0b-df3788807db8.png)

![image](https://user-images.githubusercontent.com/25798404/51440384-7238ab00-1d01-11e9-97fb-f2989aa5bcf0.png)

"
25053,eager execution numpy issues,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12.0
- Doc Link: https://www.tensorflow.org/tutorials/eager/eager_basics#numpy_compatibility


**Describe the documentation issue**
Running the following code:
```
import tensorflow as tf
import numpy as np

tf.enable_eager_execution
#tf.enable_eager_execution()
# Gives following erro:    ValueError: tf.enable_eager_execution must be called at program startup.

ndarray = np.ones([3, 3])

print(""TensorFlow operations convert numpy arrays to Tensors automatically"")
tensor = tf.multiply(ndarray, 42)
print(tensor)


print(""And NumPy operations convert Tensors to numpy arrays automatically"")
print(np.add(tensor, 1))

print(""The .numpy() method explicitly converts a Tensor to a numpy array"")
print(tensor.numpy())
```

Traceback error:
```
Traceback (most recent call last):

  File ""<ipython-input-33-4feddfe31f00>"", line 1, in <module>
    runfile('****/t.py', wdir='****')

  File ""C:\Users\****\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\Users\****\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""****/t.py"", line 21, in <module>
    print(tensor.numpy())

AttributeError: 'Tensor' object has no attribute 'numpy'
```

1st issue: 
In docs it says I should use 
tf.enable_eager_execution()
But if I use it I get following error:
ValueError: tf.enable_eager_execution must be called at program startup.

2nd issue:
Doc says I should get real tensorvalues for arrays, but I get:
```
TensorFlow operations convert numpy arrays to Tensors automatically
Tensor(""Mul_10:0"", shape=(3, 3), dtype=float64)
And NumPy operations convert Tensors to numpy arrays automatically
Tensor(""add_10:0"", shape=(3, 3), dtype=float64)
```

3rd issue:
Based on above Traceback issue:
AttributeError: 'Tensor' object has no attribute 'numpy'

I had to install tensorflow-eigen          1.12.0, since I could not run without it

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
No
"
25052,"[XLA] erf, lgamma, and others do not compute correct values in f16 mode","In https://github.com/google-research/bert/pull/255 @thorjohnsen noticed something was wrong with XLA's `erf` implementation in f16 mode.

At first I thought this was just a precision issue, but upon investigation (trying all f16 inputs) I think the problem is worse than this.  Instead, I notice that for ~4476 inputs to erf with large absolute value (i.e. close to +/- inf) over/underflow to nan instead of +/-1.  And with the exception of these nans, all other inputs to erf have small relative error (less than 0.1%).

I notice a similar problem with lgamma, where negative numbers with large absolute value overflow to nan instead of inf.

acosh has the opposite problem, many inputs which cmath returns nan for, we return -inf.  That's probably not as bad but still may be something we should fix.

The lgamma issue at least can be ""fixed"" by doing the computation in f32 (i.e. `static_cast<f16>(lgamma(static_cast<f32>(x))`) but I don't immediately think this is the right thing to do.  As a starting point, I need to understand the algorithms better.

Many functions also have one or two wrong answers, e.g. when presented with +/-inf.  I suspect that the f32 versions have these problems too.  The f32 versions may also have the same under/overflow problems as the f16 versions, just at more extreme values.  I haven't tested yet.

The issue I'm observing broadly occurs on both CPU and GPU, although CPU in some cases gives slightly different errors (e.g. its acosh seems to be fine, but its asinh has a similar problem [?], returning finite values instead of nan)."
25050,Error: No gradient defined for operation (op type: CropAndResizeGradImage),"I'm running FasterRCNN model with second ordered differentiation. 
but an error occur: 
`No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)` 

I have been looking on the internet and haven't found a solution 
please help
thank you"
25049,provide a better error when keras.layers.Dot is called with a tuple instead of a list,"- TensorFlow version: 1.12

`keras.layers.Dot` (functional API) is intended to be called with a list of layers: `Dot(axes=0)([a, b])`.

If you call it with multiple args (`Dot(axes=0)(a, b)`), it provides a useful error message: ```ValueError: A `Dot` layer should be called on a list of 2 inputs.```.

If you call it with a tuple of layers (`Dot(axes=0)((a, b))`, it fails with a cryptic error message: `TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`.

There is code at the beginning of `Dot.build` that is intended to help with tuples as well:

```python
  @tf_utils.shape_type_conversion
  def build(self, input_shape):
    # Used purely for shape validation.
    if not isinstance(input_shape, list) or len(input_shape) != 2:
      raise ValueError('A `Dot` layer should be called '
                       'on a list of 2 inputs.')
```

However, when `input_shape` is a tuple, `tf_utils.shape_type_conversion` fails, producing the cryptic error message above, before the helpful error message can kick in.

I just lost a couple of hours to this.

I see two possible fixes:

* Teach `tf_utils.shape_type_conversion` to understand tuples.
* Add another decorator before `tf_utils.shape_type_conversion` to diagnose tuple inputs.

This applies to other keras merge layers as well.
"
25047,Installation error with flags,N/A
25046,failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED,"<em>failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED. </em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): on keras
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: 1050Ti



**Other info / logs**
Epoch 1/10
2019-01-19 23:32:25.086846: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.70GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-19 23:32:30.159630: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 595.05MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-01-19 23:32:32.432667: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-01-19 23:32:32.437902: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0000019AF3892E90: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-01-19 23:32:32.444503: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0000019AF3892E90: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-01-19 23:32:32.452222: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.


Anyone have any suggestions, I tried updating to the latest CUDA/cuDNN but still facing this issue."
25045,No code example which allows to use tensorflow lite immediately in iOS,"**System information**
- TensorFlow version: 1.12.0
- Doc Link:
https://www.tensorflow.org/lite/tfmobile/ios_build

Most iOS developers use Swift and Objective C. But tensorflow is written in C++ and ""examples"" require strong C++ knowledge too. In the same time that code contains a lot of hardcoded data and it is very hard to understand how to use it in general!

A lot of time passed. Why not to write a class-wrapper over tensorflow to hide C++ code from developers? For example, code should convert `UIImage` to `NSString` or array of strings"
25044,[TFLite] Feature request: Add support for DepthToSpace op ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):1.13


**Provide the text output from tflite_convert**

```
...  Converting unsupported operation: DepthToSpace
```

**Any other info / logs**

DepthToSpace op has been well used for implementing subpixel convolution (aka pixel shuffle) like [this super-resolution task](https://davidreiman.github.io/posts/galaxy-image-super-resolution.html). 
I know SELECT_TF_OPS option. But its binary size is too huge for mobile application.
Would you like to add DepthToSpace to built-in ops? "
25043,tf.nn.sampled_softmax_loss use deprecated sparse_to_dense,tf.nn.sampled_softmax_loss is using the deprecated ops sparse_to_dense (from tensorflow.python.ops.sparse_ops). Could it be replaced by the `tf.sparse.SparseTensor` and `tf.sparse.to_dense` as the warning suggested?
25042,fight club probably with upgrade not see,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
25041,The scale parameter in the slim.batch_norm,"**System information**
- TensorFlow version: tensorflow-gpu 1.4.0
- Doc Link: [link](https://github.com/tensorflow/tensorflow/blob/467c1c2e215c5481372ba4e685869ae504b9e74f/tensorflow/contrib/layers/python/layers/layers.py#L226)

I try to know the details of slim.batch_norm's parameters. We know the formula of the BN is like y = gamma * x + beta. And when the BN layer before the non-linear layer (like the relu), why the gamma need to set 1.0? Does it will harm the performance? In addition, in the doc, the relu is linear? why is the scaling done by the relu?
```
 scale: If True, multiply by `gamma`. If False, `gamma` is
            not used. When the next layer is linear (also e.g. `nn.relu`), this can be
            disabled since the scaling can be done by the next layer.
```"
25040,tf.hessians vs tf.hessian_vector_product won't give the same result,"I want to test whether the hessian of an objective loss is equal to the hessian vector product (HVP) of the same loss when we pass a vector of ones to the HVP function.
Here is my simple code to test this idea and surprisingly the result of the two are not the same.
I've put enough comments in the code to follow the problem easier and the hessian_vector_product function is taken from the tensorflow implementation in [here](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/gradients_impl.py):

```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
from tensorflow import gradients

def hessian_vector_product(ys, xs, v):
  """"""Multiply the Hessian of `ys` wrt `xs` by `v`.
  This is an efficient construction that uses a backprop-like approach
  to compute the product between the Hessian and another vector. The
  Hessian is usually too large to be explicitly computed or even
  represented, but this method allows us to at least multiply by it
  for the same big-O cost as backprop.
  Implicit Hessian-vector products are the main practical, scalable way
  of using second derivatives with neural networks. They allow us to
  do things like construct Krylov subspaces and approximate conjugate
  gradient descent.
  Example: if `y` = 1/2 `x`^T A `x`, then `hessian_vector_product(y,
  x, v)` will return an expression that evaluates to the same values
  as (A + A.T) `v`.
  Args:
    ys: A scalar value, or a tensor or list of tensors to be summed to
        yield a scalar.
    xs: A list of tensors that we should construct the Hessian over.
    v: A list of tensors, with the same shapes as xs, that we want to
       multiply by the Hessian.
  Returns:
    A list of tensors (or if the list would be length 1, a single tensor)
    containing the product between the Hessian and `v`.
  Raises:
    ValueError: `xs` and `v` have different length.
  """""" 

  # Validate the input
  length = len(xs)
  if len(v) != length:
    raise ValueError(""xs and v must have the same length."")

  # First backprop
  grads = gradients(ys, xs)

  # grads = xs

  assert len(grads) == length

  elemwise_products = [
      math_ops.multiply(grad_elem, array_ops.stop_gradient(v_elem))
      for grad_elem, v_elem in zip(grads, v) if grad_elem is not None
  ]

  # Second backprop  
  grads_with_none = gradients(elemwise_products, xs)
  return_grads = [
      grad_elem if grad_elem is not None \
      else tf.zeros_like(x) \
      for x, grad_elem in zip(xs, grads_with_none)]
  
  return return_grads


k = 1
tf.reset_default_graph()
a = tf.get_variable('a', [k])
#b = tf.get_variable('b', [k])
b = tf.multiply(a,a) # b = a^2
c = tf.multiply(tf.multiply(a,a),a) + tf.multiply(b,b) # c = a^3 + b^2

# Calculate the hessian of c w.r.t. a and b
# This is equivalent to the following matrix:
#    -               -
#   | 6a+12a^2    4a  |
# H=|                 |
#   | 4a          2.0 | 
#    -               -

# However the following line returns this matrix or basically just the diagonal values:
#       -               -
#      | 6a+12a^2     0  |
# hess=|                 |
#      |    0        2.0 | 
#       -               -
hess = tf.hessians(c, [a,b])

# while the following line which calculates Hv or H.1 = H, returns this vector:
#       -                     -
# hvp= | 6a+12a^2+4a    2.0+4a |
#       -                     -

v = [np.ones((k),dtype=np.float32),np.ones((k),dtype=np.float32)]
hvp = hessian_vector_product(c, [a,b], v)

# which is basically the summation of derivatives in hessian matrix along the rows.

# According to this observation (and similar tests), 
# the result of tf.hessians always return zero for off-diagonal elements,
# which shouldn't be the case
# On the other hand hessian vector product (HVP) function, only returns the summation of hessians over rows
# and we can't access to off-diagonal elements of the hessian matrix using this function, too.

sess = tf.Session()
sess.run(tf.global_variables_initializer())

_a, _hess, _hvp = sess.run([a, hess, hvp])
```

This problem will be avoided if the variable `b` is not depended on `a` and thus the hessian matrix only should have values on the diagonals and all the other values will be zero. Therefore, HVP and hessian matrix would have the same values. However, as we all know this is a rare scenario and in most of the real-world applications trainable variables are depended to each other."
25039,dense to sparse,"Since we have sparse to dense op, I think it would be helpful if there is a dense_to_sparse op in Tensorflow. Just like we have a tensor A and it can be converted to a sparse tensor by using a function such as tf. sparse_to_dense(A)."
25036,Keras subclassed model layers' output shape detection (e.g. for summary),"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Currently output shapes inference of all layers is essentially disabled if model is subclassed, which results in a rather vague ""multiple"" in model summary. Example:

```python
import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense = tf.keras.layers.Dense(1)

    def call(self, inputs, **kwargs):
        return self.dense(inputs)

model = MyModel()
model.build(input_shape=(None, 1))
model.summary()
```

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                multiple                  2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
```

After investigating source code I understand why this happens from a technical point of view, but it is quite frustrating for an end user, especially given that it is quite trivial to infer the shapes manually with given information. I think there should be a way to force `layer.output_shape` recalculation if `model.build()` is manually called. There is of course a risk of mismatching actual input shapes, but that would be responsibility of the `.build()` caller to enforce. 

Alternatively, allow runtime input(s) shape argument to be passed to `model.summary()`, that could be used for one-time shape detection and validation.

**Will this change the current api? How?**

No, not directly. Could possibly affect internal checks if `layer.output_shape` is manually overriden. However, currently that parameter is completely unused for subclassed models, so risk should be low.

**Who will benefit with this feature?**

End users of subclassed model providers who could be interested in inspecting model topology.

**Any Other info.**

I am willing to contribute either option with a little bit of guidance on the direction."
25034,Keras_to_tpu_model: validation_split parameter does not remove validation samples from training set,"**System information**
- Using unmodified Google Colab TPU runtime

== cat /etc/issue ===============================================
Linux 07e8e3e8ed41 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04.1 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 07e8e3e8ed41 4.14.79+ #1 SMP Wed Dec 19 21:19:13 PST 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
mesh-tensorflow          0.0.5                
msgpack-numpy            0.4.3.2              
numpy                    1.14.6               
protobuf                 3.6.1                
tensorflow               1.12.0               
tensorflow-hub           0.2.0                
tensorflow-metadata      0.9.0                
tensorflow-probability   0.5.0                

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.


== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7

**Describe the current behavior**
When using the `tf.contrib.tpu.keras_to_tpu_model` API for training models on the TPU, providing a `validation_split` parameter does not remove the validation samples from the training set.  This causes the reported validation loss/metrics to be artificially high since we are training on samples that should have been withheld.

**Describe the expected behavior**
Specifically, when calling the fit method with a `validation_split` parameter, a proportion of the samples equal to (count of samples) * (validation_split) should be removed from the training data.

**Code to reproduce the issue**
A Colab notebook that reproduces the issue is here:
https://colab.research.google.com/drive/13g_eEYBUGw9GR6XVjop4TOQEKEDCJQVG

Specifically, a dummy dataset with 256 samples with validation_split = 1/8 should result in 224 training samples and 32 validation samples, but currently shows 256 training samples and 32 validation samples.

**Other info / logs**
I am very grateful for to everyone that is working on this interface.  This makes the on-ramp to TPU training much more manageable."
25033,BatchNormalization layer not being folded when converting quantization-aware trained pb to tflite using tflite_convert,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 26
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-dev20190118
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.4.1
- GPU model and memory: Nvidia Titan RTX


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When converting a pb model file (that includes BatchNorm layers) to tflite, the resulting tflite graph continues to contain batchnorm layers in the form of mul and add ops.

**Describe the expected behavior**
Batch normalization ops should have been folded into weights and biases of previous layers in the tflite graph in order to optimize inference latency.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25029,ImportError: cannot import name 'pywrap_tensorflow',"**NOTE:**  this is not related to the common case of being inside the tensorflow folder.

**System information**
- Mac OSX 10.11.6
- installed 1.5 with pip (I use old version because old Mac OSX version)
- TensorFlow version:
- Python version: 3.6.6
- Inside virtualenv and with pip

**Describe the problem**
I have flask app which I install as python package and TF is installed as part of the required packages. When I run the flask app, I get: 

```
Traceback (most recent call last):
  File ""studio/app.py"", line 9, in <module>
    from simple_neural_net import simple_neural_net
  File ""/Users/mikko/dev/studio/simple_neural_net.py"", line 3, in <module>
    from keras.models import Sequential
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/utils/conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/backend/__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/mikko/dev/studio_test/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name 'pywrap_tensorflow'
```
The problem manifest in the environment regardless of how I try to import tensorflow (i.e. not just inside the flask app). On the same machine I have several envs (including one I installed yesterday) which work. 
"
25027,you mindmodeling must stop generate sound wave plz in future,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): last
- Are you willing to contribute it (Yes/No): maybe i can, if you math teory about security



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?** yes archvie current. close do

**Who will benefit with this feature?** deads people

**Any Other info.** you, theserflow kill 1 peple
"
25026,Can not build debug wheel due to zip overflow(linux),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.4
- TensorFlow installed from: Source
- TensorFlow version: 3ed46c325f70e2f1521b850b43d2b174101d9472
- Python version: 3.4
- Bazel version (if compiling from source):  0.19.2 (not from source)
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the problem**

Cant create wheel file for a build with debug symbols.

Command:
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`
Output:
`
Fri Jan 18 17:01:09 EET 2019 : === Preparing sources in dir: /tmp/tmp.Qpy8sDXUC9
/tmp/tensorflow /tmp/tensorflow
/tmp/tensorflow
Fri Jan 18 17:01:22 EET 2019 : === Building wheel
warning: no files found matching '*.pyd' under directory '*'
warning: no files found matching '*.pd' under directory '*'
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*' under directory 'tensorflow/include/Eigen'
warning: no files found matching '*.h' under directory 'tensorflow/include/google'
warning: no files found matching '*' under directory 'tensorflow/include/third_party'
warning: no files found matching '*' under directory 'tensorflow/include/unsupported'
Traceback (most recent call last):
  File ""setup.py"", line 295, in <module>
    keywords='tensorflow tensor machine learning',
  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/setuptools/__init__.py"", line 143, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib64/python3.4/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/usr/lib64/python3.4/distutils/dist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""/usr/lib64/python3.4/distutils/dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/bdist_wheel.py"", line 250, in run
    wf.write_files(archive_root)
  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py"", line 122, in write_files
    self.write(path, arcname)
  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py"", line 136, in write
    self.writestr(zinfo, data, compress_type)
  File ""/home1/private/mavridis/.local/lib/python3.4/site-packages/wheel/wheelfile.py"", line 139, in writestr
    ZipFile.writestr(self, zinfo_or_arcname, bytes, compress_type)
  File ""/usr/lib64/python3.4/zipfile.py"", line 1447, in writestr
    data = co.compress(data) + co.flush()
OverflowError: Size does not fit in an unsigned int
`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
First build TF with debug symbols, no GPU support (runs succesfully):

`bazel build --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false --config=opt -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package`

Build pip wheel file (FAILS):
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`


**Any other info / logs**

Thanks in advance"
25024,Tensorflow GPU build for Java and Cuda 9.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.11
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): MinGW
- CUDA/cuDNN version: 9.1/7.1
- GPU model and memory: 1050 Ti 3GB

I want to build Tensorflow 1.11 with GPU support and the appropriate Java JNI libraries. The build tool that I'm using is CMake. To build the jni-libraries, is there any options I have to supply to CMake, or are these made as default in the build process?  Bazel for windows did not work out for me. 

I have been at this for some (a frustrating amount of time) time now. Any help is appreciated. Solving this issue seems problematic at best. Is what I'm trying to do possible at all? Has anyone else managed to do this? All other issues I have found on this haven't helped. 

Thank you. 
"
25022,Bazel Build Tensorflow error,"
root@gpu-B85M-D3H:/home/gpu/Downloads/tensorflow# bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
WARNING: Processed legacy workspace file /home/gpu/Downloads/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
Starting local Bazel server and connecting to it...
ERROR: /home/gpu/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 

/home/gpu/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
INFO: Elapsed time: 4.158s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (198 packages loaded)
    currently loading: tensorflow/core/kernels ... (2 packages)



kindly help me to get sloved "
25021,"[TFLite, Quantization, Performance] float32 nodes faster than uint8.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0, b'v1.12.0-6341-g8a5d48a'
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
I converted my TF model to two TFLite models: float32 and uint8. Then I compared them and nodes DEPTHWISE_CONV2D and SOFTMAX with float32 were faster than with uint8.

**Describe the expected behavior**
I thought, that at least conv2d with uint8 will faster than with float32.

**Code to reproduce the issue**
My code:
```
import tensorflow as tf

for fl_quant in (0, 1):
    X = tf.placeholder(tf.float32, [100, 10, 10, 1], 'X')

    W1 = tf.Variable(tf.random_normal([3, 3, 1, 32]), name='W1')
    B1 = tf.Variable(tf.random_normal([32]), name='B1')

    W2 = tf.Variable(tf.random_normal([3200, 10]), name='W2')
    B2 = tf.Variable(tf.random_normal([10]), name='B2')

    XW1 = tf.nn.conv2d(X, W1, [1, 1, 1, 1], 'SAME')
    XWB1 = tf.nn.relu6(tf.nn.bias_add(XW1, B1))
    XWB1 = tf.reshape(XWB1, [100, 3200])
    XWB2 = tf.nn.bias_add(tf.matmul(XWB1, W2), B2)
    Result = tf.nn.softmax(XWB2)

    if fl_quant:
        tf.contrib.quantize.create_eval_graph(tf.get_default_graph())

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
        converter_dot = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])
        converter_tflite = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])
    tf.reset_default_graph()
    if fl_quant:
        converter_dot.inference_type = tf.lite.constants.QUANTIZED_UINT8
        converter_dot.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
        converter_dot.quantized_input_stats = {'X': (127, 127)}
        converter_dot.default_ranges_stats = (0, 255)

        converter_tflite.inference_type = tf.lite.constants.QUANTIZED_UINT8
        converter_tflite.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
        converter_tflite.quantized_input_stats = {'X': (127, 127)}
        converter_tflite.default_ranges_stats = (0, 255)
    converter_dot.output_format = tf.lite.constants.GRAPHVIZ_DOT
    model_dot = converter_dot.convert()
    model_tflite = converter_tflite.convert()
    if fl_quant:
        open(""quant_model.dot"", ""wb"").write(model_dot)
        open(""quant_model.tflite"", ""wb"").write(model_tflite)
    else:
        open(""model.dot"", ""wb"").write(model_dot)
        open(""model.tflite"", ""wb"").write(model_tflite)
```

**Other info / logs**
Benchmark for float32:
```
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [model.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model model.tflite
resolved reporter
Initialized session in 0.378ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=302 first=5983 curr=1592 min=1586 max=5983 avg=1652.99 std=360

Running benchmark for at least 50 iterations and at least 1 seconds
count=626 first=1675 curr=1587 min=1586 max=1684 avg=1592.71 std=14

============================== Run Order ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         45.131%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         98.456%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]

============================== Top by Computation Time ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         53.325%             0.000              1       [BiasAdd_1]
               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         98.456%             0.000              1       [Relu6]
                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]

Number of nodes executed: 3
============================== Summary by node type ==============================
                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]
                 FULLY_CONNECTED                1            0.849          53.363%         53.363%          0.000              1
               DEPTHWISE_CONV_2D                1            0.718          45.129%         98.492%          0.000              1
                         SOFTMAX                1            0.024           1.508%        100.000%          0.000              1

Timings (microseconds): count=626 first=1675 curr=1587 min=1585 max=1684 avg=1592.5 std=14
Memory (bytes): count=0
3 nodes observed


Average inference timings in us: Warmup: 1652.99, Init: 378, no stats: 1592.71
```

Benchmark for uint8:
```
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [quant_model.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model quant_model.tflite
resolved reporter
Initialized session in 0.311ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=159 first=9037 curr=3022 min=3020 max=9037 avg=3152.67 std=598

Running benchmark for at least 50 iterations and at least 1 seconds
count=316 first=3222 curr=3194 min=3020 max=3254 avg=3162.37 std=67

============================== Run Order ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]

============================== Top by Computation Time ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]

Number of nodes executed: 3
============================== Summary by node type ==============================
                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]
               DEPTHWISE_CONV_2D                1            2.829          89.497%         89.497%          0.000              1
                 FULLY_CONNECTED                1            0.283           8.953%         98.450%          0.000              1
                         SOFTMAX                1            0.049           1.550%        100.000%          0.000              1

Timings (microseconds): count=316 first=3222 curr=3194 min=3019 max=3254 avg=3162.23 std=67
Memory (bytes): count=0
3 nodes observed


Average inference timings in us: Warmup: 3152.67, Init: 311, no stats: 3162.37
```

"
25020,installation error,"~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-7-822a3379d161> in <module>
----> 1 import tensorflow as tf
      2 import pandas as pd
      3 import tflearn
      4 from tflearn.layers.conv import conv_3d, max_pool_3d
      5 from tflearn.layers.core import input_data, dropout, fully_connected

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     26 
     27 # pylint: disable=g-bad-import-order
---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     29 
     30 from tensorflow._api.v1 import app

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long"
25018,error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory,"techv@techversant-pc:~/Desktop/django-deepspeech-server/tensorflow$ tensorflow/contrib/makefile/gen/bin/benchmark  --graph=$HOME/graphs/inception/tensorflow_inception_graph.pb
tensorflow/contrib/makefile/gen/bin/benchmark: error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory
"
25016,import tensorflow in python3 -> TypeError: __new__() got an unexpected keyword argument 'serialized_options',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>


**Describe the current behavior**

I have the following issue when importing tensorflow in python3 since some days ago:
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 59, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/node_def_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/tensor_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/core/framework/resource_handle_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __new__() got an unexpected keyword argument 'serialized_options'

```

**Describe the expected behavior**
```
>>> import tensorflow as tf
>>> 
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution: Ubuntu 18.04 (4.15.0-43-generic x86_64 GNU/Linux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO 
- TensorFlow installed from (source or binary): NO (using pip3)
- TensorFlow version (use command below): 1.12.0 (command below is not working as tf doesn't load)
- Python version: Python 3.6.7
- Bazel version (if compiling from source): NO
- GCC/Compiler version (if compiling from source): NO
- CUDA/cuDNN version: NO CUDA
- GPU model and memory: Intel HD Graphics 5500:
```
$ sudo lshw -C display
  *-display                 
       description: VGA compatible controller
       product: HD Graphics 5500
       vendor: Intel Corporation
       physical id: 2
       bus info: pci@0000:00:02.0
       version: 09
       width: 64 bits
       clock: 33MHz
       capabilities: msi pm vga_controller bus_master cap_list rom
       configuration: driver=i915 latency=0
       resources: irq:44 memory:f0000000-f0ffffff memory:e0000000-efffffff ioport:3000(size=64) memory:c0000-dffff

```


<em>You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"":</em>

This doesn't work as tf doesn't load.


**Code to reproduce the issue**
`>>> import tensorflow as tf`

**Other info / logs**
<em>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</em>

```
$ pip3 list | grep tensor
tensorboard                   1.12.0     
tensorflow                    1.12.0 
```
Everything is up-to-date for tensorflow:
```
$ sudo -H pip3 install --upgrade tensorflow
Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (1.12.0)
Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.1)
Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.6.1)
Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)
Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)
Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.6)
Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)
Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)
Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)
Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.4)
Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.5)
Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.30.0)
Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)
Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.6.1->tensorflow) (39.0.1)
Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)
Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)

```"
25012,TF Build Fails: missing input file '@enum34_archive//:LICENSE',"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Cent OS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version: 2.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): GCC 6.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Tensorflow build fails with this error:
ERROR: missing input file '@enum34_archive//:LICENSE'
ERROR: /ec/fm/disks/nrvlab_300G_work01/mabuzain/pr/private-tensorflow/tensorflow/tools/pip_package/BUILD:240:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@enum34_archive//:LICENSE'
"
25010,Location of Policy Gradient Algorithm in python/grappler for device placement,"I have been going over this paper: [Device Placement Optimization with Reinforcement Learning](https://arxiv.org/abs/1706.04972) and I have been reviewing the corresponding code within [python/grappler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/grappler). It seems that the starting point for this code is [graph_placer.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/graph_placer.py), which sets up the RL model for device placement.

I want to see what happens when I adjust the reinforcement model optimization policies. According to the paper, it is currently using a generic policy gradient algorithm. I'm having a hard time location the mechanism of this algorithm in grappler. Can someone point me towards the location (line number and file) of this code and help explain to me how it works?"
25007,FAILED: Build did NOT complete successfully,"**System information**
- OS Platform and Distribution: Ubuntu 16.04.5 LTS
- TensorFlow installed from: Docker image `tensorflow/tensorflow:nightly-devel-gpu`
- TensorFlow version: `master` branch
- Python version: 2 and 3
- Installed using: Docker


**Describe the problem**

I want to submit pull requests and am unable to run the TensorFlow unit tests. I will be happy to add directions to the documentation once I solve the problem.

I set up an Azure virtual machine with Tesla M60 GPUs (the most recent that my academic subscription allows) with the image `Data Science Virtual Machine` on a `Standard NV6 (6 vcpus, 56 GB memory)`. I ssh into it, run these commands, and get the error in the title.

```bash
sudo docker pull tensorflow/tensorflow:nightly-devel-gpu
sudo nvidia-docker run -it abbaea1b533c
```

Then, inside the Docker container:

```bash
export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH""
export flags=""--config=opt --config=cuda -k""
cd /tensorflow/
bazel test ${flags} //tensorflow/python/...
```

I attach the whole console output, which ends with

```
FAILED: Build did NOT complete successfully
```
[failed_build_log.txt](https://github.com/tensorflow/tensorflow/files/2770095/failed_build_log.txt)


How can I set up a machine that runs TensorFlow's unit tests?"
25006,install TensorFlow without using GPU,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14
- TensorFlow installed from (source or binary): 
`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.1-py3-none-any.whl`
- TensorFlow version: 1.10.1
- Python version: 3.6.3
- Installed using virtualenv? pip? conda?: pip (and I also tried pip3)
- Bazel version (if compiling from source): 0.21.0(added this information in 1/20)
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:

**Describe the problem**

I want to use TensorFlow in my laptop, and I would like to use TensorFlow without using GPU.

When I started Python on the command line, the following error occurred.
I have found a number of responses to similar errors, but they are all considered to be using GPUs because they are using CUDA.
In addition, although I got information that it operates by invalidating SIP by #19720, it did not solve my problem because I don't have directory like named 'cuda'.

Also, because of eGPU 's circumstances, mojave got information that GPEN version of TensorFlow will not work. I would like to ask about this as well.

```
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct  6 2017, 12:04:38) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/yuichikato/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/yuichikato/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/yuichikato/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/yuichikato/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib
  Referenced from: /Users/yuichikato/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Reason: image not found


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```"
25003,Unable to figure out CTS tests for Android NNAPI for  SOFTMAX inputs/outputs ,"Have been using Android P for our internal Product Ramp up  and looked into following code where I am unable to figure out , how below  softmax input  and output could match like what the mathematical formula here ?...any one could help me to understand or any link for documentations around it ?
 http://androidxref.com/9.0.0_r3/xref/frameworks/ml/nn/runtime/test/generated/examples/softmax_float_1.example.cpp 


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25002,Tensorflow segmentation fault with RTX 2080Ti CUDA 9.0,"<em>Recently, I try to run my previous programming with our new multiple GPUs servers with RTX 2080 Ti. I do not make any about my code which can run successfully on Cuda 9.0 with Tesla V100. I am not sure what the problem is and it seems that there is a problem with Cuda support. When I run the optimizer of Tensorflow, I get the error of segmentation fault. Self-attention GAN is one example that I cannot run successfully. This link is the source code. https://github.com/taki0112/Self-Attention-GAN-Tensorflow</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Conda
- TensorFlow version (use command below): 1.10.1
- Python version:Python 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): GCC 7.2.0
- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176
- GPU model and memory: RTX 2080Ti

**Describe the current behavior**
Tensorflow can build graph successfully, but when it comes to flow the data into model and update parameters,  it always gives the segmentation fault error. The error is as the following.

generator/attention/h_conv/bias:0 (float32_ref 256) [256, bytes: 1024]
generator/attention/gamma:0 (float32_ref 1) [1, bytes: 4]
generator/up_conv_2/kernel:0 (float32_ref 3x3x256x128) [294912, bytes: 1179648]
generator/up_conv_2/bias:0 (float32_ref 128) [128, bytes: 512]
generator/batch_norm_2/beta:0 (float32_ref 128) [128, bytes: 512]
generator/batch_norm_2/gamma:0 (float32_ref 128) [128, bytes: 512]
generator/up_conv_3/kernel:0 (float32_ref 3x3x128x64) [73728, bytes: 294912]
generator/up_conv_3/bias:0 (float32_ref 64) [64, bytes: 256]
generator/batch_norm_3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/batch_norm_3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/G_conv_logit/kernel:0 (float32_ref 3x3x64x3) [1728, bytes: 6912]
generator/G_conv_logit/bias:0 (float32_ref 3) [3, bytes: 12]
Total size of variables: 19748741
Total bytes of variables: 78994964
 [*] Reading checkpoints...
 [*] Failed to find a checkpoint
 [!] Load failed...
Segmentation fault (core dumped)


**Describe the expected behavior**
The program should run successfully.

**Code to reproduce the issue**
 Self-attention GAN is one example that I cannot run successfully. This link is the source code. https://github.com/taki0112/Self-Attention-GAN-Tensorflow. Maybe this code is a little bit complex, but it can show you where the problem is.

**Other info / logs**
It seems that there is a version mismatching between the CUDA version and the RTX support, but I am not sure."
25001,close,
25000,"""no such package '@com_google_protobuf//'"" while running ""bazel test"" (r1.13)","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow version: 1.13
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: created virtualenv for build process
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0.130 / 7.4.2.24
- GPU model and memory: Tesla P40 (inside virtual machine via Nvidia Virtual GPU), 8GB memory



**Describe the problem**
During the build process described on the tensorflow website for r1.13, the command `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...` fails due to not finding the package `'@com_google_protobuf//'`. The full error message is:
```
ERROR: /home/cgv/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: no such package '@com_google_protobuf//': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replacement.
Use --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.
ERROR: Analysis of target '//tensorflow/core:example_protos' failed; build aborted: Analysis failed
```
If I use the switch `--incompatible_remove_native_http_archive=false` as the error message suggests, I receive different errors:
```
ERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:597:1: Traceback (most recent call last):
	File ""/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD"", line 597
		internal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)
	File ""/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/protobuf.bzl"", line 266, in internal_gen_well_known_protos_java
		Label((""%s//protobuf_java"" % REPOSITOR...))
	File ""/home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/protobuf.bzl"", line 266, in Label
		REPOSITORY_NAME
The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false
ERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:android' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/cgv/.cache/bazel/_bazel_cgv/ad496aba49fed364b29fcfdc32e3267a/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows_msvc' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'
ERROR: /home/cgv/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: Target '@com_google_protobuf//:protoc' contains an error and its package is in error
ERROR: Analysis of target '//tensorflow/core:example_protos' failed; build aborted: Analysis failed
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Install `python3-dev` and other dependencies directly from Ubuntu via apt
2. Install `bazel` from repository https://storage.googleapis.com/bazel-apt via apt
3. Clone the r1.13 branch of TensorFlow via `git clone https://github.com/tensorflow/tensorflow.git --branch r1.13 --single-branch`
4. Create a virtual environment with `python3 -m venv tensorflow-build` and activate it
5. Install the remaining dependencies via pip as stated on https://www.tensorflow.org/install/source
`pip install -U --user pip six numpy wheel mock`
`pip install -U --user keras_applications==1.0.6 --no-deps`
`pip install -U --user keras_preprocessing==1.0.5 --no-deps`
6. run `./configure` in the cloned repo (full output in [configure.log](https://github.com/tensorflow/tensorflow/files/2769428/configure.log))
7. run `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`

Then after a few seconds of loading packages and analyzing (and a few deprecation warnings), the described error occurs.

Please let me know if I can provide any further information. Thanks!"
24999,chief_training_hooks for TPUEstimatorSpec,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes, very much!

**Describe the feature and the current behavior/state.**
Currently, `training_chief_hooks` is not part of the [TPUEstimatorSpec API](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec).
However I saw that they are indeed used, when wrapping the user-supplied `model_fn`:
https://github.com/tensorflow/tensorflow/blob/9331096d56002c7fcb8bae411684b8b78fc196c4/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L2755-L2764
From looking at the code, I feel it would be a doable task to extend it in this direction.
In my use-case, I would like to provide an additional, custom `CheckpointSaverHook`.

**Will this change the current api? How?**
yes, extend it:
``` python
TPUEstimatorSpec(
    mode,
    predictions=None,
    loss=None,
    train_op=None,
    eval_metrics=None,
    export_outputs=None,
    scaffold_fn=None,
    host_call=None,
    # new:
    training_chief_hooks=None,
    # end
    training_hooks=None,
    evaluation_hooks=None,
    prediction_hooks=None
)
```

**Who will benefit with this feature?**
Everyone using TPUs and being a bit frustrated that the API is more restrictive than for more general-purpose accelerators (e.g. GPUs).

**Any Other info.**
nope"
24998,TPUEstimatorSpec supports hooks or not?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Links:
  - https://www.tensorflow.org/guide/using_tpu
  - https://cloud.google.com/ml-engine/docs/tensorflow/using-tpus
  - https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec


**Describe the documentation issue**
> The tf.train.SessionRunHook are unsupported, so these fields are omitted

The first link from above claims that no hooks are supported whatsover.
> TPUEstimator handles many of the details of running on TPU devices, such as replicating inputs and models for each core, and returning to host periodically to run hooks.

The second link from above claims that they are indeed supported.
The documentation of the `TPUEstimatorSpec` itself does not mention anything about that.

Which of the two is now the case? Which hooks are supported (`training_hooks`, `evaluation_hooks`, `prediction_hooks`)?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes absolutely! If you clarify the current level of support, I gladly update the documentation!"
24997,Chief exit without waiting other workers when distributed training with ParameterServerStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): compile using tags/v1.10.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.15.0
- CUDA/cuDNN version: cuda 9.1/cuDNN 7

**Describe the current behavior**
I'm using `tf.Estimator.train_and_evaluate` API and trying to make it work in a distributed environment. According to the doc, the configuration should be simple: set correct TF_CONFIG and pass the DistributeStrategy. 

I have 1chief, 2worker, 1ps, 1evaluator, the training starts with no error, everything seems right, but the training does not stop correctly. Based on my observation, when the training data is not balanced distributed among workers and chief, more specific, when the chief has fewer data. it will exit without waiting other workers. I draw a picture to show the problem:

![xnip2019-01-21_18-33-16](https://user-images.githubusercontent.com/1506580/51469063-6275a100-1dab-11e9-94ef-5731357f5905.jpg)

**Describe the expected behavior**

because only the chief does the checkpoint, I think the chief must wait others finishing, otherwise the model is partially trained and evaluator will not stop.

**Code to reproduce the issue**
```
def create_estimator_and_specs(run_config):
    model_params = tf.contrib.training.HParams(
        batch_size=FLAGS.batch_size,
        learning_rate=FLAGS.learning_rate)

    estimator = tf.estimator.Estimator(
        model_fn=model_fn,
        config=run_config,
        params=model_params
    )

    train_spec = tf.estimator.TrainSpec(
        input_fn=generate_tfrecord_dataset(
            mode=tf.estimator.ModeKeys.TRAIN,
            directory=""inputs"",
            batch_size=FLAGS.batch_size,
            train_size=FLAGS.train_size,
            epoch=FLAGS.epoch),
        max_steps=FLAGS.max_steps
    )

    eval_spec = tf.estimator.EvalSpec(
        input_fn=generate_tfrecord_dataset(
            mode=tf.estimator.ModeKeys.EVAL,
            directory=""inputs"",
            batch_size=FLAGS.batch_size,
            train_size=FLAGS.eval_size,
            epoch=FLAGS.epoch),
        throttle_secs=FLAGS.checkpoint_secs
    )

    return estimator, train_spec, eval_spec


train_distribute = ParameterServerStrategy(num_gpus_per_worker=FLAGS.gpu_per_worker)
eval_distribute = MirroredStrategy(num_gpus_per_worker=FLAGS.gpu_per_worker,
                                   cross_tower_ops=cross_tower_ops_lib.AllReduceCrossTowerOps())
run_config = tf.estimator.RunConfig(
    model_dir=FLAGS.model_dir,
    save_checkpoints_secs=FLAGS.checkpoint_secs,
    save_summary_steps=FLAGS.summary_steps,
    keep_checkpoint_max=FLAGS.max_checkpoints,
    train_distribute=train_distribute,
    eval_distribute=eval_distribute)

estimator, train_spec, eval_spec = create_estimator_and_specs(run_config=run_config)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

TF_CONFIG:
```
chief node:
 {u'cluster': {
   u'ps': [u'ps:12615'], 
   u'chief': [u'chief:20396'], 
   u'worker': [u'worker1:18339', u'worker2:11609'], 
   u'evaluator': [u'evaluator:24352']
   }, 
   u'task': {u'index': 0, u'type': u'chief'}
 }

 woker 0:
 {u'cluster': {
   u'ps': [u'ps:12615'], 
   u'chief': [u'chief:20396'], 
   u'worker': [u'worker1:18339', u'worker2:11609'], 
   u'evaluator': [u'evaluator:24352']
   }, 
   u'task': {u'index': 0, u'type': u'worker'}
 }

 worker 1:
 {u'cluster': {
   u'ps': [u'ps:12615'], 
   u'chief': [u'chief:20396'], 
   u'worker': [u'worker1:18339', u'worker2:11609'],
   u'evaluator': [u'evaluator:24352']
   }, 
   u'task': {u'index': 1, u'type': u'worker'}
 }

evaluator:
{u'cluster': {
  u'ps': [u'ps:12615'], 
  u'chief': [u'chief:20396'], 
  u'worker': [u'worker1:18339', u'worker2:11609'],
  u'evaluator': [u'evaluator:24352']
  }, 
  u'task': {u'index': 0, u'type': u'evaluator'}
}
```


**Other info / logs**
log screenshot
![image](https://user-images.githubusercontent.com/1506580/51469382-3575be00-1dac-11e9-9dc7-9e183d3da095.png)

I did post [a stackoverflow question](https://stackoverflow.com/questions/54252279/estimator-api-training-data-is-unbalance-split-among-workers-chief-may-finish) but no response so far. Please help."
24995,Tensorflow does not build in a python3,"**Ubuntu 18.04LTS** 
**TensorFlow installed from (source or binary):**
Source
**TensorFlow version (use command below):**
1.12.0
**Python version:**
3.6.7
**Bazel version (if compiling from source):**
0.19.0

**Source code / logs**

> ERROR: /home/sonfire/tensorflow/tensorflow/core/BUILD:2534:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 127)
> /usr/bin/env: 'python': No such file or directory
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 595.106s, Critical Path: 14.90s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
> INFO: 438 processes: 438 local.
> FAILED: Build did NOT complete successfully
> 

I don`t used env"
24993,ERROR: The 'test' command is only supported from within a workspace.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 16.04.5 LTS
- TensorFlow installed from: Docker image `tensorflow/tensorflow:nightly-devel-gpu`
- TensorFlow version: `master` branch
- Python version: 2 and 3
- Installed using: Docker


**Describe the problem**

I set up an Azure virtual machine with `Data Science Virtual Machine` on a `Standard NV6 (6 vcpus, 56 GB memory)`. I ssh into it, run these commands, and get the error in the title.

```
miguelmorin@ds-docker-gpu:~$ sudo docker pull tensorflow/tensorflow:nightly-devel-gpu
...
miguelmorin@ds-docker-gpu:~$ sudo docker images
REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE
tensorflow/tensorflow   nightly-devel-gpu   abbaea1b533c        6 weeks ago         3.82GB
miguelmorin@ds-docker-gpu:~$ sudo docker run -it abbaea1b533c
root@fa1e7cf2c349:~# export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH""
root@fa1e7cf2c349:~# export flags=""--config=opt --config=cuda -k""
root@fa1e7cf2c349:~# bazel test ${flags} //tensorflow/python/...
Extracting Bazel installation...
ERROR: The 'test' command is only supported from within a workspace.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
```

It also fails running the container with `nvidia-docker` instead of `docker`:

```
miguelmorin@ds-docker-gpu:~$ sudo nvidia-docker run -it abbaea1b533c
root@148ea4778aab:~# export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH""
root@148ea4778aab:~# 
root@148ea4778aab:~# export flags=""--config=opt --config=cuda -k""
root@148ea4778aab:~# bazel test ${flags} //tensorflow/python/...
Extracting Bazel installation...
ERROR: The 'test' command is only supported from within a workspace.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
```"
24991,Non-utilisation of available cores on multi-core Intel CPU for Intel Optimized TF build,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):     Yes (custom code)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary( Intel Optimized TF)
- TensorFlow version (use command below): 1.12
- Python version: 3.6.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A(no GPU) but Intel i5 7200U @ 2.50GHz * 4  with 7 GB memory and 64-bit OS


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
The load does not get shared by all the available cores(mostly one core at a time), as a result the performance is poor.

**Describe the expected behavior**
Expect the load to be shared by all the available cores and the performance to scale up with increasing number of cores.

**Code to reproduce the issue**
Follow this link and run the scripts as mentioned: https://github.com/mystic123/tensorflow-yolo-v3https://github.com/mystic123/tensorflow-yolo-v3

**Other info / logs**
I have used  KMP, OMP and ConfigProto settings and nothing seems to improve the performance.
"
24990,How to generate .pbtxt with .cc REGISTER_OP file,"I want to understand exactly  how user_ops generated,  and I found .cc .h op file 
can generated by .pbtxt ,but I'm not konw how to generate .pbtxt with .cc 
REGISTER_OP file ."
24988,how to use tensorflow read a image from remote hdfs cluster,"I need to read the image dataset directly from HDFS and use the tensorflow related API for preprocessing, but according to the official website's tensorflow and HDFS API, I did not get a specific answer. So can explain, for example: how to use the tensorflow related API to read a picture from HDFS and open it with other libraries."
24987,Running in google colab and got the errror:  'tensorflow.python.framework.ops.EagerTensor' has no len(),"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: '1.12.0'
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb

**Describe the documentation issue**
By running on google CoLab this notebook ( https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/eager/custom_training_walkthrough.ipynb#scrollTo=iDuG94H-C122 ), I have received the following error

![image](https://user-images.githubusercontent.com/19335547/51309762-17445100-1a80-11e9-8199-195b3d460e2f.png)

If one converts the tensors to numpy, it works normally.

```
plt.scatter(features['petal_length'].numpy(),
            features['sepal_length'].numpy(),
            c=labels.numpy(),
            cmap='viridis')

plt.xlabel(""Petal length"")
plt.ylabel(""Sepal length"");
```

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
24986,TFLite GPU Delegate error Initializing,"Dear Tensorflow developers,

After trying out the gpu-delegate demo on Android, I reimplemented in my app to try it out on my own model.

My model is the stripped ssd as the official tflite file did.
At the interpreter initialization stage, the gpu delegate seems not working.
On logcat I saw : `Failed to apply delegate: GpuDelegate Prepare: Node is already a consumer of the valueNode number 77 (GpuDelegate) failed to prepare.`

But it worked fine on the official mobilenet-ssd model.

What does this error message suppose to mean?
Or is there anything I need to be careful on TFLite converting?
Anything would be helpful.
Thanks!"
24985,how to flatten a tensor in block-wise,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version : 1.12
- Are you willing to contribute it : Yes



**Describe the feature and the current behavior/state.**
A is a tensor with shape `[n,h,w,block_size*block_size]`. I hope to flatten it into `[n,h*block_size,w*block_size,1]`. In Numpy, this can be implemented by:
```python
import numpy as np
A = np.ones((1,2,2,4))
A_flat= np.zeros((1,4,4,1))
block_size = 2
for n in range(A.shape[0]):
    for h in range(A.shape[1]):
        for w in range(A.shape[2]):
            A_flat[n:n+1, h*block_size:(h+1)*block_size, w*block_size:(w+1)*block_size, 0:1] \
                = A[n:n+1, h:h+1, w:w+1, :].reshape(1, block_size, block_size, 1)
```
How can i do it using tensorflow if these functions exist?

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Some researchers of image processing
**Any Other info.**
"
24984,Unable to install tensorflow 2.0 nightly for python 3.4,"OS version: Ubuntu 14.04
When I run I get the below output
 pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: )
No matching distribution found for tf-nightly-2.0-preview
"
24982,Enable reference code of kernels(internal) in TF-Lite,"Hi,
I want to benchmark the model based on reference code and Neon optimized code for kernels.So that I  want to enable the reference code implementation while executing the Tensorflow Lite model.

Is there is any way to run reference code ?
I tried to make changes in build file but there is some compilation issues.
Thanks and Regards,
Amar kumar
"
24981,Error in tf.nn.softmax_cross_entropy_with_logits_v2 due to tensor shape change,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Cent OS 6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed from source
- TensorFlow version (use command below):  tensorflow-1.8.0
- Python version: python 3.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.0
- GPU model and memory: Tesla V100 (16GB)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Error in tf.nn.softmax_cross_entropy_with_logits_v2
Below is outcome of before and after the optimization.

dimension of final output for vgg is [?, 3]
dimension of final output for yolo is [?, 1, 1, 3]

`softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=vgg_pred, labels=y, dim=-1)`
print(softmax)

run 1
vgg output
[ 1110.82873535     0.          1372.21044922  1051.81994629  5117.62011719]
after optimization
[    0.          8890.65722656     0.             0.          6899.49121094]
yolo output
[[[       0. ]] [[ 8094592. ]] [[       0. ]] [[ 8089163.5]] [[ 8069554.5]]]
after optimization
**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**

run2
vgg output
[    0.             0.          2072.43017578     0.          2563.55004883]
after optimization
[  8293.29980469  11375.015625        0.          12365.3359375   1054.58935547]
yolo output
[[[ 8573584.]] [[       0.]] [[       0.]] [[ 8399384.]] [[ 9033760.]]]
after optimization
**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**

run3
vgg output
[    0.          2648.55541992     0.          2143.43066406     0.        ] 
after optimization
[ 12912.1875          0.          13529.96679688      0.          12147.125     ]
yolo output
[[[ 7737216.]] [[ 9160056.]] [[ 7647175.]] [[ 7012569.]] [[ 8301956.]]]
after optimization
**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**

run4
vgg output
[ 1804.08349609     0.             0.             0.          2595.27880859]
after optimization
[     0.          11371.57226562  10606.33691406  11048.16992188      0.        ]
yolo output
[[[ 8434419. ]] [[ 7721689.5]] [[       0. ]] [[ 8754676. ]] [[ 6599649.5]]]
after optimization
**[[[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]] [[ 1.09861231]]]**

**Describe the expected behavior**
I believe softmax outcome should be different in each run. It doesn't happen when I use tf.nn.softmax_cross_entropy_with_logits, which is to be removed in future version.

**Other info / logs**
Architecture of vgg and yolo is bit different. Both are manually editted by me. Training vgg gives good enough result; however, training yolo gives poor outcome. Particularly, error is not properly calculated, so the network is not trained at all."
24980,MKL-backed tensorflow throws errors on softmax on empty tensors; GPU and Eigen versions do not,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary from anaconda
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Calling `tf.nn.softmax` on a tensor that has a dimension 0 throws an exception at runtime, when Tensorflow is backed by MKL. The same op returns an empty array of suitable size (i.e., the same size as the input tensor) under GPU and Eigen.

Note that this happens no matter which axis the softmax is taken across (the 0-length dim or a nonzero-length one).

Stacktrace:
```
python test.py 
2019-01-16 17:55:50.289518: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-16 17:55:50.294890: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-01-16 17:55:50.303432: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at mkl_softmax_op.cc:167 : Aborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164
Traceback (most recent call last):
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164
	 [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_0_0, DMT/_0)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    print(sess.run(softmax, {tensor: x}))
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164
	 [[node Softmax (defined at test.py:5)  = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_0_0, DMT/_0)]]

Caused by op 'Softmax', defined at:
  File ""test.py"", line 5, in <module>
    softmax = tf.nn.softmax(tensor)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1722, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1673, in _softmax
    return compute_op(logits, name=name)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 7138, in softmax
    ""Softmax"", logits=logits, name=name)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/matt/anaconda3/envs/learn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:164
	 [[node Softmax (defined at test.py:5)  = _MklSoftmax[T=DT_FLOAT, _kernel=""MklOp"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_0_0, DMT/_0)]]
```

**Describe the expected behavior**
`tf.nn.softmax` on an empty tensor returns an empty tensor on all configurations.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import numpy as np

tensor = tf.placeholder(tf.float32, [10, None])
softmax = tf.nn.softmax(tensor)
x = np.random.randn(10, 0)

with tf.Session() as sess:
    print(sess.run(softmax, {tensor: x}))
```
**Other info / logs**
This may be related to https://github.com/tensorflow/tensorflow/issues/23145 which has a similar error message but seems to be caused by a different situation.
"
24977,"""Upgrade to Tensorflow 2.0"" documentation lists wrong TF version","**System information**
- TensorFlow version: 1.12
- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md


**Describe the documentation issue**
The document says that `Note: tf_upgrade_v2 is installed automatically by pip install for TensorFlow 1.12 and later.` but looking at the setup.py files this console command is not installed by 1.12, but will be installed in 1.13

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
No
"
24976,Linking to both tensorflow and protobuf causes segmentation fault during static initializers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 4.18.10-1rodete2-amd64 (Debian-derived)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly Jan 15, 2018 (protobuf built from HEAD Jan 15)
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): gcc 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Aborts on SIGSEGV

**Describe the expected behavior**
Exits cleanly

**Details**
I want to create an application that calls the C API but also can parse protocol buffers on its own behalf. For that want to link dynamically to tensorflow and statically to protobuf. When I do this, it seems like protobuf may be tricking libtensorflow.so into thinking that it has run some static initializers that it in fact has not run (on the static variables needed by its own internal copy of protobuf).

The segfault is only on Linux. Linking the same way on Windows works fine.

I have varied libtensorflow and protobuf versions, and it seems to happen with all of them. It also happens whether I choose static or dynamic linking for my binary's copy of protobuf.

I also tried building my own liba.so that itself statically links protobuf and then a binary that linked dynamically to ""a"" and statically to protobuf. This worked, which is pointing away from this being a purely protobuf issue.

**Code to reproduce the issue**

* bash
```
c++ -o main \
  -L$TF_DIR/lib -I$TF_DIR/include \
  -L$PROTO_DIR/lib -I$PROTO_DIR/include \
  main.cc -l tensorflow -l protobuf

LD_LIBRARY_PATH=$TF_DIR/lib:$PROTO_DIR/lib ./main
```

Removing -lprotobuf from the above command will get rid of the segfault.

* main.cc
```
int main(int argc, char** argv) {}
```

**Other info / logs**

Program received signal SIGSEGV, Segmentation fault.
0x00007fffed8f20b8 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std::un
ique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >) ()
   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so
(gdb) bt
#0  0x00007fffed8f20b8 in tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::string_view, std
::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >) ()
   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so
#1  0x00007fffed88336a in tensorflow::kernel_factory::OpKernelRegistrar::OpKernelRegistrar(tensorflow::KernelDef const*, absl::string_view
, tensorflow::OpKernel* (*)(tensorflow::OpKernelConstruction*)) ()
   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so
#2  0x00007fffed85f806 in _GLOBAL__sub_I_dataset.cc ()
   from /usr/local/google/home/mattharvey/no_backup/libtensorflow/lib/libtensorflow_framework.so
#3  0x00007ffff7de88aa in call_init (l=<optimized out>, argc=argc@entry=1, argv=argv@entry=0x7fffffffdc68, env=env@entry=0x7fffffffdc78)
    at dl-init.c:72
#4  0x00007ffff7de89bb in call_init (env=0x7fffffffdc78, argv=0x7fffffffdc68, argc=1, l=<optimized out>) at dl-init.c:30
#5  _dl_init (main_map=0x7ffff7ffe170, argc=1, argv=0x7fffffffdc68, env=0x7fffffffdc78) at dl-init.c:120
#6  0x00007ffff7dd9c5a in _dl_start_user () from /lib64/ld-linux-x86-64.so.2
#7  0x0000000000000001 in ?? ()
#8  0x00007fffffffdf2e in ?? ()
#9  0x0000000000000000 in ?? ()

   0x00007fffed8f20a0 <+80>:    mov    0x50(%r15),%rax
   0x00007fffed8f20a4 <+84>:    lea    -0xa0(%rbp),%rbx
   0x00007fffed8f20ab <+91>:    mov    %rbx,%rdi
   0x00007fffed8f20ae <+94>:    mov    (%rax),%r8
   0x00007fffed8f20b1 <+97>:    mov    0x48(%r15),%rax
   0x00007fffed8f20b5 <+101>:   mov    (%rax),%rsi
=> 0x00007fffed8f20b8 <+104>:   mov    -0x18(%r8),%r9

How did -0x18(%r8) get illegal?

(gdb) info register r8
r8             0x0                 0

-0x18 is certainly illegal. Where did it come from? 0x50(%r15) if we trace through the above.

(gdb) info register r15
r15            0x555555768d10      93824994413840

(gdb) x/2 0x555555768d60
0x555555768d60: 0xee2c0bc0      0x00007fff

(gdb) x/2 0x00007fffee2c0bc0
0x7fffee2c0bc0 <google::protobuf::internal::fixed_address_empty_string>:        0x00000000      0x00000000

... the 0x0 that ended up in r8.

Zoom out to find lots of stuff uninitialized:

(gdb) x/64x 0x7fffee4ddb00
0x7fffee4ddb00 <google::protobuf::_DoubleValue_default_instance_>:      0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb10 <google::protobuf::_DoubleValue_default_instance_+16>:   0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb20 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb30 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb40 <google::protobuf::internal::RepeatedPrimitiveDefaults::default_instance()::instance>:   0x00000000      0x00000000      0x00000000        0x00000000
0x7fffee4ddb50 <guard variable for google::protobuf::internal::RepeatedStringTypeTraits::GetDefaultRepeatedField()::instance>:  0x000000000x00000000      0x00000000      0x00000000
0x7fffee4ddb60 <guard variable for google::protobuf::internal::(anonymous namespace)::Register(google::protobuf::MessageLite const*, int, google::protobuf::internal::ExtensionInfo)::local_static_registry>:     0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb70 <_ZStL8__ioinit>:        0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddb80 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu>: 0x00000000      0x00000000      0x00000000        0x00000000
0x7fffee4ddb90 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu+16>:      0x00000000      0x000000000x00000000      0x00000000
0x7fffee4ddba0 <google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::mu+32>:      0x00000000      0x000000000x00000000      0x00000000
0x7fffee4ddbb0 <guard variable for google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*)::runner>:  0x000000000x00000000      0x00000000      0x00000000
0x7fffee4ddbc0 <google::protobuf::internal::fixed_address_empty_string>:        0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddbd0 <google::protobuf::internal::implicit_weak_message_default_instance>:    0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddbe0 <google::protobuf::internal::implicit_weak_message_default_instance+16>: 0x00000000      0x00000000      0x00000000      0x00000000
0x7fffee4ddbf0 <google::protobuf::ShutdownProtobufLibrary()::is_shutdown>:      0x00000000      0x00000000      0x00000000      0x00000000
"
24974,GetTempFilename is not implemented,"**System information**
- Windows 10:
- TensorFlow installed from (source or binary):
- master branch:
- python 3.6:

**tensorflow\core\lib\io\path.cc(290) : error C4716: 'tensorflow::io::GetTempFilename': must return a value**

**bazel build --config=opt -c fastbuild //tensorflow:libtensorflow.so**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://user-images.githubusercontent.com/1705364/51275349-4e8b0300-1997-11e9-9991-5a86eb694e02.png)

"
24972,tflite 0.0.0-gpu-experimental still long interference time,"I tested tflite 0.0.0-gpu-experimental on Android.
Comparing to version 1.12.0, the speed up is quite poor.
Average of total interference time on the devices I tested:

device | Android v | tflite 1.12.0 time | tflite gpu exp time
--------- | ------------ | --------------------- | ------------------------
Huawei p10 lite (was-lx1) | 7.0, API 24 | ~651ms | ~605ms
LGE LG-M200 | 7.0, API 24 | ~1207ms | ~1390ms
Sony Xperia M5 (e5603) | 6.0, API 23 | ~690ms | ~609ms
Xiaomi Redmi 3 | 5.1.1, API 22 | ~942ms | ~800ms
Alcatel TCL 5033D | 8.1.0, API 27 | ~1128ms |  ~1032ms

whereas for the same model on CoreML (iOS platform) it takes much less on an old iPhone.
"
24971,TensorFlow unit tests crash or take a long time on Azure Standard A1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 16.04 LTS
- TensorFlow installed from: source
- TensorFlow version: `master` branch
- Python version: 2 and 3
- Installed using: I cloned the repo, `cd`-ed into it, and ran the unit tests with Docker: `tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...`, as mentioned at the end of the [contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md)
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): latest (from Docker image)
- CUDA/cuDNN version: I believe this is not installed
- GPU model and memory: no GPU listed for this virtual machine in [Microsoft Docs](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-previous-gen)



**Describe the problem**

Hello, I am a new contributor here trying to run the unit tests before submitting pull requests and confirming that they do not break the tests. I'll be happy to add the solution to the documentation when we find one.

I am running the unit tests of TensorFlow according to the [contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md). I set up a `Docker on Ubuntu Server` virtual machine (`Standard A1 (1 Core, 1.75 GiB memory)`) on Azure, and ran these commands:

    git clone https://github.com/tensorflow/tensorflow.git
    cd tensorflow/
    tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...

My first attempt crashed with these lines at the end:

    Analyzing: 7629 targets (561 packages loaded, 37218 targets configured)
    java.lang.OutOfMemoryError: GC overhead limit exceeded
    Dumping heap to /home/mmorin/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_mmorin/eab0d61a99b6696edb3d2aff87b585e8/java_pid25131.hprof ...
    Heap dump file created [617886369 bytes in 9.053 secs]
    Internal error thrown during build. Printing stack trace: java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)
    	at com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)
    	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)
    	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)
    	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
    	at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
    
    INFO: Elapsed time: 873.635s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (561 packages loaded, 37254 targets configured)
    Internal error thrown during build. Printing stack trace: java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)
    	at com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)
    	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)
    	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)
    	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
    	at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
    java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:115)
    	at com.google.devtools.build.lib.analysis.configuredtargets.RuleConfiguredTarget.<init>(RuleConfiguredTarget.java:142)
    	at com.google.devtools.build.lib.analysis.RuleConfiguredTargetBuilder.build(RuleConfiguredTargetBuilder.java:174)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:53)
    	at com.google.devtools.build.lib.rules.python.PyBinary.create(PyBinary.java:36)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:323)
    	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:207)
    	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:636)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:783)
    	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:326)
    	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)
    	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)
    	at java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
    	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
    GC overhead limit exceeded
    
    ERROR: bazel ran out of memory and crashed.
    FAILED: Build did NOT complete successfully (561 packages loaded, 37254 targets configured)

My second attempt reached more targets configured, but seems stuck or very slow after running for 4 hours as it is now incrementing targets one by one:

```
Analyzing: 7629 targets (561 packages loaded, 35347 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35347 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35348 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35349 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35351 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35354 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35358 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35361 targets configured)
Analyzing: 7629 targets (561 packages loaded, 35365 targets configured)
```

The Docker container was at the first of these lines after 1 hour, so the others took 3 hours.

How long should TensorFlow unit tests take to run? Has anyone succeeded in running them on Azure, and if so, which image and machine did you use?

**Any other info / logs**
I attach the two log for the crash and for the long time running:

[TensorFlow unit test crash log output.txt](https://github.com/tensorflow/tensorflow/files/2765390/TensorFlow.unit.test.crash.log.output.txt)
[long time log.txt](https://github.com/tensorflow/tensorflow/files/2765391/long.time.log.txt)
"
24969,"Inconsistencies between tf.contrib.layer.fully_connected, tf.layers.dense, tf.contrib.slim.fully_connected, tf.keras.layers.Dense","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave (10.14.2 (18C54))
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0
- Python version: Python 3.5.6 :: Anaconda, Inc.
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


I am trying to implement policy gradient for a contextual bandit problem (https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c).

I am defining a model in tensorflow to solve this problem using a single **fully-connected** layer.

I am trying out different APIs from tensorflow, but want to avoid using the `contrib` package since it is not tensorflow-supported. I am interested in using the `keras` API since I am already familiar with the functional interface, and it is now implemented as `tf.keras`. However, I can only seem to get results to work when using `tf.contrib.slim.fully_connected`, or `tf.contrib.layers.fully_connected` (the former calls the latter).


The following two snippets work correctly (`one_hot_encoded_state_input` and `num_actions` both adhere to the expected tensor shapes for the layers).

```python
import tensorflow.contrib.slim as slim
action_probability_distribution = slim.fully_connected(
    one_hot_encoded_state_input, \
    num_actions, \     
    biases_initializer=None, \
    activation_fn=tf.nn.sigmoid, \
    weights_initializer=tf.ones_initializer())
```

and 

```python
from tensorflow.contrib.layers import fully_connected
action_probability_distribution = fully_connected(
    one_hot_encoded_state_input,
    num_actions,\
    biases_initializer=None, \
    activation_fn=tf.nn.sigmoid, \
    weights_initializer=tf.ones_initializer())
```

On the other hand, neither of the following work:

```python
action_probability_distribution = tf.layers.dense(
    one_hot_encoded_state_input, \
    num_actions, \
    activation=tf.nn.sigmoid, \
    bias_initializer=None, \
    kernel_initializer=tf.ones_initializer())
```

nor

```python
action_probability_distribution = tf.keras.layers.Dense(
    num_actions, \
    activation='sigmoid', \
    bias_initializer=None, \
    kernel_initializer = 'Ones')(one_hot_encoded_state_input)
```

The last two cases use tensorflow's high level APIs `layers` and `keras`. Ideally, I would like to know if **I am incorrectly implementing the first two cases using the last two cases**, and if the only issue I am having is that **the latter two are not equivalent to the former two**.

For completeness, here is the entire code needed to run this (Note: python 3.5.6 and tensorflow 1.12.0 were used).

```python
import tensorflow as tf
import numpy as np
tf.reset_default_graph()

num_states = 3
num_actions = 4
learning_rate = 1e-3

state_input = tf.placeholder(shape=(None,),dtype=tf.int32, name='state_input')
one_hot_encoded_state_input = tf.one_hot(state_input, num_states)

# DOESN'T WORK
action_probability_distribution = tf.keras.layers.Dense(num_actions, activation='sigmoid', bias_initializer=None, kernel_initializer = 'Ones')(one_hot_encoded_state_input)

# WORKS
# import tensorflow.contrib.slim as slim
# action_probability_distribution = slim.fully_connected(one_hot_encoded_state_input,num_actions,\
#     biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())

# WORKS
# from tensorflow.contrib.layers import fully_connected
# action_probability_distribution = fully_connected(one_hot_encoded_state_input,num_actions,\
#     biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())

# DOESN'T WORK
# action_probability_distribution = tf.layers.dense(one_hot_encoded_state_input,num_actions, activation=tf.nn.sigmoid, bias_initializer=None, kernel_initializer=tf.ones_initializer())

action_probability_distribution = tf.squeeze(action_probability_distribution)
action_chosen = tf.argmax(action_probability_distribution)

reward_input = tf.placeholder(shape=(None,), dtype=tf.float32, name='reward_input')
action_input = tf.placeholder(shape=(None,), dtype=tf.int32, name='action_input')
responsible_weight = tf.slice(action_probability_distribution, action_input, [1])
loss = -(tf.log(responsible_weight)*reward_input)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
update = optimizer.minimize(loss)


bandits = np.array([[0.2,0,-0.0,-5],
                    [0.1,-5,1,0.25],
                    [-5,5,5,5]])

assert bandits.shape == (num_states, num_actions)

def get_reward(state, action): # the lower the value of bandits[state][action], the higher the likelihood of reward
    if np.random.randn() > bandits[state][action]:
        return 1
    return -1

max_episodes = 10000
epsilon = 0.1

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    rewards = np.zeros(num_states)
    for episode in range(max_episodes):
        state = np.random.randint(0,num_states)
        action = sess.run(action_chosen, feed_dict={state_input:[state]})
        if np.random.rand(1) < epsilon:
            action = np.random.randint(0, num_actions)

        reward = get_reward(state, action)
        sess.run([update, action_probability_distribution, loss], feed_dict = {reward_input: [reward], action_input: [action], state_input: [state]})
        
        rewards[state] += reward
        
        if episode%500 == 0:
            print(rewards)
```

When using the chunks commented `# THIS WORKS`, the agent learns and maximizes reward across all three states. On the other hand, those commented `# THIS DOESN'T WORK# don't learn and typically converge extremely quickly to choosing one action.



Question asked on stackoverflow: https://stackoverflow.com/questions/54221778/inconsistencies-between-tf-contrib-layer-fully-connected-tf-layers-dense-tf-co

I will link here if it is answered."
24968,tf.data.Dataset.from_tensor_slices().batch() broken when batch size equals dataset size,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: OS X 10.13.6 
- Mobile device if the issue happens on mobile device: X
- TensorFlow installed from: Binary
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Bazel version:x
- GCC/Compiler version: x
- CUDA/cuDNN version: x
- GPU model and memory: x

**Describe the current behavior**
When using tf.data.Dataset.from_tensor_slices(Data).batch(batchsize) using a batchsize which equals to the number of samples,  I get an out of range error:
```
OutOfRangeError: End of sequence
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,2], [?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorV2)]]
```
I think this due to the iterator wanting to construct a second batch, but there's nothing left due to the size of the first batch. In other words, it's impossible right now (I think) to use the tf.data pipeline without minibatching.

**Describe the expected behavior**
No error; the iterator produces a single batch.

**Code to reproduce the issue**
```
dataset = tf.data.Dataset.from_tensor_slices(np.arange(10)).batch(10)
iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)
x = iterator.get_next()

use_test_set = iterator.make_initializer(dataset)
with tf.Session() as sess:
    sess.run(use_test_set)
    print(sess.run(x)) #This runs
    print(sess.run(x)) #This line gives an error
```
**Other info / logs**
None.
"
24966,What input pre-processing is required for TensorFlow Lite pre-trained models,"TensorFlow Lite framework has some pre-trained models here https://www.tensorflow.org/lite/models

How do I know what pre-processing to input of the network is needed? (scale, mean values)
I'm talking about .pb file with FakeQuant* operation.
I need this to understand where those `min` and `max` parameters of FakeQuant* operation trained with pre-processing or not 

Thank you for your answer!"
24964,Change target name of tensorflow.dll in cmake installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.8
- Python version: 3.6.3
- Installed using virtualenv? pip? conda?: cmake
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): Visual Studio 2015
- CUDA/cuDNN version: 9.0/7.0
- GPU model and memory: GeForce GTX 1080 Ti (11 GB)



**Describe the problem**

I am trying to build two different versions of TensorFlow 1.8 (one with GPU support and one without). Even if it is deprecated, I use the cmake installation since I had a lot of troubles with bazel. And the cmake installation allows to directly build a shared library as well as a tensorflo.dll file, which suits our need for integration in our applications in Windows.

However, the two versions (with or without GPU) output a dll with the same name (tensorflow.dll), and we will need to have the two different dlls in our applications. I was searching for a way to change the target name (tensorfflow_cpu.dll and tensorflow_gpu.dll for instance), is it possible? I already tried to change the target name in the Visual Studio projet (i.e. tensorflow), but the .lib seems to always reference tensorflow.dll.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24962,Support model.fit using targets in a dictionary,"This feature relates to the TensorFlow implementation of the Keras API, specifically relating to the use of `model.fit`
Supporting this feature will improve the simplicity of the use case with TFRecord and multi-task problems (or multiple output streams)


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes (if my suggested implementation is sufficient)



**Describe the feature and the current behavior/state.**
Currently `model.fit` supports tensor/list/dict for the inputs `x`, but a dictionary cannot be used for the labels `y`.

**Will this change the current api? How?**
The API will not change, and it will more closely represent the underlying Keras API https://keras.io/models/model/#fit
""y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays""

**Who will benefit with this feature?**
Users with multi-task learning problems (or problems with multiple output steams)
Implementations using protos (tf.Example or tf.SequenceExample) and/or TFRecord
Implementations using tf.data.Dataset

**Any Other info.**
As per the handling of `inputs` in [`training_arrays._prepare_feed_values`](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_arrays.py#L389), we can change [line 429](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_arrays.py#L429) to the following
```
targets = training_utils.ModelInputs(targets).as_list()
```

If this solution is valid, it may be appropriate to modify [`training_utils.ModelInputs`](https://github.com/tensorflow/tensorflow/blob/aeddcb55fadc0ad9085d71d736ba49c6b69620bf/tensorflow/python/keras/engine/training_utils.py#L1306), either with an abstract `ModelTensors` class, or with a refactored `ModelInputs` class that does not use the word ""inputs""."
24960,"tf.dataset + tf.estimator  slow, starving CPU/GPU","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution  Linux centos
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.9.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cudatoolkit   8.0    and cudnn 7.1.3  
- GPU model and memory:  Tesla M40


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I used the gpu to train the wide deep model with tf.dataset and tf.estimator .In the case that I have opened 4 gpus with os.environ['CUDA_VISIBLE_DEVICES'] = ""0,1,2,3"", but only one gpu utilization is 15%, the others are 0.Is it because the network structure of the deep network part cannot be parallel. So you can only use one gpu. Is there any other way to speed up the training of the model?
**Describe the expected behavior**


"
24959,cannot open tensorflow as non-sudor,">>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


I can import tensorflow as sudo, but it fails when I import tensorflow as non-sudo."
24958,"Why compiling tensorflow from source failed which every step was done successfully as in the documentation, in both Windows 10 and Ubuntu","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64Bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.18.0, 0.16. 0.21.0
- GCC/Compiler version (if compiling from source): 7.4, 5.4
- CUDA/cuDNN version: 10.0, 7.4.2
- GPU model and memory: GTX 1080 16GB



**Describe the problem**

I have tried the steps on the official site, and many more tutorials, but it was unable to build successfully.

every time almost throw new kind of error. this is the last error:

> 1 error detected in the compilation of ""C:/Users/adil/AppData/Local/Temp/nvcc_inter_files_tmp_dir/eye_functor_gpu.cu.cpp1.ii"".
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 171.890s, Critical Path: 58.48s
> INFO: 698 processes: 698 local.
> FAILED: Build did NOT complete successfully

everything installed correctly. 

I think there are a lot of bugs with tensorflow which let me dislike this framework, very hard to compile and also very hard to get the environment ready, it relays on a lot of third party tools which leads sometimes the versions miss match, and also it is long time which Cuda 10 has been come out, but the tf still using 9.0, the same with visual studio 2015 which now it is a long time that people using 2017. Everything that is used in tf is very old, which some user must uninstall their new version and install the older one. Tensorflow must consider this problem to be compatible with new tools and systems.

I am waiting when tf start to support cuda 10?
"
24957,native tf and tf.keras optimizer and gradient calculation problem,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Win 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
pip install 
- TensorFlow version (use command below):
1.6 and 1.10
- Python version:
3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9.0/7.05
- GPU model and memory:
GTX 1080 Ti

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
train model using tf and tf.keras
**weights updates** and **gradients** are different

**test result as following chart:**
![gradients](https://user-images.githubusercontent.com/39117820/51234690-7a62b480-19a8-11e9-8f46-94b247d91273.JPG)


**Describe the expected behavior**
Since tf.keras using tf as backend operation, weights updates and gradient calculation should be identical.
**Code to reproduce the issue**

```
###imports and default settings
import tensorflow as tf
from tensorflow.contrib import keras
import numpy as np
import sys,os
import keras.backend as K
slim = tf.contrib.slim
img_size=10
datas = np.array([np.ones((100)),np.zeros((100))])
labels = np.array([[1.0,0.0],[0.0,1.0]])
lr = 1e-3
beta_1=0.9
beta_2=0.999
epsilon=1e-08
seed=10
relu_function = tf.nn.relu
```
```
##keras model

tf.reset_default_graph()
with tf.get_default_graph().as_default() as keras_graph:
    with tf.Session(graph=keras_graph).as_default() as keras_sess:
        init = keras.initializers.RandomNormal(seed=seed)
        inputs = keras.layers.Input(shape=(100,))
        x = keras.layers.Dense(units=2,kernel_initializer=init,bias_initializer=init)(inputs)
        pred = keras.layers.Activation(activation='softmax')(x)
        model = keras.models.Model(inputs,pred)
        
        #tf optimizer
#         optimizer = tf.train.AdamOptimizer(lr,beta1=beta_1,beta2=beta_2,epsilon=epsilon)

        ###keras optimizer
        optimizer = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon)

        #use differnet loss 
        model.compile(optimizer=optimizer,loss=keras.losses.categorical_crossentropy)

        #get first cnn layer weights
        keras_variable_name = [x.name for x in tf.trainable_variables()[:2]]
        keras_each_layer_tensors = tf.trainable_variables()[:2]
        keras_weights_list=[]
        keras_gradients_list = []
        ####gradient
        #get gradient tensor
        keras_gradients_tensor = model.optimizer.get_gradients(loss=model.loss(model.targets[0],model.outputs[0]),
                                                        params=keras_each_layer_tensors)
        
        keras_sess = keras.backend.get_session()
        #get layer weights before training
        keras_weights_list.append(keras_sess.run(keras_each_layer_tensors))
        #get gradients before training
        keras_gradients_list.append(keras_sess.run(keras_gradients_tensor,feed_dict={model.inputs[0]:datas,model.targets[0]:labels}))
        #### update weights
        model.fit(x=datas,y=labels,epochs=1,verbose=2)
        #get layer weights after 1 epoch training
        keras_weights_list.append(keras_sess.run(keras_each_layer_tensors))
        #get gradients after 1 epoch training
        keras_gradients_list.append(keras_sess.run(keras_gradients_tensor,feed_dict={model.input:datas,model.targets[0]:labels}))

        keras_weights_list = np.array(keras_weights_list)
        keras_gradients_list = np.array(keras_gradients_list)
        keras_weights_update = keras_weights_list[1]-keras_weights_list[0]
  ```
```
###### tf model
tf.reset_default_graph()
with tf.get_default_graph().as_default() as tf_graph:
    with tf.Session(graph=tf_graph).as_default() as tf_sess:
        initializer = tf.initializers.random_normal(seed=seed)
        
        ##placeholders
        tf_input = tf.placeholder(tf.float32, [None, 100],
                    name='input')
        tf_label = tf.placeholder(tf.float32,[None,2],name='label')
        tf_lr = tf.placeholder(tf.float32,[],name='lr')
        #model
        x = tf.layers.dense(inputs=tf_input,units=2,kernel_initializer=initializer,bias_initializer=initializer)
        pred = tf.nn.softmax(x)
        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_label,logits=x)
#         loss = keras.losses.categorical_crossentropy(y_pred=pred,y_true=tf_label)
        tf_loss = tf.reduce_mean(loss)
#         tf_optimizer = tf.keras.optimizers.Adam(lr=lr)
        tf_optimizer = tf.train.AdamOptimizer(learning_rate=lr,beta1=beta_1,beta2=beta_2,epsilon=epsilon,name='optimizer')
        train = tf_optimizer.minimize(loss=tf_loss)
        
        #get tensors
        tf_each_layer_tensors = tf.trainable_variables()
        tf_variable_name = [x.name for x in tf.trainable_variables()]
        #get gradient tensor
#         tf_gradient_tensor  = tf.gradients(tf_loss,tf_each_layer_tensors)
        tf_gradient_tensor  = keras.backend.gradients(tf_loss,tf_each_layer_tensors)
        #init
        tf_sess.run(tf.global_variables_initializer())
        
        tf_weights_list=[]
        tf_gradients_list=[]
        #get init weight before training
        tf_weights_list.append(tf_sess.run(tf_each_layer_tensors))
        
        tf_gradients_list.append(tf_sess.run(tf_gradient_tensor,feed_dict={tf_label:labels,tf_input:datas}))

        loss_value,_ = tf_sess.run([tf_loss,train],feed_dict={tf_input:datas,tf_label:labels})
        print('loss:',loss_value)
        
        tf_weights_list.append(tf_sess.run(tf_each_layer_tensors))
        
        tf_gradients_list.append(tf_sess.run(tf_gradient_tensor,feed_dict={tf_label:labels,tf_input:datas}))
        
        tf_weights_list = np.array(tf_weights_list)
        tf_gradients_list = np.array(tf_gradients_list)
        
        tf_weights_update = tf_weights_list[1]-tf_weights_list[0]
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24955,"fatal error LNK1120, LNK2019, LNK2001","
**System information**
- OS Platform and Distribution :    windows 10
- TensorFlow installed from :          source
- TensorFlow version:                     r1.12
- Python version:                            3.5.4
- Installed using :                            exe
- Bazel version :                               0.15.0
- CUDA/cuDNN version:                 10.0, 7.3
- GPU model and memory:             gtx1080ti

after i change the patch from [Aleksandr Sokolovskii's blog](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) for modify eigen's error.
and add three lines code from this [#23655](https://github.com/tensorflow/tensorflow/issues/23655) issues of cielavenir's description ,
last ,i run "" bazel build --config=opt --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so"" ,at last,i get the below error:

INFO: From Linking tensorflow/core/liblib_internal_impl.a:
android_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
random_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Linking tensorflow/libtensorflow_framework.so:
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/liblibtensorflow_framework.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/liblibtensorflow_framework.so.exp
libcuda_platform.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function ""public: virtual bool __cdecl stream_executor::cuda::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)"" (?DoMatMul@CudnnSupport@cuda@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)
ERROR: D:/tf3/tensorflow-r1.12/tensorflow/cc/BUILD:456:1: Linking of rule '//tensorflow/cc:ops/math_ops_gen_cc' failed (Exit 1120): link.exe failed: error executing command
  cd C:/users/swls/_bazel_swls/5dz6uozl/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python35/python.exe
    SET PYTHON_LIB_PATH=C:/Python35/libs/python35.lib
    SET TEMP=C:\Users\swls\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\swls\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc /SUBSYSTEM:CONSOLE -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc-2.params
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
libmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)"" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z) referenced in function ""public: static class tensorflow::shape_inference::DimensionHandle __cdecl tensorflow::shape_inference::InferenceContext::DimKnownRank(class tensorflow::shape_inference::ShapeHandle,__int64)"" (?DimKnownRank@InferenceContext@shape_inference@tensorflow@@SA?AVDimensionHandle@23@VShapeHandle@23@_J@Z)
libcc_op_gen_main.a(cc_op_gen_main.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)"" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)
libcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)"" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::LogMessageFatal::LogMessageFatal(char const *,int)"" (??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z)
libmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol ""public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)"" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ) referenced in function ""public: void __cdecl tensorflow::internal::LogMessageFatal::`vbase destructor'(void)"" (??_DLogMessageFatal@internal@tensorflow@@QEAAXXZ)
libcc_op_gen_main.a(cc_op_gen_main.o) : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)"" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)
libcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)"" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)
libop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol ""public: virtual __cdecl tensorflow::internal::LogMessageFatal::~LogMessageFatal(void)"" (??1LogMessageFatal@internal@tensorflow@@UEAA@XZ)
libmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)"" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z) referenced in function ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)"" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)
libcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)"" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const *)"" (??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z)
libmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)"" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ) referenced in function ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)"" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)
libcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)"" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ)
libop_gen_lib.a(op_gen_lib.o) : error LNK2001: unresolved external symbol ""public: __cdecl tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder(void)"" (??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ)
libmath_ops_op_lib.lo(math_ops.o) : error LNK2019: unresolved external symbol ""public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)"" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ) referenced in function ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * __cdecl tensorflow::internal::MakeCheckOpString<int,int>(int const &,int const &,char const *)"" (??$MakeCheckOpString@HH@internal@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBH0PEBD@Z)
libcc_op_gen_main.a(cc_op_gen.o) : error LNK2001: unresolved external symbol ""public: class std::basic_ostream<char,struct std::char_traits<char> > * __cdecl tensorflow::internal::CheckOpMessageBuilder::ForVar2(void)"" (?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ)
......

libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""bool __cdecl tensorflow::str_util::CUnescape(class absl::string_view,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?CUnescape@str_util@tensorflow@@YA_NVstring_view@absl@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@1@Z) referenced in function ""bool __cdecl tensorflow::ConvertLine(class absl::string_view,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?ConvertLine@tensorflow@@YA_NVstring_view@absl@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@5@@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""bool __cdecl tensorflow::str_util::ConsumePrefix(class absl::string_view *,class absl::string_view)"" (?ConsumePrefix@str_util@tensorflow@@YA_NPEAVstring_view@absl@@V34@@Z) referenced in function ""bool __cdecl tensorflow::ConsumeEquals(class absl::string_view *)"" (?ConsumeEquals@tensorflow@@YA_NPEAVstring_view@absl@@@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""bool __cdecl tensorflow::str_util::EndsWith(class absl::string_view,class absl::string_view)"" (?EndsWith@str_util@tensorflow@@YA_NVstring_view@absl@@0@Z) referenced in function ""class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::WordWrap(class absl::string_view,class absl::string_view,int)"" (?WordWrap@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@Vstring_view@absl@@0H@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""public: static bool __cdecl google::protobuf::TextFormat::ParseFromString(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class google::protobuf::Message *)"" (?ParseFromString@TextFormat@protobuf@google@@SA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessage@23@@Z) referenced in function ""public: class tensorflow::Status __cdecl tensorflow::ApiDefMap::LoadApiDef(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)"" (?LoadApiDef@ApiDefMap@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""class tensorflow::Status __cdecl tensorflow::ReadFileToString(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > *)"" (?ReadFileToString@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV45@@Z) referenced in function ""public: class tensorflow::Status __cdecl tensorflow::ApiDefMap::LoadFile(class tensorflow::Env *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)"" (?LoadFile@ApiDefMap@tensorflow@@QEAA?AVStatus@2@PEAVEnv@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
libop_gen_lib.a(op_gen_lib.o) : error LNK2019: unresolved external symbol ""class tensorflow::OpDef_AttrDefDefaultTypeInternal tensorflow::_OpDef_AttrDef_default_instance_"" (?_OpDef_AttrDef_default_instance_@tensorflow@@3VOpDef_AttrDefDefaultTypeInternal@1@A) referenced in function ""void __cdecl tensorflow::`anonymous namespace'::InitApiDefFromOpDef(class tensorflow::OpDef const &,class tensorflow::ApiDef *)"" (?InitApiDefFromOpDef@?A0x86251cb0@tensorflow@@YAXAEBVOpDef@2@PEAVApiDef@2@@Z)
bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/math_ops_gen_cc : fatal error LNK1120: 119 unresolved externals
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 2672.865s, Critical Path: 259.43s
INFO: 3296 processes: 3296 local.
FAILED: Build did NOT complete successfully

there is ""fatal error LNK1120: 119 unresolved externals"" errors,i think it must missing something,,,
is there anyone can help me? any response will appreciate!
"
24954,AttributeError: module 'tensorflow' has no attribute 'keras'(Tensorflow 1.4.0 has problem with new Model directory),"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not custom code / object_detection

**OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04**

**TensorFlow installed from (source or binary): binary**

**TensorFlow version (use command below): 1.4.0**

**Python version: 3.5**

**CUDA/cuDNN version: 8.0/6.0**

Architecture        :   x86_64
CPU op-mode(s):   32-bit, 64-bit
Byte Order:            Little Endian
CPU(s)               :                6
On-line CPU(s) list:   0-5
Thread(s) per core:    1
Core(s) per socket:    6
Socket(s)              :    1
NUMA node(s)     :    1
Vendor ID            :  GenuineIntel
CPU family           :     6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2603 v3 @ 1.60GHz
Stepping              :              2
CPU MHz            :               1596.347
CPU max MHz    :           1600.0000
CPU min MHz     :           1200.0000
BogoMIPS          :              3192.69
Virtualization      :        VT-x 
L1d cache          :             32K
L1i cache           :             32K
L2 cache            :              256K
L3 cache            :              15360K
NUMA node0 CPU(s):     0-5

Running `nvidia-smi` command :
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K2200        Off  | 00000000:02:00.0  On |                  N/A |
| 42%   43C    P8     1W /  39W |    309MiB /  4040MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       967      G   /usr/lib/xorg/Xorg                           178MiB |
|    0      1829      G   compiz                                       127MiB |
+-----------------------------------------------------------------------------+

```

Exact command to reproduce:
python object_detection/builders/model_builder_test.py
When i run the test program with tensorflow 1.4 i get the following error:

```
Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 23, in <module>
    from object_detection.builders import model_builder
  File ""/home/models/research/object_detection/builders/model_builder.py"", line 22, in <module>
    from object_detection.builders import box_predictor_builder
  File ""/home/models/research/object_detection/builders/box_predictor_builder.py"", line 21, in <module>
    from object_detection.predictors import convolutional_box_predictor
  File ""/home/models/research/object_detection/predictors/convolutional_box_predictor.py"", line 19, in <module>
    from object_detection.core import box_predictor
  File ""/home/models/research/object_detection/core/box_predictor.py"", line 137, in <module>
    class KerasBoxPredictor(tf.keras.Model):
AttributeError: module 'tensorflow.python.keras' has no attribute 'Model'
```

The solution i found when i searched was to update the tensorflow version.

But when I update tensorflow to the latest version(1.12) i get another error:
```
Traceback (most recent call last):
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/i.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/.virtualenvs/basic/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/.virtualenvs/basic/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 20, in <module>
    import tensorflow as tf
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/.virtualenvs/basic/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/.virtualenvs/basic/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/.virtualenvs/basic/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

Error seems like the higher tensorflow version is expecting CUDA 9.0.

So my inference is like the new tensorflow version doesnot support CUDA 8.0.
Downgrading the tensorflow version has problems with the new Models problem. 

What could be the solution?
Is this because  the CUDA runtime version is 9.0 (driver version 384.130) and the installed CUDA version is 8.0 and updating the CUDA version is the only solution?


NB: Is there any necessity for GPUs used in distributed training to have the same CUDA, tensorflow version?
"
24953,Multiple parameter servers are not sharing the load when running TensorFlow distributed,"This is a performance issue question. 
I have one, two, and four parameter servers experiments to test performance.
In the two and four parameter server networks utilization, only one parameter server has a highly utilized network and the rest is having low utilization. 
For example:
in 4 parameter server tasks with 4 worker tasks. Each task is on one machine.

After running the experiments, I collected the network utlization:

first ps == 693.7K
second ps == 93.7 M
third ps == 15.5 M
fourth ps == 4.1
all four workers have 28.5 M utilization of the network. 
if we take 28.5 * 4 = 114 M. 
Then 114 M = (93.7+15.5+4.1+693.7K).

**System information**
- I wrote my own example with MNIST dataset and fully connected layers
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Pip3 installed 
- TensorFlow version (use command below): 1.11
- Python version: 3.6
- CPU
- Bandwidth is 108 M for every connection. 

My question here is why ps did not share tensor size equally. 

This is the part of my code that relates to this question. In Tensorflow, there is a Round-Robin fashion for tf.Variables(...) to place variables on parameter servers.


with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % (FLAGS.task_index),cluster=cluster)):

I am using MNIST dataset with minibatch 100. In my case, I will run 600 iterations to go through all dataset( 60000 samples in the data / 100 ) = 600 times. That means the worker will push and pull the model from each parameter server 600 times. 

      with tf.name_scope('input'):
        X = tf.placeholder(dtype=tf.float32,shape=[None, 784])
        Y = tf.placeholder(dtype=tf.float32,shape=[None, 10])
        keep_prob = tf.placeholder(tf.float32, name = 'keep_prob') # dropout (keep probability)

      with tf.name_scope(""weights""):
        W1 = tf.Variable(tf.truncated_normal([784, 256], stddev=0.1)) 
        W2 = tf.Variable(tf.truncated_normal([256, 128], stddev=0.1))
        W3 = tf.Variable(tf.truncated_normal([128, 64], stddev=0.1)) 
        W4 = tf.Variable(tf.truncated_normal([64, 10], stddev=0.1)) 

      with tf.name_scope(""biases""):
        b1 = tf.Variable(tf.zeros([256]))
        b2 = tf.Variable(tf.zeros([128]))
        b3 = tf.Variable(tf.zeros([64]))
        b4 = tf.Variable(tf.zeros([10]))

      with tf.name_scope(""softmax""):
        XX = tf.reshape(X, [-1, 784])
        z2 = tf.nn.sigmoid(tf.matmul(XX, W1) + b1)
        z3 = tf.nn.sigmoid(tf.matmul(z2, W2) + b2)
        z4 = tf.nn.sigmoid(tf.matmul(z3, W3) + b3)
        z5 = tf.matmul(z4,W4) + b4
      
      # define the loss function ...
      with tf.name_scope('cross_entropy'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z5, labels=Y))

      # create an optimizer then wrap it with SynceReplicasOptimizer
      with tf.name_scope(""train""):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)

        # global_step tells the graph where it is in training
        global_step = tf.train.get_or_create_global_step()
        
        #Synchronization
        optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers, total_num_replicas=num_workers)
        
        # averages gradients
        opt = optimizer.minimize(loss, global_step=global_step)



"
24951,embedding_lookup return a SparseTensor,"Neither embedding_lookup nor embedding_lookup_sparse can return a sparse tensor. Could you add a feature to support SparseTensor returned?

I don't mean just convert the dense to sparse. It's **sparse lookup**."
24949,Tensorflow random categorical,"Hi,

I'm running 
https://www.tensorflow.org/tutorials/sequences/text_generation

When I arrive at the line the following error is produced. Is there an import missing?
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

AttributeError                            Traceback (most recent call last)
<ipython-input-23-60a341594c52> in <module>
----> 1 sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
      2 sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

AttributeError: module 'tensorflow._api.v1.random' has no attribute 'categorical'

**System information**
- TensorFlow version: 1.12 Jupyter NoteBooks on Ubuntu
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
24948,Linking of rule '//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: macOS 10.13.5
- TensorFlow installed from: source
- TensorFlow version: 1.11.0
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.15.2
- GCC/Compiler version (if compiling from source): XCODE 8.3.2
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.3.2 / NCCL 2.3.5
- GPU model and memory: GTX 1080 Ti 11GB

Trying to build tensorflow 1.11 from source and the following error occurred:

tensorflow/tensorflow/contrib/nccl/BUILD:24:1: Linking of rule '//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed (Exit 1)
ld: library not found for -l:libnccl.so.2.3.5

Thanks a lot


"
24946,tensorflow/third_party/toolchains/gpus/cuda/BUILD,tensorflow/third_party/toolchains/gpus/cuda/BUILD file contains hard coded paths which would not necessarily true for all systems. Anyway I thought these files were generated through repository gen rules in `third_party/gpus/cuda`. Isn't it the case anymore?
24945,How to eliminate duplicate codes for shape inference,"**System information**
- TensorFlow version (you are using): tf 2.0-preview
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

When we are looking into issue: https://github.com/tensorflow/addons/issues/12, as @jaingaurav said,  SetShapeFn only works in graph constructing stage. So for eager mode, we have to add the duplicate logic (codes) in the `Compute` method of kernel. 

My question is, can we find a solution to get rid of the duplicate codes for shape inference? Say, SetShapeFn also works in eager mode?

cc @jaingaurav @karmel @seanpmorgan "
24940,NNAPI doesn't support tensors with rank 0 - Mobilenet,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10 for retraining, Linux Ubuntu 16.04 for converting to tflite model
- Mobile device: Sony Xperia Z1 - Android 9 (AICP ROM)
- TensorFlow installed from : binary
- TensorFlow version: b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6

**Describe the current behavior**
I retrained ""https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/quantops/classification/1"" using this script: https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py.

Then I converted the output to tflite format using the following command:
```
tflite_convert \
  --output_file=graph.tflite \
  --graph_def_file=retrained_graph.pb \
  --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
  --input_arrays=Placeholder \
  --output_arrays=final_result \
  --input_shapes=1,160,160,3 \
  --mean_values=128 \
  --std_dev_values=127 
```
I moved graph.tflite (the output of the above command) to assets folder of this project: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo

I did all the necessary changes (such as changing the graph name, input size, etc.) in the ImageClassifierQuantizedMobileNet class.

Everything is right as long as CPU is selected from the device list, but as soon as I select NNAPI, the app crashes.

I think the problem is from the pre-trained models that are available on tfhub. Since when I convert the stock mobilenet_v1_1.0_224.pb from ""http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz ""
the problem is gone.

Log:
```
1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: NNAPI doesn't support tensors with rank 0 (index 91 name module_apply_default/hub_input/Mul/y)
1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:742.
1975-04-19 08:19:20.212 14862-15282/android.example.com.tflitecamerademo E/tflite: Failed to build graph for NNAPI
    
    --------- beginning of crash
1975-04-19 08:19:20.217 14862-15282/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 14862
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: 
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:140)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)
        at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.runInference(ImageClassifierQuantizedMobileNet.java:95)
        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:125)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:804)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.access$1200(Camera2BasicFragment.java:78)
        at com.example.android.tflitecamerademo.Camera2BasicFragment$8.run(Camera2BasicFragment.java:699)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```
"
24938,tf.keras.backend.zeros implementation ends up tracking tensors as well in graph mode ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.6

**Describe the current behavior**

Here is my custom layer:

```bash
class ReshapeLayer(Layer):
    def __init__(self, **kwargs):
        super(ReshapeLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) >= 2
        super(ReshapeLayer, self).build(input_shape)

    def call(self, x):
        s = K.shape(x)
        # zeros_w = K.zeros((s[0], 1, s[2], s[3]), tf.float32) # does not work
        zeros_w = tf.zeros((s[0], 1, s[2], s[3]), tf.float32)
        r = K.concatenate([x, zeros_w], 1)

        s = K.shape(r)
       #  zeros_h = K.zeros((s[0], s[1], 1, s[3]), tf.float32)  # does not work
        zeros_h = tf.zeros((s[0], s[1], 1, s[3]), tf.float32)
        r = K.concatenate([r, zeros_h], 2)
        return r

    def compute_output_shape(self, input_shape):
        shape = tf.TensorShape(input_shape).as_list()
        shape[1] = shape[1] + 1
        shape[2] = shape[2] + 1
        return tf.TensorShape(shape)
```

Please note the commented lines i.e. K.zeros vs tf.zeros

In graph mode, if I use K.zeros even though the graph gets built, later on I get an exception with long stack trace (probably because this layer gets used many times in my network) that Tensor object does not have is_initialized property

```
AttributeError: 'Tensor' object has no attribute 'is_initialized'
```

K.zeros works in eager mode.

Usage of tf.zeros work fine in both graph and eager mode.

After debugging the tensorflow code I figured that towards the very end when keras tries to initialize the **variables** it sees some entries that are of type **Tensor**

Those entries are the ones generated by K.zeros.

I then looked at the implementation of K.zeros https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/backend.py#L1010

In that it clear says that it could return either a variable or tensor based on the input (i.e. shape). This is correct however it **seems** like that irrespective of the return value being a variable or tensor it still ends up tracking it (via track_variable) in graph mode.

```bash
# code of zeros in tf.keras.backend.py
with ops.init_scope():
    if dtype is None:
      dtype = floatx()
    tf_dtype = dtypes_module.as_dtype(dtype)
    v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)
    if py_all(v.shape.as_list()):
      return variable(v, dtype=dtype, name=name)
    track_variable(v)
    return v
```

```bash
# code of track_variable in tf.keras.backend.py
def track_variable(v):
  """"""Tracks the given variable for initialization.""""""
  if context.executing_eagerly():
    return
  graph = v.graph if hasattr(v, 'graph') else ops.get_default_graph()
  if graph not in _GRAPH_VARIABLES:
    _GRAPH_VARIABLES[graph] = weakref.WeakSet()
  _GRAPH_VARIABLES[graph].add(v)
```

During debugging I can see that since the tensor is part of the collection how invoking is_initialized on it would result in error. 

Based on the code flow I would think that if `K.zeros` is going to return a tensor then it should not track it (i.e add it to the variables collection).

**Part of the stack trace**

```
return get_session().run(tensors)  File ""/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 469, in get_session
    _initialize_variables(session)  File ""/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 731, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])  File ""/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 731, in <listcomp>
    [variables_module.is_variable_initialized(v) for v in candidate_vars])  File ""/Users/ksachdeva/mlenv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 189, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
```"
24936,High memory usage of tensorflow.python.ops.lookup_ops.index_table_from_file compared to vocabulary_file size on disc,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
    - OSX Mojave 10.14.2
    - 2.3 GHz Intel Core i5
    - 8 GB 2133 MHz LPDDR3
    - Number of Processors: 1
    - Total Number of Cores: 2
- TensorFlow installed from (source or binary):
    - pip installed into anaconda virtual env 
- TensorFlow version (use command below): v1.11.0-rc2-4-gc19e29306c 1.11.0
- Python version: Python 3.6.6 :: Anaconda, Inc.
- Bazel version (if compiling from source): N/A 
- GCC/Compiler version (if compiling from source): c++ (GCC) 4.8.5
- CUDA/cuDNN version: N/A 
- GPU model and memory: N/A

**Describe the current behavior**
Note: while the example code is python, this problem also exists once the graph is deserialised and appears to be language independent.

This issue originates from trying to serve a Tensorflow model containing tensorflow.python.ops.lookup_ops.index_table_from_file to check the occurrence of a word in a vocabulary. The memory usage of the model when making predictions increases to ~8x the size of the vocabulary on disc (350MB vocab on disc, 3GB+ total memory usage when running).

The cause of this seems to be the size of the hash table created when initialising index_table_from_file. Using the debugger, the memory increase occurs on initialisation. 

**Describe the expected behavior**
Memory usage on the same order as it's on disc usage. 

**Code to reproduce the issue**
*This generates and saves an 84MB file*
Running the following opens a Tensorflow debug session.

    import tensorflow as tf
    import numpy as np
    import string
    import random
    from tensorflow.python.ops.lookup_ops import index_table_from_file
    from tensorflow.python import debug as tf_debug

    lookup_table_filename = ""./lookup_table.csv""
    data_filename = './vocab_feature.npz'

    def generate_data():
        """"""creates the required datafiles""""""
        letters = [i for i in string.ascii_letters]
        vocab = set([
            i + i + j + j + k + k + l + l
            for i in letters
            for j in letters
            for k in letters
            for l in letters
        ])
        
        positive_vocab = set(random.sample(vocab, 1000000))
        with open(lookup_table_filename, 'w') as f:
            f.write('\n'.join(positive_vocab))
            f.write('\n')
            f.write('\n'.join(map(str, range(10**7))))
            f.write('\n')

        vocab_feature = np.random.choice(list(vocab), size=1000, replace=True)
        np.savez(file=data_filename, vocab_feature=vocab_feature)

    # Run the first time
    generate_data()

    vocab_feature = np.load(data_filename)['vocab_feature']
    lookup_table = index_table_from_file(
        vocabulary_file=lookup_table_filename,
        default_value=-1,
        num_oov_buckets=0,
        vocab_size=None,
        name='vocab_table',
        key_dtype=tf.string  
    )
    text = tf.placeholder(dtype=tf.string, shape=[None, ])
    index = lookup_table.lookup(text)
    sess = tf.Session()
    sess = tf_debug.LocalCLIDebugWrapperSession(sess)
    sess.run(tf.tables_initializer())
    # sess.run(tf.global_variables_initializer())
    np_index = sess.run(index, feed_dict={text: vocab_feature})
    sess.close()
    print(np_index)

**Other info / logs**
**Things I've tried**
I have attempted to change the parallelism threads via ConfigProto to see if this reduces the memory usage but without improvement."
24934,Building from source the shared library for Raspberry-Pi,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch 9.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.9.0
- Python version: 3.5
- Installed using virtualenv? pip? conda?: none
- Bazel version (if compiling from source): 0.11.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**
I've been trying to compile tensorflow from source on my raspberry-pi for quite some time and did not succeeded yet.
I followed many different tutorials such as [this](https://www.youtube.com/watch?v=WqCnW_2XDw8) or [this](https://gist.github.com/EKami/9869ae6347f68c592c5b5cd181a3b205) but none of them allowed me to build it successfully.
I arrived to the point that I decided to ask for help. My latest state is trying to build tensorflow 1.9.0 using bazel 0.11.0 (as stated on tensorflow's website that these two versions are compatible). The bazel build goes flawlessly but for the tensorflow build, after ~20'000 seconds, shows the final error : 
tensorflow/tensorflow/BUILD:506:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1).

The goal would be to load and run a pre-trained model on my own c++ project. The pre-trained model was trained using keras with tensorflow backend version 1.10.1.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The steps are : 
```
git clone https://github.com/tensorflow/tensorflow.git
git checkout v1.9.0
grep -Rl 'lib64' | xargs sed -i 's/lib64/lib/g'
./configure
bazel build -c opt --config=monolithic --local_resources 1024,1.0,1.0 --verbose_failures //tensorflow:libtensorflow_cc.so
```


**Any other info / logs**

ERROR: /home/pi/cleanTF/tensorflow/tensorflow/BUILD:506:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/280abdd112ab916e808922e4ee91ed95/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -shared -o bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so -z defs -Wl,--version-script tensorflow/tf_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_cc.so -pthread-pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread-pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/arm-opt/bin/tensorflow/libtensorflow_cc.so-2.params)
bazel-out/arm-opt/bin/tensorflow/core/kernels/_objs/list_kernels/tensorflow/core/kernels/list_kernels.pic.o:list_kernels.cc:function tensorflow::TensorListStack<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 18838.252s, Critical Path: 14426.26s
FAILED: Build did NOT complete successfully
"
24933,"Export model with ""REGRESS"" URI not working in tensorflow-serving","Hi,

I've created the linear regression model which I want to serve using the tf-serving for which I have exported the model using the export model and **REGRESS_METHOD_NAME** in the signature_def after saving which the log is:

```
inputs {
  key: ""inputs""
  value {
    name: ""data_pipeline/IteratorGetNext:0""
    dtype: DT_FLOAT
    tensor_shape {
      dim {
        size: 1
      }
    }
  }
}
outputs {
  key: ""outputs""
  value {
    name: ""prediction/add:0""
    dtype: DT_FLOAT
    tensor_shape {
      dim {
        size: 1
      }
    }
  }
}
method_name: ""tensorflow/serving/regress""
```

After deploying the saved_model with tf-serving for prediction I'm using 

1. Working API for PREDICTION - with **predict** in uri
```
import json
data = json.dumps({""signature_name"": ""tensorflow/serving/regress"", ""instances"": [150]})
data

import requests
headers = {""content-type"": ""application/json""}
json_response = requests.post('http://localhost:8502/v1/models/linear:predict', data=data, headers=headers)
predictions = json.loads(json_response.text)
predictions
```
**Output**:
```
{
    ""predictions"": [465.759
    ]
}
```

2. Not working API for PREDICTION - with **regress** in uri
```
import json
data = json.dumps({""signature_name"": ""tensorflow/serving/regress"", ""instances"": [150]})
data

import requests
headers = {""content-type"": ""application/json""}
json_response = requests.post('http://localhost:8502/v1/models/linear:regress', data=data, headers=headers)
predictions = json.loads(json_response.text)
predictions
```
**Output**:
```
{
""error"": ""JSON Value: {\n \""signature_name\"": \""tensorflow/serving/regress\"",\n \""instances\"": [\n 150\n ]\n} When method is classify, key \\'examples\\' is expected and was not found""
}

```

tensorflow version: 1.10.0
tf-serving version: Latest docker image
"
24931,unexpected keyword argument 'serialized_options',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24930,Continuously differentiable eigendecomposition,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes, but I'd probably need some guidance.



**Describe the feature and the current behavior/state.**
Currently, when any two eigenvalues are equal, the `tf.gradients( tf.self_adjoint_eig(matrix), matrix) ` function returns NaN. 
It was proposed to return a subgradient (https://github.com/tensorflow/tensorflow/issues/12853) in this case, which equals `v_k^T*v_k` for kth eigenvalue. However, there [exists](https://www.win.tue.nl/analysis/reports/rana06-33.pdf) a procedure which allows to generate continuously differentiable eigendecomposition. This allows to have calculate true gradient (of a particluar decomposition) as soon as eigenbasis exists. Intuitively, if we choose a particular consistent way to choose eigenvectors from subspace, the eigendecomposition would be well defined.

Possibly related issue (same about SVD): https://github.com/tensorflow/tensorflow/issues/13641
**Will this change the current api? How?** The eigendecomposition will probably change in cases it is not unique.

**Who will benefit with this feature?** People dealing with optimizations based on eigenvalues/eigenvectors."
24928,Compiler error : To build Android TensorFlow library on mac os,"
OS platform : mac os 10.13.6
TensorFlow : source code
TensorFlow : 1.12

Build Cmd:
NDK_ROOT=/Users/cheer67/android_ndk/android-ndk-r15c tensorflow/contrib/makefile/compile_android_protobuf.sh -c

Error Log:

checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether UID '501' is supported by ustar format... yes
checking whether GID '20' is supported by ustar format... yes
checking how to create a ustar tar archive... gnutar
checking for arm-linux-androideabi-gcc...  arm-linux-androideabi-gcc --sysroot /Users/cheer67/android_ndk/android-ndk-r15c/platforms/android-21/arch-arm
checking whether the C compiler works... no
configure: error: in `/Users/cheer67/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf':
configure: error: C compiler cannot create executables
See `config.log' for more details
cheer67de-MacBook-Pro:tensorflow-master cheer67$ 


[config.log](https://github.com/tensorflow/tensorflow/files/2758840/config.log)
"
24926,I have installed python 3.6.5 on windows and installed tensorflow. tensorflow not getting imported and throwing error,"






PFB the error message:
C:\Users\hp>python
Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
24924,Connection loops occur while Reading TFRecord from S3 ,"Connection loops occur while Reading TFRecord from S3 
note that i've successfully installed aws credential already and have no trouble reading data from S3 
( using method from tensorflow.python.lib.io import file_io )

the problem occurs  when i pass my tfrecords S3 Address directly through input_fn to tensorflow.**estimator**, it takes about 2 hours while do mnist classification training which takes 5 minutes only when same tfrecord from local disc



part of my src are as below: 

    data_path = s3://mybucket/mydirectory/my.tfrecord
    tensorflow.run_train_with_validation(model.train_input_fn,
                                         data_path,
                                         model.vali_input_fn,
                                         steps=steps,
                                         ..........)


read data from S3 -  logs as below : (over 2H)


2019-01-09 11:26:09.896151: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
2019-01-09 11:26:10.150447: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/user/.aws/config and using profilePrefix = 1
2019-01-09 11:26:10.150481: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/user/.aws/credentials and using profilePrefix = 0
2019-01-09 11:26:10.150493: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/user/.aws/credentials for credentials file and /home/rubenchu//.aws/config for the config file , for use with profile default
2019-01-09 11:26:10.150501: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http
2019-01-09 11:26:10.150527: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2019-01-09 11:26:10.150537: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2019-01-09 11:26:10.150550: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.152144: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2019-01-09 11:26:10.154101: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.154241: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2019-01-09 11:26:10.154253: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.288550: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.288649: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.348006: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.348144: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.362769: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.362973: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.396204: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.396380: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.409498: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.409625: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.423777: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.423993: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.442704: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.442862: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.462751: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.462928: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.479342: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2019-01-09 11:26:10.479519: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2019-01-09 11:26:10.500023: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
............
....

..


..



read data from local - logs as below (ONLY 5 MIN)

2019-01-09 11:17:17.072920: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
2019-01-09 11:17:17.907945: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.
2019-01-09 11:17:17.907945: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.
2019-01-09 11:17:18.158344: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.
2019-01-09 11:17:18.158344: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.
2019-01-09 11:17:18.360677: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.
2019-01-09 11:17:35,059|INFO|TrainingAndValiCheckpointSaverListener : end
2019-01-09 11:17:35,059|INFO|TrainingAndValiCheckpointSaverListener : end


......


does anyone have the same issue?
"
24923,tf_upgrade_v2 AnnotationError ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version: 1.13
- Python version: 3.4.3
- Installed using virtualenv? pip? conda?: virtualenv 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**
Error found when run update the code for 2.0 preview


**Provide the exact sequence of commands / steps that you executed before running into the problem**

`tf_upgrade_v2 --intree tensorlayer --outtree tensorlayer2`

**Any other info / logs**
```bash
(env3)myhost:tensorlayer2 haodong$ tf_upgrade_v2 --intree tensorlayer --outtree tensorlayer2
/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  return f(*args, **kwds)
Line 98:11: Added keywords to args of function 'tf.reduce_sum'
Line 117:9: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'
Line 118:16: Added keywords to args of function 'tf.reduce_mean'
Line 118:31: Renamed 'tf.log' to 'tf.math.log'
Traceback (most recent call last):
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/bin/tf_upgrade_v2"", line 11, in <module>
    sys.exit(main())
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py"", line 97, in main
    args.input_tree, args.output_tree, args.copy_other_files, args.in_place)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 570, in process_tree
    _, l_report, l_errors = self.process_file(input_path, output_path)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 441, in process_file
    temp_file)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 489, in process_opened_file
    self.update_string_pasta("""".join(lines), in_filename))
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py"", line 453, in update_string_pasta
    t = pasta.parse(text)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/__init__.py"", line 25, in parse
    annotator.visit(t)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1153, in visit
    super(AstAnnotator, self).visit(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 245, in visit
    return visitor(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 47, in wrapped
    f(self, node, *args, **kwargs)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 211, in visit_Module
    self.generic_visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 253, in generic_visit
    self.visit(item)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1153, in visit
    super(AstAnnotator, self).visit(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 245, in visit
    return visitor(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 90, in wrapped
    f(self, node, *args, **kwargs)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 400, in visit_FunctionDef
    self.visit(stmt)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1153, in visit
    super(AstAnnotator, self).visit(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 245, in visit
    return visitor(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 90, in wrapped
    f(self, node, *args, **kwargs)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 294, in visit_With
    return self.visit_With_3(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 344, in visit_With_3
    self.visit(stmt)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1153, in visit
    super(AstAnnotator, self).visit(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 245, in visit
    return visitor(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 90, in wrapped
    f(self, node, *args, **kwargs)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 282, in visit_For
    self.visit(stmt)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1153, in visit
    super(AstAnnotator, self).visit(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 127, in visit
    super(BaseVisitor, self).visit(node)
  File ""/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py"", line 245, in visit
    return visitor(node)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 90, in wrapped
    f(self, node, *args, **kwargs)
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 219, in visit_If
    default=':\n')
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1334, in attr
    attr_parts.append(self.token(attr_val))
  File ""/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py"", line 1260, in token
    token_val, token.src, token.start[0], token.line))
pasta.base.annotate.AnnotationError: Expected ':' but found 'b"":""'
line 564:             if line.startswith(b"":""):  # Skip comments.
```"
24920,how to use resnet_v1,"
"
24919,About the MFCC feature in input_data.py,"Dear,
In the script,
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py](url)
is there any related pdf or other file about the mfcc feature extraction,
Could U help me about this, please?
Any advice or suggestion will be okey.
Thx."
24916,The Object Detection Tutorial is broken,"**System information**
- TensorFlow version: 1.13.0-dev20190114
- Doc Link: https://www.tensorflow.org/lite/
**Describe the documentation issue**
TFLite's home page links to this Object Detection tutorial: https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 
The tutorial is not compatible with the latest TensorFlow (1.13). It has multiple references to the TOCO convertor which used to be in tensorflow/contrib/lite/toco:toco but has been recently removed. I believe, it has been replaced by TFLiteConverter which has a different interface, so the tutorial needs to be updated. Plus, it would make a lot of sense to provide an iOS specific object detection tutorial. You already have both Android and iOS image classifier tutorials, so it would be sweet to give object detection the same coverage.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

This tutorial must be fixed by somebody who knows how to get TFLiteConverter to work. I gave it a shot but was unable to convert the post-processing operation. I already filed issue 24910 with all the details on this."
24913,"When I compile andorid so using bazel build and reference this so in a new project, the generated so will be linked to the absolute path.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Linux Ubuntu 16.04
- NA
- TensorFlow installed from source
- Python version: Python 2.7.12
- Bazel version: release 0.21.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 4.9.3-13ubuntu2) 4.9.3
- CUDA/cuDNN version: 
- GPU model and memory:

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Recurring step
1.  Use the following command to compile libtensorflow_demo.so in tensorflow project

bazel build -c opt //tensorflow/examples/android:tensorflow_demo    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=arm64-v8a

2. use libtensorflow_inference.so with ndk-build and get libCameraTest.so
the Fragment of Android.mk like this
include $(CLEAR_VARS)
LOCAL_MODULE := libtensorflow_demo-prebuild
LOCAL_SRC_FILES := $(LOCAL_PATH)/lib/libtensorflow_demo.so
LOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/include
include $(PREBUILT_SHARED_LIBRARY)

include $(CLEAR_VARS)
LOCAL_PATH := $(MY_PATH)
LOCAL_MODULE := CameraTest
LOCAL_SHARED_LIBRARIES  += libtensorflow_demo-prebuild
LOCAL_CPPFLAGS  += -std=c++11
LOCAL_CFLAGS := -g
LOCAL_LDLIBS    += -llog -landroid -lz 
#LOCAL_SHARED_LIBRARIES += libc libdl

LOCAL_ARM_MODE := arm  
include $(BUILD_SHARED_LIBRARY)

3. use objdump -T libCameraTest.so | grep Needed
i got the follow result
 NEEDED               D:/workspace/new_invisioncontroller/InVisionBTControl_ball_tensorflow/InVisionBTControl_ball_tensorflow/InVisionBTControl_ball_tensorflow/app/build/intermediates/ndkBuild/debug/obj/local/arm64-v8a/libtensorflow_demo.so

**Other info / logs**

"
24912,"When passing tf.data.Dataset instance to model.predict method which is instantiated by tf.keras.Sequential, tf.keras.Model, subclassing tf.keras.Model, return of model.predict is different from return of model.call ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.6


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When passing `tf.data.Dataset` instance to `model.predict` method which is instantiated by `tf.keras.Sequential`, `tf.keras.Model`, subclassing `tf.keras.Model`, return of model.predict is different from return of `model.call`

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
```
```python
1.12.0
```
```python
x = np.random.randn(60,30).astype(np.float32)
y = np.random.randint(low=0, high=10, size = 60).astype(np.int32)

x_tr = x[0:20]
y_tr = y[0:20]
x_val = x[20:40]
y_val = y[20:40]
x_tst = x[40:60]
y_tst = y[40:60]

print(x_tr.shape, y_tr.shape)
print(x_val.shape, y_val.shape)
print(x_tst.shape, y_tst.shape)


tr_dataset = tf.data.Dataset.from_tensor_slices((x_tr,y_tr))
tr_dataset = tr_dataset.batch(batch_size=4).repeat()
val_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val))
val_dataset = tr_dataset.batch(batch_size=4).repeat()
tst_dataset = tf.data.Dataset.from_tensor_slices((x_tst,y_tst))
tst_dataset = tst_dataset.batch(batch_size=4)

print(tr_dataset)
print(val_dataset)
print(tst_dataset)
```
```python
(20, 30) (20,)
(20, 30) (20,)
(20, 30) (20,)
<RepeatDataset shapes: ((?, 30), (?,)), types: (tf.float32, tf.int32)>
<RepeatDataset shapes: ((?, ?, 30), (?, ?)), types: (tf.float32, tf.int32)>
<BatchDataset shapes: ((?, 30), (?,)), types: (tf.float32, tf.int32)>
```
```python
class Model(keras.Model):
    def __init__(self, num_classes):
        super(Model, self).__init__()
        self.dense = keras.layers.Dense(units=10, activation='softmax')
    def call(self, inputs):
        score = self.dense(inputs)
        return score
  
model = Model(10)
model.compile(optimizer=tf.train.GradientDescentOptimizer(.1),
              loss=keras.losses.sparse_categorical_crossentropy)
model.fit(tr_dataset, epochs=5, steps_per_epoch=20//4,
          validation_data=val_dataset, validation_steps=20//4)
```
```python
Epoch 1/5
5/5 [==============================] - 0s 36ms/step - loss: 2.5626 - val_loss: 1.9200
Epoch 2/5
5/5 [==============================] - 0s 2ms/step - loss: 1.9345 - val_loss: 1.4256
Epoch 3/5
5/5 [==============================] - 0s 2ms/step - loss: 1.4506 - val_loss: 1.0624
Epoch 4/5
5/5 [==============================] - 0s 1ms/step - loss: 1.0906 - val_loss: 0.8049
Epoch 5/5
5/5 [==============================] - 0s 2ms/step - loss: 0.8314 - val_loss: 0.6274
```
```python
# yhat_from_call_method
sess = keras.backend.get_session()
x_tst_tensor = tf.convert_to_tensor(x_tst)
yhat_from_call_method = sess.run(model(x_tst_tensor))
yhat_from_call_method = np.argmax(yhat_from_call_method, axis = -1)
print(yhat_from_call_method)
```
```python
[7 4 7 1 8 8 3 4 8 9 8 4 0 7 1 7 4 3 0 0]
```
```python
# yhat_from_predict_method 
yhat_from_predict_method = model.predict(tst_dataset, steps=20//4)
yhat_from_predict_method = np.argmax(yhat_from_predict_method, axis =-1)
print(yhat_from_predict_method)
```
```python
[8 4 2 4 1 8 4 8 8 7 6 1 9 1 1 3 6 5 4 1]
```
```python
print(yhat_from_call_method == yhat_from_predict_method)
```
```python
[False  True False False False  True False False  True False False False
 False False  True False False False False False]
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24910,TFLiteConverter unable to convert object detection model from export_tflite_ssd_graph.py,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.13.0-dev20190114

**Provide the text output from tflite_convert**

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)

Also, please include a link to a GraphDef or the model if possible.
ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03

**Any other info / logs**
I trained the above mentioned model on my images, and then exported the graph with the --add_postprocessing_opt option (which I assume adds the TFLite_Detection_PostProcess op):
python /tf/models/research/object_detection/export_tflite_ssd_graph.py \
    --pipeline_config_path /tf/notebooks/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config \
    --trained_checkpoint_prefix /tf/notebooks/model.ckpt \
    --output_directory /tf/notebooks/tflite \
    --add_postprocessing_op=true

Then I used the following Python code in order to convert the model to TFLite format:

import tensorflow as tf
graph_def_file = ""/tf/notebooks/scripts/both_training/tflite/tflite_graph.pb""
input_arrays = [""normalized_input_image_tensor""]
output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
input_shapes = {""normalized_input_image_tensor"" : [1, 640, 640, 3]}

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""/tf/notebooks/tflite/model.tflite"", ""wb"").write(tflite_model)

I tried this with and without target_ops=SELECT_TF_OPS and the convert() call always crashes due to TFLite_Detection_PostProcess operation. The TFLite file is not created. Please help!"
24906,tf.contrib.eager.defun does not handle variables as documented,"TensorFlow version: 1.12.0

The docs for `defun` contain this example:
```python
import tensorflow as tf

tf.enable_eager_execution()

def fn():
  x = tf.Variable(0.0)
  x.assign_add(1.0)
  return x.read_value()

# `fn` is a Python function, so x is created, initialized, and destroyed upon
# every invocation
assert fn().numpy() == fn().numpy() == 1.0

compiled = tf.contrib.eager.defun(fn)

# Compiling `fn` with `defun` hoists all variables outside of the generated
# graph, so initialization happens exactly once.
assert compiled().numpy() == 1.0
assert compiled().numpy() == 2.0
```

**Describe the expected behavior**
The example should run as is.

**Describe the current behavior**
But instead it throws this error:
> AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call."
24905,"Build AVX512 Error (v1.13.0): target specific option mismatch  _mm512_xor_ps (__m512 __A, __m512 __B)","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.4.103-6.38-default x86_64
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.0
- Python version: 3.5
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0

**Describe the problem**
The build stoped, complaining about ""error: inlining failed in call to always_inline '__m512 _mm512_xor_ps(__m512, __m512)': target specific option mismatch
 _mm512_xor_ps (__m512 __A, __m512 __B)""

See the following error message for details: 
ERROR: /lus/theta-fs0/projects/datascience/hzheng/build/tensorflow/1.13/tensorflow/compiler/xla/service/cpu/BUILD:538:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1): gcc failed: error executing command
  (cd /lus/theta-fs0/projects/datascience/hzheng/bazel_cache/_bazel_hzheng/49b930504bfa76e4e005c923edac9eb5/execroot/org_tensorflow && \
  exec env - \
    PATH=/bin:/usr/bin \
    PWD=/proc/self/cwd \
    PYTHONPATH=/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/home/hzheng/mylibs/python:/home/hzheng/mylibs/python: \
    PYTHON_BIN_PATH=/opt/intel/python/2017.0.035/intelpython35/bin/python \
    PYTHON_LIB_PATH=/soft/datascience/tensorflow/tf1.12 \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /opt/gcc/7.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_syc\
l -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -g -mavx -mavx2 -mfma -mavx512f -mavx512pf -mavx512cd -mavx512er '-mtune=knl' -DEIGEN_USE_VML -DEIGEN_AVOID_STL_ARRAY -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/xla/service/cpu/runtime_fft.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from /opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/immintrin.h:57:0,
                 from external/eigen_archive/Eigen/src/Core/util/ConfigureVectorization.h:313,
                 from external/eigen_archive/Eigen/Core:22,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/types.h:21,
                 from ./tensorflow/compiler/xla/executable_run_options.h:20,
                 from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:20:
/opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/avx512dqintrin.h: In function 'Packet Eigen::internal::pconj(const Packet&) [with Packet = Eigen::internal::Packet8cf]':
/opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/avx512dqintrin.h:435:1: error: inlining failed in call to always_inline '__m512 _mm512_xor_ps(__m512, __m512)': target specific option mismatch
 _mm512_xor_ps (__m512 __A, __m512 __B)
 ^~~~~~~~~~~~~

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --verbose_failures --config=mkl -c opt --copt=-g --strip=never --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt='-mtune=knl' --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
"
24898,TensorFlow tfrecord OutOfRange error,"In [src/datasets/h36m_edit.py](https://github.com/MatthewD1993/hmr_extend/blob/5a496695c0ebdc8207a847f4d83c49426a25e934/src/datasets/h36m_edit.py#L196-L229):
```
with tf.Session() as sess:
    reader = tf.TFRecordReader()
    coder = ImageCoder()

    fqueue = tf.train.string_input_producer(files, num_epochs=1, shuffle=False, name=""input"")
    _, example_serialized = reader.read(fqueue)

    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    fidx = 0
    total_imgs = 0
    image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d = parse_example_proto(example_serialized)

    while not coord.should_stop():
        fidx += 1
        tf_filename = out_path% fidx

        print('Starting tfrecord file %s \n' % tf_filename)
        with tf.python_io.TFRecordWriter(tf_filename) as writer:
            for i in tqdm(range(train_shards)):  # min(train_shards, image_bs.shape[0])
                image_v, image_size_v, label_v, center_v, fname_v, pose_v, shape_v, gt3d_v, has_smpl3d_v = sess.run(
                    [image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d])
                image_s = coder.encode_jpeg(image_v)
                example = convert_to_example_wmosh(image_s, fname_v, image_size_v[0], image_size_v[1],
                                                   label_v, center_v, gt3d_v, pose_v, shape_v)
                writer.write(example.SerializeToString())
                total_imgs += 1

    coord.request_stop()
    coord.join(threads)
```

Sometimes the inner loop stops before it reaches the maximum iter limit (train_shards) 500. 
```
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 225.07it/s]
Starting tfrecord file /home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0011.tfrecord 

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 478/500 [00:02<00:00, 225.58it/s]Starting tfrecord file /home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0012.tfrecord 

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 230.37it/s]
```
And when it writes to the number 625 tfrecord file, there is OutOfRange error (it supposes to finish with more than 3000  tfrecord files, cause human36m train has 1559985 images and each tfrecord contains 500 images). I guess it's because the *image queue* is not handled correctly, maybe the producer is too slow?

```
/home/cdeng/tf_datasets/tf_records_human36m_wjoints/train_modified/train_0625.tfrecord 
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/500 [00:00<00:01, 221.50it/s]2019-01-13 22:47:40.946736: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input)]]
2019-01-13 22:47:40.946816: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input)]]

Traceback (most recent call last):
  File ""/home/cdeng/star_repos/hmr/src/datasets/h36m_edit.py"", line 233, in <module>
    [image, image_size, label, center, fname, pose, shape, gt3d, has_smpl3d])
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input)]]
	 [[Node: ParseSingleExample/ParseExample/ParseExample/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_52_ParseSingleExample/ParseExample/ParseExample"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'ReaderReadV2', defined at:
  File ""/home/cdeng/star_repos/hmr/src/datasets/h36m_edit.py"", line 204, in <module>
    _, example_serialized = reader.read(fqueue)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 194, in read
    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 423, in _reader_read_v2
    queue_handle=queue_handle, name=name)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/cdeng/.virtualenvs/hmr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): FIFOQueue '_0_input' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input)]]
	 [[Node: ParseSingleExample/ParseExample/ParseExample/_21 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_52_ParseSingleExample/ParseExample/ParseExample"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]


Process finished with exit code 1
```


"
24897,Fusing context and cell output in AttentionWrapper ,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
`AttentionWrapper` fuses the `cell_output` and the `context` vector using an `array_ops.concat` operation when specifying an attention layer: [link](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L1067)

In my case, this behaviour is not flexible enough, I and would like to be able to define my own function to replace the `concat` operation.

**Will this change the current api? How?**
Similarly to `cell_input_fn`, I would like to have a `attention_layer_fn` in `AttentionWrapper`, which by default will perform a `concat`, preserving the old behaviour. This function would be passed to `_compute_attention`, which could be modified like this:

```
if attention_layer is not None:
  attention_layer_input = attention_layer_fn(cell_output, context)
  attention = attention_layer(attention_layer_input)
else:
  attention = context
```
where
```
if attention_input_fn is None:
  attention_input_fn = (
      lambda cell_output, context: array_ops.concat([cell_output, context], 1))
```

**Who will benefit with this feature?**
TensorFlow users designing new attention architectures may find this feature useful.
The users have to implement their own function taking `cell_output` and `context` as inputs.

**Any Other info.**
I am not a super experienced developer, so I would also like to ask for advice.

I implemented the feature locally and successfully trained a network, but ran into a new problem.

The main goal would be to debug/visualise some internal ops defined by the user inside the new function, just like with `alignment_history`. And here is where things get complicated.

Since these operations are created inside the `while_loop` of `dynamic_rnn`, I'd have to use a `TensorArray` and write the variables at every step (otherwise it says that the operation has been marked as not fetchable), but this also requires `state.time` when calling `write()`. Then the object returned by `write()` should be added to the `AttentionWrapperState`, to show up in the final_state when running `dynamic_rnn`. Is there a simpler way to visualise all the necessary ops inside the envisaged `attention_input_fn` ?


"
24895,Issue with training of object detection api (ssd_mobilenet_v2_coco.config),"Command Used:
python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config

Please guide me if i am doing something wrong.

Error Trace:
Traceback (most recent call last):
  File ""train.py"", line 163, in <module>
    tf.app.run()
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 91, in main
    FLAGS.pipeline_config_path)
  File ""/home/tayyab/Desktop/models/object_detection/utils/config_util.py"", line 43, in get_configs_from_pipeline_file
    text_format.Merge(proto_str, pipeline_config)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 536, in Merge
    descriptor_pool=descriptor_pool)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 590, in MergeLines
    return parser.MergeLines(lines, message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 623, in MergeLines
    self._ParseOrMerge(lines, message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 638, in _ParseOrMerge
    self._MergeField(tokenizer, message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 763, in _MergeField
    merger(tokenizer, message, field)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 837, in _MergeMessageField
    self._MergeField(tokenizer, sub_message)
  File ""/home/tayyab/Desktop/tensor_android/local/lib/python2.7/site-packages/google/protobuf/text_format.py"", line 730, in _MergeField
    (message_descriptor.full_name, name))
google.protobuf.text_format.ParseError: 86:7 : Message type ""object_detection.protos.SsdFeatureExtractor"" has no field named ""use_depthwise""."
24893,the results of tf.image.decode_image() and opencv.imread() are different ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.10
- **Python version**: python2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda9.0-cudnn7.0
- **GPU model and memory**: V100,P40
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```
### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

the results of tf.image.decode_image() and opencv.imread() are different. i read a image file by 
tf.gfile.GFile(filename,'r').read() + tf.image.decode_image() and opencv2.imread() respectively. and i get the different results. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

image_data = tf.gfile.GFile(filename,'r').read()
tf.reset_default_graph()  
graph = tf.Graph()        
with graph.as_default() as g:   
  with tf.Session(graph=g) as session:
      image = session.run(tf.image.decode_image(image_data, channels = 3))
      print(image)

[[[  0   0   5]
  [ 79  80  85]
  [ 66  63  70]
  ...
  [233 236 215]
  [233 236 215]
  [233 236 215]]

 [[  0   0   5]
  [ 78  79  84]
  [ 64  61  68]
  ...
  [233 236 215]
  [233 236 215]
  [233 236 215]]

 [[  0   1   6]
  [ 77  78  83]
  [ 61  58  65]
  ...
  [233 236 215]
  [233 236 215]
  [233 236 215]]

 ...

 [[  0   2   0]
  [134 140 138]
  [133 137 136]
  ...
  [ 69  58  75]
  [ 69  60  79]
  [ 72  63  82]]

 [[  0   2   0]
  [134 140 138]
  [133 137 136]
  ...
  [ 41  28  45]
  [ 49  38  55]
  [ 61  49  69]]

 [[  0   3   1]
  [129 135 133]
  [139 143 142]
  ...
  [ 29  15  32]
  [ 25  12  29]
  [ 22   9  27]]]

image = cv2.imread(FLAGS.image_dir + '18455308.jpg')
image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
print(image)

[[[  0   0   7]
  [ 80  80  88]
  [ 66  63  72]
  ...
  [235 237 216]
  [235 237 216]
  [235 237 216]]

 [[  0   0   7]
  [ 79  79  87]
  [ 64  61  70]
  ...
  [235 237 216]
  [235 237 216]
  [235 237 216]]

 [[  0   2   9]
  [ 77  77  85]
  [ 61  58  67]
  ...
  [235 237 216]
  [235 237 216]
  [235 237 216]]

 ...

 [[  0   1   0]
  [135 139 138]
  [134 138 137]
  ...
  [ 69  57  77]
  [ 72  60  82]
  [ 75  63  85]]

 [[  0   1   0]
  [135 139 138]
  [134 138 137]
  ...
  [ 41  28  46]
  [ 51  38  58]
  [ 62  49  69]]

 [[  0   2   1]
  [131 135 134]
  [140 144 143]
  ...
  [ 29  15  32]
  [ 25  12  30]
  [ 22   9  27]]]

i am wondering is that normal? and i found this may cause different predict results with the same model. "
24892,"""Transient arrays with strings are not supported yet"" when converting GraphDef to tflite file","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/7.4
- GPU model and memory: GTX1080Ti 11GB


**Describe the current behavior**
No file was created.

**Describe the expected behavior**
A .tflite file is converted.

**Code to reproduce the issue**
The model cann't be provided.

**Other info / logs**
```
 CUDA_VISIBLE_DEVICES=""2"" tflite_convert \
  --output_file=frozen.tflite \
  --graph_def_file=model.pb \
  --input_arrays=input_image \
  --output_arrays=train_net/recon_image
2019-01-14 17:09:08.298351: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-01-14 17:09:08.549870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:82:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2019-01-14 17:09:08.549910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-01-14 17:09:08.874792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-14 17:09:08.874835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-01-14 17:09:08.874846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-01-14 17:09:08.875110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
b'2019-01-14 17:09:13.458649: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-01-14 17:09:13.697666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\npciBusID: 0000:82:00.0\ntotalMemory: 10.92GiB freeMemory: 10.56GiB\n2019-01-14 17:09:13.697710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2019-01-14 17:09:14.017280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2019-01-14 17:09:14.017321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n2019-01-14 17:09:14.017329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n2019-01-14 17:09:14.018988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10209 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\n2019-01-14 17:09:14.053557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Round\n2019-01-14 17:09:14.061350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ImageSummary\n2019-01-14 17:09:14.061401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: train_net/recon_image\n2019-01-14 17:09:14.067392: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 168 operators, 287 arrays (0 quantized)\n2019-01-14 17:09:14.070050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 168 operators, 287 arrays (0 quantized)\n2019-01-14 17:09:14.105968: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting ""build_towers/tower_0/train_net_inference_one_pass/train_net/random_uniform_1/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this\n2019-01-14 17:09:14.135825: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 47 operators, 89 arrays (0 quantized)\n2019-01-14 17:09:14.136127: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting ""build_towers/tower_0/train_net_inference_one_pass/train_net/random_uniform_1/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this\n2019-01-14 17:09:14.136547: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 47 operators, 89 arrays (0 quantized)\n2019-01-14 17:09:14.137116: F tensorflow/contrib/lite/toco/tooling_util.cc:1674] Transient arrays with strings are not supported yet\nAborted (core dumped)\n'
None
```

What does ""Transient arrays with strings are not supported yet"" means? What should I modify in my model?"
24890,"XLA Compiler Error ""(...)C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command""","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1809 Betriebssystembuild: 17763.253
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source (install impossible)
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: virtualenv: yes; pip: yes anaconda: no
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: RTX 2070 8GB
- Microsoft Visual Studio Version: VS 2017 Enterprise (I am student)



**Describe the problem**
If i build the pip package, than  following error occured:

**Provide the exact sequence of commands / steps that you executed before running into the problem**

ERROR: F:/*/tensorflow/tensorflow/compiler/xla/BUILD:711:1: C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/bjoern_std/_bazel_bjoern_std/h2irjqdy/execroot/org_tensorflow

**Any other info / logs**
Log one step before the error and one step after:
""(...)Hinweis: Einlesen der Datei:    external/com_google_absl\absl/numeric/int128_no_intrinsic.inc
ERROR: F:/*/tensorflow/tensorflow/compiler/xla/BUILD:711:1: C++ compilation of rule '//tensorflow/compiler/xla:window_util' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/*/_bazel_bjoern_std/h2irjqdy/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;F:\Windows Kits\10\include\10.0.17763.0\ucrt;F:\Windows Kits\10\include\10.0.17763.0\shared;F:\Windows Kits\10\include\10.0.17763.0\um;F:\Windows Kits\10\include\10.0.17763.0\winrt;F:\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;F:\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;F:\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Team Tools\Performance Tools;F:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;F:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\;F:\Windows Kits\10\bin\10.0.17763.0\x64;F:\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages
    SET TEMP=C:\Users\*~1\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.5
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\*~1\AppData\Local\Temp
  C:/Program Files/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/_objs/window_util/window_util.o /c tensorflow/compiler/xla/window_util.cc
Execution platform: @bazel_tools//platforms:host_platform
Hinweis: Einlesen der Datei: .\tensorflow/compiler/xla/window_util.h(...)""
""(...) Hinweis: Einlesen der Datei:   external/com_google_absl\absl/strings/string_view.h
Hinweis: Einlesen der Datei:    C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\include\cassert
Hinweis: Einlesen der Datei:     F:\Windows Kits\10\include\10.0.17763.0\ucrt\assert.h
Hinweis: Einlesen der Datei: .\tensorflow/core/platform/logging.h
Hinweis: Einlesen der Datei:  .\tensorflow/core/platform/default/logging.h
Hinweis: Einlesen der Datei:   .\tensorflow/core/platform/macros.h
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 536,864s, Critical Path: 314,01s
INFO: 2538 processes: 2538 local.
FAILED: Build did NOT complete successfully(...)""

NOTE from threadauthor: I have short the log, because they have only read acknowledges!
A star (*) in the path means, that i anonymised my account name!
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24888,Tflite Op missing:Stack,"**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installed from ( pip ):
- TensorFlow version ( 1.10 ):


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack.

```
"
24887,Distributed training fails when using CollectiveAllReduceStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
16.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
tensorflow (1.12.0rc0)
- Python version:
Python3.4
- Bazel version (if compiling from source):
bazel 0.16
- GCC/Compiler version (if compiling from source):
gcc4.8.5
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am trying to employing CollectiveAllReduceStrategy upon tensorflow official model resnet following the instructions from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#multi-worker-training.
Code: https://github.com/threeleafzerg/models
Steps:
1) Prepare dataset: python3 cifar10_download_and_extract.py --${PWD}/cifar_data
2) Start Worker1: https://github.com/threeleafzerg/models/blob/master/official/resnet/worker1.sh
3) Start Worker2: https://github.com/threeleafzerg/models/blob/master/official/resnet/worker2.sh
I expect that the distributed training could start successfully. 

But unfortunately, I got python exceptions. 
  File ""/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 403, in _init_from_arg s
    initial_value() if init_from_fn else initial_value,
  File ""/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py"", lin e 180, in _overridden_initial_value_fn
    group_size, group_key, collective_instance_key)
  File ""/home/zhouhaiy/.local/lib/python3.4/site-packages/tensorflow/python/ops/collective_ops.py"", line 94, in broadcast_send
    'Parameter group_size to broadcast_send must be at least 2.')
ValueError: Parameter group_size to broadcast_send must be at least 2.

**Describe the expected behavior**
Distributed training can start successfully. 

**Code to reproduce the issue**
I have uploaded my experiment code in my private branch: https://github.com/threeleafzerg/models


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24886,tf-nightly-2.0-preview failed to install on windows1809,"**System information**
- OS Platform and Distribution:Windows 10 1809 x64
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly-2.0-preview
- Python version: 3.6
- Installed using virtualenv? pip? conda?: use pip
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.0 
- GPU model and memory: gtx 1070 



**Describe the problem**
When I try to install tf-nightly-2.0-preview on windows 10(1809) using follow command.
`pip install tf-nightly-2.0-preview`
I meet problem about follow error information:
`Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\anqin\\AppData\\Local\\Temp\\pip-install-hhl6pefq\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-1.13.0.dev20190113.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'`

Does anyone know how to solve it?
"
24885,Complete support for building C++ API on windows using Bazel,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.11.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Feature: Correctly use Bazel to build Tensorflow C++ API on Windows.

Current state: Now we can build Tensorflow C++ API on Windows using Bazel by `bazel build //tensorflow:libtensorflow_cc.so` (I even wrote a [script](https://github.com/guikarist/tensorflow-windows-build-script) to do this). However, there are some problems:

1. [necessary symbols are not exported](https://github.com/tensorflow/tensorflow/blob/ac4d3682939dac303082097d25ebda805409acde/tensorflow/BUILD#L521) and [#23542](https://github.com/tensorflow/tensorflow/issues/23542)
1. The built libraries have `.so` extensions which are not for Windows platform, although after being renamed to `.dll` the dynamic libraries can be used.
1. There is no information about how to use built third-party `.a` files.

**Will this change the current api? How?**

Yes. The API may become `//tensorflow:libtensorflow_cc.dll`.

**Who will benefit with this feature?**

1. Everyone who wants to build Tensorflow C++ API on Windows.
1. Everyone who wants to use the latest Tensorflow C++ API on Windows (the last version of C++ API which can be built officially on Windows is 1.10.0).

**Any Other info.**
No."
24882,Tensorflow import issue,"**System information**
-Windows 10
-Python 3.6
-TensorFlow installed using pip3
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**
I am getting Importerror as  **DLL load failed: A dynamic link library (DLL) initialization routine failed.**
```
# Copy and paste here
import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Selvi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 


Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24881,Support per channel quantized ops,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The current quantization Ops -  [tf.quantization](https://www.tensorflow.org/api_docs/python/tf/quantization), [tf.nn.quantized_conv2d](https://www.tensorflow.org/api_docs/python/tf/quantization) support only per layer quantization. For per channel quantization the `min` and `max` values should be of dimension `[d]`, one for each channel instead of being a single `float` value now.

**Will this change the current api? How?**
Yes. The ops will take `[d]` dimensional `min` and `max` values, one per channel. And the `output_min` and `output_max` would also be  `[d]` dimensional .
**Who will benefit with this feature?**
Anyone who wants to use per channel quantization.
**Any Other info.**
"
24879,how to control the dimension of operation tf.spectral.fft2() ,"Hi TF, I am new here and wondering that

for a tf.tensor (placeholder) normally defined as shape=[batch_size, image_height, image_width, channel_number], along which dimensions would fft2 operate on?

what if I just want to do 2d fft in image domain, i.e. the middle two dimensions of the above fourth-order tensor?

huge thanks"
24877,TFLite Upsample Nearest Neighbors,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos
- TensorFlow installed from (source or binary): 1.12 /  tf-nightly-1.13.0.dev20190112
- TensorFlow version (or github SHA if from source):

command: tflite_convert --keras_model_file=model.h5 --output_file=model.tflite --input_array=input_1 --inference_type=FLOAT --input_shape=1,512,512,3

I'm trying to convert a keras model that has an upsample nearest neighbors op. In tf 1.12 this op is not supported, and I saw it was added later on but not yet in the official release. When I used tf-nightly I get an error:

error:
2019-01-13 09:12:33.214920: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-01-13 09:12:33.215726: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-01-13 09:12:33.275871: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node initial_conv/kernel/Assign doesn't exist in graph

Is there some commit I can use to convert my model?
"
24875,How to define a variable with different value for each tower under MirroredStrategy,"I tried to train model on multi-gpus with tf estimator and MirroredStrategy. And I found that variables of the model are wrapped as MirroredVariables which are always in sync among towers.

However, there is a variable that needs to have different value for each tower on every step of training, How to define such variable ? that is, a variable with locality T, refers to (https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/DistributionStrategy)"
24874,Eager mode / AutoGraph control dependencies,"In the current main examples of `autograph` which uses eager mode when calling `opt.minimize` there is this comment:
```
  # Autograph automatically adds the necessary `tf.control_dependencies` here.
  # (Without them nothing depends on `opt.minimize`, so it doesn't run.)
  # This makes it much more like eager-code.
```
So my question is could you clarify how this magic is happening behind the scenes (e.g. what are the conditions for which autograph creates these dependencies and if one wants to somehow force in eager mode some form of dependencies, how to achieve it. For instance consider the following ""python"" code:
```
def some_fn(x, m, t):
   if t % 100 == 0:
       m = m * 0.9 + x ** 2 * 0.1
   return x / tf.sqrt(m)
```
Here we want the final value of` to always be either the input or the updated value dependable on `t`. How can we force this control dependency? "
24873,tf.sparse.to_dense warns that you should use itself,"
**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/sparse/to_dense
- Implemented here: https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L1115


**Describe the documentation issue**
Hi,
The function [`tf.sparse.to_dense`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L1115), is a wrapper for `tf.sparse_to_dense`. However, [`tf.sparse_to_dense`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L832) is deprecated and produces a warning that the user should use `tf.sparse.to_dense` instead.

So, the user uses the correct function `tf.sparse.to_dense` but still gets a warning that she should use it.

Example code that produces that warning:
`
s = tf.SparseTensor(indices=[[1,1],[2,2],[3,3]], values=[4,5,6],dense_shape=[3,3])
tf.sparse.to_dense(s)
`

results in a warning:
WARNING:tensorflow:From /home/urialon/.local/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.

So I am not sure - is this the right usage, and just the warnings are wrong?
Thanks!"
24872,Unable to load native tensorflow," Mac OS X EI Captain.Python Anaconda 3.6, am getting the following error when i run ""from keras.models import Sequential""

Using TensorFlow backend.
ImportError Traceback (most recent call last)
/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in ()
57
---> 58 from tensorflow.python.pywrap_tensorflow_internal import *
59 from tensorflow.python.pywrap_tensorflow_internal import version

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in ()
27 return _mod
---> 28 _pywrap_tensorflow_internal = swig_import_helper()
29 del swig_import_helper

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
23 try:
---> 24 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
25 finally:

/Applications/Anaconda/anaconda/lib/python3.6/imp.py in load_module(name, file, filename, details)
241 else:
--> 242 return load_dynamic(name, filename, file)
243 elif type_ == PKG_DIRECTORY:

/Applications/Anaconda/anaconda/lib/python3.6/imp.py in load_dynamic(name, path, file)
341 name=name, loader=loader, origin=path)
--> 342 return _load(spec)
343

ImportError: dlopen(/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
Referenced from: /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
Expected in: /usr/lib/libSystem.B.dylib
in /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

During handling of the above exception, another exception occurred:

ImportError Traceback (most recent call last)
in ()
----> 1 from keras.models import Sequential
2 from keras.layers import Dense
3 from keras.layers import LSTM

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/init.py in ()
1 from future import absolute_import
2
----> 3 from . import utils
4 from . import activations
5 from . import applications

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/utils/init.py in ()
4 from . import data_utils
5 from . import io_utils
----> 6 from . import conv_utils
7
8 # Globally-importable utils.

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/utils/conv_utils.py in ()
7 from six.moves import range
8 import numpy as np
----> 9 from .. import backend as K
10
11

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/backend/init.py in ()
87 elif _BACKEND == 'tensorflow':
88 sys.stderr.write('Using TensorFlow backend.\n')
---> 89 from .tensorflow_backend import *
90 else:
91 # Try and load external backend.

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in ()
3 from future import print_function
4
----> 5 import tensorflow as tf
6 from tensorflow.python.framework import ops as tf_ops
7 from tensorflow.python.training import moving_averages

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/init.py in ()
22
23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import
25
26 try:

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/init.py in ()
47 import numpy as np
48
---> 49 from tensorflow.python import pywrap_tensorflow
50
51 from tensorflow.python.tools import component_api_helper

/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in ()
72 for some common reasons and solutions. Include the entire stack trace
73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74 raise ImportError(msg)
75
76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
File ""/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/Applications/Anaconda/anaconda/lib/python3.6/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/Applications/Anaconda/anaconda/lib/python3.6/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: dlopen(/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
Referenced from: /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)
Expected in: /usr/lib/libSystem.B.dylib
in /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
24871,Documentation on reducing the binary size of tflite library on iOS,"**System information**
- TensorFlow version: r1.12
- Doc Link: https://www.tensorflow.org/lite/

**Describe the documentation issue**

The tflite library seems to add around 2.5 MB to my iOS binary, which is far bigger than the 700 kB claimed in the documentation. Are there custom build options for tflite like there was with tfmobile that can help get this library size down? I haven't been able to find documentation on how to do this.

For context, I've tried using both the Pod and also used the build_ios_universal_lib.sh script, but the library size still seems to be around 2.5 MB."
24870,tensorflow installed but cannot be imported,"**System information**
- Microsoft Windows 10 Home Single Language
- TensorFlow installed from binary
- TensorFlow version: 1.12.0 but also tried bit 1.5.0 and 1.11.0
- Python version: Python 3.6.8 and also tried 3.5.2
- Installed using virtualenv? pip? conda?: tried using pip and conda
- CUDA/cuDNN version: cannot find a cuda or cudnn file
- GPU model and memory: Nvidia geforce 840 - Total memory 6071 MB of which 2010MB is Video RAM

The installation of tensorflow using the `pip install tensorflow` command was successful but when I test the command `import tensorflow as tf` with `cmd.exe`, I get the following result:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Bruno\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
ModuleNotFoundError: No module named 'tensorflow.python'
```
I tried so many things that i am quite desperate. Can someone please suggest a solution?"
24868,TF tutorial for Custom Layers seems not work for multiple GPUs case,"In the tutorial [Custom layers](https://www.tensorflow.org/tutorials/eager/custom_layers), it says to extend the `tf.keras.Layer` class. However, Keras layer doesn't have `_reuse` attribute, and it doesn't work for multiple GPUs case. I check that in the `tf.layers.conv` definition script, it inherits from two classes: 

```
@tf_export('layers.Conv1D')
class Conv1D(keras_layers.Conv1D, base.Layer):
```

The second class provides support for variable reuse."
24866,Gradient descent in the body of while loop (v2) causes error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6, TensorFlow r1.12
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-rc0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 10.0.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When using `tf.while_loop` (v1), we are able to call gradients in the body of the while loop. In other words, we were able to embed the training into our TensorFlow graph.

If we replace `tf.while_loop` with `while_v2.while_loop` then we end up getting an error:

`In op 'train/update_W/ApplyGradientDescent', input types ([tf.float32, tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32, tf.float32])`

Since this shows that gradient descent is being fed a list of length 3, but the placeholders have a length of 2, I think it might be something around the counter getting added. However, I'm not certain and any insights you guys have would be useful!

**Describe the expected behavior**
We expect to get the same results when we use v1 or v2. We know that you guys are working on v2 of the while loops, and we're really excited for it. We just wanted to make certain this was on your radar as a use case, since we didn't see it in the RFC ðŸ˜„ 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
def body(loopVar):
    y = x * W
    loss = tf.reduce_mean((y - y_)**2)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
    train_op = optimizer.minimize(loss, name='train')

    with tf.control_dependencies([train_op]):
        return loopVar + 1

x = tf.placeholder(tf.float32, name='input')
y_ = tf.placeholder(tf.float32, name='target')
W = tf.Variable(5., name='W')

counter = tf.Variable(0, name='counter')
z = while_v2.while_loop(lambda v: v < 6, body, [counter])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(W))
    print(sess.run(z, feed_dict={x: [1.0, 2.0], y_:[2.0, 4.0]}))
    print(sess.run(W))
```

(If we replace `while_v2.while_loop` with `tf.while_loop` then we get the expected results of seeing the value of W move towards 2)

**Other info / logs**
"
24865,[TF2.0] Custom variables,"Hello all,

I'm testing new features of TF2.0. Especially, I'm interested in creating custom variables. According to [Variables RFC](https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md) description, it must be easy to do:
> tf.Variable will become an abstract base class with a well-defined interface and `a scoped factory to construct instances: 1. Users will be able to implement their own variable-like objects by subclassing tf.Variable and adding a scoped factory function to use those variables, 2. ...

But, I'm getting errors and because of lack of the documentation, I can not figure out either I do something wrong or this feature doesn't work at the moment.

I'm using tensorflow from pip `tf-nightly-2.0-preview`.

version 1:
```python
In [74]: class Variable(metaclass=tf.Variable): pass
In [75]: v = Variable(1.0)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-75-5f9f01fedcf6> in <module>
----> 1 v = Variable(1.0)

TypeError: 'ResourceVariable' object is not callable
```


version 2:
```python
In [78]: class Variable(tf.Variable):
    ...:     pass
    ...:

In [79]: v = Variable(10.)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-79-9441da3b11b5> in <module>
----> 1 v = Variable(10.)

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    200       return cls._variable_v2_call(*args, **kwargs)
    201     else:
--> 202       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    203
    204

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation)
    395       RuntimeError: If eager execution is enabled.
    396     """"""
--> 397     raise NotImplementedError
    398
    399   def __repr__(self):

NotImplementedError:
```

I would be grateful for the clarity about the status of this RFC part and, in case it is implemented, some guidance how it should be done.

Thanks!"
24863,AttributeError: 'MirroredStrategy' object has no attribute 'read_var',"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.0/7.3 (drivers 410.72)
- GPU model and memory: Gtx 1080Ti 11178 MiB


**Describe the current behavior**
`AttributeError: 'MirroredStrategy' object has no attribute 'read_var'` when trying to use MirroredStrategy.

**Describe the expected behavior**
No error. I just compiled the new master branch, and it was working before.

**Code to reproduce the issue**
```
 n_gpus = 2
 strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=n_gpus) if n_gpus > 1 else None
 run_config = tf.estimator.RunConfig(
     model_dir=model_dir,
      save_checkpoints_steps=save_steps,
      save_summary_steps=summary_steps,
      train_distribute=strategy,
      keep_checkpoint_max=None)
#...  Use this RunConfig in EstimatorSpec ...

```

**Other info / logs**
```
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
2019-01-11 18:35:33.974554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:17:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2019-01-11 18:35:34.097498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:65:00.0
totalMemory: 10.91GiB freeMemory: 10.42GiB
2019-01-11 18:35:34.098592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1
2019-01-11 18:35:34.099646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-11 18:35:34.099665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 
2019-01-11 18:35:34.099694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y 
2019-01-11 18:35:34.099743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N 
2019-01-11 18:35:34.100040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2019-01-11 18:35:34.100319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2019-01-11 18:35:34.101973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1
2019-01-11 18:35:34.102046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-11 18:35:34.102073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 
2019-01-11 18:35:34.102087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y 
2019-01-11 18:35:34.102132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N 
2019-01-11 18:35:34.102368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2019-01-11 18:35:34.102507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
2019-01-11 18:35:34.180174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1
2019-01-11 18:35:34.180243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-11 18:35:34.180256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 
2019-01-11 18:35:34.180265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y 
2019-01-11 18:35:34.180274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N 
2019-01-11 18:35:34.180495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
2019-01-11 18:35:34.180773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)

""siknet.py"" in <module>
  585:  tf.app.run()
  ./siknet.py
""app.py"" in run
  125:  _sys.exit(main(argv))
  /usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py
""siknet.py"" in main
  578:  sync_replicas=FLAGS.sync_replicas
  ./siknet.py
""utils.py"" in train_and_eval
  681:  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  /home/rpng/github/siknet/utils.py
""training.py"" in train_and_evaluate
  471:  return executor.run()
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py
""training.py"" in run
  610:  return self.run_local()
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py
""training.py"" in run_local
  711:  saving_listeners=saving_listeners)
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py
""estimator.py"" in train
  354:  loss = self._train_model(input_fn, hooks, saving_listeners)
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py
""estimator.py"" in _train_model
 1181:  return self._train_model_distributed(input_fn, hooks, saving_listeners)
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py
""estimator.py"" in _train_model_distributed
 1256:  self._train_distribution.read_var(global_step_tensor))
  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py
AttributeError: 'MirroredStrategy' object has no attribute 'read_var'
```
"
24857,Getting wrong predictions after saving and reloading an Estimator,"I am trying to save an `Estimator` and then load it to predict as required. Part where I train the model:

    classifier = tf.estimator.Estimator(model_fn=bag_of_words_model)
    
    # Train
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""words"": x_train},  # x_train is 2D numpy array of shape (26, 5)
        y=y_train,                   # y_train is 1D panda series of length 26
        batch_size=1000,
        num_epochs=None,
        shuffle=True)
    
    classifier.train(input_fn=train_input_fn, steps=300)

I then save the model as follows:

    def serving_input_receiver_fn():
        serialized_tf_example = tf.placeholder(dtype=tf.int64, shape=(None, 5), name='words')
        receiver_tensors = {""predictor_inputs"": serialized_tf_example}
        features = {""words"": tf.tile(serialized_tf_example, multiples=[1, 1])}
        return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
    
    full_model_dir = classifier.export_savedmodel(export_dir_base=""E:/models/"",
                                                  serving_input_receiver_fn=serving_input_receiver_fn)

I now load the model and give the test set to it for prediction:

    from tensorflow.contrib import predictor
    
    classifier = predictor.from_saved_model(""E:\\models\\1547122667"")
    predictions = classifier({'predictor_inputs': x_test})
    print(predictions)

This gives me predictions like:

    {'class': array([ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0, 15,  0,
            0,  5,  0, 20,  0,  5,  0,  0,  0], dtype=int64),
    'prob': array([[9.9397606e-01, 6.5355714e-05, 2.2225287e-05, ..., 1.4510043e-07,
                1.6920333e-07, 1.4865007e-07],
               [9.9886864e-01, 1.4976941e-06, 7.0847680e-05, ..., 9.4182191e-08,
                1.1828639e-07, 9.5683227e-08],
               [9.9884748e-01, 2.1105163e-06, 1.1994909e-05, ..., 8.3957858e-08,
                1.0476184e-07, 8.5592234e-08],
               ...,
               [9.6145850e-01, 6.9048328e-05, 1.1446012e-04, ..., 7.3761731e-07,
                8.8173107e-07, 7.3824998e-07],
               [9.7115618e-01, 2.9716679e-05, 5.9592247e-05, ..., 2.8933655e-07,
                3.4183532e-07, 2.9737942e-07],
               [9.7387028e-01, 6.9163914e-05, 1.5800977e-04, ..., 1.6116818e-06,
                1.9025001e-06, 1.5990496e-06]], dtype=float32)}

`class` and `prob` are two things that I am predicting. Now, if I predict the output with the same test set without saving and loading the model:

    # Predict.
    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""words"": x_test}, y=y_test, num_epochs=1, shuffle=False)
    predictions = classifier.predict(input_fn=test_input_fn)
    print(predictions)

then I get the output as follows:

    {'class': 0, 'prob': array([9.9023646e-01, 2.6038184e-05, 3.9950578e-06, ..., 1.3950405e-08,
           1.5713249e-08, 1.3064114e-08], dtype=float32)}
    {'class': 1, 'prob': array([2.0078469e-05, 9.9907070e-01, 8.9245419e-05, ..., 6.6533559e-08,
           7.1365662e-08, 6.8764685e-08], dtype=float32)}
    {'class': 2, 'prob': array([3.0828053e-06, 9.6484597e-05, 9.9906868e-01, ..., 5.9190391e-08,
           6.0995028e-08, 6.2322023e-08], dtype=float32)}
    {'class': 3, 'prob': array([7.4923842e-06, 1.1112734e-06, 1.1697492e-06, ..., 4.4295877e-08,
           4.4563325e-08, 4.0475427e-08], dtype=float32)}
    {'class': 4, 'prob': array([4.6085161e-03, 2.8403942e-05, 2.0638861e-05, ..., 7.6083229e-09,
           8.5255349e-09, 6.7836012e-09], dtype=float32)}
    {'class': 5, 'prob': array([6.2119620e-06, 7.2357750e-07, 2.6231232e-06, ..., 7.4999367e-09,
           9.0847436e-09, 7.5630142e-09], dtype=float32)}
    {'class': 6, 'prob': array([4.4882968e-06, 2.2007227e-06, 8.3352124e-06, ..., 2.3130213e-09,
           2.3657243e-09, 2.0045692e-09], dtype=float32)}
    {'class': 7, 'prob': array([1.88617545e-04, 9.01482690e-06, 1.47353385e-05, ...,
           3.38567552e-09, 3.97709154e-09, 3.37017392e-09], dtype=float32)}
    {'class': 8, 'prob': array([1.9843496e-06, 4.5909755e-06, 4.8804057e-05, ..., 2.2636470e-08,
           2.0094852e-08, 2.0215294e-08], dtype=float32)}
    {'class': 9, 'prob': array([2.5907659e-04, 4.4661370e-05, 6.9490757e-06, ..., 1.6249915e-08,
           1.7579131e-08, 1.5439820e-08], dtype=float32)}
    {'class': 10, 'prob': array([3.6456138e-05, 7.5861579e-05, 3.0208937e-05, ..., 2.7859956e-08,
           2.5423596e-08, 2.8662368e-08], dtype=float32)}
    {'class': 11, 'prob': array([1.1723863e-05, 9.1407037e-06, 4.8835855e-04, ..., 2.3693143e-08,
           2.0524153e-08, 2.3223269e-08], dtype=float32)}
    {'class': 12, 'prob': array([1.2886175e-06, 2.6652628e-05, 2.7812246e-06, ..., 4.8295210e-08,
           4.4282604e-08, 4.7342766e-08], dtype=float32)}
    {'class': 13, 'prob': array([3.3486103e-05, 1.3361238e-05, 3.6493871e-05, ..., 2.2195401e-09,
           2.4768412e-09, 2.0150714e-09], dtype=float32)}
    {'class': 14, 'prob': array([4.6108948e-05, 3.0377207e-05, 2.0945006e-06, ..., 4.2276231e-08,
           5.2376720e-08, 4.4969173e-08], dtype=float32)}
    {'class': 15, 'prob': array([1.7165689e-04, 2.9350400e-05, 3.2283624e-05, ..., 7.1849078e-09,
           7.6871531e-09, 6.6224697e-09], dtype=float32)}
    {'class': 16, 'prob': array([5.9876328e-07, 3.0931276e-06, 1.5760432e-05, ..., 4.0450086e-08,
           4.2720632e-08, 4.6017195e-08], dtype=float32)}
    {'class': 17, 'prob': array([2.6658317e-04, 9.9656281e-05, 4.0355867e-06, ..., 1.2873563e-08,
           1.4808875e-08, 1.2155732e-08], dtype=float32)}
    {'class': 18, 'prob': array([1.4914459e-04, 2.1025437e-06, 1.2505146e-05, ..., 9.8899635e-09,
           1.1115599e-08, 8.9312255e-09], dtype=float32)}
    {'class': 19, 'prob': array([2.5615416e-04, 2.3750392e-05, 2.2886352e-04, ..., 3.9635733e-08,
           4.5139984e-08, 3.8605780e-08], dtype=float32)}
    {'class': 20, 'prob': array([6.3949975e-04, 2.3652929e-05, 7.8577641e-06, ..., 2.0959168e-09,
           2.5495863e-09, 2.0428985e-09], dtype=float32)}
    {'class': 21, 'prob': array([8.2179489e-05, 8.4409467e-06, 5.4756888e-06, ..., 2.2360982e-09,
           2.4820561e-09, 2.1206517e-09], dtype=float32)}
    {'class': 22, 'prob': array([3.9681905e-05, 2.4394642e-06, 8.9102805e-06, ..., 2.0282410e-08,
           2.1132811e-08, 1.8368105e-08], dtype=float32)}
    {'class': 23, 'prob': array([3.0794261e-05, 6.5104805e-06, 3.3528936e-06, ..., 2.0360846e-09,
           1.9360573e-09, 1.7195430e-09], dtype=float32)}
    {'class': 24, 'prob': array([3.4596618e-05, 2.2907707e-06, 2.5318438e-06, ..., 1.1038886e-08,
           1.2148775e-08, 9.9556408e-09], dtype=float32)}
    {'class': 25, 'prob': array([1.4846727e-03, 1.9189476e-06, 5.3232620e-06, ..., 3.1966723e-09,
           3.5612517e-09, 3.0947123e-09], dtype=float32)}

which is correct. Notice the difference between two outputs is that the `class` in the second one is increasing 1 by 1 while the `class` in the first case shows 0s at most places.

Why is there a difference in the prediction? Am I saving the model in a wrong way?


@ispirmustafa @random-forests @DynamicWebPaige"
24856,Eager Execution Guide link is broken,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: v2.0
- Doc Link: https://github.com/ehennis/Blog/tree/master/TensorFlow


**Describe the documentation issue**
I was looking through the eager execution documentation and one of the links is missing the extension 'ipynb'. If you click on the ""eager execution guide"" the link is broken.

Markdown Page: https://github.com/ehennis/Blog/tree/master/TensorFlow.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
24855,"[Error, Quantization] tf.nn.quantized_conv2d with filter type tf.qint8","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0, b'v1.12.0-5132-g02b966e'
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
Error with using `tf.nn.quantized_conv2d()`. If both input and filter have type `tf.quint8` then everything is fine. But if filter have type `tf.qint8` then error occurs. Tensorflow was built with `--config=mkl`. There is kernel for filter with type `tf.qint8`:
```
Registered devices: [CPU]
Registered kernels:
  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QINT8]; out_type in [DT_QINT32]
  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QUINT8]; out_type in [DT_QINT32]

         [[QuantizedConv2D]]
```

**Code to reproduce the issue**
```
import tensorflow as tf

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.random_normal([3, 3, 1, 4])
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')
qW = tf.quantize(W, min_W, max_W, tf.qint8, mode='MIN_FIRST')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
AW = tf.dequantize(*qAW, mode='MIN_FIRST')

tf.Session().run(AW)
```

**Error log**
```
Traceback (most recent call last):
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Missing 0-th output from {{node QuantizedConv2D}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    tf.Session().run(AW)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Missing 0-th output from node QuantizedConv2D (defined at test.py:13)

Errors may have originated from an input operation.
Input Source operations connected to node QuantizedConv2D:
 Const_3 (defined at test.py:9)
 random_normal_1/stddev (defined at test.py:7)
 Const_1 (defined at test.py:5)
 random_normal/stddev (defined at test.py:3)
 Const (defined at test.py:4)
 Const_2 (defined at test.py:8)
```
"
24854,"second max_pool probably is missing in ""create_conv_model"" function in tensorflow/examples/speech_commands/models.py","in function `create_conv_model()` which is defined in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py

It seems that the second max pool is missing.
Due to the documentation of the function this should be the layout of the graph:

&nbsp;(fingerprint_input)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
 [Conv2D]<-(weights)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
 [BiasAdd]<-(bias)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Relu]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[MaxPool]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
[Conv2D]<-(weights)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
[BiasAdd]<-(bias)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Relu]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[MaxPool] <================is this missing???
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
[MatMul]<-(weights)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v
[BiasAdd]<-(bias)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v

After the first_dropout, max pooling is correctly performed:
`line 272: max_pool = tf.nn.max_pool(first_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')`

But I could't find any max pooling after the second_dropout."
24850,Too many LSTM implementations (6),"**System information**
- TensorFlow version: v1.8
- Doc Link: [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMCell), [tf.contrib.rnn.LSTMBlockCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockCell), [tf.contrib.rnn.LSTMBlockFusedCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell), [tf.nn.rnn_cell.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell), [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [tf.contrib.cudnn_rnn.CudnnLSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM), [tf.contrib.cudnn_rnn.CudnnLSTMSaveable](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTMSaveable)

Ignoring any fancy versions like Grid LSTM cells and LSTM cells somehow combined with convolutions there are **6!** different LSTM implementations in tensorflow (see documentation links above). Some are better documented than others, some claim to be faster than others (there seems to be the order  tf.contrib.rnn.LSTMCell < tf.contrib.rnn.LSTMBlockCell < tf.contrib.rnn.LSTMBlockFusedCell but then why keep the slower implementations instead of just replacing the slower ones with faster implementations?), all have slightly different APIs.

What I am missing is a guide that tells me:
* What is the fastest implementation on what hardware (I might be willing to deal with poor documentation if the result is at least fast, but having to deal with poor documentation just to find out that the performance is bad sucks).
* What is the stable implementation which I can expect to be maintained for a longer time period.
* What implementations are just included for historic reasons or to maintain backwards compatibility (and therefor should be avoided when starting a new project).
* Maybe there are good reasons (that I am not aware of) to keep multiple implementations in parallel because they all have different tradeoffs. In this case I would like to know more what the differences are between the implementations. This can just be a table with pros and cons.

The state of affairs is probably similar for GRU cells and simple RNN cells. The situation with the LSTM cells is just exemplary ..."
24849,GradientTape return none for variable not being present in expression,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Currently, Gradient tape return none for derivative respect to variables not being present in expression. Semantically, the gradient should return zero for variables. The current behavior make programmer have to checkout whether the output is none and replace it with zero.

**Will this change the current api? How?**
no change to current api

**Who will benefit with this feature?**
people who use gradient tape will get intuitive result. 

**Any Other info.**
"
24847,"Upgrade tensor flow from 1.9 to 1.12, Using tf.keras : ImportError: cannot import name 'Layer'","'from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional'


---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-b2fcd5276851> in <module>()
----> 1 from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional
      2 from tensorflow.keras.models import Model
      3 from tensorflow.keras.optimizers import Adam
      4 
      5 from tensorflow.keras.losses import binary_crossentropy

/usr/local/lib/python3.5/dist-packages/tensorflow/keras/__init__.py in <module>()
     18 from . import estimator
     19 from . import initializers
---> 20 from . import layers
     21 from . import losses
     22 from . import metrics

/usr/local/lib/python3.5/dist-packages/tensorflow/keras/layers/__init__.py in <module>()
      6 from __future__ import print_function
      7 
----> 8 from tensorflow.python.estimator.keras import Layer
      9 from tensorflow.python.keras import Input
     10 from tensorflow.python.keras.applications.densenet import Activation

ImportError: cannot import name 'Layer'"
24846,gpu installation automation,"installation with gpu is very difficult. could you please publish a one-command script that does the whole process? see a list of the steps here (i wrote them):

https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions"
24845,"Different with keras.layers.Dropout and tensorflow.keras.layers.Dropout, rate argument","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.2
- CUDA/cuDNN version: 9.0.176 
- GPU model and memory: NVIDIA 1060


* keras version
```python
from keras import layers
from keras import models

model = models.Sequential()
model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))
model.add(layers.Dropout(rate=1.0))
model.add(layers.Flatten())
model.add(layers.Dense(units=1, activation=None))
```
it's fine, but

* tensorflow version
```python
from tensorflow.keras import layers
from tensorflow.keras import models

model = models.Sequential()
model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))
model.add(layers.Dropout(rate=1.0))
model.add(layers.Flatten())
model.add(layers.Dense(units=1, activation=None))
```
it occurred ValueError like below:
```sh
ValueError: keep_prob must be a scalar tensor or a float in the range (0, 1], got 0
```

When I see the [source code line](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/layers/core.py#L142), why it works like `1 - self.rate`?

Related issue #24526 "
24844,"BUS Error, likely with blas","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v1.12.0-rc2-3-ga6d8ffae09 1.12.0
- Python version:
3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
no Cuda
- GPU model and memory:
not using GPU


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I'm trying Magenta from Google Brain. When I run `python onsets_frames_transcription_transcribe.py --acoustic_run_dir /Users/lorenzori/Downloads/maestro_checkpoint  ~/Downloads/test_audio.wav` the script starts, then it ends with: 
```
/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
2019-01-10 17:49:57.458805: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Restoring parameters from /Users/lorenzori/Downloads/maestro_checkpoint/train/model.ckpt-maestro
INFO:tensorflow:Starting transcription for /Users/lorenzori/Downloads/test_audio.wav...
INFO:tensorflow:Processing file...
INFO:tensorflow:Running inference...
[1]    20612 bus error  python onsets_frames_transcription_transcribe.py --acoustic_run_dir
```

Using lldb I obtain the following info:
```
Process 19280 stopped
* thread #25, stop reason = EXC_BAD_ACCESS (code=2, address=0x700001382000)
    frame #0: 0x0000000110a24805 libopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn + 1541
libopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn:
->  0x110a24805 <+1541>: xchgq  %rdi, -0x40(%rsi)
    0x110a24809 <+1545>: xorl   %edi, %edi
    0x110a2480b <+1547>: xchgq  %rdi, (%rsi)
    0x110a2480e <+1550>: addq   $0x200, %rsi              ; imm = 0x200
Target 0: (python) stopped.
(lldb) bt
* thread #25, stop reason = EXC_BAD_ACCESS (code=2, address=0x700001382000)
  * frame #0: 0x0000000110a24805 libopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn + 1541
    frame #1: 0x00000001108f3e26 libopenblasp-r0.3.0.dev.dylib`cblas_dgemm + 854
    frame #2: 0x0000000105564285 multiarray.cpython-36m-darwin.so`cblas_matrixproduct + 4917
    frame #3: 0x0000000105529d37 multiarray.cpython-36m-darwin.so`PyArray_MatrixProduct2 + 215
    frame #4: 0x000000010552ed1f multiarray.cpython-36m-darwin.so`array_matrixproduct + 191
    frame #5: 0x00000001000d1cbe Python`_PyCFunction_FastCallDict + 463
    frame #6: 0x00000001001362d6 Python`call_function + 489
    frame #7: 0x000000010012f18b Python`_PyEval_EvalFrameDefault + 4811
    frame #8: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719
    frame #9: 0x000000010013713b Python`fast_function + 218
    frame #10: 0x00000001001362ad Python`call_function + 448
    frame #11: 0x000000010012f224 Python`_PyEval_EvalFrameDefault + 4964
    frame #12: 0x00000001001373db Python`_PyFunction_FastCall + 121
    frame #13: 0x00000001001362ad Python`call_function + 448
    frame #14: 0x000000010012f18b Python`_PyEval_EvalFrameDefault + 4811
    frame #15: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719
    frame #16: 0x000000010013730b Python`_PyFunction_FastCallDict + 449
    frame #17: 0x0000000100099f21 Python`_PyObject_FastCallDict + 196
    frame #18: 0x0000000100182073 Python`partial_call + 258
    frame #19: 0x0000000100099da2 Python`PyObject_Call + 101
    frame #20: 0x000000010012f3f4 Python`_PyEval_EvalFrameDefault + 5428
    frame #21: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719
    frame #22: 0x000000010013730b Python`_PyFunction_FastCallDict + 449
    frame #23: 0x0000000100099f21 Python`_PyObject_FastCallDict + 196
    frame #24: 0x000000010009a044 Python`_PyObject_Call_Prepend + 156
    frame #25: 0x0000000100099da2 Python`PyObject_Call + 101
    frame #26: 0x00000001000e4460 Python`slot_tp_call + 50
    frame #27: 0x0000000100099da2 Python`PyObject_Call + 101
    frame #28: 0x00000001212b078e _pywrap_tensorflow_internal.so`tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*) + 974
    frame #29: 0x000000012bd82422 libtensorflow_framework.so`tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 6690
    frame #30: 0x000000012bd895ba libtensorflow_framework.so`std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 58
    frame #31: 0x000000012bdde824 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1876
    frame #32: 0x000000012bdddfd4 libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52
    frame #33: 0x000000012be00070 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96
    frame #34: 0x00007fffb96b893b libsystem_pthread.dylib`_pthread_body + 180
    frame #35: 0x00007fffb96b8887 libsystem_pthread.dylib`_pthread_start + 286
    frame #36: 0x00007fffb96b808d libsystem_pthread.dylib`thread_start + 13
```

**Describe the expected behavior**
The script should run

**Code to reproduce the issue**
python onsets_frames_transcription_transcribe.py --acoustic_run_dir <checkpoint_dir> <wav_file>
"
24843,Add MetaOptimizer option to save the graph after each optimization pass,"Adding an option to MetaOptimizer to dump the graph after each optimization pass will help in development of optimizers and understanding of how the graph is modified before being executed. Currently only final graph is saved if VLOG level is 1. Extending it to the per optimization level is much more useful than saving the final graph, since it is already possible to get final graph through RunMetadata."
24841,Fix support for newer bazel versions - Issue 23673 closed with incorrect solution.,"I believe https://github.com/tensorflow/tensorflow/issues/23673 was closed incorrectly.

The problem is newer versions of bazel are not reading the bazel.rc that provides:
```
tools/bazel.rc:build --define=grpc_no_ares=true
```

The solution can not be to install ares as this option is supposed to disable the need for it.

Please fix the build process to support newer versions of bazel (in my case, using 0.19.0)"
24839,importing tf-nightly-gpu error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Wheel
- TensorFlow version: tf-nightly-gpu 1.13.0.dev20181225
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip on a clean conda env
- CUDA/cuDNN version: 10
- GPU model and memory: rtx 2070 8GB GDDR6



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Created a new conda env, installed python 3.6.8 on it. Pip installed the windows wheel after clearing cache. Went into python in the console, imported tensorflow and got the following error.

**Any other info / logs**
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 18:50:55) [MSC v.1915 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
24838,importing tf-nightly-gpu error,"os: windows 10
Anaconda clean env
pip 10.4
python 3.6.8

I installed the windows python 3.6 wheel https://pypi.org/project/tf-nightly-gpu/1.13.0.dev20181225/#files and installed it in a clean environment. Importing Tensorflow yields me this error:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\envs\tf\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
24837,"[TF Java, Control Flow] Constant nodes created via TF_AddGradients are not in the correct frame","**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
We are using TF Java's `graph.addGradients` which calls upon c_api.h's `TF_AddGradients`. Currently the function works fine when called in the outermost frame (frame '')). 

However, we would like to call TF Java's `graph.addGradients` on nodes that are in the same while loop frame. This gives us the error:
```
[error] Exception in thread ""main"" java.lang.IllegalArgumentException: {{node Gradients/Multiply_1}} has inputs from different frames. The input {{node Gradients/Div_2}} is in frame ''. The input {{node Sub_1}} is in frame 'While'.
```
The issue here is that specifically constant nodes created for the gradient subgraph via `TF_AddGradients`  are not in the correct frame. (The error mentions Multiply and Div nodes because the incorrect frame of constant nodes propagates to those nodes.) Unlike other nodes (Multiply, Sub, Div...), constant nodes do *not* take inputs from other nodes and therefore will always be in the outermost frame (frame '') unless a control input is specified.

**Will this change the current api? How?**
Adding a control edge from a node (let's call this node a pivot) within the while loop frame (frame 'While') to each constant node created by TF_AddGradients to forces the constant node into the while loop frame and our graph will run without error. 

Support for this can be added within the `TF_AddGradients` function.

There is already a loop in the function that does a sanity check on each node. Some simple logic can be added to add a control edge from each newly created constant node to a pivot.

The pivot can be chosen as one of the given inputs to `TF_AddGradients`. This way we ensure that all nodes in the subgraph are in the same frame as the inputs. (Inputs to `TF_AddGradients` must be in the same frame.)

**Who will benefit with this feature?**
Those who want greater usability/flexibility in addGradients will benefit as this will allow addGradients to be called in frames other than the outermost.

**Any Other info.**

Here is a hack-y solution that assumes the pivot is the node created immediately before `TF_AddGradients` is called (rather than using one of the inputs as the pivot).

In `c_api.cc/TF_AddGradients`:
```
...
    // Assumes the pivot is the node created immediately before the gradient nodes.
    // TODO: Use input as pivot.
    Node* pivot = g->graph.FindNodeId(first_new_node_id - 1);

    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Add control edge from each newly created constant node to the pivot to force
      // the nodes to be in the same frame as pivot.
      if (n->IsConstant()) {
        g->graph.AddControlEdge(pivot, n);
      }
...
```
"
24836,Bazel build failed building Android demo,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 2.7.12
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Running the following command:

```
bazel build --cxxopt='--std=c++11' -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures
```

causes the following build errors:
```
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:
bazel-out/android-armeabi-v7a-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
bazel-out/android-armeabi-v7a-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
ERROR: /home/glorenz/.cache/bazel/_bazel_glorenz/701fde1b3ff5c4a449c0c2cab4570a97/external/com_google_absl/absl/base/BUILD.bazel:29:1: C++ compilation of rule '@com_google_absl//absl/base:spinlock_wait' failed (Exit 1): clang failed: error executing command 
  (cd /home/glorenz/.cache/bazel/_bazel_glorenz/701fde1b3ff5c4a449c0c2cab4570a97/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \
    ANDROID_NDK_API_LEVEL=14 \
    ANDROID_NDK_HOME=/home/glorenz/Android/Sdk/ndk-bundle \
    ANDROID_SDK_API_LEVEL=23 \
    ANDROID_SDK_HOME=/home/glorenz/Android/Sdk \
    PATH=/home/glorenz/Workspace/cat-local/cat_test_1/bin:/home/glorenz/bin:/home/glorenz/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/glorenz/.dotnet/tools:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.d '-frandom-seed=bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.o' -iquote external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/bin/external/bazel_tools '--std=c++11' -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/com_google_absl/absl/base/internal/spinlock_wait.cc -o bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/spinlock_wait.o)
In file included from external/com_google_absl/absl/base/internal/spinlock_wait.cc:27:
In file included from external/com_google_absl/absl/base/internal/spinlock_linux.inc:17:
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:28:21: error: field has incomplete type 'struct robust_list'
 struct robust_list __user *next;
                    ^
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:27:8: note: definition of 'robust_list' is not complete until the closing '}'
struct robust_list {
       ^
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:28:27: error: expected ';' at end of declaration list
 struct robust_list __user *next;
                          ^
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/linux/futex.h:37:27: error: expected ';' at end of declaration list
 struct robust_list __user *list_op_pending;
                          ^
3 errors generated.
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 176.016s, Critical Path: 25.68s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 340 processes: 328 local, 12 worker.
FAILED: Build did NOT complete successfully
```
This happens when using the most recent commit (with the hashcode `3c4b35297e4b44baaf4cc7ed5b099c586ffa049b`).
However, the build suceeds if the cloned repository is hard reset to the commit with the following hashcode:
`30ebb9b712b2008525944deb47dee8b653f522a3`.
This also happened with bazel 0.21.0, which is the latest version to date.

**Full Log**
[stack.txt](https://github.com/tensorflow/tensorflow/files/2746756/stack.txt)
"
24835,Windows 10/Anaconda3: Missing File on install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Installed from pip
- TensorFlow version: 2 Preview
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
I created a virtual environment and tried to install the preview that Martin Wicke suggested on twitter. It failed with a missing file.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Commands from Anaconda3 prompt:
conda create -n tf_daily python=3.6
activate tf_daily
pip install tf-nightly-2.0-preview

Error Message:
(tf_daily) C:\Users\nurl_>pip install tf-nightly-2.0-preview
Collecting tf-nightly-2.0-preview
  Using cached https://files.pythonhosted.org/packages/a6/d9/0499db98422207eb3d7643ee3b8152dd503a85dc2f958f77834aa0a3fcde/tf_nightly_2.0_preview-1.13.0.dev20190108-cp36-cp36m-win_amd64.whl
Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\nurl_\\AppData\\Local\\Temp\\pip-install-r8cg1kry\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-1.13.0.dev20190108.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'

"
24834,tf-nightly-2.0-preview failed to install on my Windows 7/64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7/64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24833,Triplet loss semi hard function does not work,The triplet loss semi hard function does not work in the line where have an assertion which checking variable dimension
24832,"Could NOT find CUDA: Found unsuitable version ""9.2""  ON WINDOWS SOURCE BUILD WITH CMAKE","
**System information**
- Windows 10
- TensorFlow installed from source: r1.8
- TensorFlow version: r1.8
- Python version: 3.6.5
- Bazel version (if compiling from source): not use bazel
- GCC/Compiler version (if compiling from source): cmake 3.13.0-rc1
- CUDA/cuDNN version: 9.2/7.1.4
- GPU model and memory: 2G

when i try to build rensorflow on windows,there is the error:

D:\tf3\tensorflow-r1.8\tensorflow\contrib\cmake\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/tf3/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/python36.dll -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2""
-- Building for: Visual Studio 15 2017
-- The C compiler identification is MSVC 19.16.27026.1
-- The CXX compiler identification is MSVC 19.16.27026.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success
CMake Error at C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:137 (message):
  **Could NOT find CUDA: Found unsuitable version ""9.2"", but required is exact
  version ""9.0"" (found C:/Program Files/NVIDIA GPU Computing
  Toolkit/CUDA/v9.2)**
Call Stack (most recent call first):
  C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:376 (_FPHSA_FAILURE_MESSAGE)
  C:/cmake-3.13.0-rc1-win64-x64/share/cmake-3.13/Modules/FindCUDA.cmake:1099 (find_package_handle_standard_args)
  CMakeLists.txt:310 (find_package)


-- Configuring incomplete, errors occurred!
See also ""D:/tf3/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log"".
See also ""D:/tf3/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log"".

**and then, I change the CUDA version to 9.0,like this: (changed virtual path)**

D:\tf3\test\tensorflow-r1.8\tensorflow\contrib\cmake\build>""C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin\amd64\vcvars64.bat""
D:\tf3\test\tensorflow-r1.8\tensorflow\contrib\cmake\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/tf3/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0""
-- Building for: Visual Studio 15 2017
-- The C compiler identification is MSVC 19.16.27026.1
-- The CXX compiler identification is MSVC 19.16.27026.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success
-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0 (found suitable exact version ""9.0"")
CMake Error at CMakeLists.txt:322 (message):
  Selected compiler (or version) is not supported for CUDA


-- Configuring incomplete, errors occurred!
See also ""D:/tf3/test/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log"".
See also ""D:/tf3/test/tensorflow-r1.8/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log"".

so how can i to do it correctly?,please help....
"
24831,Build a Convolutional Neural Network using Estimators,"Hello

I'm learning CNN with mnist from this [Documentation](https://www.tensorflow.org/tutorials/estimators/cnn). I followed the tutorial step by step. After I finished the code and run it, the error comes: 

**ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [100,10].**

Below is the code I think is related code:

`loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)`

The labels are from the mnist dataset:

`train_labels = np.asarray(mnist.train.labels, dtype=np.int32)`

     train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": train_data},
        y=train_labels,
        batch_size=100,
        num_epochs=None,  
        shuffle=True)     
    mnist_classifier.train(
        input_fn=train_input_fn,
        steps=20000,
        hooks=[logging_hook])`




"
24830,TfLiteCameraDemo performance of NNAPI on Honor10 View (Kirin 970),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO.
I used the TF code from the TFLiteCameraDemo example. Only modified the line 47 in interpreter.cc where I set UseNNAPI(true);
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Honor 10 View (with Kirin970 and NPU)
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): tf: 1.5.0
- Python version: 2.7.15rc1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): GCC 7.3.0
- CUDA/cuDNN version: None
- GPU model and memory: None

I compiled the demo app twice using two different bazel commands:

case 1. bazel build -c opt --cxxopt='--std=c++11' \
  //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo

case 2. bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a

**Describe the current behavior**
I was switching in the app between CPU and NNAPI and checked the execution time.
In case 1 the CPU was faster then NNAPI
In case 2 both CPU and NNAPI gave approximately the same execution time. No real difference in was seen. 

**Describe the expected behavior**
Expected behavior is that the NNAPI time should be much shorter than CPU


"
24828,"Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source and Binary (tried both)
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: Cudnn - 7.4 ,  CUDA- 9.0
- GPU model and memory: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225 8GB




**Describe the problem**
I tried installting tensorflow 1.12 using both pip install and building from source.However when I am trying to run faster rcnn model  i get following error message:
Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.

I only get this with tf 1.12 and python 3.6 ,it works fine with python 3.6


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 103, in worker
    initializer(*initargs)
  File ""detection_app.py"", line 67, in worker
    output_q.put(y.get_stats_and_detection(frame))
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 142, in get_stats_and_detection
    boxes, scores, classes, num = self.processFrame(img)
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 76, in processFrame
    feed_dict={self.image_tensor: image_np_expanded})
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

Caused by op 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D', defined at:
  File ""detection_app.py"", line 94, in <module>
    pool = Pool(args.num_workers, worker, (input_q, output_q))
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py"", line 119, in Pool
    context=self.get_context())
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 174, in __init__
    self._repopulate_pool()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 239, in _repopulate_pool
    w.start()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/context.py"", line 277, in _Popen
    return Popen(process_obj)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/popen_fork.py"", line 73, in _launch
    code = process_obj._bootstrap()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/multiprocessing/pool.py"", line 103, in worker
    initializer(*initargs)
  File ""detection_app.py"", line 62, in worker
    y = DetectorAPI()
  File ""/home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py"", line 36, in __init__
    tf.import_graph_def(od_graph_def, name='')
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3440, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3299, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/home/user/anaconda3/envs/tf_faust/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D (defined at /home/user/faster_rcnn_inception_v2_coco_2018_01_28/base_model.py:36)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, FeatureExtractor/MobilenetV1/Conv2d_0/weights/read/_4__cf__7)]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_21/Gather/GatherV2_2/_211}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7500_...GatherV2_2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopPostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Assert/Assert/data_0/_11)]]

"
24827,tensorflow.keras.backend.clip has different semantics than keras.backend.clip.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):

TF built from master.

- Are you willing to contribute it (Yes/No):

No

**Describe the feature and the current behavior/state.**

Currently `tensorflow.keras.backend.clip` takes a tensor and two floating point python values for min and max. But a recent PR in keras-team/keras allowed now to pass three tensors for elementwise clipping. The PR I'm talking about is https://github.com/keras-team/keras/pull/11442 . If tensorflow.keras wants to stay compatible with the keras API, this feature should be implemented in `tensorflow.keras`

**Will this change the current api? How?**

No change to the API.

**Who will benefit with this feature?**

All people currently using keras contrib. I need tensorflow.keras API to be entirely compatible with keras-team/keras to avoid using two codebases to support keras and tf.keras. Some layers use this clipping feature and don't work when I change the imports to tf.keras.

**Any Other info.**

"
24826,tf.estimator rebuild graph at per training step,"hello everyone:
    i'm working on reading source code of tf.estimator, I found a little problem of this implementationï¼Œ

```python
  def _train_model_default(self, input_fn, hooks, saving_listeners):
    """"""Initiate training with `input_fn`, without `DistributionStrategies`.
    Args:
      input_fn: A function that provides input data for training as minibatches.
      hooks: List of `tf.train.SessionRunHook` subclass instances. Used for
        callbacks inside the training loop.
      saving_listeners: list of `tf.train.CheckpointSaverListener` objects. Used
        for callbacks that run immediately before or after checkpoint savings.

    Returns:
      Loss from training
    """"""
    worker_hooks = []
    with ops.Graph().as_default() as g, g.device(self._device_fn):
      random_seed.set_random_seed(self._config.tf_random_seed)
      global_step_tensor = self._create_and_assert_global_step(g)

      # Skip creating a read variable if _create_and_assert_global_step
      # returns None (e.g. tf.contrib.estimator.SavedModelEstimator).
      if global_step_tensor is not None:
        training_util._get_or_create_global_step_read(g)  # pylint: disable=protected-access
      
      features, labels, input_hooks = (
          self._get_features_and_labels_from_input_fn(
              input_fn, model_fn_lib.ModeKeys.TRAIN))
      worker_hooks.extend(input_hooks)
    
      estimator_spec = self._call_model_fn(
          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
      global_step_tensor = training_util.get_global_step(g)
      return self._train_with_estimator_spec(estimator_spec, worker_hooks,
                                             hooks, global_step_tensor,
                                             saving_listeners)
```
 this code is part of Estimator.training, look at this
```python
 estimator_spec = self._call_model_fn(
          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
```
it will be called at every training step, that is a build graph operation, i think is called one time at the begining is better, am I right?"
24825,Unsupported data type in placeholder op:2,"When I tried to convert a .pb to .tflite, an error occurred.
The command line shows below:

 > tflite_convert \
>   --output_file=model_file.tflite \
>   --graph_def_file=model_file.pb \
>   --input_arrays=input1,input2 \
>   --output_arrays=output1,output2 \
>   --input_shapes=512,512,3:2

And the errors :

> RuntimeError: TOCO failed see console for info.
> 2019-01-10 16:03:15.168961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] **Unsupported data type in placeholder op: 2**
> 2019-01-10 16:03:15.168987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] **Unsupported data type in placeholder op: 2**
> 2019-01-10 16:03:15.168995: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
> 2019-01-10 16:03:15.169013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
> 2019-01-10 16:03:15.169019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
> 2019-01-10 16:03:15.169025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
> 2019-01-10 16:03:15.177018: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 740 operators, 1073 arrays (0 quantized)
> 2019-01-10 16:03:15.191193: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 740 operators, 1073 arrays (0 quantized)
> 2019-01-10 16:03:15.205171: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 126 operators, 296 arrays (0 quantized)
> 2019-01-10 16:03:15.207152: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 126 operators, 296 arrays (0 quantized)
> 2019-01-10 16:03:15.208723: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:164] **An array, detect/cast, still does not have a known data type after all graph transformations have run.**
> Aborted (core dumped)
> 
> None

I have no idea about this error, can anyone offer help.
"
24824,ValueError: as_list() is not defined on an unknown TensorShape.,"I want to test some example with my training model ,but it occur error during test ,console tell me the wrong in my code is focus on line 119:

line112: cell_fw = LSTMCell(self.hidden_dim,forget_bias=0.4)
line113: cell_bw = LSTMCell(self.hidden_dim,forget_bias=0.4)
line114: (output_fw_seq, output_bw_seq), _ = 
            tf.nn.bidirectional_dynamic_rnn(
line115:    cell_fw=cell_fw,
line116:    cell_bw=cell_bw,
line117:    inputs=self.word_embeddings,
line118:    sequence_length=self.sequence_lengths,
line119     dtype=tf.float32)
The console error as follows:

 File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 416, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 632, in dynamic_rnn
    dtype=dtype)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py"", line 697, in _dynamic_rnn_loop
    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 812, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape."
24823,tf.contrib.integrate.odeint can only accept time points in increasing order,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

currently, tensorflow support odeint function. but it only supports input time points in ascending order. This restriction make solving back propagation of ODENet(https://arxiv.org/abs/1806.07366) impossible. ODENet requires giving initial condition at latter time point and integrate to get the function value of an earlier time point. besides, odeint function of scipy supports input time points given in any order.

**Will this change the current api? How?**

this change would not change the current api. it just loose the restriction to the input time points.

**Who will benefit with this feature?**

researcher who is doing researches with ODENet will be happy to see it.

**Any Other info.**
"
24822,`pip install tensorflow` still install 1.5.0,"I installed tensorflow via `pip install tensorflow`, but found it was still using a very old version tensorflow, and this caused the `tf.enable_eager_execution()` failed. Can we upgrade the tensorflow to a new version?

```
>>> tf.VERSION
'1.5.0'
>>> tf.enable_eager_execution()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'enable_eager_execution'
```"
24821,tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.,"I am  trying to install  tensorflow after activating conda environment using pip
$ conda activate /home/eartaert/anaconda3/
$ pip install --upgrade /home/eartaert/Downloads/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl

 but the following error is coming 

tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
 

uname -a results are as follows 
Linux ubuntu 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

can anyone help please

"
24819,Invalid argument: Must have updates.shape = indices.shape[:batch_dim] + params_shape[slice_dim:],"I want to test some example with my training model ,but it occur error during test ,console tell me the wrong in my code is focus on line 94:

 line90ï¼švar_output_2 = tf.Variable(0, dtype=tf.float32,name=""a"",trainable=False,validate_shape=False)
 line91: row_vector = tf.gather_nd(word_embeddings, self.word_error_embed, name=""a_word_error_embed"")
 line92: sum_all = tf.reduce_sum(row_vector, 1, name=""a_reduce_sum"")
 line93: var_output_3=tf.assign(var_output_2, word_embeddings, validate_shape=False)
 line94: word_embeddings = tf.scatter_nd_update(var_output_3, self.error_word, sum_all)

The console error as follows:

tensorflow.python.framework.errors_impl.InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + params_shape[slice_dim:], got updates.shape: [7,300], indices.shape: [7,1], params_shape: [7,9,300], slice_dim: 1, and batch_dim: 1
     [[Node: words/ScatterNdUpdate = ScatterNdUpdate[T=DT_FLOAT, Tindices=DT_INT32, _class=[""loc:@words/a""], use_locking=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](words/Assign, _arg_word_error_embed_2_0_4/_57, words/a_reduce_sum)]]"
24818,eigvalsh documentation error,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link:

https://www.tensorflow.org/api_docs/python/tf/linalg/eigvalsh

**Describe the documentation issue**

Note: If your program backpropagates through this function, you should replace it with a call to tf.linalg.eigvalsh (possibly ignoring the second output) to avoid computing the eigen decomposition twice. This is because the eigenvectors are used to compute the gradient w.r.t. the eigenvalues. See _SelfAdjointEigV2Grad in linalg_grad.py.

This warning is quite ambiguous since
1) ling.eigvalsh returns only one output, while linalg.eigh returns two outputs
2) it circularly references itself

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes if it devs agree that there's an issue.
"
24817,TFDBG Segmentation Fault,"TFDBG encounters a seg fault (see output at bottom) when debugging my TF code. Incidentally this code hangs on a GPU, but code and tfdbg both run without incident on CPU only (setting tf.device). As a side note, is tensorflow ever supposed to produce different execution results on CPU and GPU?

- OS Platform and Distribution: AWS Deep Learning Base AMI (Ubuntu) Version 14.0 (ami-012b19f1736b6aae8)
- TensorFlow installed from (source or binary): tensorflow-gpu==1.12.0 installed by pip
- Python version: Python 3.6.7
- CUDA/cuDNN version: 9.0/7.3.1
- GPU model and memory: Tesla V100 with 16130MiB

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem. <== It's not clear why or where the seg fault occurs so I can't generate a minimal test case. I'd be happy to share my full code (a single python file) with you (the TF developers, eg via email) but don't want to post it on github at present.

**Other info / logs**

// Started tfdbg, called run() once to initialize the variables, then invoke_stepper and:

tfdbg> step
  -->(1 / 126)  [  H   ] model/transpose/sub/y
     (2 / 126)  [      ] model/transpose/Range/start
     (3 / 126)  [      ] model/TensorArrayStack/range/start

Continued to model/transpose/sub/y:

Stepper used feeds:
  (No feeds)

Tensor ""model/transpose/sub/y"":

1

tfdbg> s
     (1 / 126)  [  H   ] model/transpose/sub/y
  -->(2 / 126)  [  H   ] model/transpose/Range/start
     (3 / 126)  [      ] model/TensorArrayStack/range/start
     (4 / 126)  [      ] model/TensorArrayStack/range/delta

Continued to model/transpose/Range/start:

Stepper used feeds:
  (No feeds)

Tensor ""model/transpose/Range/start"":

0

tfdbg> s
     (1 / 126)  [  H   ] model/transpose/sub/y
     (2 / 126)  [  H   ] model/transpose/Range/start
  -->(3 / 126)  [  H   ] model/TensorArrayStack/range/start
     (4 / 126)  [      ] model/TensorArrayStack/range/delta
     (5 / 126)  [      ] model/transpose/Range/delta

Continued to model/TensorArrayStack/range/start:

Stepper used feeds:
  (No feeds)

Tensor ""model/TensorArrayStack/range/start"":

0

tfdbg> s
     (2 / 126)  [  H   ] model/transpose/Range/start
     (3 / 126)  [  H   ] model/TensorArrayStack/range/start
  -->(4 / 126)  [  H   ] model/TensorArrayStack/range/delta
     (5 / 126)  [      ] model/transpose/Range/delta
     (6 / 126)  [      ] model/TensorArray/size

Continued to model/TensorArrayStack/range/delta:

Stepper used feeds:
  (No feeds)

Tensor ""model/TensorArrayStack/range/delta"":

1

tfdbg> s
     (3 / 126)  [  H   ] model/TensorArrayStack/range/start
     (4 / 126)  [  H   ] model/TensorArrayStack/range/delta
  -->(5 / 126)  [  H   ] model/transpose/Range/delta
     (6 / 126)  [      ] model/TensorArray/size
     (7 / 126)  [      ] model/TensorArray

Continued to model/transpose/Range/delta:

Stepper used feeds:
  (No feeds)

Tensor ""model/transpose/Range/delta"":

1

tfdbg> s
     (4 / 126)  [  H   ] model/TensorArrayStack/range/delta
     (5 / 126)  [  H   ] model/transpose/Range/delta
  -->(6 / 126)  [  H   ] model/TensorArray/size
     (7 / 126)  [      ] model/TensorArray
     (8 / 126)  [      ] model/value/weights

Continued to model/TensorArray/size:

Stepper used feeds:
  (No feeds)

Tensor ""model/TensorArray/size"":

1

tfdbg> s
Segmentation fault (core dumped)
ubuntu@ip-10-0-0-9:~$
"
24816,"Wrong Error Raised: ""The graph couldn't be sorted in topological order""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip from anaconda
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9.0
- GPU model and memory: GeForce GTX 1080Ti / Tesla K80


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

The tensorflow will raise `The graph couldn't be sorted in topological order` Error when executing the optimizer. While The error doesn't occur on tensorflow 1.10.0. This error is also posed [here](https://stackoverflow.com/questions/54088172/why-tensorflow-throws-the-graph-couldnt-be-sorted-in-topological-order/54120941#54120941) by another user.

**Describe the expected behavior**

The error should not be raised because there is no loop in the computation graph.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Reference: https://stackoverflow.com/questions/54088172/why-tensorflow-throws-the-graph-couldnt-be-sorted-in-topological-order/54120941#54120941
```
import tensorflow as tf
print(tf.__version__)
activation = tf.nn.relu
img_plh = tf.placeholder(tf.float32, [None, 3, 3, 3])
label_plh = tf.placeholder(tf.float32, [None])
layer = img_plh
buffer = []
ks_list = list(range(1, 10, 1)) + list(range(9, 0, -1))
for ks in ks_list:
    buffer.append(tf.layers.conv2d(layer, 9, ks, 1, ""same"", activation=activation))
layer = tf.concat(buffer, 3)
layer = tf.layers.conv2d(layer, 1, 3, 1, ""valid"", activation=activation)
layer = tf.squeeze(layer, [1, 2, 3])
loss_op = tf.reduce_mean(tf.abs(label_plh - layer))
optimizer = tf.train.AdamOptimizer()
train_op = optimizer.minimize(loss_op)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    result = sess.run(train_op, {img_plh: np.zeros([2, 3, 3, 3], np.float32), label_plh: np.zeros([2], np.float32)})
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24815,"If a branch contains summaries, tf.cond doesn't support Operations as branches","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.2)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0
- Python version: 3.6.6 (Anaconda)

**Describe the current behavior**
The code below produces a `TypeError`. The bug seems to rely on a combination of:
1. `tf.cond`
2. a `tf.contrib.summary` in one of the branches 
3. `tf.group`

**Describe the expected behavior**
The code should run successfully.

**Code to reproduce the issue**
```
import tensorflow as tf

writer = tf.contrib.summary.create_file_writer('tb')
with writer.as_default(), tf.contrib.summary.always_record_summaries():
    op = tf.cond(
        tf.random.normal([]) >= 0,
        lambda: tf.group(tf.contrib.summary.scalar('loss', 0.2)),
        tf.no_op)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.contrib.summary.summary_writer_initializer_op())
sess.run(op)
```

**Other info / logs**
Here's the traceback:
```
Traceback (most recent call last):
  File ""repro.py"", line 8, in <module>
    tf.no_op)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2086, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1941, in BuildCondBranch
    original_result)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 381, in map_structure
    structure[0], [func(*x) for x in entries])
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 381, in <listcomp>
    structure[0], [func(*x) for x in entries])
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 81, in identity
    return gen_array_ops.identity(input, name=name)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3454, in identity
    ""Identity"", input=input, name=name)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/dmz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6168, in _operation_conversion_error
    name, as_ref))
TypeError: Can't convert Operation 'cond/group_deps' to Tensor (target dtype=None, name='input', as_ref=False)
```"
24814,TensorFlow binary was not compiled to use : AVX2 AVX512F FMA,"Using the virtualenv install procedure (https://www.tensorflow.org/install/pip)
(venv) user@ubuntu:~$ pip install --upgrade tensorflow

Installing collected packages: tensorflow
Successfully installed tensorflow-1.12.0

```
(venv) user@ubuntu:~$ python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""

2019-01-09 21:39:35.073563: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
tf.Tensor(276.40582, shape=(), dtype=float32)
```

I don't know if this is a warning or that TensorFlow is in fact installed.
Ubuntu 18.04

```
(venv) user@ubuntu:~$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.12.0-0-ga6d8ffae09 1.12.0
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
24811,tensorflow ends with a ValueError exception,"I am experimenting with tensorflow/examples/get_started/regression/linear_regression_categorical.py and according to https://www.tensorflow.org/api_guides/python/regression_examples it ought to be fairly straightforward.
My script (for qsub submission) is as follows:
#PBS -N my_project_tf_l0
cd /home/u22835/tensorflow-master/tensorflow/examples/get_started/regression/
echo start
./linear_regression_categorical.py
echo end

The error file shows the traceback with the culprit function:
ValueError: Tensor(â€œbuffer_size:0â€, shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(â€œMapDataset_1:0â€, shape=(), dtype=variant).

to exclude an issue with tensorflow version, I went ahead and upgraded tensorflow to 1.70 as explained in https://access.colfaxresearch.com/?p=learn, namely:

export CC=/glob/development-tools/versions/gcc-6.4.0/bin/gcc
export LD_LIBRARY_PATH=/glob/development-tools/versions/gcc-6.4.0/lib64/:$LD_LIBRARY_PATH
source /glob/development-tools/parallel-studio/compilers_and_libraries/linux/mpi/bin64/mpivars.sh
pip install â€“user /glob/deep-learning/versions/intel-tensorflow/tensorflow-1.7.0/tensorflow-1.7.0-cp36-cp36m-linux_x86_64.whl

as a result:
python -c â€˜import tensorflow as tf; print(tf.__version__)â€™
â€¦
ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8â€² not found (required by /home/u22835/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

Failed to load the native TensorFlow runtime.

I assumed the exception reported in the previous post might be related to the tensorflow version (I saw a tutorial hinting in that direction). The upgrade completed but now I have a broken environment, and so assuming what I am doing here is nonsense, how do I revert back to the previous tensorflow version?

If indeed the error reported before is related to an (older) version, which version is then required?"
24809,regarding tensor flow detect code ,"hello guys 
how can we get TFdetect code only and 
how is it installing 4 apps  at a time 

thank in advance for your generous help"
24808,"InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]          [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]","Wenn  I ran following the bashscript, I got the following the errors. I want to train my dataset using the transferlearning of Inception-v3.
How do I go about correcting this? Thank you very much.

Bashcript:
python3 train_image_classifier.py \
 --train_dir=satellite/train_dir \
 --dataset_name=satellite \
 --dataset_split_name=train \
 --dataset_dir=satellite/data \
 --model_name=inception_v3 \
 --checkpoint_path=satellite/pretrained/inception_v3.ckpt \
 --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
 --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
 --max_number_of_steps=100000 \
 --batch_size =32 \
--learning_rate=0.001 \
--learning_rate_decay_type=fixed \
--save_summaries_secs=2 \
-log_every_n_steps=10 \
--optimizer=rmsprop \
--weight_decay=0.00004

Errors:
Caused by op 'save/Assign_31', defined at:
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 583, in main
    init_fn=_get_init_fn(),
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 377, in _get_init_fn
    ignore_missing_vars=FLAGS.ignore_missing_vars)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 695, in assign_from_checkpoint_fn
    write_version=saver_pb2.SaverDef.V1)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 422, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 113, in restore
    self.op.get_shape().is_fully_defined())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\state_ops.py"", line 219, in assign
    validate_shape=validate_shape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_state_ops.py"", line 63, in assign
    use_locking=use_locking, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 589, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 748, in train
    master, start_standard_services=False, config=session_config) as sess:
  File ""C:\ProgramData\Anaconda3\lib\contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1005, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 833, in stop
    ignore_live_threads=ignore_live_threads)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 994, in managed_session
    start_standard_services=start_standard_services)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\supervisor.py"", line 731, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\session_manager.py"", line 289, in prepare_session
    init_fn(sess)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 697, in callback
    saver.restore(session, model_path)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

Caused by op 'save/Assign_31', defined at:
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 593, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 583, in main
    init_fn=_get_init_fn(),
  File ""D:\python\Py Code\Laufzeit\train_image_classifier.py"", line 377, in _get_init_fn
    ignore_missing_vars=FLAGS.ignore_missing_vars)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\variables.py"", line 695, in assign_from_checkpoint_fn
    write_version=saver_pb2.SaverDef.V1)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1284, in __init__
    self.build()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 781, in _build_internal
    restore_sequentially, reshape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 422, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 113, in restore
    self.op.get_shape().is_fully_defined())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\state_ops.py"", line 219, in assign
    validate_shape=validate_shape)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_state_ops.py"", line 63, in assign
    use_locking=use_locking, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1,1,2048,2] rhs shape= [1,1,2048,1001]
         [[Node: save/Assign_31 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Logits/Conv2d_1c_1x1/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Logits/Conv2d_1c_1x1/weights, save/RestoreV2:31)]]

"
24807,Installation failed of C API on Ubuntu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:/
- TensorFlow installed from (source or binary): Tensorflow C API
- TensorFlow version: Latest
- Python version:
- Installed using virtualenv? pip? conda?:No
- Bazel version (if compiling from source):/
- GCC/Compiler version (if compiling from source): Update
- CUDA/cuDNN version: NaN
- GPU model and memory: NaN



**THE GUIDE IS NOT COMPLETE / THAT IS NOT WORKING*

**TF_VERSION ISN'T RECOGNIZE AS A FUNCTION**


**LOVE**
"
24806,TENSORFLOW- GPU BUILD ERROR ERROR: C:/tensorflow/tensorflow/tensorflow/python/BUILD:5651:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Exit 2): bash.exe failed: error executing command,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.17.2
- GCC/Compiler version (if compiling from source): vs2015, i have no idea
- CUDA/cuDNN version: 10, 7.4.2
- GPU model and memory: 6gb nvidia 1060 with max q, 32gb ram



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Follow tutorial on https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows error at step 12

```
python configure.py
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/C:/Users/Telo%20Anderson/_bazel_Telo%20Anderson/install/3a7287a15200b68eb8146fc168635be1/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.17.2 installed.
Please specify the location of python. [Default is D:\Anaconda3\envs\opencv-test\python.exe]:


Found possible Python library paths:
  D:\Anaconda3\envs\opencv-test\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Anaconda3\envs\opencv-test\lib\site-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: Y
Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.4.2


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y
Eigen strong inline overridden.

```
```
bazel --batch build --config=opt --config=cuda --config=mkl --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
```
bazel --batch build --config=opt --config=cuda --config=mkl --verbose_failures //tensorflow/tools/pip_package:build_pip_package
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/C:/Users/Telo%20Anderson/_bazel_Telo%20Anderson/install/3a7287a15200b68eb8146fc168635be1/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at D:\Visual Studio 14.0\VC\
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at D:\Visual Studio 14.0\VC\
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (235 packages loaded)
WARNING: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/external/grpc/bazel/grpc_build_system.bzl:172:12
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (311 packages loaded)
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (311 packages loaded).
INFO: Found 1 target...
[3 / 20] [-----] Expanding template tensorflow/create_tensorflow.python_api_1.temp ... (2 actions, 0 running)
[3 / 21] [-----] Expanding template tensorflow/create_tensorflow.python_api_1.temp ... (2 actions, 0 running)
[388 / 1,995] Compiling external/flatbuffers/src/idl_gen_lobster.cpp; 4s local ... (8 actions running)
[396 / 1,995] Compiling external/flatbuffers/src/idl_gen_lua.cpp; 5s local ... (8 actions running)
[402 / 1,995] Compiling external/flatbuffers/src/idl_parser.cpp; 10s local ... (8 actions running)
[411 / 1,995] Compiling external/flatbuffers/src/idl_gen_js.cpp; 9s local ... (8 actions running)
INFO: From Compiling tensorflow/contrib/lite/profiling/time.cc:
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(35): warning C4820: '_timespec64': '4' bytes padding added after data member '_timespec64::tv_nsec'
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(42): warning C4820: 'timespec': '4' bytes padding added after data member 'timespec::tv_nsec'
tensorflow/contrib/lite/profiling/time.cc(32): warning C4365: 'return': conversion from '__int64' to 'uint64_t', signed/unsigned mismatch
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(278): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(283): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(288): warning C4514: 'fpclassify': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(293): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(298): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(303): warning C4514: 'signbit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(308): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(313): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(318): warning C4514: '_fpcomp': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(794): warning C4514: '_chgsignl': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(801): warning C4514: '_copysignl': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\math.h(855): warning C4514: '_hypotl': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(17): warning C4514: 'abs': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(22): warning C4514: 'pow': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(30): warning C4514: 'abs': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(35): warning C4514: 'acos': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(40): warning C4514: 'acosh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(45): warning C4514: 'asin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(50): warning C4514: 'asinh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(55): warning C4514: 'atan': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(60): warning C4514: 'atanh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(65): warning C4514: 'atan2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(70): warning C4514: 'cbrt': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(75): warning C4514: 'ceil': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(80): warning C4514: 'copysign': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(86): warning C4514: 'cos': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(91): warning C4514: 'cosh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(96): warning C4514: 'erf': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(101): warning C4514: 'erfc': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(106): warning C4514: 'exp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(111): warning C4514: 'exp2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(116): warning C4514: 'expm1': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(121): warning C4514: 'fabs': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(126): warning C4514: 'fdim': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(131): warning C4514: 'floor': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(136): warning C4514: 'fma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(142): warning C4514: 'fmax': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(147): warning C4514: 'fmin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(152): warning C4514: 'fmod': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(157): warning C4514: 'frexp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(162): warning C4514: 'hypot': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(167): warning C4514: 'ilogb': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(172): warning C4514: 'ldexp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(177): warning C4514: 'lgamma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(182): warning C4514: 'llrint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(187): warning C4514: 'llround': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(192): warning C4514: 'log': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(197): warning C4514: 'log10': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(202): warning C4514: 'log1p': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(207): warning C4514: 'log2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(212): warning C4514: 'logb': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(217): warning C4514: 'lrint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(222): warning C4514: 'lround': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(227): warning C4514: 'modf': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(232): warning C4514: 'nearbyint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(237): warning C4514: 'nextafter': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(242): warning C4514: 'nexttoward': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(248): warning C4514: 'pow': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(254): warning C4514: 'pow': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(262): warning C4514: 'remainder': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(267): warning C4514: 'remquo': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(273): warning C4514: 'rint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(278): warning C4514: 'round': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(283): warning C4514: 'scalbln': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(288): warning C4514: 'scalbn': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(293): warning C4514: 'sin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(298): warning C4514: 'sinh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(303): warning C4514: 'sqrt': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(308): warning C4514: 'tan': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(313): warning C4514: 'tanh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(318): warning C4514: 'tgamma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(323): warning C4514: 'trunc': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(328): warning C4514: 'abs': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(333): warning C4514: 'acos': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(338): warning C4514: 'acosh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(343): warning C4514: 'asin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(348): warning C4514: 'asinh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(353): warning C4514: 'atan': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(358): warning C4514: 'atanh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(363): warning C4514: 'atan2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(369): warning C4514: 'cbrt': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(374): warning C4514: 'ceil': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(379): warning C4514: 'copysign': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(385): warning C4514: 'cos': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(390): warning C4514: 'cosh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(395): warning C4514: 'erf': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(400): warning C4514: 'erfc': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(405): warning C4514: 'exp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(410): warning C4514: 'exp2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(415): warning C4514: 'expm1': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(420): warning C4514: 'fabs': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(425): warning C4514: 'fdim': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(431): warning C4514: 'floor': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(436): warning C4514: 'fma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(442): warning C4514: 'fmax': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(448): warning C4514: 'fmin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(454): warning C4514: 'fmod': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(460): warning C4514: 'frexp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(466): warning C4514: 'hypot': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(472): warning C4514: 'ilogb': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(477): warning C4514: 'ldexp': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(483): warning C4514: 'lgamma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(488): warning C4514: 'llrint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(493): warning C4514: 'llround': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(498): warning C4514: 'log': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(503): warning C4514: 'log10': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(508): warning C4514: 'log1p': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(513): warning C4514: 'log2': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(518): warning C4514: 'logb': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(523): warning C4514: 'lrint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(528): warning C4514: 'lround': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(533): warning C4514: 'modf': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(539): warning C4514: 'nearbyint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(544): warning C4514: 'nextafter': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(550): warning C4514: 'nexttoward': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(556): warning C4514: 'pow': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(562): warning C4514: 'pow': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(571): warning C4514: 'remainder': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(577): warning C4514: 'remquo': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(583): warning C4514: 'rint': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(588): warning C4514: 'round': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(593): warning C4514: 'scalbln': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(599): warning C4514: 'scalbn': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(605): warning C4514: 'sin': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(610): warning C4514: 'sinh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(615): warning C4514: 'sqrt': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(620): warning C4514: 'tan': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(625): warning C4514: 'tanh': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(630): warning C4514: 'tgamma': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\cmath(635): warning C4514: 'trunc': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdlib.h(354): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdlib.h(359): warning C4514: 'abs': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdlib.h(364): warning C4514: 'div': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdlib.h(369): warning C4514: 'div': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_memcpy_s.h(64): warning C4514: 'memmove_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(100): warning C4514: '_vcwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(127): warning C4514: '_vcwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(154): warning C4514: '_vcwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(167): warning C4514: '_cwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(185): warning C4514: '_cwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(202): warning C4514: '_cwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(220): warning C4514: '_cwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(237): warning C4514: '_cwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(255): warning C4514: '_cwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(303): warning C4514: '_vcwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(335): warning C4514: '_vcwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(348): warning C4514: '_cwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(371): warning C4514: '_cwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(393): warning C4514: '_cwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wconio.h(411): warning C4514: '_cwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wio.h(227): warning C4514: '_wopen': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wio.h(240): warning C4514: '_wsopen': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(309): warning C4514: 'vfwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(340): warning C4514: 'vfwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(371): warning C4514: '_vfwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(385): warning C4514: '_vwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(399): warning C4514: 'vwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(412): warning C4514: '_vwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(428): warning C4514: 'vwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(443): warning C4514: '_vwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(457): warning C4514: '_vwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(470): warning C4514: '_fwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(489): warning C4514: 'fwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(507): warning C4514: '_fwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(528): warning C4514: 'fwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(548): warning C4514: '_fwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(567): warning C4514: '_fwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(585): warning C4514: '_wprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(603): warning C4514: 'wprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(620): warning C4514: '_wprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(640): warning C4514: 'wprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(659): warning C4514: '_wprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(677): warning C4514: '_wprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(726): warning C4514: 'vfwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(759): warning C4514: 'vfwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(774): warning C4514: '_vwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(788): warning C4514: 'vwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(801): warning C4514: '_vwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(817): warning C4514: 'vwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(832): warning C4514: '_fwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(851): warning C4514: 'fwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(869): warning C4514: '_fwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(890): warning C4514: 'fwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(910): warning C4514: '_wscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(928): warning C4514: 'wscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(945): warning C4514: '_wscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(965): warning C4514: 'wscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1092): warning C4514: '_vsnwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1120): warning C4514: '_vsnwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1169): warning C4514: '_vswprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1221): warning C4514: '_vswprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1274): warning C4514: 'vswprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1321): warning C4514: '_vswprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1356): warning C4514: '_vscwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1389): warning C4514: '_vscwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1403): warning C4514: '__swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1423): warning C4514: '_swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1444): warning C4514: '_swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1463): warning C4514: 'swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1506): warning C4514: '_swprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1528): warning C4514: 'swprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1557): warning C4514: '_swprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1578): warning C4514: '_swprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1598): warning C4514: '_swprintf_c_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1619): warning C4514: '_swprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1639): warning C4514: '_snwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1665): warning C4514: '_snwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1690): warning C4514: '_snwprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1712): warning C4514: '_snwprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1740): warning C4514: '_scwprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1759): warning C4514: '_scwprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1777): warning C4514: '_scwprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1796): warning C4514: '_scwprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1820): warning C4514: 'swprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1835): warning C4514: 'vswprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1846): warning C4514: '_swprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1861): warning C4514: '_vswprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1919): warning C4514: 'vswscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(1954): warning C4514: 'vswscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2017): warning C4514: '_swscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2037): warning C4514: 'swscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2056): warning C4514: '_swscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2078): warning C4514: 'swscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2099): warning C4514: '_snwscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2125): warning C4514: '_snwscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2150): warning C4514: '_snwscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstdio.h(2171): warning C4514: '_snwscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(159): warning C4514: 'wcsnlen_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(240): warning C4514: '_wcstok': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(254): warning C4514: 'wcstok': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(532): warning C4514: 'wcschr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(538): warning C4514: 'wcspbrk': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(544): warning C4514: 'wcsrchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wstring.h(551): warning C4514: 'wcsstr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wtime.h(185): warning C4514: '_wctime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_wtime.h(192): warning C4514: '_wctime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\sys/stat.h(236): warning C4514: 'fstat': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\sys/stat.h(241): warning C4514: 'stat': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\wchar.h(181): warning C4514: 'fwide': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\wchar.h(190): warning C4514: 'mbsinit': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\wchar.h(269): warning C4514: 'wmemchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(644): warning C4514: 'vfprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(675): warning C4514: 'vfprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(706): warning C4514: '_vfprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(720): warning C4514: '_vprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(734): warning C4514: 'vprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(747): warning C4514: '_vprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(763): warning C4514: 'vprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(778): warning C4514: '_vprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(792): warning C4514: '_vprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(805): warning C4514: '_fprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(824): warning C4514: 'fprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(848): warning C4514: '_fprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(869): warning C4514: 'fprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(889): warning C4514: '_fprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(908): warning C4514: '_fprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(926): warning C4514: '_printf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(944): warning C4514: 'printf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(961): warning C4514: '_printf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(981): warning C4514: 'printf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1000): warning C4514: '_printf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1018): warning C4514: '_printf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1066): warning C4514: 'vfscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1100): warning C4514: 'vfscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1116): warning C4514: '_vscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1130): warning C4514: 'vscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1143): warning C4514: '_vscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1159): warning C4514: 'vscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1174): warning C4514: '_fscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1193): warning C4514: 'fscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1211): warning C4514: '_fscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1232): warning C4514: 'fscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1252): warning C4514: '_scanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1270): warning C4514: 'scanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1287): warning C4514: '_scanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1307): warning C4514: 'scanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1465): warning C4514: 'vsprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1506): warning C4514: 'vsprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1553): warning C4514: '_vsprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1591): warning C4514: '_vsnprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1619): warning C4514: 'vsnprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1664): warning C4514: '_vscprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1729): warning C4514: '_vsnprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1744): warning C4514: '_sprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1769): warning C4514: 'sprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1803): warning C4514: '_sprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1826): warning C4514: 'sprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1854): warning C4514: '_sprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1875): warning C4514: '_sprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1895): warning C4514: '_snprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1932): warning C4514: 'snprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1952): warning C4514: '_snprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(1982): warning C4514: '_snprintf_c_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2003): warning C4514: '_snprintf_c': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2023): warning C4514: '_snprintf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2045): warning C4514: '_snprintf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2073): warning C4514: '_scprintf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2091): warning C4514: '_scprintf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2108): warning C4514: '_scprintf_p_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2126): warning C4514: '_scprintf_p': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2174): warning C4514: 'vsscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2235): warning C4514: '_sscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2254): warning C4514: 'sscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2272): warning C4514: '_sscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2293): warning C4514: 'sscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2321): warning C4514: '_snscanf_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2345): warning C4514: '_snscanf': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2369): warning C4514: '_snscanf_s_l': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\stdio.h(2393): warning C4514: '_snscanf_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\corecrt_memory.h(97): warning C4514: 'memchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\string.h(373): warning C4514: 'strnlen_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\string.h(517): warning C4514: 'strchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\string.h(523): warning C4514: 'strpbrk': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\string.h(529): warning C4514: 'strrchr': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\string.h(535): warning C4514: 'strstr': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(86): warning C4514: 'operator new': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(92): warning C4514: 'operator delete': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(101): warning C4514: 'operator new[]': unreferenced inline function has been removed
D:\Visual Studio 14.0\VC\INCLUDE\vcruntime_new.h(107): warning C4514: 'operator delete[]': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(475): warning C4514: 'ctime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(486): warning C4514: 'difftime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(495): warning C4514: 'gmtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(505): warning C4514: 'localtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(516): warning C4514: '_mkgmtime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(524): warning C4514: 'mktime': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(531): warning C4514: 'time': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(539): warning C4514: 'timespec_get': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(549): warning C4514: 'ctime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(559): warning C4514: 'gmtime_s': unreferenced inline function has been removed
C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt\time.h(568): warning C4514: 'localtime_s': unreferenced inline function has been removed
[698 / 3,331] Compiling external/flatbuffers/src/idl_gen_general.cpp; 9s local ... (8 actions running)
[767 / 3,523] Compiling external/grpc/src/core/lib/gpr/log_windows.cc; 1s local ... (6 actions running)
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Executing genrule //tensorflow/core:version_info_gen:
fatal: Invalid path '/c/users/telo anderson/_bazel_telo anderson/wvk7snnt/execroot/org_tensorflow/C:': No such file or directory
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_windows.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
[1,369 / 5,608] Executing genrule @local_config_cuda//cuda:cuda-include; 8s local ... (8 actions running)
[1,453 / 5,641] Executing genrule @local_config_cuda//cuda:cuda-include; 20s local ... (8 actions running)
INFO: From Compiling external/com_google_absl/absl/strings/internal/utf8.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/ostringstream.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
[1,653 / 6,004] Executing genrule @local_config_cuda//cuda:cuda-include; 34s local ... (8 actions running)
INFO: From Compiling tensorflow/contrib/lite/kernels/internal/tensor_utils.cc:
cl : Command line warning D9002 : ignoring unknown option '-O3'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_posix.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/com_google_absl/absl/numeric/int128.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Linking external/protobuf_archive/python/google/protobuf/internal/_api_implementation.so:
   Creating library bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/lib_api_implementation.so.ifso and object bazel-out/x64_windows-opt/bin/external/protobuf_archive/python/google/protobuf/internal/lib_api_implementation.so.exp
ERROR: C:/tensorflow/tensorflow/tensorflow/python/BUILD:5651:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Exit 2): bash.exe failed: error executing command
  cd C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\msys64\usr\local\bin;C:\msys64\usr\bin;C:\msys64\usr\bin;C:\msys64\opt\bin;D:\Anaconda3\envs\opencv-test;D:\Anaconda3\envs\opencv-test\Library\mingw-w64\bin;D:\Anaconda3\envs\opencv-test\Library\usr\bin;D:\Anaconda3\envs\opencv-test\Library\bin;D:\Anaconda3\envs\opencv-test\Scripts;D:\Anaconda3\envs\opencv-test\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;D:\Anaconda3;D:\Anaconda3\Library\mingw-w64\bin;D:\Anaconda3\Library\bin;D:\Anaconda3\Scripts;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\WINDOWS\System32\OpenSSH;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;D:\CMake\bin;D:\nodejs;C:\Users\Telo Anderson\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm;C:\Program Files (x86)\Windows Kits\8.1\Windows Performance Toolkit;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\bazel;C:\msys64\usr\bin;D:\Git\cmd;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;C:\Users\Telo Anderson\AppData\Local\Microsoft\WindowsApps;C:\Users\Telo Anderson\AppData\Local\Programs\Microsoft VS Code\bin;D:\IntelliJ IDEA 2018.3\bin;D:\PyCharm 2018.3\bin;C:\Users\Telo Anderson\AppData\Roaming\npm;D:\Anaconda3\lib\site-packages\pywin32_system32;C:\msys64\usr\bin\site_perl;C:\msys64\usr\bin\vendor_perl;C:\msys64\usr\bin\core_perl
    SET PYTHON_BIN_PATH=D:/Anaconda3/envs/opencv-test/python.exe
    SET PYTHON_LIB_PATH=D:/Anaconda3/envs/opencv-test/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; PYTHONHASHSEED=0 bazel-out/x64_windows-opt/bin/external/cython/cython_binary.exe --cplus tensorflow/python/framework/fast_tensor_util.pyx --output-file bazel-out/x64_windows-opt/genfiles/tensorflow/python/framework/fast_tensor_util.cpp
D:/Anaconda3/envs/opencv-test/python.exe: can't open file 'C:\users\telo': [Errno 2] No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 144.965s, Critical Path: 41.46s
INFO: 343 processes: 343 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

```

Please fix asap, extra info will be provided on request"
24804,Regarding merging tensorflow and Arcore in android app,"hello guys i am tryng to merge tensorflow and arcore in android app.i am facing this error ->
Error: Program type already present: android.support.design.widget.CoordinatorLayout$Behavior

and I think that we are merging two diffrent technology so there will be some gradle dependencies  are missing anyone of you has tried doing this can u suggest how can I fix

thanks in advance "
24803,TfLiteCameraDemo on android 8.0 and 9.0 ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:centos 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Pixel 2
- **TensorFlow installed from (source or binary)**:source 
- **TensorFlow version (use command below)**:1.9
- **Python version**:python3.6
- **Bazel version (if compiling from source)**: 0.180
- **GCC/Compiler version (if compiling from source)**:7.3
- **CUDA/cuDNN version**: 9.0/7.13
- **GPU model and memory**:Tesla P4 8G

### Describe the problem
I try to run TfLiteCameraDemo project on Google Pixel 2  Android  8.1 and  9.0
when  I set  NNAPI  True,  Android 8.1 display bad performance ,because cpu fallback.
but  Android 9.0 performance is the same  as  NNAPI False. why?
Is there any method to get cpu fallback message ?"
24800,Tensor conversion requested dtype float32_ref for Tensor with dtype float32?,"   word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,ids=self.word_ids)
  word_embeddings_modify = tf.scatter_nd_update(word_embeddings, self.error_word, sum_all)
  

error:
  Tensor conversion requested dtype float32_ref for Tensor with dtype float32

from the error it seems word_embeddings in function scatter_nd_update dtype is tf.float_32,but scatter_nd_update should accepted word_embeddings dtype  tf.float_32_ref ,how can i change word_embeddings's dtype from tf.float_32 to tf.float_32_ref before use tf.scatter_nd_update
   
"
24795,Recurrent batch normalization : While_loop and control_depenecies bug,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.8.0
- Python version: 2.7
- Bazel version (if compiling from source): compiled from source 
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CUDA/cuDNN version: 9.1
- GPU model and memory:  Tesla K80 


**Describe the current behavior**
Adding batch normalization to a bidirectional RNN prevent me from updating the moving_mean and moving_variances during training and inference correctly.
The problem is that both the ```tf.control_dependencies``` and the ``while_loop```  of the dynamic_rnn are executed at the same time. The gradient will find nodes that have input from different frames and will throw errors. 

My error message is  : 
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: node train/update (defined at ...)  has inputs from different frames. 
The input node bidirectional_rnn/fw/fw/while/fw/bnlstm_cell/bnlstm_cell/state/batch_normalization/AssignMovingAvg (defined at ... )  is in frame 'train/Listener/features/layer1/BLSTM/bidirectional_rnn/fw/fw/while/while_context'. The input node train/apply_gradients/Assign (defined at ... )  is in frame ''.
```

**Describe the expected behavior**
The expected behaviour is being able to add the control_dependencies when we have the while_loop context. 
Adding clearer documentation about how to handle the moving_mean and moving_variance in case you have a distributed training.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
def _update(self, loss, learning_rate, cluster):
    '''
    create the op to update the model

    args:
        loss: the loss to minimize
        learning_rate: the learning rate
        cluster: the tf cluster

    returns: the update op
    '''

    #create the optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate)

    #create an optimizer that aggregates gradients
    if int(self.conf['numbatches_to_aggregate']) > 0:
        if 'local' in cluster.as_dict():
            num_workers = 1
        else:
            num_workers = len(cluster.as_dict()['worker'])

        optimizer = tf.train.SyncReplicasOptimizer(
            opt=optimizer,
            replicas_to_aggregate=int(
                self.conf['numbatches_to_aggregate']),
            total_num_replicas=num_workers)


    tf.summary.scalar('training_loss', loss,
                      collections=['training_summaries'])

    #get the list of trainable variables
    trainable = tf.trainable_variables()

    #get the list of variables to be removed from the trainable
    #variables
    untrainable = tf.get_collection('untrainable')

    #remove the variables
    trainable = [var for var in trainable
                 if var not in untrainable]

    #compute the gradients
    grads_and_vars = optimizer.compute_gradients(
        loss=loss,
        var_list=trainable)

    with tf.variable_scope('clip'):
        #clip the gradients
        grads_and_vars = [(tf.clip_by_value(grad, -1., 1.), var)
                          for grad, var in grads_and_vars]


    #opperation to apply the gradients
    apply_gradients_op = optimizer.apply_gradients(
        grads_and_vars=grads_and_vars,
        name='apply_gradients')

    #all remaining operations with the UPDATE_OPS GraphKeys
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    print(""update_ops {}"".format(update_ops))
    print(""################"")
    #create an operation to update the gradients, the batch_loss
    #and do all other update ops
    update_op = tf.group(
        *([apply_gradients_op] + update_ops),
        name='update')

    return update_op

```
**Other info / logs**

related issues : 
1) https://github.com/tflearn/tflearn/issues/540#issue-197855026
2) https://github.com/tensorflow/tensorflow/issues/6087#issue-193534788

"
24794,Build from v1.12.0 failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 4.4.131-1.el7.elrepo.x86_64
- TensorFlow installed from (source or binary): source
- TensorFlow version: tag v1.12.0
- Python version: python2.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): gcc 4.9.2


**Describe the problem**
try to build from source enable avx512f while local cpu not support
**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Build command:
`bazel build --config=opt  --copt=-mavx512f --incompatible_package_name_is_a_function=false  //tensorflow:libtensorflow_cc.so`
- Build error:
`ERROR: /home/tensorflow/tensorflow/compiler/xla/client/lib/BUILD:67:1: C++ compilation of rule '//tensorflow/compiler/xla/client/lib:conv_grad_size_util' failed (Exit 1)
In file included from external/eigen_archive/Eigen/Core:403:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/types.h:21,
                 from ./tensorflow/compiler/xla/client/padding.h:24,
                 from ./tensorflow/compiler/xla/client/lib/conv_grad_size_util.h:19,
                 from tensorflow/compiler/xla/client/lib/conv_grad_size_util.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AVX512/PacketMath.h:120:1: error: 'Packet Eigen::internal::pset1(const typename Eigen::internal::unpacket_traits<Packet>::type&) [with Packet = __vector(16) float; typename Eigen::internal::unpacket_traits<Packet>::type = float]' conflicts with a previous declaration
 }
 ^
In file included from external/eigen_archive/Eigen/Core:402:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/types.h:21,
                 from ./tensorflow/compiler/xla/client/padding.h:24,
                 from ./tensorflow/compiler/xla/client/lib/conv_grad_size_util.h:19,
                 from tensorflow/compiler/xla/client/lib/conv_grad_size_util.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AVX/PacketMath.h:120:41: note: previous declaration 'Packet Eigen::internal::pset1(const typename Eigen::internal::unpacket_traits<Packet>::type&) [with Packet = __vector(8) float; typename Eigen::internal::unpacket_traits<Packet>::type = float]'
 template<> EIGEN_STRONG_INLINE Packet8f pset1<Packet8f>(const float&  from) { return _mm256_set1_ps(from); }
                                         ^
In file included from external/eigen_archive/Eigen/Core:403:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/types.h:21,
                 from ./tensorflow/compiler/xla/client/padding.h:24,
                 from ./tensorflow/compiler/xla/client/lib/conv_grad_size_util.h:19,
                 from tensorflow/compiler/xla/client/lib/conv_grad_size_util.cc:16:
external/eigen_archive/Eigen/src/Core/arch/AVX512/PacketMath.h:118:31: note: -fabi-version=6 (or =0) avoids this error with a change in mangling`

- Build command:
`bazel build --config=opt  --copt=-mavx512f --cxxopt=-fabi-version=6 --incompatible_package_name_is_a_function=false  //tensorflow:libtensorflow_cc.so`
- Build error:
`ERROR: /home/tensorflow/tensorflow/core/BUILD:2551:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 1)
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:403:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/tensor_shape.h:21,
                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,
                 from ./tensorflow/core/framework/attr_value_util.h:23,
                 from tensorflow/core/framework/op_def_builder.cc:21:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'Packet Eigen::internal::ploaddup(const typename Eigen::internal::unpacket_traits<Packet>::type*) [with Packet = __vector(16) float; typename Eigen::internal::unpacket_traits<Packet>::type = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:470:77: error: '_mm512_castsi512_ps' was not declared in this scope
   __m512 even_elements = _mm512_castsi512_ps(_mm512_cvtepu32_epi64(low_half));`
"
24792,Import failed while under Windows remote desktop environment,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 Pro 1803 17134.472
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow==1.8
- TensorFlow version: 1.8
- Python version: 3.6.8 x64
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Intel HD Graphics, 8 GB system memory installed



**Describe the problem**

Okay I just did a search about ""is issue: remote desktop"" and no related topic.
I didn't expect a fix at all. The easiest way to workaround is run the code on physical machine instead of microsoft remote desktop. So this is just a post about ""Hey there is a error but it's really not important"".

Another project rely on tensorflow and failed to run during import step.
The error only occur when the code run from mstsc remote desktop.

I'm sorry about that I don't exactly know what is tensorflow or related information, I just a end user but not a developer.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
> venv\Scripts\activate
> python frequency_response.py -H
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\AutoEq-master\venv\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\AutoEq-master\venv\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""frequency_response.py"", line 18, in <module>
    import tensorflow as tf
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\AutoEq-master\venv\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\AutoEq-master\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\AutoEq-master\venv\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
24789,TensorRT's create_inference_graph() produced output with too big size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7
- CUDA/cuDNN version: 9.0
- GPU model and memory: Titan V

**Describe the current behavior**

I have tried to optimize my custom frozen model to run on TensorRT using `create_inference_graph()`, however, the output was larger than the original model (my model is around 200MB, but after converting it's more than 2GB). When I increased `minimum_segment_size` to 30 or 40, the size was samller but still slightly bigger than the original one.

**Describe the expected behavior**
Smaller size of output graph

**Code to reproduce the issue**

```
trt_graph = trt.create_inference_graph(
        input_graph_def=frozen_graph,
        outputs,
        max_batch_size=64,
        max_workspace_size_bytes=1 << 25,
        precision_mode='FP16',
        minimum_segment_size=10
)
```

**Other info / logs**
Because the model was way too big, I couldn't serialize it to .pb file, so that I had this error:
`[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/message_lite.cc:289] Exceeded maximum protobuf size of 2GB: 2756916500`
"
24785,Autonomous name change in tf.layers,"Hi,

I am confused about the reuse and naming of the variables in Tensorflow. It seems that it autonomously adds an ""_n"" to some variable names which destroy the whole weight sharing process in many networks. I have a sample piece of code here to explain what I mean:
```

import tensorflow as tf
input = tf.placeholder(tf.float32, [None, 100], name = 'input')

for t in range(0, 10):
    for i in range(0, 2):
        with tf.variable_scope('InnerLoop_' + str(i) + '/', reuse=tf.AUTO_REUSE): #tried with and without  '/'
            h = tf.layers.dense(input, 100, name='myDense')
            print(h)
```
In fact, I need to define 2 dense layers in the inner loop (the '+str(i)' in the variable_scope name supports taht) and these layers must be shared and have the SAME NAME in all of 10 time steps (outter loop).
The output of the print is:

```
Tensor(""InnerLoop_0/myDense/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_1/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_1/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_2/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_2/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_3/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_3/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_4/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_4/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_5/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_5/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_6/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_6/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_7/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_7/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_8/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_8/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_0/myDense_9/BiasAdd:0"", shape=(?, 100), dtype=float32)
Tensor(""InnerLoop_1/myDense_9/BiasAdd:0"", shape=(?, 100), dtype=float32)
```
As you can see, the dense layer name (myDense) follows by a ""_t"", in which t is the outer loop indicator.
Why does it autonomously add something to the name of a layer? I tried to put the outer loop also in a for loop but it doesn't solve the problem.

However, the documentation states: 

> name: String, the name of the layer.

> reuse: Boolean, whether to reuse the weights of a previous layer by the same name.

There is nothing mentioned in the document that Tensorflow changes the naming, and how is it possible to do weight sharing when it changes the naming and builds actual new weight matrices?
 

I was hoping if you guys can point me to part of the documentation which explains what is happening. Or tell me if there is any logic behind what is happening."
24784,./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (-608567296 vs. 0) Aborted (core dumped)," Hey @ all, 

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3
- TensorFlow version (use command below): tensorflow 1.12.0 | tensorflow-base 1.12.0 | tensorflow-gpu 1.12.0
- Python version: 3.6.6
- cuDNN version: 7.1.2
- CUDA version: 10.0.130
- GPU model and memory: 4 NVIDIA Tesla P100 GPUs

**Describe the current behavior**
Tensorflow code that runs fine with CPUs crashes with GPUs and returns error:
_./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (-608567296 vs. 0)
Aborted (core dumped)_

**Other info / logs**
2019-01-08 23:55:06.274513: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-01-08 23:55:09.733690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:61:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-08 23:55:10.097244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:62:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-08 23:55:10.475129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:89:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-08 23:55:10.874821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8a:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-08 23:55:10.875006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3
2019-01-08 23:55:12.284202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-08 23:55:12.284249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 
2019-01-08 23:55:12.284272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y 
2019-01-08 23:55:12.284278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y 
2019-01-08 23:55:12.284283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y 
2019-01-08 23:55:12.284289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N 
2019-01-08 23:55:12.285567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15127 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2019-01-08 23:55:12.286328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15127 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2019-01-08 23:55:12.286894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15127 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2019-01-08 23:55:12.287333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15127 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
2019-01-08 23:56:07.648451: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (-608567296 vs. 0)
Aborted (core dumped)
stemper@escher-01:~/rough_calibration/code> python3 main_train_single_nn.py 
2019-01-09 00:04:59.095834: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-01-09 00:05:02.355953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:61:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-09 00:05:02.731000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:62:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-09 00:05:03.118000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:89:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-09 00:05:03.516085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0000:8a:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-01-09 00:05:03.516355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3
2019-01-09 00:05:05.109673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-09 00:05:05.109719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 
2019-01-09 00:05:05.109745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y 
2019-01-09 00:05:05.109750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y 
2019-01-09 00:05:05.109758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y 
2019-01-09 00:05:05.109768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N 
2019-01-09 00:05:05.110930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15127 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 6.0)
2019-01-09 00:05:05.111860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15127 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 6.0)
2019-01-09 00:05:05.112798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15127 MB memory) -> physical GPU (device: 2, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)
2019-01-09 00:05:05.113263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15127 MB memory) -> physical GPU (device: 3, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0)
2019-01-09 00:32:29.229486: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (-608567296 vs. 0)
Aborted (core dumped)"
24781,"Failed to build, multiple definition of mkl_dnn","Failed to build, multiple definition of mkl_dnn, what could be the work around? Thanks

Following is the information that might be helpful

Bazel version

-bash-4.2$ bazel --help
INFO: Invocation ID: 46e9d78c-edd1-4c29-ae93-1d51511023b9
                                              [bazel release 0.21.0- (@non-git)]

Following is  .tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""/opt/hadoop/anaconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/opt/hadoop/anaconda3/lib/python3.7/site-packages""
build --python_path=""/opt/hadoop/anaconda3/bin/python""
build --define with_jemalloc=true
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=false
build:s3 --define with_s3_support=false
build:xla --define with_xla_support=false
build:gdr --define with_gdr_support=false
build:verbs --define with_verbs_support=false
build --action_env TF_NEED_OPENCL=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build:opt --cxxopt=-mavx --copt=-mavx --host_cxxopt=-march=native --host_copt=-march=native
build:opt --cxxopt=-mavx2 --copt=-mavx2 --host_cxxopt=-march=native --host_copt=-march=native
build:opt --cxxopt=-mfma --copt=-mfma --host_cxxopt=-march=native --host_copt=-march=native
build:opt --cxxopt=-mfpmath=both --copt=-mfpmath=both --host_cxxopt=-march=native --host_copt=-march=native
build:mkl --define using_mkl=true
build:mkl -c opt
build:mkl --copt=""-DEIGEN_USE_VML""
build:monolithic --define framework_shared_object=false
build --define framework_shared_object=true
build:android --crosstool_top=//external:android/crosstool
build:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
build:android_arm --config=android
build:android_arm --cpu=armeabi-v7a
build:android_arm64 --config=android
build:android_arm64 --cpu=arm64-v8a# Release 1.12.0

Tensorflow source release 
# Release 1.12.0

Following is the bazel commandline run:

bazel build -c opt --jobs 1 --local_resources 2048,0.5,1.0 --verbose_failures --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package


Following is the excerpt of error:

ERROR: /opt/hadoop/tensorflow/tensorflow/python/BUILD:4058:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command
  (cd /opt/hadoop/.cache/bazel/_bazel_hadoop/822cf5f4e40bf2cdc8cd17b156fe2dd7/execroot/org_tensorflow && \
  exec env - \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PATH=/bin:/usr/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/opt/hadoop/anaconda3/bin/python \
    PYTHON_LIB_PATH=/opt/hadoop/anaconda3/lib/python3.7/site-packages \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -shared -o bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib' -Lbazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Lbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib -Wl,--version-script bazel-out/k8-opt/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)
Execution platform: @bazel_tools//platforms:host_platform
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/batch_normalization.pic.o: multiple definition of 'mkldnn_batch_normalization_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/batch_normalization.pic.o: multiple definition of 'mkldnn_batch_normalization_backward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn::impl::conv_desc_init(mkldnn_convolution_desc_t*, mkldnn_prop_kind_t, mkldnn_alg_kind_t, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, int const*, int const*, int const*, int const*, mkldnn_padding_kind_t)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_backward_data_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_backward_data_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_backward_weights_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_backward_weights_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution_relu.pic.o: multiple definition of 'mkldnn_convolution_relu_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution_relu.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_backward_data_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_backward_data_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_backward_weights_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_backward_weights_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_eltwise_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_eltwise_backward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_relu_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_relu_backward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_concat_implementation_list() const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_reorder_implementation_list() const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_sum_implementation_list() const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_implementation_list() const'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_get_count'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn::impl::engine_factories'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_get_kind'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_destroy'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_backward_data_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_backward_weights_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/lrn.pic.o: multiple definition of 'mkldnn_lrn_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/lrn.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/lrn.pic.o: multiple definition of 'mkldnn_lrn_backward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/lrn.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_view_primitive_desc_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_equal'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_get_size'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_get_data_handle'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_set_data_handle'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_concat_primitive_desc_create_v2(mkldnn_primitive_desc**, mkldnn_memory_desc_t const*, int, int, mkldnn_primitive_desc const**, mkldnn_primitive_attr const*)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_concat_primitive_desc_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_sum_primitive_desc_create_v2(mkldnn_primitive_desc**, mkldnn_memory_desc_t const*, int, float const*, mkldnn_primitive_desc const**, mkldnn_primitive_attr const*)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_sum_primitive_desc_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::memory_desc_wrapper(mkldnn::impl::memory_pd_t const*)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::memory_desc_wrapper(mkldnn::impl::memory_pd_t const*)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::compute_blocking(mkldnn_memory_desc_t&)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_status2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_dt2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_rmode2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_fmt2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_prop_kind2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_prim_kind2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_alg_kind2str'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/pooling.pic.o: multiple definition of 'mkldnn_pooling_forward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/pooling.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/pooling.pic.o: multiple definition of 'mkldnn_pooling_backward_desc_init'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/pooling.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_desc_destroy'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_primitive_desc'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_input_at'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_output'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_destroy'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_at'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn::impl::scales_t::set(int, int, float const*)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn::impl::scales_t::scale(float)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops::append_sum(float)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops::append_eltwise(float, mkldnn_alg_kind_t, float, float)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr::set_round_mode(mkldnn_round_mode_t)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr::set_post_ops(mkldnn_post_ops const&)'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_clone'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_destroy'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_int_output_round_mode'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_int_output_round_mode'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_output_scales'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_output_scales'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_post_ops'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_post_ops'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops_create'
/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops_destr"
24779,assert_shapes for validating tensor shapes and their relationships to other tensors ,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I am under the impression that many errors users of TensorFlow (and users of libraries making use of TensorFlow) run into has their source in mistakes in passing it tensors with the correct/expected dimension order (and number of dimensions), especially when many tensors are passed together that have relationships expected of them. Due to broadcasting (something very useful and powerful in itself) or sheer misfortune, these mistakes can easily end up hidden as they may not raise any errors neither at graph construction time nor at graph runtime.

What I suggest is an assert function something like the following (see 'assert_shapes' below): 
```python
def model(x, y, param, other_param):
    assert_shapes({
        (""x"", x): ('N', 'Q'),
        (""y"", y): ('N', 'D'),
        (""param"", param): 'Q',
        (""other param"", other_param): 2,
    })
    ...
```
where x, y, param and other_param are tensors. The value of an entry in the dict above are representing the corresponding tensor shape: symbolically or explicitly. In the example above the first dimension of x and y are to always match in size, the second dimension of x must match the size of param and other_param must always have size two. 

If the caller of the code above writes something like:
```python
m = model(
    x=tf.ones([10, 2]),
    y=tf.ones([1, 10]),
    param=tf.ones([2]),
    other_param=tf.ones([2])
)
```
this can raise an error as: AssertionError: Tensor 'y' dim 0 was of size 1 but was expected to be 10 as declared by 'x' dim 0.

A quick demonstrative implementation with some more examples of this can be found here: https://github.com/bodin-e/tensorcheck

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Users interested in catching mistakes in their (or others) code early and explicitly, where the mistakes have to do with tensor shapes and relationships between tensors' shapes. "
24778,SetDefaultDevice doesn't seem to work; Multiple Tensorflow Session on Separate GPU's Cannot Seem to Speed up Inference,"I am trying to speed up Tensorflow inference by creating multiple Sessions, with each Session loading its own graph on its own GPU. When I ran this same model for a batch of 10 images on a single GPU, it took about 2700 ms. I was hoping I can run 2 batches, one per GPU and process 20 images in the same time frame. Instead, the run time actually took about 5300 ms. So it seems like I was not able to get the speed up I was hoping for.

I am running Tensorflow 1.7 with 2 Quadro GV100's. I did not get any error messages running my code. Below is my code:

	auto options = SessionOptions();
	options.config.mutable_gpu_options()->set_visible_device_list(""0,1"");

	NewSession(options, &m_session[0]);
	NewSession(options, &m_session[1]);
    
	GraphDef graph_def0;
	graph::SetDefaultDevice(""/device:GPU:0"", &graph_def0);
	ReadBinaryProto(Env::Default(), graphPath, &graph_def0);
	m_session[0]->Create(graph_def0);
	
	GraphDef graph_def1;
	graph::SetDefaultDevice(""/device:GPU:1"", &graph_def1);
	ReadBinaryProto(Env::Default(), graphPath, &graph_def1);
	m_session[1]->Create(graph_def1);
	
	//list0 and list1 are list of images, CallSessionRun()'s 2nd arg is index into m_session
	std::future<std::vector<std::vector<tf_detection>>> fut0 = std::async([&]()->std::vector<std::vector<tf_detection>>{
		auto detections = CallSessionRun(list0, 0);
		return detections;
	});

	std::future<std::vector<std::vector<tf_detection>>> fut1 = std::async([&]()->std::vector<std::vector<tf_detection>>{
		auto detections = CallSessionRun(list1, 1);
		return detections;
	});

	auto ans0 = fut0.get();
	auto ans1 = fut1.get();

graph::SetDefaultDevice is supposed to dedicate a GPU for a graph and calling m_session[i]->run() in std::async is supposed to utilize each session concurrently. But it didn't seem to work. Am I missing something?

The fact that the run time stays pretty much the same seems to suggest that graph::SetDefaultDevice() does not work and I am only using 1 GPU instead of 2.

Thank you very much for your help in advance!"
24775,How to freeze low_latency_svdf model?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OSX Mojave 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip install tensorflow (binary)
- TensorFlow version (use command below):
latest
- Python version:
both 2.7 and 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Trained the low_latency_svdf model according to the tutorial (https://www.tensorflow.org/tutorials/sequences/audio_recognition)

python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_svdf \
--how_many_training_steps=100000,35000 \
--learning_rate=0.01,0.005

Then try to freeze the graph after finished training:

python tensorflow/examples/speech_commands/freeze.py \
--model_architecture=low_latency_svdf \
--start_checkpoint=/tmp/speech_commands_train/low_latency_svdf.ckpt-135000 \
--output_file=/tmp/my_frozen_graph.pb

Now the errors are:

""  File ""/Users/michaelhe/ml/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1582, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [256,12] rhs shape= [256,16]""

**Describe the expected behavior**
Should generate pb file

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
24774,"why is tensorflow.map_fn slow, what is wrong with following code?","I am trying to use tensorflow map_fn to do parallel computation. However it seems to me that the performance gain is not significant.

Here are example code running Python 3.6.5, Tensorflow version 1.12.0 on Ubuntu 14.04 LTS, 28 duo cores (Intel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz) = 56 processors

These same codes running on Amazon AWS SagerMaker ml-p3-xlarge even took longer time, 227 seconds.

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-0-ga6d8ffa' 1.12.0


```
import tensorflow as tf
import time
# version 1
tic = time.time()
elems = np.array(range(1,1000000), dtype=np.float64)
output = tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64,  parallel_iterations=56)
sess = tf.Session()

res = sess.run(output)
toc = time.time() - tic
print(""elapsed="", toc)  # 29.47 (seconds)

# version 2
tic = time.time()
elems = np.array(range(1,1000000), dtype=np.float64)
output = tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64,   parallel_iterations=56)
n_cpus=28


with  tf.Session(
config=tf.ConfigProto(log_device_placement=True, 
device_count={ ""CPU"": n_cpus },
inter_op_parallelism_threads=n_cpus,
intra_op_parallelism_threads=1,

)) as sess:
res = sess.run(output)

toc = time.time() - tic
print(""elapsed="", toc)  # 29.26 (seconds)

# version 3
tic = time.time()
elems = np.array(range(1,1000000), dtype=np.float64)
x6 = [ x**6 for x in elems]
toc = time.time() - tic
print(""elapsed time="", toc) # 0.5 seconds
```

What is problem with the above codes? without map_fn, sequential execution version 3 only 0.5 (seconds). 


"
24772,tfdbg gets stuck with nested tf.scan calls,"**tfdbg gets stuck with nested tf.scan calls**

I was running https://github.com/hmishra2250/NTM-One-Shot-TF implementation of MANN on Tensorflow and had some problems using tfdbg with it. The code runs fine without the debugger mode, but gets stuck after the first .run() call when wrapped with the debugger. I've tried greatly reducing parameter and batch sizes to see if memory or performance was the issue, but the problem persisted.

After some tweaking, I believe the source of the debugger not responding is due to nested tf.scan calls (file mann/Model.py, fuction step() calls update_tensor from MANN/utils/tf-utils.py). The reason for that is that the program responded fine when I replaced the inner tf.scan call with a tf.zeros corresponding to the needed output.

The weird thing is that https://github.com/vineetjain96/one-shot-mann implementation, which is based on the code mentioned before, works perfectly on tfdbg. The main difference between the implementations is that the second one only uses simple tf.scan calls, without nesting. I find this behavior undesirable and thus report it in this issue.

- Linux Ubuntu 16.04
- TensorFlow 1.12.0 installed from source
- Python 2.7.12
- Bazel 0.21.0
- GCC 5.4.0
- CUDA 10
- cuDNN 7.4
- GeForce GTX 1080; 8GB RAM GDDR5x; PCI-EXP
- Code to reproduce: wrap https://github.com/hmishra2250/NTM-One-Shot-TF main file (Omniglot.py) with tfdbg and run it.

"
24771,tf.contrib.estimator.early_stopping doesn't work with tf.keras.estimator.model_to_estimator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary/pip
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5

**Describe the current behavior**
tf.keras.estimator.model_to_estimator returns tensorflow.python.estimator.estimator.Estimator class.
tensorflow.contrib.estimator.make_early_stopping_hook checks for tf.estimator.Estimator class.

TypeError: `estimator` must have type `tf.estimator.Estimator`. Got: <class 'tensorflow.python.estimator.estimator.Estimator'>

**Code to reproduce the issue**
model = tf.keras.applications.MobileNetV2()
model.compile(tf.keras.optimizers.Adam(),""categorical_crossentropy"")
estimator = tf.keras.estimator.model_to_estimator(model)
early_stop = tf.contrib.estimator.stop_if_no_decrease_hook(estimator, ""loss"",1) <--crashes
"
24770,TOCO can't convert SpaceToBatchND when current tensor is not a multiply of value,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 1.12

I have this command working:
'''
toco --graph_def_file=/Users/yuval/dev/research-serialization/person_production.pb --output_file=person_production.tflite --input_array=input_1 --output_array=results/truediv  --inference_type=FLOAT --input_shape=**1,512,512,3**
'''
while this doesn't
'''
toco --graph_def_file=/Users/yuval/dev/research-serialization/person_production.pb --output_file=person_production.tflite --input_array=input_1 --output_array=results/truediv  --inference_type=FLOAT --input_shape=**1,800,800,3**
'''
I have some downsampling in my graph, and have dilated convolution with dilation 16 on tesor with sepcial dimensions of size 1/8 of the original. (i.e. 100 in the case that doesn't work) could this be fixed? "
24763,tf.estimator support multi saver and train_op for training multi model at the same time,
24761,TensorFlowLite: failed to load native library: no tensorflowlite_jni in java.library.path,"TensorFlowLite: failed to load native library: no tensorflowlite_jni in java.library.path
"
24760,Bug related to tf.sparse_tensor_dense_matmul(),"**Example**
The following example has some bugs that I can not solve.

```python
import tensorflow as tf
import numpy as np
import scipy.sparse as sp
from scipy.sparse import csc_matrix

def sparse_to_tuple(sparse_mx):
    """"""Convert sparse matrix to tuple representation.""""""
    def to_tuple(mx):
        if not sp.isspmatrix_coo(mx):
            mx = mx.tocoo()
        coords = np.vstack((mx.row, mx.col)).transpose()
        values = mx.data
        shape = mx.shape
        return coords, values, shape

    if isinstance(sparse_mx, list):
        for i in range(len(sparse_mx)):
            sparse_mx[i] = to_tuple(sparse_mx[i])
    else:
        sparse_mx = to_tuple(sparse_mx)

    return sparse_mx

A = csc_matrix(np.diag(np.array([1, 2, 3])))
A = sparse_to_tuple(A)

B = tf.ones([3, 2], tf.float32)
C = tf.sparse_placeholder(tf.float32, shape=(3, 3))
D = tf.sparse_tensor_dense_matmul(C, B)

# if this line, together with the last line, is commented, it can run
H = tf.matmul(tf.sparse_tensor_dense_matmul(C, B), tf.transpose(B, [1, 0]))

with tf.Session() as sess:
    out = sess.run(D, {C: A})
    print('----------------')
    print(out)
    print('----------------')
    print(sess.run(H, {C: A}))
```

**Error**
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-83-02386eb1014c> in <module>()
     32 
     33 with tf.Session() as sess:
---> 34     out = sess.run(D, {C: A})
     35     print('----------------')
     36     print(out)

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     collected into this argument and passed back.
    899 
--> 900     Args:
    901       fetches: A single graph element, a list of graph elements,
    902         or a dictionary whose values are graph elements or lists of graph

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1111                 ' is not compatible with Tensor type ' + str(subfeed_dtype) +
   1112                 '. Try explicitly setting the type of the feed tensor'
-> 1113                 ' to a larger type (e.g. int64).')
   1114 
   1115           is_tensor_handle_feed = isinstance(subfeed_val,

ValueError: Tensor Tensor(""Const_132:0"", shape=(2,), dtype=int64) may not be fed.

**System information**
tesorflow-gpu = 1.9.0
python = Python 3.6.5 :: Anaconda, Inc.
Ubuntu 16.04


"
24759,"tf.function raises OSError ""could not get source code"" when run in a Python shell","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.function` raises an `OSError` exception with the message `""could not get source code""` when I run it in a Python shell. The error does not occur in Jupyter or ipython, and it does not occur when I revert back to TF 1.12.0.

**Describe the expected behavior**
I expect no error.

**Code to reproduce the issue**

```python
Run this in a Python shell:

import tensorflow as tf

@tf.function
def foo():
    return 42

foo()
```

**Other info / logs**

Here is the output of the above commands when I run them in a Python shell:

```pycon
(tf2) $ python
Python 3.6.6 (default, Jun 28 2018, 05:43:53)
[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> @tf.function
... def foo():
...     return 42
...
>>> foo()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 336, in __call__
    self._initialize(args, kwds)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 309, in _initialize
    *args, **kwds))
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1024, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1247, in _maybe_define_function
    arg_names=arg_names),
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 456, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 261, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 449, in wrapper
    ), *args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 293, in converted_call
    experimental_partial_types=partial_types)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 415, in to_graph
    arg_values, arg_types)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 175, in entity_to_graph
    node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 338, in function_to_graph
    node, source = parser.parse_entity(f)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py"", line 34, in parse_entity
    source = tf_inspect.getsource(entity)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py"", line 327, in getsource
    return _inspect.getsource(tf_decorator.unwrap(object)[1])
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py"", line 968, in getsource
    lines, lnum = getsourcelines(object)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py"", line 955, in getsourcelines
    lines, lnum = findsource(object)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py"", line 786, in findsource
    raise OSError('could not get source code')
OSError: could not get source code
```

Below is the output when I run the exact same commands in an ipython shell (within the same virtualenv):

```pycon
(tf2) $ ipython
Python 3.6.6 (default, Jun 28 2018, 05:43:53)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow as tf

In [2]: @tf.function
   ...: def foo():
   ...:     return 42
   ...:

In [3]: foo()
2019-01-08 13:02:33.785802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Out[3]: <tf.Tensor: id=5, shape=(), dtype=int32, numpy=42>
```

The result is also good in Jupyter (within the same virtualenv).

This is the output when I switch to another virtualenv based on TF 1.12.0:

```pycon
(tf1) $ python
Python 3.6.6 (default, Jun 28 2018, 05:43:53)
[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> @tf.contrib.eager.defun
... def foo():
...     return 42
...
>>> foo()
2019-01-08 12:55:53.087099: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<tf.Tensor: id=5, shape=(), dtype=int32, numpy=42>
```"
24756, tf.train.init_from_checkpoint incorrectly raises ValueError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffa 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: CUDA 9.1, cuDNN 7.1.3
- GPU model and memory: 1080Ti (11178MiB)

**Describe the current behavior**
1. Train a model.
2. Add a new variable to the computation graph.
3. Attempt to initialize the updated graph using `tf.train.init_from_checkpoint`.
4. Notice that a `ValueError` is raised instead of `tf.errors.OpError`.

```
  File "".../model.py"", line 53, in warmup
    tf.train.init_from_checkpoint(self._model_dir, {'/': 'model/'})
  File "".../tensorflow/python/training/checkpoint_utils.py"", line 187, in init_from_checkpoint
    _init_from_checkpoint, ckpt_dir_or_file, assignment_map)
  File "".../tensorflow/python/training/distribute.py"", line 1053, in merge_call
    return self._merge_call(merge_fn, *args, **kwargs)
  File "".../tensorflow/python/training/distribute.py"", line 1061, in _merge_call
    return merge_fn(self._distribution_strategy, *args, **kwargs)
  File "".../tensorflow/python/training/checkpoint_utils.py"", line 268, in _init_from_checkpoint
    tensor_name_in_ckpt, ckpt_dir_or_file
ValueError: Tensor params (params in /) is not found in [...] checkpoint
```

**Describe the expected behavior**
Documentation for `tf.train.init_from_checkpoint` states that `tf.errors.OpError` should be raised. It's unclear if the docs are wrong or the implementation is wrong but I think it's the implementation."
24755,how to read tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24754,Support first_n in tf.print,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): no

**Describe the feature and the current behavior/state.**
`tf.Print` is deprecated and `tf.print` doesn't support `first_n`. Would be good to support `first_n` in `tf.print` as well.

**Will this change the current api? How?**
Add a `first_n` parameter to tf.print.

**Who will benefit with this feature?**
Anyone who has used `first_n` in tf.Print.

**Any Other info.**
See Goomics #50."
24753,Why name scope will add the prefix to the keras layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.0
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: 7.4

**Describe the current behavior**
```
import tensorflow as tf


def conv_block(inputs, filters, kernel_size, strides, scope):
    """"""Create a simple Conv --> BN --> ReLU6 block""""""
    with tf.name_scope(scope):
        x = tf.keras.layers.Conv2D(filters, kernel_size, strides)(inputs)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation(tf.nn.relu6)(x)
        return x


if __name__ == '__main__':
    inputs = tf.keras.Input(shape=[224, 224, 3], batch_size=1, name='inputs')
    hidden = conv_block(inputs, 32, 3, 2, scope='block_1')
    outputs = conv_block(hidden, 64, 3, 2, scope='block_2')
    model = tf.keras.Model(inputs, outputs)
    for weight in model.weights:
        print(weight)
```
output:
```
<tf.Variable 'block_1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>
<tf.Variable 'block_1/conv2d/bias:0' shape=(32,) dtype=float32>
<tf.Variable 'block_1/batch_normalization/gamma:0' shape=(32,) dtype=float32>
<tf.Variable 'block_1/batch_normalization/beta:0' shape=(32,) dtype=float32>
<tf.Variable 'block_2/conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>
<tf.Variable 'block_2/conv2d_1/bias:0' shape=(64,) dtype=float32>
<tf.Variable 'block_2/batch_normalization_1/gamma:0' shape=(64,) dtype=float32>
<tf.Variable 'block_2/batch_normalization_1/beta:0' shape=(64,) dtype=float32>
<tf.Variable 'block_1/batch_normalization/moving_mean:0' shape=(32,) dtype=float32>
<tf.Variable 'block_1/batch_normalization/moving_variance:0' shape=(32,) dtype=float32>
<tf.Variable 'block_2/batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32>
<tf.Variable 'block_2/batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32>
```

**Describe the expected behavior**
```
import tensorflow as tf


def conv_block(inputs, filters, kernel_size, strides):
    """"""Create a simple Conv --> BN --> ReLU6 block""""""
    x = tf.keras.layers.Conv2D(filters, kernel_size, strides)(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(tf.nn.relu6)(x)
    return x


if __name__ == '__main__':
    inputs = tf.keras.Input(shape=[224, 224, 3], batch_size=1, name='inputs')
    hidden = conv_block(inputs, 32, 3, 2)
    outputs = conv_block(hidden, 64, 3, 2)
    model = tf.keras.Model(inputs, outputs)
    for weight in model.weights:
        print(weight)
```

output
```
<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>
<tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>
<tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>
<tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>
<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>
<tf.Variable 'conv2d_1/bias:0' shape=(64,) dtype=float32>
<tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32>
<tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32>
<tf.Variable 'batch_normalization/moving_mean:0' shape=(32,) dtype=float32>
<tf.Variable 'batch_normalization/moving_variance:0' shape=(32,) dtype=float32>
<tf.Variable 'batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32>
<tf.Variable 'batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32>
```

indeed, i think only variable scope will add a prefix to the variable name, but not name scope.
"
24752,saved_model_portable_proto target doesn't exist,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: 2.7.12
- Bazel version (if compiling from source):
Build label: 63d51e23a26a25116e659a75113ee654a62213c5
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 3 01:24:53 2019 (1546478693)
Build timestamp: 1546478693
Build timestamp as int: 1546478693

- GCC/Compiler version (if compiling from source): 8.2.0

**Describe the problem**

Dangling reference to `//tensorflow/core:saved_model_portable_proto` within `if_mobile()` guard here:
https://github.com/tensorflow/tensorflow/blob/fbbecc81be3b1eab0c15020ce3b0566e60e25c7f/tensorflow/cc/saved_model/BUILD#L50

Seems similar to:
https://github.com/tensorflow/tensorflow/issues/4312

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Caused failure of unrelated `genquery` rule; even though if_mobile was disabled, it seems to error out on undefined dependencies."
24751,Instantiating Separate Session for Each GPU,"
I'm trying to speed up Tensorflow inference by using multiple GPUs and instantiating a separate Session for each GPU.

I am using Tensorflow 1.7 and 2 Quadro GV100's for this. The GV100's are device 0 and device 1, respectively.

My C++ code looks something like the following:

auto options0 = SessionOptions();
options0.config.mutable_gpu_options()->set_visible_device_list(""0"");
NewSession(options0, &m_session0);

auto options1 = SessionOptions();
options1.config.mutable_gpu_options()->set_visible_device_list(""1"");
NewSession(options1, &m_session1);
However, when I execute this code, I get the following error message:

name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627 pciBusID: 0000:18:00.0 totalMemory: 31.87GiB freeMemory: 31.33GiB 2019-01-07 17:56:09.453901: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0 2019-01-07 17:56:10.196800: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-01-07 17:56:10.202994: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917] 0 2019-01-07 17:56:10.206965: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0: N 2019-01-07 17:56:10.211407: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30421 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:18:00.0, compute capability: 7.0) 17:56:11.099: Choosing GPU: 1 2019-01-07 17:56:11.280313: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties: name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627 pciBusID: 0000:3b:00.0 totalMemory: 31.87GiB freeMemory: 31.33GiB 2019-01-07 17:56:11.291274: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 1 2019-01-07 17:56:12.076807: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-01-07 17:56:12.082561: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917] 1 2019-01-07 17:56:12.088587: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 1: N 2019-01-07 17:56:12.095142: F C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first->second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1

My question is, what is the proper way to assign/dedicate a GPU to each Tensorflow session?

Thank you very much for your help in advance!"
24745,NcclAllReduce Registered Multiple TImes,"This is likely a problem between the chair and keyboard, but bear with me. I try to simply:

`from tensorflow.contrib import nccl`

This causes code to run that registers an operation called NcclAllReduce multiple times because of various imports:


```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/gradescan/.local/lib/python2.7/site-packages/tensorflow/contrib/nccl/__init__.py"", line 30, in <module>
    from tensorflow.contrib.nccl.python.ops.nccl_ops import all_max
  File ""/home/gradescan/.local/lib/python2.7/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py"", line 52, in <module>
    @ops.RegisterGradient('NcclAllReduce')
  File ""/home/gradescan/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2494, in __call__
    _gradient_registry.register(f, self._op_type)
  File ""/home/gradescan/.local/lib/python2.7/site-packages/tensorflow/python/framework/registry.py"", line 61, in register
    (self._name, name, function_name, filename, line_number))
KeyError: ""Registering two gradient with name 'NcclAllReduce'! (Previous registration was in <module> /home/gradescan/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py:72)""
```

Seems like one should be able to register multiple functions with the tensoflow RegsiterGradient to the same operation. Any help pointing me to documentation I can review, or general thoughts on how to resolve this issue would be appreciated."
24744,why is there different latency for post-training quantization and quantization-aware trained models? ,"**System information**
- TensorFlow version: TFlite 
- Doc Link:https://www.tensorflow.org/lite/performance/model_optimization

in the model optimization documentation page:
https://www.tensorflow.org/lite/performance/model_optimization
Table 1 gives a latency of 145ms for post-training quantization and 80.2 for quantization-aware training. 

it is unclear to me why there should be a difference in latency as both methods results in models that use 8-bit operation. The documentation implies that the benefit of using quantization aware training is to increase model accuracy when using int8, but from the table it seems that latency is also improved. why is that?

"
24743,load model's checkpoint,"hi 
how can i restore checkpoints to use this in pyhton?"
24742,Inconsistent behavior of tf.function when using autograph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.function` converts almost identical functions into completely different graphs. It seems like a bug, but perhaps it's just a bit too complicated for me. If it's working as expected, then I think the documentation really needs to be expanded, with detailed examples and clear guidelines.

**Describe the expected behavior**
All the following functions should return almost identical graphs.

**Code to reproduce the issue**

```python
import tensorflow as tf

@tf.function
def foo1(x):
    for i in range(10):
        x = x + 1
    return x

@tf.function
def foo2(x):
    for i in range(tf.constant(10)):
        x = x + tf.constant(1)
    return x

@tf.function
def foo3():
    x = 0
    for i in range(10):
        x = x + 1
    return x

@tf.function
def foo4():
    x = tf.constant(0)
    for i in range(tf.constant(10)):
        x = x + tf.constant(1)
    return x

def _print_sub_ops(op, indent=0):
    """"""Recursively print an op's inputs""""""
    print(""  ""*indent, op.name)
    for ts in op.inputs:
        _print_sub_ops(ts.op, indent + 1)

def print_graph(func, *args):
    print(func.__name__)
    ops = func.get_concrete_function(*args).graph.get_operations()
    _print_sub_ops(ops[-1])   # or just print(ops) if you prefer
    print()

print_graph(foo1, tf.constant(0))
print_graph(foo2, tf.constant(0))
print_graph(foo3)
print_graph(foo4)
```

**Other info / logs**

Below is the output of this program. Notice that:
* foo1 is horrible, autograph did not generate a `tf.while_loop`. Imagine a loop with 10000 iterations, it would just blow up.
* foo2 is pretty good, but it's odd that I have to wrap every integer into a tf.constant.
* foo3 is perfect, it even reduced the whole graph to a single constant, congrats.
* foo4 is virtually identical to foo2, which is pretty good, but why didn't it get the same magic as foo3?

```
foo1
 Identity
   add_9
     add_8
       add_7
         add_6
           add_5
             add_4
               add_3
                 add_2
                   add_1
                     add
                       x
                       add/y
                     add_1/y
                   add_2/y
                 add_3/y
               add_4/y
             add_5/y
           add_6/y
         add_7/y
       add_8/y
     add_9/y

foo2
 Identity
   while/Identity_2
     while
       while/loop_counter
       Const_1
       x
       maximum_iterations
       range
         range/start
         Maximum
           Const
           Maximum/y
         range/delta

foo3
 Identity
   Const

foo4
 Identity
   while/Identity_2
     while
       while/loop_counter
       Const_2
       Const
       maximum_iterations
       range
         range/start
         Maximum
           Const_1
           Maximum/y
         range/delta
```"
24741,"When passing tf.data.Dataset instance to model.fit method which is instantiated by tf.keras.Sequential, tf.keras.Model, subclassing tf.keras.Model, passing metrics argument to 'accuracy' in model.compile method provokes TypeError ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.6


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When passing `tf.data.Dataset` instance to `model.fit` method which is instantiated by `tf.keras.Sequential`, `tf.keras.Model`, subclassing `tf.keras.Model`, passing `metrics` argument to `'accuracy'` in `model.compile` method provokes `TypeError`. But passing `np.array` to `model.fit` method doesn't provoke `TypeError`.

**Code to reproduce the issue**
**`tf.data.Dataset`** provkes `TypeError`. (because I think `tf.keras.metrics.sparse_categorical_accuracy`

```python
import numpy as np
import tensorflow as tf
keras = tf.keras

# tf.data.Dataset instance
tr_data = np.random.random((1000, 32)).astype(np.float32)
tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)
tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))
tr_dataset = tr_dataset.batch(batch_size=32)
tr_dataset = tr_dataset.repeat()

val_data = np.random.random((100, 32)).astype(np.float32)
val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)
val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))
val_dataset = val_dataset.batch(batch_size=100).repeat()

# Training
model = keras.Sequential()
model.add(keras.layers.Dense(units=64, activation='relu'))
model.add(keras.layers.Dense(units=64, activation='relu'))
model.add(keras.layers.Dense(units=10, activation='softmax'))
model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), 
              loss=keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,
          validation_data = val_dataset, validation_steps = 1)
```
output

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    509                 as_ref=input_arg.is_ref,
--> 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1145     if ret is None:
-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1147 

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    982         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--> 983         (dtype.name, t.dtype.name, str(t)))
    984   return t

ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: 'Tensor(""metrics/acc/ArgMax:0"", shape=(?,), dtype=int64)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-3-ef35f2c07d04> in <module>
      9 
     10 model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,
---> 11           validation_data = val_dataset, validation_steps = 1)

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
   1534         steps_name='steps_per_epoch',
   1535         steps=steps_per_epoch,
-> 1536         validation_split=validation_split)
   1537 
   1538     # Prepare validation data.

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)
    990         x, y, sample_weight = next_element
    991     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,
--> 992                                                      class_weight, batch_size)
    993     return x, y, sample_weights
    994 

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_weights(self, x, y, sample_weight, class_weight, batch_size)
   1078                      metrics=self.metrics,
   1079                      loss_weights=self.loss_weights,
-> 1080                      target_tensors=target_tensors)
   1081 
   1082     # In graph mode, if we had just set inputs and targets as symbolic tensors

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)
    472     self._setattr_tracking = False  # pylint: disable=protected-access
    473     try:
--> 474       method(self, *args, **kwargs)
    475     finally:
    476       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    646         targets=self.targets,
    647         skip_target_indices=skip_target_indices,
--> 648         sample_weights=self.sample_weights)
    649 
    650     # Prepare gradient updates and state updates.

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _handle_metrics(self, outputs, skip_target_indices, targets, sample_weights, masks)
    311         metric_results.extend(
    312             self._handle_per_output_metrics(self._per_output_metrics[i], target,
--> 313                                             output, output_mask))
    314         metric_results.extend(
    315             self._handle_per_output_metrics(

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)
    268               metric_fn)
    269           metric_result = weighted_metric_fn(
--> 270               y_true, y_pred, weights=weights, mask=mask)
    271 
    272         if not context.executing_eagerly():

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)
    596     """"""
    597     # score_array has ndim >= 2
--> 598     score_array = fn(y_true, y_pred)
    599     if mask is not None:
    600       mask = math_ops.cast(mask, y_pred.dtype)

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py in sparse_categorical_accuracy(y_true, y_pred)
    660     y_pred = math_ops.cast(y_pred, K.floatx())
    661 
--> 662   return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())
    663 
    664 

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py in equal(x, y, name)
   2732   if _ctx is None or not _ctx._eager_context.is_eager:
   2733     _, _, _op = _op_def_lib._apply_op_helper(
-> 2734         ""Equal"", x=x, y=y, name=name)
   2735     _result = _op.outputs[:]
   2736     _inputs_flat = _op.inputs

/usr/local/var/pyenv/versions/3.6.6/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    544                   ""%s type %s of argument '%s'."" %
    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--> 546                    inferred_from[input_arg.type_attr]))
    547 
    548           types = [values.dtype]

TypeError: Input 'y' of 'Equal' Op has type int64 that does not match type int32 of argument 'x'.
```

**`np.array`** doesn't provoke `TypeError`
```python
import numpy as np
import tensorflow as tf
keras = tf.keras

# np.array 
tr_data = np.random.random((1000, 32)).astype(np.float32)
tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)
# tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))
# tr_dataset = tr_dataset.batch(batch_size=32)
# tr_dataset = tr_dataset.repeat()

val_data = np.random.random((100, 32)).astype(np.float32)
val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)
# val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))
# val_dataset = val_dataset.batch(batch_size=100).repeat()

# Training
model = keras.Sequential()
model.add(keras.layers.Dense(units=64, activation='relu'))
model.add(keras.layers.Dense(units=64, activation='relu'))
model.add(keras.layers.Dense(units=10, activation='softmax'))
model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), 
              loss=keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

model.fit(x=tr_data, y=tr_label, epochs=5, batch_size=32, validation_data=(val_data, val_label))
# model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,
#           validation_data = val_dataset, validation_steps = 1)
```
output
```python
Train on 1000 samples, validate on 100 samples
Epoch 1/5
1000/1000 [==============================] - 0s 171us/step - loss: 2.3126 - acc: 0.1160 - val_loss: 2.3027 - val_acc: 0.1000
Epoch 2/5
1000/1000 [==============================] - 0s 43us/step - loss: 2.3078 - acc: 0.1200 - val_loss: 2.2976 - val_acc: 0.0900
Epoch 3/5
1000/1000 [==============================] - 0s 45us/step - loss: 2.3049 - acc: 0.1240 - val_loss: 2.2953 - val_acc: 0.0800
Epoch 4/5
1000/1000 [==============================] - 0s 42us/step - loss: 2.3030 - acc: 0.1190 - val_loss: 2.2926 - val_acc: 0.0900
Epoch 5/5
1000/1000 [==============================] - 0s 46us/step - loss: 2.3009 - acc: 0.1200 - val_loss: 2.2925 - val_acc: 0.1000
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24739, AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsSequenceForData',"I have followed the TensorFlow tutorial, ""first_steps_with_tensor_flow.ipynb""  When I ran the code below local , I got  the error messages behind the code below:

from __future__ import print_function

import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
import tensorflow as tf
from tensorflow.python.data import Dataset


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-f9edf7e7a602> in <module>
----> 1 from tensorflow.python.data import Dataset

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/__init__.py in <module>
     23 
     24 # pylint: disable=unused-import
---> 25 from tensorflow.python.data import experimental
     26 from tensorflow.python.data.ops.dataset_ops import Dataset
     27 from tensorflow.python.data.ops.iterator_ops import Iterator

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/experimental/__init__.py in <module>
     66 # pylint: disable=unused-import
     67 
---> 68 from tensorflow.python.data.experimental.ops.batching import dense_to_sparse_batch
     69 from tensorflow.python.data.experimental.ops.batching import map_and_batch
     70 from tensorflow.python.data.experimental.ops.batching import unbatch

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/batching.py in <module>
     20 import numpy as np
     21 
---> 22 from tensorflow.python.data.experimental.ops import get_single_element
     23 from tensorflow.python.data.experimental.ops import grouping
     24 from tensorflow.python.data.ops import dataset_ops

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/get_single_element.py in <module>
     18 from __future__ import print_function
     19 
---> 20 from tensorflow.python.data.ops import dataset_ops
     21 from tensorflow.python.data.util import nest
     22 from tensorflow.python.data.util import sparse

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in <module>
     26 
     27 from tensorflow.python.compat import compat
---> 28 from tensorflow.python.data.ops import iterator_ops
     29 from tensorflow.python.data.util import nest
     30 from tensorflow.python.data.util import random_seed

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py in <module>
     22 
     23 from tensorflow.python.compat import compat
---> 24 from tensorflow.python.data.ops import optional_ops
     25 from tensorflow.python.data.util import nest
     26 from tensorflow.python.data.util import sparse

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/ops/optional_ops.py in <module>
     20 import abc
     21 
---> 22 from tensorflow.python.data.util import structure
     23 from tensorflow.python.framework import dtypes
     24 from tensorflow.python.framework import ops

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/util/structure.py in <module>
     20 import abc
     21 
---> 22 from tensorflow.python.data.util import nest
     23 from tensorflow.python.framework import dtypes
     24 from tensorflow.python.framework import ops

~/zhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py in <module>
     98 
     99 # See the swig file (../../util/util.i) for documentation.
--> 100 is_sequence = _pywrap_tensorflow.IsSequenceForData
    101 
    102 # See the swig file (../../util/util.i) for documentation.

AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'IsSequenceForData'"
24738,float16 matmul is way slower than float32 matmul on CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
float16 matmul is way slower than float32 matmul on CPU

**Code to reproduce the issue**
```
import tensorflow as tf
import time
from datetime import timedelta


a = tf.random.normal(shape=[1, 768, 768], dtype=tf.float16)
b = tf.random.normal(shape=[1, 768, 768], dtype=tf.float16)

c = tf.random.normal(shape=[1, 768, 768], dtype=tf.float32)
d = tf.random.normal(shape=[1, 768, 768], dtype=tf.float32)

e = tf.matmul(a, b)
f = tf.matmul(c, d)

config = tf.ConfigProto(
    intra_op_parallelism_threads=24,
    inter_op_parallelism_threads=24,
    allow_soft_placement=True,
    device_count={""GPU"": 0},
)

with tf.Session(config=config) as sess:
    for i in range(100):
        if i % 2:
            print(""16bit -- "", end="""")
            op = e
        else:
            print(""32bit -- "", end="""")
            op = f
        start = time.monotonic()
        sess.run(op)
        end = time.monotonic()
        print(i, timedelta(seconds=end - start))
```       
output         
```
2019-01-07 16:06:19.698878: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow bi
nary was not compiled to use: AVX512F
32bit -- 0 0:00:00.017297
16bit -- 1 0:00:00.275746
32bit -- 2 0:00:00.002908
16bit -- 3 0:00:00.261320
32bit -- 4 0:00:00.003028
16bit -- 5 0:00:00.253561
32bit -- 6 0:00:00.002849
16bit -- 7 0:00:00.256515
32bit -- 8 0:00:00.006011
16bit -- 9 0:00:00.255613
32bit -- 10 0:00:00.003996
16bit -- 11 0:00:00.242231
32bit -- 12 0:00:00.003338
```"
24735,Error while building from source with Bazel: in target '//external:cc_toolchain': no such package '@local_config_cc//',"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.20.0


**Describe the problem**
I followed the instructions to build TF from source with bazel. But this error occurs:

> C:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow/.bazelrc
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: 1a8498c3-1a43-45b0-abe9-317aa5b27e0b
ERROR: in target '//external:cc_toolchain': no such package '@local_config_cc//': Traceback (most recent call last):
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/cc_configure.bzl"", line 51
                configure_windows_toolchain(repository_ctx)
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 347, in configure_windows_toolchain
                _find_missing_vc_tools(repository_ctx, vc_path)
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 257, in _find_missing_vc_tools
                find_msvc_tool(repository_ctx, vc_path, tool)
        File ""C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 230, in find_msvc_tool
                repository_ctx.path((vc_path + ""\\Tools\\MSVC"")).readdir()
C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)
INFO: Elapsed time: 4,086s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded)
    Fetching @local_config_cc; fetching

(I did the default configuration)

The weird thing is that it seems to look for a file in MSVC 2017, while I have MSVC 2015 Update 3 installed (I also tried with MSVC 2017, same error).

Note that I had tried building with Cuda 9.0 before and had a similar error ( issue #24549 ).

Is there anything I can do ?

Thanks.

"
24734,"Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp Aborted (core dumped)","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip install 
- TensorFlow version (use command below):1.12
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


TF version = 1.12
tfnightly = 1.13.0.dev20190104
python 3.5
Model ssd_mobilenet_v1_coco_2018_01_28

```
import tensorflow as tf
import sys
def convert(frozen):
	graph_def_file = frozen+""/frozen_inference_graph.pb""
	input_arrays = [""image_tensor""]
	output_arrays = [""detection_boxes"",""detection_scores"",""detection_classes"",""num_detections""]
	converter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file,input_arrays,output_arrays)#,input_shapes={""image_tensor"":[None,300,300,3]})
	tflite_model = converter.convert()
	open(frozen+""/model.tflite"", ""wb"").write(tflite_model)

if __name__=='__main__':
	frozen_path = sys.argv[1]
	convert(frozen_path)
```
Error

> 2019-01-07 09:01:08.263004: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2019-01-07 09:01:08.268020: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300030000 Hz
> 2019-01-07 09:01:08.268185: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x56b0400 executing computations on platform Host. Devices:
> 2019-01-07 09:01:08.268255: I tensorflow/compiler/xla/service/service.cc:168] StreamExecutor device (0): , 
> 2019-01-07 09:01:09.412404: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
> 2019-01-07 09:01:09.412562: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
> 2019-01-07 09:01:11.571194: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:584] Optimization results for grappler item: graph_to_optimize
> 2019-01-07 09:01:11.571245: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] model_pruner: Graph size after: 5957 nodes (-3), 10020 edges (-3), time = 68.272ms.
> 2019-01-07 09:01:11.571253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] function_optimizer: Graph size after: 5957 nodes (0), 10020 edges (0), time = 13.564ms.
> 2019-01-07 09:01:11.571259: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] constant folding: Graph size after: 5958 nodes (1), 10022 edges (2), time = 539.625ms.
> 2019-01-07 09:01:11.571265: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] shape_optimizer: Graph size after: 5958 nodes (0), 10022 edges (0), time = 31.515ms.
> 2019-01-07 09:01:11.571271: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] arithmetic_optimizer: Graph size after: 4306 nodes (-1652), 8412 edges (-1610), time = 243.152ms.
> 2019-01-07 09:01:11.571280: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] loop_optimizer: Graph size after: 4289 nodes (-17), 8380 edges (-32), time = 43.908ms.
> 2019-01-07 09:01:11.571289: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] dependency_optimizer: Graph size after: 4272 nodes (-17), 8358 edges (-22), time = 103.724ms.
> 2019-01-07 09:01:11.571299: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] layout: Graph size after: 4272 nodes (0), 8358 edges (0), time = 15.684ms.
> 2019-01-07 09:01:11.571309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] memory_optimizer: Graph size after: 4272 nodes (0), 8358 edges (0), time = 263.292ms.
> 2019-01-07 09:01:11.571319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] model_pruner: Graph size after: 4258 nodes (-14), 8332 edges (-26), time = 43.246ms.
> 2019-01-07 09:01:11.571329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] function_optimizer: Graph size after: 4258 nodes (0), 8332 edges (0), time = 11.187ms.
> 2019-01-07 09:01:11.571338: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] constant folding: Graph size after: 4258 nodes (0), 8332 edges (0), time = 305.64ms.
> 2019-01-07 09:01:11.571348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] shape_optimizer: Graph size after: 4258 nodes (0), 8332 edges (0), time = 22.274ms.
> 2019-01-07 09:01:11.571358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] arithmetic_optimizer: Graph size after: 4256 nodes (-2), 8328 edges (-4), time = 192.768ms.
> 2019-01-07 09:01:11.571367: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:586] dependency_optimizer: Graph size after: 4256 nodes (0), 8328 edges (0), time = 100.01ms.
> Traceback (most recent call last):
> File ""export_graph.py"", line 15, in 
> convert(frozen_path)
> File ""export_graph.py"", line 10, in convert
> tflite_model = converter.convert()
> File ""/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/lite.py"", line 499, in convert
> **converter_kwargs)
> File ""/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 445, in toco_convert_impl
> input_data.SerializeToString())
> File ""/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/lite/python/convert.py"", line 208, in toco_convert_protos
> ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
> tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
> 2019-01-07 09:01:12.809603: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for ReadVariableOp
> Aborted (core dumped)
> "
24732,ARM6/RPI: Executor failed to create kernel _FusedConv2D,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ARM 6 (Raspberry PI Zero W)

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No

- TensorFlow installed from (source or binary):
Source at 92b598a32d5f8c636b9eb1cdc7afcb958395500f

- TensorFlow version (use command below):
n/a

- Python version:
n/a

- Bazel version (if compiling from source):
n/a

- GCC/Compiler version (if compiling from source):
n/a

- CUDA/cuDNN version:
n/a

- GPU model and memory:
n/a


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
n/a

**Describe the current behavior**
My use case is running TensorFlow as a C-library using Go interface. I am performing an image inferencing task using a model (`.pb file`). I see that my `libtensorflow.so` build works fine on ARM7 but fails on ARM6 with the following error:
```
2019-01-07 04:07:27.533445: E tensorflow/core/common_runtime/executor.cc:634] Executor failed to create kernel. Not found: No registered '_FusedConv2D' OpKernel for CPU devices compatible with node {{node module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Relu}}
	.  Registered:  <no registered kernels>

	 [[module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Relu]]
2019/01/07 04:07:27 session run:No registered '_FusedConv2D' OpKernel for CPU devices compatible with node {{node module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Relu}}
	.  Registered:  <no registered kernels>

	 [[module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Relu]]
```

**Describe the expected behavior**
expecting ARM6 build of `libtensorflow.so` and `libtensorflow_framework.so` to work the same as ARM7 build.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
24731,"cannot locate ""pip_test/whl/*.whl"" while executing parameterized_docker_build.sh","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.12
- Python version: 2.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I modify `ci_build/Dockerfile.gpu`, change the ENV part
```
# Configure the build for our CUDA configuration.
ENV TF_NEED_CUDA 1
ENV TF_NEED_TENSORRT 1
ENV TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0
ENV TF_CUDA_VERSION=9.0
ENV TF_CUDNN_VERSION=7
ENV NO_TEST_ON_INSTALL=1
ENV TF_GPU_COUNT=1
```

then I execute `parameterized_docker_build.sh` as below
```
TF_DOCKER_BUILD_TYPE=gpu TF_DOCKER_BUILD_IS_DEVEL=no TF_DOCKER_BUILD_DEVEL_BRANCH=r1.12 ./parameterized_docker_build.sh
```
after pip test finished, I got the following error
```
Parameterized build ends with SUCCESS at: Fri Jan  4 11:35:46 UTC 2019 (Elapsed time: 1271 s)
~/tf_builder/tensorflow/tensorflow/tools/docker
ls: cannot access 'pip_test/whl/*.whl': No such file or directory
ERROR: Cannot locate the locally-built pip whl file
```

**Any other info / logs**
I found in `parameterized_docker_build.sh`

https://github.com/tensorflow/tensorflow/blob/38c91321421694432117b077294df43aa31d1193/tensorflow/tools/docker/parameterized_docker_build.sh#L250-L265

before the `ci_parameterized_build.sh ` execute , the script enterd the folder `${SCRIPT_DIR}/../../../`, and left the folder immediately after build finish.

then the script will check whether the whl is existed by `ls pip_test/whl/*.whl | head -1`

I think the whl existence check should also be executed inside `push pop` section, not outside.Am I right?

My source code is in directory `~/tf_builder/tensorflow/`
So the building script is in `~/tf_builder/tensorflow/tensorflow/tools/docker`
After building finish, I can find whl in `~/tf_builder/tensorflow/pip_test/whl/`
"
24730,Depthwise Conv Tensor Allocation Failure,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

`java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/contrib/lite/kernels/depthwise_conv.cc:92 NumDimensions(input) != 4 (0 != 4)Node number 0 (DEPTHWISE_CONV_2D) failed to prepare.`

I construct the model with:
```
interpreter = Interpreter(loadModelFile(""model.tflite""), object: Interpreter.Options() {
                override fun setAllowFp16PrecisionForFp32(allow: Boolean): Interpreter.Options {
                    return super.setAllowFp16PrecisionForFp32(true)
                }

                override fun setUseNNAPI(useNNAPI: Boolean): Interpreter.Options {
                    return super.setUseNNAPI(true)
                }

            })
```

and invoke it with:

```            
imgData = ByteBuffer.allocateDirect(4 * 100 * 100).apply { order(nativeOrder()) }.asFloatBuffer()
            labelProbArray = IntArray(1)
interpreter.run(imgData, labelProbArray as Any)
```

**Describe the expected behavior**
Expected behavior is that the byte buffer would be converted to tensors of float32 under the hood as per the placeholder op. 

**Other info / logs**
[model.dot.pdf](https://github.com/tensorflow/tensorflow/files/2730812/model.dot.pdf)

"
24729,Speech recognition with pretrained model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>



**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):

**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**


Train the acoustic model with 30 words in the speech command database.

The pretrained model file ""conv_actions_labels.txt"" only contains 10 words, however, the speech command database (http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz) contains 30 words. If I follow the train process in the tutorial (https://www.tensorflow.org/tutorials/sequences/audio_recognition) , the trained model can only handle 10 words, right?

How can I train the model to recognize other words in the speech command database? (which include words like forward, tree, etc...)

Thanks."
24728,backpropagation of gradients into labels in tensorflow tf.nn.ctc_loss(),"**System information**
- TensorFlow version (you are using): 1.12
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- Python version: 2.7

**Describe the feature and the current behavior/state.**

I am working with tensorflow ctc loss function `tf.nn.ctc_loss(labels,inputs,__)`

Here `inputs` argument is the output from some RNN1 and `labels` argument is true target that we want to achieve.
In my case `labels` is also generated from some other RNN2.

But after training, only RNN1 weights are updated by tensorflow backpropagation and RNN2 weights are not changing at all. I think tensorflow does not support backpropagation of **gradients** into `labels` for **ctc_loss** as done in `tf.nn.softmax_cross_entropy_with_logits_v2()`.

What is the possible workaround for it?

**Will this change the current api? How?**

New parameter can be added to existing api `tf.nn.ctc_loss()` to enable or disable gradients backpropagation along `labels`.

**Who will benefit with this feature?**
A wast community of DL developers working with ctc_loss as labels can be generated dynamically from another neural network."
24727,Run tflite model on self-developed embedded system,"Hi experts,
I have self-developed an embedded system. I want to run tflite model on this system for detection or classification. But I can't how to do it. Could you please give me advises and instruction of solving this problem? 

Thanks."
24726,PolymorphicFunction.get_concrete_function() creates a different concrete function for each python scalar value,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
- Python version:
3.6.6
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.function` returns a `PolymorphicFunction`.  When called, this function creates a concrete function to call for the specific types and shapes of the arguments, by calling `PolymorphicFunction.get_concrete_function()`, and subsequently, it reuses the same concrete function when it encounters the same types and shapes again.

Unfortunately, when passed python scalars or arrays of scalars, it seems to create a new function for each different value, instead of each different type (see code example below). Similarly, it creates a new function for [1.0, 2.0], and [3.0, 4.0].

I understand that there may be a good technical reason for this, but even if there is, I fear that users will pass Python scalar values or arrays (instead of tensors or NumPy arrays) to @tf.functions and they won't understand why their system blows up (slowed down by all the traces and new concrete functions piling up in RAM all the time).

**Describe the expected behavior**
I expect `PolymorphicFunction.get_concrete_function()` to return the same concrete function whether I pass it 1, or 2, or tf.constant(3) or tf.constant(4) as an argument.  I expect a different concrete function to be returned when I pass it [1.0], but this second concrete function should be the same as for the arguments [2.0], or tf.constant([3.0]), or tf.constant([4.0]) or np.array([5.0]), or or np.array([6.0])

**Code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

@tf.function
def square(x):
    print(""Calling"")
    tf.get_logger().warning(""Tracing"")
    return tf.square(x)

print(""Square of int32 scalars"")
for i in range(3):
    square(tf.constant(i))
print(""OKAY"")
    
print()
print(""Square of float32 scalars"")
for i in range(3):
    square(tf.constant(i, dtype=tf.float32))
print(""OKAY"")

print()
print(""Square of float32 arrays of shape [2]"")
for i in range(3):
    square(tf.constant([i, i], dtype=tf.float32))
print(""OKAY"")

print()
print(""Square of float32 NumPy arrays of shape [2]"")
for i in range(3):
    square(tf.constant(np.array([i, i]), dtype=tf.float32))
print(""OKAY"")

print()
print(""Square of python integers 1, 2, 3"")
for i in range(3):
    square(i)
print(""ERROR! Why is there one trace per call?"")

print()
print(""Square of python integers 1, 2, 3 (AGAIN)"")
for i in range(3):
    square(i)
print(""ERROR! No traces anymore, which means that one Function was cached for each specific integer 1, 2, 3."")

print()
print(""Square of python integers arrays of shape [2]"")
for i in range(3):
    square([i, i])
print(""ERROR! Why is there one trace per call?"")
```

**Other info / logs**
Here is the output of this program:

```
Square of int32 scalars
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 scalars
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 arrays of shape [2]
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 NumPy arrays of shape [2]
Calling
Calling
Calling
OKAY

Square of python integers 1, 2, 3
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
ERROR! Why is there one trace per call?

Square of python integers 1, 2, 3 (AGAIN)
Calling
Calling
Calling
ERROR! No traces anymore, which means that one Function was cached for each specific integer 1, 2, 3.

Square of python integers arrays of shape [2]
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
ERROR! Why is there one trace per call?
```

"
