Issue Number,Issue Title,Issue Body
28739,Assigning to ressource variables with varying shapes violates the shape information of the tensor of which is assigned from.,"Hi,
I ran into problems when assigning to variables from tensors with dynamic shape. In the end, it boiled down to using resource variables and having a strided slice tensor from which to assign from, see the upcoming MWE. When assigning to resource variables from a strided slice tensor, the shape information of the tensor becomes corrupt.

**System information**
- Unexpected behaviour in custom code (minimum working example see below)
- Windows 10
- TensorFlow installed from pip wheels
- TensorFlow version:
  - tf.version.VERSION = 1.13.1
  - tf.version.GIT_VERSION = b'unknown'
  - tf.version.COMPILER_VERSION = MSVC 190024215
  - Sanity check: array([1])
- Python version: Python 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)] on win32
- also occurs without GPU support


**Describe the current behavior**

Output is
> *results: [1 2] [array([1]), array([1, 2]), array([2])]
> var_value_before [3]
> var_value_after [1 2]

Note the contradiction that the tensor with elements [1, 2] has shape [1]

**Describe the expected behavior**

Expected output
> *results: [1 2] [array([**2**]), array([1, 2]), array([2])]
> var_value_before [3]
> var_value_after [1 2]

**Code to reproduce the issue**

```
import tensorflow as tf

def mwe():
  u = tf.range(3, 4)   # Some tensor of shape (1,)
  v = tf.range(1, 3)   # Some tensor of shape (2,)

  # Forget about shape of v (Otherwise assign_op will not build with use_resource=True)
  v = tf.placeholder_with_default(v, (None,))

  # Random stride (leaving this out will not result in error)
  value_to_assign = v[:]
  var = tf.Variable(u, use_resource=True, validate_shape=False)
  assign_op = tf.assign(var, value_to_assign, validate_shape=False)

  observed_ops = [tf.shape(value_to_assign), value_to_assign, tf.shape(v)]

  with tf.Session() as sess:
    tf.initializers.global_variables().run()
    var_value_before = sess.run(var)
    results = sess.run([assign_op, observed_ops])
    var_value_after = sess.run(var)
    print('*results:', *results)
    print('var_value_before', var_value_before)
    print('var_value_after', var_value_after)

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
mwe()
```

**Note**
Either `use_resource=False`, leaving out the assign_op when running the session or leaving out the stride op `v[:]` will yield the expected behaviour. This may be tested with the following routine:

```
def mwe_options():
  for use_resource in [0, 1]:
    for additional_stride in [0, 1]:
      u = tf.range(3, 4)   # Some tensor of shape (1,)
      v = tf.range(1, 3)   # Some tensor of shape (2,)

      # Forget about shape of v (Otherwise assign_op will not build with use_resource=True)
      v = tf.placeholder_with_default(v, (None,))

      # Random stride (leaving this out will not result in error)
      value_to_assign = v[:] if additional_stride else v
      var = tf.Variable(u, use_resource=use_resource, validate_shape=False)
      assign_op = tf.assign(var, value_to_assign, validate_shape=False)

      observed_ops = [tf.shape(value_to_assign), value_to_assign]

      with tf.Session() as sess:
        tf.initializers.global_variables().run()
        for add_assign_op in [0, 1]:
          if add_assign_op:
            _, result = sess.run([assign_op, observed_ops])
          else:
            result = sess.run(observed_ops)
          var_value = sess.run(var)
          print(f'use_resource={use_resource}, additional_stride={additional_stride}, add_assign_op={add_assign_op}: {result}, var = {var_value}')
```
Output:

> use_resource=0, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=0, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=0, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=0, additional_stride=1, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=1, additional_stride=0, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=1, additional_stride=0, add_assign_op=1: [array([2]), array([1, 2])], var = [1 2]
> use_resource=1, additional_stride=1, add_assign_op=0: [array([2]), array([1, 2])], var = [3]
> use_resource=1, additional_stride=1, add_assign_op=1: [array([1]), array([1, 2])], var = [1 2]"
28738,keras fit_generator ignores verbose parameter for validation_data,"Setting `verbose=0` in `tf.keras.model.fit_generator()` has no effect if `validation_data` is provided.
The progress is still displayed for each validation step.
This is annoying if you have a callback that is responsible for printing progress.

**System information**
- running in docker container tensorflow/tensorflow:latest-py3-jupyter
- `tf.GIT_VERSION`: v1.13.1-0-g6612da8951
- `tf.VERSION`: 1.13.1 
- `tf.keras.__version__`: 2.2.4-tf

**Describe the current behavior**
Progress-bar is displayed regardless of `verbose=0`

**Describe the expected behavior**
Nothing should be printed

**Code to reproduce the issue**
```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])
model.compile(loss=""mae"", optimizer=""adam"")

def generator():
    i=0
    while 1:
        yield (np.array([i]),[i])
        i+=1
valData = (np.arange(10), np.arange(10))

history = model.fit_generator(generator(), steps_per_epoch=5, verbose=0, validation_data=valData)
```
Output:
> 10/10 [==============================] - 0s 6ms/sample - loss: 11.3572

Omitting `validation_data`  stops the output from appearing

Link to an example notebook:
https://colab.research.google.com/drive/1SrTdFXD_SCxu-gvo-h2YjFiAkO1CDaLG"
28735,Wrong MD5 checksum for flatbuffers when building Tensorflow Lite Micro using make,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: ba63891c8b
- Python version: NA
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
When running
make -f tensorflow/lite/experimental/micro/tools/make/Makefile test
the command fails when downloading the flatbuffers archive, since there is a mismatch with the MD5 sums.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
make -f tensorflow/lite/experimental/micro/tools/make/Makefile test

**Any other info / logs**
tensorflow/lite/experimental/micro/tools/make/download_and_extract.sh ""https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"" ""7e8191b24853d75de2af87622ad293ba"" tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp                        
downloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip
tensorflow/lite/experimental/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz"" ""3811552512049fac3af419130904bc55"" tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers                           
downloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz
Checksum error for 'http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz'. Expected 3811552512049fac3af419130904bc55 but found 02c64880acb89dbd57eebacfd67200d8                                                                                               
tensorflow/lite/experimental/micro/tools/make/Makefile:198: recipe for target 'tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers' failed                                                                                                                                 
make: *** [tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers] Error 1

"
28734,Persistent colors on detected objects when using TFLite demo app,"<em>I would like persisent colors on detected objects when using the TFLite demo app on Android</em>


**System information**
- TensorFlow version (you are using): 1.13.1

- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Detected objects in the TFLite demo app show rectangles around them with a confidence number. These rectangles change color and make a certain use case I have rather difficult. I'd like the colors to be persisent, let's say I have 3 labels:
Label A - GREEN
Label B - RED
Label C - BLUE
and so on.

**Will this change the current api? How?**
I don't want to change the current API but I would appreciate a suggestion on how to change this.

**Who will benefit with this feature?**
I will.

**Any Other info.**
No.
"
28733,tf.maximum/ tf.nn.relu not giving expected result,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab, CPU versio
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.7 (default)

**Describe the current behavior**
I was testing Relu and tf.maximum for a transformation, but I did not get the results I expect. 

Running this code for example,
`with tf.Session() as sess:
    a = tf.random.normal([5])
    print(sess.run(a))
    print(sess.run(tf.nn.relu(a)))
    print(sess.run(tf.maximum(a, tf.zeros_like(a))))
`
Neither maximum nor relu gives the right/expected result (i.e. max(0, a)). Running similar code in either pytorch or numpy does give me the correct result. I've read the documentation but I couldn't find anything describing this. Am I missing something or is maximum/relu broken?"
28732,Tensorflow Type error when trying to iterate the Tensors in loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes. It is mentioned as above in the code. I want to go as it is with loop and conditions. I know there are short hand functions but I want to customize it my way later during the go.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Desktop
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No it did not as I am using Desktop system for the development
- TensorFlow installed from (source or binary):
Using Pip on Python 3.5.0 64 bit
- TensorFlow version (use command below):
Tensorflow version is 1.12.0
- Python version:
Python 3.5.0
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source):?
- CUDA/cuDNN version: No usng CPU version
- GPU model and memory: There isn't any as I faced issue during the go.
I have the following scenario:

```
y = tf.placeholder(tf.float32, [None, 1],name=""output"")
layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.leaky_relu, name=""layer""+str(layer))
         for layer in range(2)]
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, 100]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, 1)
outputs = tf.reshape(stacked_outputs, [-1, 2, 1])
outputs = tf.identity(outputs[:,1,:], name=""prediction"")
loss = Custom_loss(y,outputs)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss,name=""training_op"")
```
The custom loss function I tried is:

```
def Custom_loss(y,outputs):
    hold_loss = []
    for exp,pred in zip(y,outputs):
        if exp >= pred:
            result = tf.pow(pred * 0.5,2) - exp
            hold_loss.append(result)
        else:
            hold_loss.append(tf.subtract(pred-exp))
    return tf.reduce_mean(hold_loss)
```
Now when I am trying to implement this I am getting the following error:

`TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.`
I have tried implementing the tf.map_fn() but there is the same error I encounter. I have used the following question:
[How to explain the result of tf.map_fn?](https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn)

Kindly, help me get through this issue? How I can iterate the tensor? What way is best for the custom loss function implementation?
I am willing to have the custom function become a part of the graph so that I can train it effectively."
28730,use GradientTape to compute gradient cost all memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip3
- TensorFlow version (use command below): v1.13.1-0-g6612da8951' 1.13.1
- Python version:3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.130/ 7.5.0
- GPU model and memory: GeForce GTX 1060 5GB

**Describe the current behavior**

When i use tf.GradientTape to compute gradients, it will use all gpu memory even though I use GPUoption in session.

**Describe the expected behavior**

GPU memory should be controled

**Code to reproduce the issue**

```
import tensorflow as tf

def mul(x):
    y = tf.layers.dense(x, 3)
    return y

x = tf.placeholder(tf.float32, [None, 2])

with tf.GradientTape() as gg:
    gg.watch(x)
    f_eval = mul(x)
    grad = gg.gradient(f_eval, x)
del gg


gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)
sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))
sess.run(tf.global_variables_initializer())
import time
time.sleep(5)
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> Observing memory usage, it always use more than 4100MB memory because my GPU only has 5G memory.
"
28729,ValueError: Invalid tensors 'outputs' were found when converting from .pb to .tflite,"I successfully retrained mobilenet quantized model (`architecture=""mobilenet_1.0_128_quantized""`) with my own image dataset:

    python3 -m scripts.retrain \
      --bottleneck_dir=tf_files/bottlenecks_quant \
      --how_many_training_steps=50000 \
      --model_dir=tf_files/models/ \
      --summaries_dir=tf_files/training_summaries/""mobilenet_1.0_128_quant"" \
      --output_graph=tf_files/retrained_graph_50000_1.0_128.pb \
      --output_labels=tf_files/retrained_labels.txt \
      --architecture=""mobilenet_1.0_128_quantized"" \
      --image_dir=images

When I try to convert .pb file to .tflite using

    toco \
      --graph_def_file=tf_files/retrained_graph_50000_1.0_128.pb \
      --output_file=tf_files/retrained_graph_50000_1.0_128.tflite \
      --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
      --inference_type=QUANTIZED_UINT8 \
      --input_shape=""1,128,128,3"" \
      --input_array=input \
      --output_array=outputs \
      --std_dev_values=127.5 --mean_value=127.5

It fails with the next error:

> ValueError: Invalid tensors 'outputs' were found.

Script uses this link to download mobilenet model: http://download.tensorflow.org/models/mobilenet_v1_1.0_128_frozen.tgz

```
data_url = 'http://download.tensorflow.org/models/mobilenet_v1_'
data_url += version_string + '_' + size_string + '_frozen.tgz'
```
 
"
28728,Error while convert the Tensorflow to tensorflowlite model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.

```
Graph_def_file = ssdlite_mobilenet_v2_coco_2018_05_09/tflite_graph.pb
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28727,Converting error for quantize aware trained tf.keras.applications.MobileNetV2,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.13
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 2080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
During the converting the quantize aware trained mobilentnetv2 model from tf.keras, it will raise the follow error massage
```
F tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)
```
**Describe the expected behavior**
It works for model built with tf.contrib.slim. I feels like the tf.contrib.quantize.create_eval_graph() doesn't support BN layer from tf.keras. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
working_path='/tmp/tflite'
tf.keras.backend.set_learning_phase(1)
inputs = tf.keras.Input(shape=(224, 224, 3), name='input')
y_true = tf.keras.Input(shape=[1000], name='label')
y_pred = keras_model.output

loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)

# quant aware training
graph = tf.get_default_graph()
tf.contrib.quantize.create_training_graph(input_graph=graph, quant_delay=0)

train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00625).minimize(loss)
saver = tf.train.Saver()
with tf.Session() as sess:
    _input = np.random.rand(10, 224, 224, 3)
    _label = np.zeros([10, 1000])
    _label[:, 2] = np.ones(10)
    sess.run(tf.global_variables_initializer())
    for i in range(3):
        _, _loss = sess.run([train_step, loss], feed_dict={inputs: _input,
                                                           y_true: _label})
        print(_loss)
    # save
    saver.save(sess, os.path.join(working_path, 'checkpoints/model.ckpt'))

'''load and convert to TFLite'''
tf.reset_default_graph()
tf.keras.backend.set_learning_phase(0)
inputs = tf.keras.Input(shape=(224, 224, 3), name='input')
keras_model = tf.keras.applications.MobileNetV2(input_tensor=inputs, alpha=1.0, weights=None, include_top=True)
output = keras_model.output

# insert fake quant nodes
graph = tf.get_default_graph()
tf.contrib.quantize.create_eval_graph(graph)
saver = tf.train.Saver()

with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    saver.restore(sess, tf.train.latest_checkpoint(os.path.join(working_path, 'checkpoints/')))

    # freeze graph
    graph_def = graph.as_graph_def()
    froze_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, [output.op.name])
    tf.io.write_graph(froze_graph, working_path, 'freeze_graph.pb')

# convert to TFLite
graph_def_file = os.path.join(working_path, 'freeze_graph.pb')
input_array = [""input""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_array, [output.op.name],
                                                      input_shapes={""input"": [1, 224, 224, 3]})

converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
converter.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
converter.quantized_input_stats = {""input"": (0., 255.)}
tfmodel = converter.convert()
open(os.path.join(working_path, ""converted_model.tflite""), ""wb"").write(tfmodel)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""/home/mgou/projects/tf-lightweight-yolov3/backbone_pretrain/train_recog_quant.py"", line 91, in <module>
    tfmodel = converter.convert()
  File ""/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 455, in convert
    **converter_kwargs)
  File ""/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/mgou/virtualenv/tf113-py36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-05-15 02:27:33.413703: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1161 operators, 1728 arrays (0 quantized)
2019-05-15 02:27:33.440573: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1161 operators, 1728 arrays (0 quantized)
2019-05-15 02:27:33.608574: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 137 operators, 260 arrays (1 quantized)
2019-05-15 02:27:33.610239: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 137 operators, 260 arrays (1 quantized)
2019-05-15 02:27:33.610999: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 76 operators, 199 arrays (1 quantized)
2019-05-15 02:27:33.611690: F tensorflow/lite/toco/tooling_util.cc:1702] Array expanded_conv_project_BN/FusedBatchNorm, which is an input to the Conv operator producing the output array block_1_expand_relu/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)
```"
28725,Autograph fails for keyword-only arguments,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): `pip install tf-nightly-gpu-2.0-preview`
- TensorFlow version (use command below): `v1.12.1-1847-gc095504 2.0.0-dev20190514`
- Python version: 3.7.3

**Describe the current behavior**
Autograph complains when compiling functions with keyword-only arguments.

Example output:
`W0515 01:46:22.158518 139635868194560 ag_logging.py:145] Entity <function f at 0x7eff8120c1e0> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function f at 0x7eff8120c1e0>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output when filing the bug report. Caused by: inconsistent nodes: None (NoneType) and None (NoneType)`

**Describe the expected behavior**
Autograph works for keyword-only arguments

**Code to reproduce the issue**
```
import tensorflow as tf
@tf.function
def f(*, a):
     return a*2

f(a=0)
```
"
28721,Failed to load the native TensorFlow runtime,"I'm trying to run tensorflow in a venv managed on PyCharm, but is giving some trouble (I cant even check the version). I have another env, this one on conda, which works fine.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): import tensorflow (for example)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce 1080 Ti - 11Gb. Driver 430.64

**Traceback**

> Traceback (most recent call last):
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\Python\Python-3.6.8\lib\imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\Python\Python-3.6.8\lib\imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: No se puede encontrar el módulo especificado.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""C:\Python\Python-3.6.8\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""C:\Python\Python-3.6.8\lib\imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""C:\Python\Python-3.6.8\lib\imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: No se puede encontrar el módulo especificado.
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
> 

I have already tried to include `bin`, `include` and `lib` (as in https://github.com/tensorflow/docs/pull/419)"
28718,Transfer Learning Using Pretrained ConvNets tutorial should use sigmoid activation,"## URL(s) with the issue:
https://www.tensorflow.org/alpha/tutorials/images/transfer_learning

## Description of issue (what needs changing):
classification head should use sigmoid activation.
### Clear description
The tutorial has this paragraph:

You don't need an activation function here because this prediction will be treated as a logit, or a raw prediciton value. Positive numbers predict class 1, negative numbers predict class 0.

I think we need sigmoid activation. 
Later we use loss=""binary_crossentropy"" in model.compile. binary_crossentropy by default has from_logits=False and expect a probability as is documented [here ](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/BinaryCrossentropy) and [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)
"
28714,tensorflow gradient tape training speed performance issue (much slower than keras .fit training),"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): binary pip3
- TensorFlow version (use command below): 2.0.0-dev20190510
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: Titan V 12GB

**Describe the current behavior**
tensorflow.keras .fit training time is way faster than tensorflow gradient tape

**Describe the expected behavior**
keras .fit is just a wrapper why would it be way faster??? They should be the same on the same dataset with the same loss func and optimizer.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tqdm import tqdm
import numpy as np
from time import perf_counter

def get_uncompiled_model():
  inputs = keras.Input(shape=(784,), name='digits')
  x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
  x = layers.Dense(64, activation='relu', name='dense_2')(x)
  outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
  model = keras.Model(inputs=inputs, outputs=outputs)
  return model

def get_compiled_model():
  model = get_uncompiled_model()
  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])
  return model

# Load a toy dataset for the sake of this example
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are Numpy arrays)
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(64)

# model = get_compiled_model()

# # keras train
# model.fit(train_dataset, epochs=3)

# manual train
model = get_uncompiled_model()
optimizer = keras.optimizers.RMSprop(learning_rate=1e-3)
loss_fn = keras.losses.SparseCategoricalCrossentropy()

@tf.function
def train():
    # Iterate over epochs.
    for epoch in range(3):
      print('Start of epoch %d' % (epoch))

      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
          logits = model(x_batch_train)  # Logits for this minibatch
          loss_value = loss_fn(y_batch_train, logits)
        grads = tape.gradient(loss_value, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
s = perf_counter()
train()
print(perf_counter() - s)

model = get_compiled_model()
s = perf_counter()
model.fit(train_dataset, epochs=3)
print(perf_counter() - s)
```
The snippet is taken from here: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#low-level_handling_of_metrics

**Other info**
GPU utilization of the gradient tape training is almost zero - with and without tf.function. Using tf.debugging.set_log_device_placement(True), I can see that the model is allocated on GPU."
28713,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
Windows 10 Pro
Tensorflow 1.13.1 (didn't work with 2.0.0a0 or 1.10.0)
python 3.6.8
installed using pip

I installed using pip following the instructions on the website. The only solutions I could find told me that i need cuDNN, but this is CPU only, or they told me that I need an AVX-enabled processor, which I have. It gave me the following error when I tried importing it in the command prompt:

Traceback (most recent call last):
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Lucas\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime."
28712,tensorflowjs_converter install fails due to tf-nightly-2.0-preview missing from Python 3.7,"**Describe the problem**

Command used:
tensorflowjs_converter --input_format=tf_frozen_model --output_node_names=final_result  output/saved_model.pb web_model

Getting below error:
tensorflowjs_converter: command not found
"
28711,Tensorflow lite demo app gives wrong result in GPU delegate (Honor Play Android 9.0 GPU Turbo),"
**System information**

- OS Platform and Distribution: Android 9.0 (API 28)
- Mobile device : Huawei Honor Play 9.0 ,GPU: Mali G72 MP12 (GPU Turbo)
- TensorFlow installed from: tensorflow-lite:0.0.0-gpu-experimental


**Describe the current behavior**

The tensorflow lite gpu delegate demo application gives incorrect results for image classification , when it is run on GPU; whereas the same float model gives correct results when it is run on CPU.Even the quantized model in the demo application gave correct inference results in this application.We even tried the official deeplab model for semantic segmentation with the same phone; but even in this scenario it gave wrong results (square/stripes), instead of correct masks.
The same model is running in other phones with Adreno GPU and also in some phones with Mali GPU (eg: Samsung A8+ Android 9.0).

**Describe the expected behavior**
The tensorflow lite gpu inference should give same results in cpu and gpu in all the android phones.

**Other info / logs**
It looks like the  phone uses a new feature called GPU Turbo .Initially the model
was working correctly with android 8.1 (stock os).But after the 9.0 upgrade the tflite models are giving wrong results.
"
28710,steps_per_epoch parameter in fit not working with tf.keras.utils.Sequence,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 14.04`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `binary (conda)`
- TensorFlow version (use command below): `1.13.1`
- Python version: `3.6`
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
setting the `steps_per_epoch` parameter in `model.fit` (`tf.keras`) to for example 10 should only perform 10 updates per epoch. When passing numpy arrays as inputs to `model.fit` this works as expected, but when using a custom `tf.keras.utils.Sequence`  instance `steps_per_epoch` does not have any effect.

**Describe the expected behavior**
When passing `steps_per_epoch` to `model.fit` as a parameter when training on data fed via a `tf.keras.utils.Sequence` instance the model should be updated exactly `steps_per_epoch` times in every epoch.

**Code to reproduce the issue**
```python
import math

import numpy as np
import tensorflow as tf


class SomeFeeder(tf.keras.utils.Sequence):
    """"""A dummy Sequence.

    Slightly modified tf.keras.utils.Sequence example.
    """"""

    def __init__(self, x_set, y_set, batch_size):
            self.x, self.y = x_set, y_set
            self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]
        return batch_x, batch_y


# Simple dummy model.
input_x = tf.keras.Input((4,))
output = tf.keras.layers.Dense(1, activation='sigmoid')(input_x)
model = tf.keras.Model(inputs=[input_x], outputs=[output])
model.compile(
    optimizer=tf.keras.optimizers.SGD(lr=0.1), loss='binary_crossentropy')

# Dummy data.
x = np.random.rand(100, 4).astype(np.float32)
y = np.random.choice([0, 1], size=(100,)).astype(np.float32)
x = SomeFeeder(x, y, batch_size=4)

# Train the model with `steps_per_epoch=5`.
model.fit(x=x, epochs=10, steps_per_epoch=5)
```

**Other info / logs**
Produces:
```
Epoch 1/10
25/25 [==============================] - 0s 13ms/step - loss: 0.7041
Epoch 2/10
25/25 [==============================] - 0s 5ms/step - loss: 0.7007
Epoch 3/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6993
Epoch 4/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6981
Epoch 5/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6939
Epoch 6/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6954
Epoch 7/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6936
Epoch 8/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6914
Epoch 9/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6928
Epoch 10/10
25/25 [==============================] - 0s 5ms/step - loss: 0.6916
```
I would have expected `5/5` instead of `25/25`
"
28709,defun + random + addition = explode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Colab or WSL Ubuntu 18.04
- Mobile device if the issue happens on mobile device:
- TensorFlow installed from: binary
- TensorFlow version: 1.13.1 or 1.14.1-dev20190514
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Using `defun` in eager mode, this very simple function seems to cause havoc. In particular, the first run takes a *long* time and a *lot* of memory, both scaling seemingly exponentially with the `n_iter` parameter below.
```python
import time
import tensorflow as tf
tf.enable_v2_behavior()

def f(D, num_iter):
    x = tf.random_uniform((D,))
    for i in range(num_iter):
        x = x + x
    return x

# DANGER: Increasing this much beyond 20 may use up all your memory...
n_iter = 22

t0 = time.time()
f(10, n_iter)
print(time.time() - t0)  # order ms

f_g = tf.contrib.eager.defun(f)
for i in range(3):
    t0 = time.time()
    f_g(10, n_iter)
    print(time.time() - t0)  # around 30s first run for n_iter=22! After that it's fast.
```
Replacing `x = tf.random_uniform((D,))` with `x = tf.ones((D,))` eliminates the problem, as does replacing `x = x + x` with `x = 2*x`.
In graph mode the problem does not occur:
```python
import time
import tensorflow as tf

def graph_fun(node):
    with tf.Session() as sesh:
        for i in range(3):
            t0 = time.time()
            sesh.run(node)
            print(time.time() - t0)

def f(D, num_iter):
    x = tf.random_uniform((D,))
    for i in range(num_iter):
        x = x + x
    return x

n_iter = 100

tf.reset_default_graph()
t0 = time.time()
fres = f(10, n_iter)
print(time.time() - t0)  # around 0.4s

graph_fun(fres)  # around 0.6 s first run, then essentially zero.
```

**Describe the expected behavior**
This very simple defun'd function should execute quickly, including the graph-building and graph optimization steps.

"
28708,gcloud ai-platform fails with TFv2 Saved Model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am following this tutorial: https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models and using a model I created in TFv2. I am able to create the Saved_Model in my Colab using the following code:
```
import time
saved_model_path = ""/content/gdrive/My Drive/Colab Notebooks/{}"".format(int(time.time()))
tf.keras.experimental.export_saved_model(restored_model, saved_model_path)
```
This created a folder with the PB file and assets and variables folder. I then upload that to my bucket in the Google Cloud. I then ran the command to predict:
`gcloud ai-platform local predict --model-dir=$MODEL_DIR --text-instances ci.txt --framework TENSORFLOW`
The ci.txt is a comma separated list of my input numbers.
I get the following error that shows my values:

> Traceback (most recent call last):
>   File ""lib/googlecloudsdk/command_lib/ml_engine/local_predict.py"", line 184, in <module>
>     main()
>   File ""lib/googlecloudsdk/command_lib/ml_engine/local_predict.py"", line 179, in main
>     signature_name=args.signature_name)
>   File ""/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py"", line 102, in local_predict
>     predictions = model.predict(instances, signature_name=signature_name)
>   File ""/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_utils.py"", line 268, in predict
>     preprocessed, stats=stats, **kwargs)
>   File ""/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py"", line 363, in predict
>     ""Exception during running the graph: "" + str(e))
> cloud.ml.prediction.prediction_utils.PredictionError: Failed to run the provided model: Exception during running the graph: invalid literal for float(): 81,71,80,76,1,3 (Error code: 2)


**Describe the expected behavior**
I expect the code to return a single number.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Here is the saved_model: 
[ver1.zip](https://github.com/tensorflow/tensorflow/files/3178529/ver1.zip)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28707,How to apply gradient clipping in TensorFlow 2.0?  ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow 2.0 Alpha
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**

I want to apply gradient clipping in TF 2.0, the best solution is to decorator optimizer with `tf.contrib.estimator.clip_gradients_by_norm` in TF 1.x.

However, I can't find this function in TF2.0 after trying many methods. As I know, the tf.contrib has been clean up in TF 2.0
"
28706,Keep version fixed when searching API docs,"When using the search bar in the [API docs](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf) with a version in the URL, any search will include pages from different API versions, which means you often end up in the wrong module and have to click on the correct version _again_.

## How to reproduce

- Go to: https://www.tensorflow.org/versions/r1.13/api_docs/python/tf
- Type in 'unsorted_segment_' and wait for the results to appear (see image)
- Choose the top 'Pages' result
- You're now in the 1.9 API rather than 1.13. 

<img width=""394"" alt=""Screenshot 2019-05-14 at 16 13 38"" src=""https://user-images.githubusercontent.com/49023008/57705141-8d8a4600-7663-11e9-8640-5a3fdde51ae8.png"">"
28705,How to build Tensorflow C++ API with Visual Studio 2017,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10 Home 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): Visual C++
- CUDA/cuDNN version: 7.4
- GPU model and memory: RTX 2080 ti



**Description of the problem**
I have created a Mask-RCNN model and need to deploy it inside a C++ application. I could not find any guide to do it for TF 2.0 so after looking around I found that we can save the model as a Tensorflow SavedModel and then load the model to predict (forward pass) using Tensorflow C++ API. 
But the problem is I cannot find a way to get Tensorflow C++ API working with Visual Studio 2017. 

I tried the steps outlined by https://joe-antognini.github.io/machine-learning/build-windows-tf
But it uses Cmake which is not supported by the Tesnorflow C++ API.

Could anyone please point me to a guide/resource that can help me solve this problem.

Or could you suggest any alternative where I could call my python model (forward pass only) from the C++ application. 


**Using CMake**

This is what I got when I used the guide above ->

[Executed command] ->  cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=""C:\Users\Soumya Mohanty\swig\swigwin-4.0.0\swig.exe"" -DPYTHON_EXECUTABLE=""C:\Anaconda3\python.exe"" -DPYTHON_LIBRARIES=""C:\Anaconda3\libs\python3.lib"" -DPYTHON_INCLUDE_DIR=""C:\Anaconda3\include"" -DNUMPY_INCLUDE_DIR=""C:\Anaconda3\Lib\site-packages\numpy""

Error Message:

-- Building for: Visual Studio 15 2017
CMake Warning at CMakeLists.txt:9 (message):
Your current cmake generator is set to use 32 bit toolset architecture.
This may cause ""compiler out of heap space"" errors when building. Consider
using the flag -Thost=x64 when running cmake. Ignore this if you are on
CMake GUI.
-- Selecting Windows SDK version 10.0.17134.0 to target Windows 10.0.14393.
-- The C compiler identification is MSVC 19.22.27706.96
-- The CXX compiler identification is MSVC 19.22.27706.96
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Performing Test MSVC_OPENMP_SUPPORT
-- Performing Test MSVC_OPENMP_SUPPORT - Success
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success
-- D:/Tensorflow/build_x64/abseil_cpp/src/abseil_cpp_build
-- Found PythonInterp: C:/ProgramData/Anaconda3/python.exe (found version ""3.6.5"")
-- Found PythonLibs: C:/ProgramData/Anaconda3/libs/python36.lib (found version ""3.6.5"")
CMake Error at tf_python.cmake:217 (message):
Python module not found: tensorflow/contrib/tpu/ops
Call Stack (most recent call first):
CMakeLists.txt:612 (include)

This fails because the new Tensorflow repo doesn't have an ""ops"" file in the  tpu folder. 

**Using Bazel**
I have been trying to build tensorflow.dll using Bazel as explained in #24885 but that too is not working. I am running command :  bazel build --config=opt //tensorflow:tensorflow.dll

Error message:
C:\tensorflow> bazel build --config=opt //tensorflow:tensorflow.dll
INFO: Build options --copt and --define have changed, discarding analysis cache.
INFO: Analysed target //tensorflow:tensorflow.dll (1 packages loaded, 8366 targets configured).
INFO: Found 1 target...
ERROR: C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/external/gif_archive/BUILD.bazel:45:1: Executing genrule @gif_archive//:windows_unistd_h failed (Exit -1). Note: Remote connection/protocol failed with: execution failed: bin failed: error executing command
  cd C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/execroot/org_tensorflow
  SET PATH=C:\msys64\usr;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\NVIDIA Corporation\NVIDIA NGX;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\Git\cmd;C:\cmake\cmake-3.14.3-win64-x64\bin;C:\Anaconda3\envs\Deep_Learning;C:\Anaconda3\envs\Deep_Learning\Scripts;C:\toolkit\Bazel-0.24.0;C:\msys64;C:\msys64\usr\bin;C:\Anaconda3\envs\Deep_Learning\Scripts;C:\Users\Soumya Mohanty\AppData\Local\Microsoft\WindowsApps;C:\Users\Soumya Mohanty\AppData\Local\Programs\Microsoft VS Code\bin
    SET PYTHON_BIN_PATH=C:/Anaconda3/envs/Deep_Learning/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/envs/Deep_Learning/lib/site-packages
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin -c source external/bazel_tools/tools/genrule/genrule-setup.sh; touch bazel-out/x64_windows-opt/genfiles/external/gif_archive/windows/unistd.h
Execution platform: @bazel_tools//platforms:host_platform
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(454): CreateProcessW(""C:\msys64\usr\bin"" -c ""source external/bazel_tools/tools/genrule/genrule-setup.sh; touch bazel-out/x64_windows-opt/genfiles/external/gif_archive/windows/unistd.h""): Access is denied.

Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 1.492s, Critical Path: 0.02s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

There is a space in the user name and that is causing the cd C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/execroot/org_tensorflow command to fail.
But I am running as Administrator so I dont really know how this user name is coming into play.


**Using Bazel with another user(name does not have spaces)**
I tried using another user which does not have a space in the user name but then I get a different error:

ERROR: C:/users/tensorflowuser/_bazel_tensorflowuser/xv6zejqw/external/nsync/BUILD:463:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/tensorflowuser/_bazel_tensorflowuser/xv6zejqw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Windows\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda3/envs/Deep_Learning/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/envs/Deep_Learning/lib/site-packages
    SET TEMP=C:\Users\TENSOR~1\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\TENSOR~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN y /TP -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/win32 -I./external/nsync//platform/msvc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix /Fobazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/nsync_semaphore_mutex.obj /c external/nsync/platform/c++11/src/nsync_semaphore_mutex.cc
Execution platform: @bazel_tools//platforms:host_platform
y
c1xx: fatal error C1083: Cannot open source file: 'y': No such file or directory
Generating Code...
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 90.347s, Critical Path: 5.10s
INFO: 120 processes: 120 local.
FAILED: Build did NOT complete successfully

**Update**
bazel build //tensorflow:libtensorflow_cc.so works and builds successfully. 

And after running the above command the build for bazel build //tensorflow:tensorflow.dll also works successfully

But I have no idea where to find it and how to link it to Visual Studio  2017, so I could write some Tensorflow code in C++ and load my SavedModel 
 
Thank you"
28704,tfexample_decoder_test.py fails on s390x for dtype float32 and image_format 'raw',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):1.12.0
- Python version:2.7.12
- Bazel version (if compiling from source):0.15.0
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609
- CUDA/cuDNN version:NA
- GPU model and memory:NA

**Describe the current behavior**
python ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype fails

**Describe the expected behavior**
It should pass on s390x like on Intel.

**Code to reproduce the issue**
python ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype 

**Other info / logs**
python ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype

======================================================================
FAIL: testDecodeExampleWithRawEncodingFloatDtype (__main__.TFExampleDecoderTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py"", line 242, in testDecodeExampleWithRawEncodingFloatDtype
    image, serialized_example = self.GenerateImage(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1591, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1561, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1496, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 1395, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 778, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=0
Mismatched value: a is different from b.
not close where = (array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]), array([1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]))
not close lhs = [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.
  16.  17.]
not close rhs = [  4.60060299e-41   8.96831017e-44   2.30485571e-41   4.60074312e-41
   5.74868682e-41   6.89663052e-41   8.04457422e-41   9.10844002e-44
   5.83080291e-42   1.15705214e-41   1.73102399e-41   2.30499584e-41
   2.87896769e-41   3.45293955e-41   4.02691140e-41   4.60088325e-41
   4.88786917e-41]
not close dif = [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.
  16.  17.]
not close tol = [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
dtype = float32, shape = (2, 3, 3)
(mismatch 94.4444444444%)
 x: array([[[  0.,   1.,   2.],
        [  3.,   4.,   5.],
        [  6.,   7.,   8.]],...
 y: array([[[  0.000000e+00,   4.600603e-41,   8.968310e-44],
        [  2.304856e-41,   4.600743e-41,   5.748687e-41],
        [  6.896631e-41,   8.044574e-41,   9.108440e-44]],...

----------------------------------------------------------------------
Ran 1 test in 2.034s

FAILED (failures=1)

"
28703,AttributeError: 'Sequential' object has no attribute 'prediction',"from flask import Flask
from flask_restful import Api, Resource, reqparse

app = Flask(__name__)
api = Api(app)

import os
import json
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import logging

from collections import Counter

import pandas as pd
import numpy as np

from keras.models import load_model
from keras import backend as K

from keras.preprocessing.image import array_to_img, img_to_array, load_img
from scipy.misc import imresize

from PIL import Image


users = [
    {
        ""name"": ""Nicholas"",
        ""age"": 42,
        ""occupation"": ""Network Engineer"",
       
    },
    {
        ""name"": ""Elvin"",
        ""age"": 32,
        ""occupation"": ""Doctor""
       
    },
    {
        ""name"": ""Jass"",
        ""age"": 22,
        ""occupation"": ""Web Developer""
      
    }
]

K.clear_session()

class User(Resource):
    def get(self, name):
        for user in users:
            if(name == user[""name""]):
                return user, 200
        return ""User not found"", 404

    def post(self, name):
        parser = reqparse.RequestParser()
        parser.add_argument(""image"")
        #get the image
        model = load_model('C:/Users/ASUS/Downloads/mse-04-0.0877.h5')
        img_height, img_width, channels = 350, 350, 3

        img = load_img('C:/xampp/htdocs/faceDetection/image/logo.png')

        img = imresize(img, size=(img_height, img_width))	  
        test_x = img_to_array(img).reshape(img_height, img_width, channels) 
        test_x = test_x / 255.
        test_x = test_x.reshape((1,) + test_x.shape)
        predicted = model.prediction(test_x)
        self.graph = tf.get_default_graph()

        return{
        ""name"": ""Nicholas"",
        ""age"": 42,
        ""occupation"": ""Network Engineer"",
        ""prediction"": str(predicted[0][0])

    }, 201


    def put(self, name):
        parser = reqparse.RequestParser()
        parser.add_argument(""age"")
        parser.add_argument(""occupation"")
        args = parser.parse_args()

        for user in users:
            if(name == user[""name""]):
                user[""age""] = args[""age""]
                user[""occupation""] = args[""occupation""]
                return user, 200
        
        user = {
            ""name"": name,
            ""age"": args[""age""],
            ""occupation"": args[""occupation""]
        
        }
        users.append(user)
        return user, 201

    def delete(self, name):
        global users
        users = [user for user in users if user[""name""] != name]
        return ""{} is deleted."".format(name), 200
      
api.add_resource(User, ""/user/<string:name>"")

app.run(debug=True)




### Here is the error...
Using TensorFlow backend.
 * Debugger is active!
 * Debugger PIN: 301-406-345
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-05-14 21:20:36.954666: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
app.py:66: DeprecationWarning: `imresize` is deprecated!
`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.
Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.
  img = imresize(img, size=(img_height, img_width))
127.0.0.1 - - [14/May/2019 21:21:13] ""[1m[35mPOST /user/Nicholas HTTP/1.1[0m"" 500 -
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 2309, in __call__
    return self.wsgi_app(environ, start_response)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 2295, in wsgi_app
    response = self.handle_exception(e)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask_restful\__init__.py"", line 269, in error_router
    return original_handler(e)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 1741, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\_compat.py"", line 34, in reraise
    raise value.with_traceback(tb)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask_restful\__init__.py"", line 269, in error_router
    return original_handler(e)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\_compat.py"", line 34, in reraise
    raise value.with_traceback(tb)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask_restful\__init__.py"", line 458, in wrapper
    resp = resource(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask\views.py"", line 88, in view
    return self.dispatch_request(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\flask_restful\__init__.py"", line 573, in dispatch_request
    resp = meth(*args, **kwargs)
  File ""C:\xampp\htdocs\faceDetection\app.py"", line 70, in post
    predicted = model.prediction(test_x)
AttributeError: 'Sequential' object has no attribute 'prediction'
"
28702,gives an error when import tensorflow,">>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\домашний\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 

C:\Users\домашний>pip freeze
absl-py==0.7.1
astor==0.7.1
et-xmlfile==1.0.1
gast==0.2.2
grpcio==1.20.1
h5py==2.9.0
jdcal==1.4.1
Keras==2.2.4
Keras-Applications==1.0.7
Keras-Preprocessing==1.0.9
Markdown==3.1
mock==3.0.5
numpy==1.16.3
openpyxl==2.6.2
Pillow==6.0.0
protobuf==3.7.1
PyYAML==5.1
scipy==1.2.1
six==1.12.0
tensorboard==1.13.1
tensorflow==1.13.1
tensorflow-estimator==1.13.0
termcolor==1.1.0
Werkzeug==0.15.2

python 3.7.0 


What could be the problem?"
28701,Possible wrong comments in minimax_discriminator_loss,"## URL(s) with the issue:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L468

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L477

## Description of issue (what needs changing):

One line 468:     `# -log((1 - label_smoothing) - sigmoid(D(x)))`

Shouldn't it be `-(1-label_smoothing) * log(sigmoid(D(x))` ?
I'm uncertain about the label_smoothing part, but I think the argument of the `log` is wrong.

On line 477: `# -log(- sigmoid(D(G(x))))`
Shouldn't it be `-log(1-sigmoid(D(G(x))`?"
28700, Shown unsupported ops after compilling  tf.lite.TFLiteConverter.from_frozen_graph on SSD_Mobilenet_v1_0.75_depth_coco model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):1.13.1


I tried running TensorflowLite converter as follow:

import tensorflow as tf

graph_def_file=""./frozen_inference_graph.pb""
input_arrays=[""image_tensor""]
output_arrays=[""detection_boxes"",""detection_scores"",""num_detections"",""detection_classes""]
input_tensor={""image_tensor"":[1,300,300,3]}

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays,input_tensor)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops=True
tflite_model = converter.convert()
open(""detect.tflite"", ""wb"").write(tflite_model)

Once it is complete, I get the following output of unsupported ops:

NonMaxSuppressionV2,TensorArrayGatherV3,TensorArrayReadV3,Size,Where,Enter,TensorArraySizeV3,LoopCond,Exit,TensorArrayV3.

This is a ssd_mobilenet_v1_0.75_depth_coco model trained through tensorflow object detection API . It is performing well on tensorflow, but it contains ops not supported by TensorflowLite. I have tried several other models from the tensorflow model zoo, and they all have similar unsupported ops.

Downloading link for the .pb file are :
[http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz](url)"
28698,ConvLSTM2D - Include feedback loop from c_tm1 (carry state),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): Yes



Currently, the ConvLSTM2D layer does not include any feedback from C_tm1 (the carry state from the previous timestamp), see https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/convolutional_recurrent.py#L717

This stands in contrast to the description of the original paper which is cited as a reference: https://arxiv.org/abs/1506.04214v1 .

The missing feedback loop is already documented ""The current implementation does not include the feedback loop on the cells output"". However, is there a reason that this feedback was omitted? If not, I don't see why it should be different from the paper....

**Will this change the current api? How?** 
We might add a boolean flag to either incorporate such feedback or not.

**Who will benefit with this feature?**
Any researcher looking for an implementation exactly like the original paper.

**Any Other info.**
"
28697,tf.lite.TFLiteConverter error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow version (use command below): tf-nightly 1.14.1.dev20190513
- Python version: 3.5.2

**Describe the current behavior**
I create a model from tensorflow, and it's a simple model.

The graph is as following:
1. Placeholder: type=tf.float32, shape=[None, 640, 480, 1]  (format: NHWC)
2. Conv2D: kernel_size = 5, filter_num = 32, stride=1, padding='same' ( tf.layer.con2d)
3. relu (tf.nn.relu)

I save this graph to test.pb. 

I want to convert this model to tflite model with int8, so i use tf.lite.converter to convert my test.pb to convert_test.tflite. 

But something strange...

When i use Netron to parse test.pb, the Conv2D layer is still Conv2D. But in my convert_test.tflite, the Conv2D layer is been changed to ""DepthwiseConv2D"".

What happen...?
And, what can i do?"
28696,TensorFlow Install failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Na
- TensorFlow installed from (source or binary): PyCharm automatic installation via PiP. 
- TensorFlow version: 2.0.0a0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: 8 GB Ram; INtel i7; NVIDIA GeForce 1060 6GB 



**Describe the problem**
After Installation via PiP the Tensorflow library is not available. 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Used the following script: 

import numpy as np
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

C:\Users\bunga\PycharmProjects\bahndaten\venv\Scripts\python.exe C:/Users/bunga/PycharmProjects/bahndaten/bahndatenleser.py
Traceback (most recent call last):
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\bunga\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\bunga\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/bunga/PycharmProjects/bahndaten/bahndatenleser.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\bunga\PycharmProjects\bahndaten\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\bunga\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\bunga\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1

"
28695,Always same random values in eager mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS X 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.1-dev20190513
- Python version: 3.6.5
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior**
Random output should be different on each call.
That is true for graph mode, but not for eager one.

**Describe the expected behavior**
Random output should be different on each call in both graph and eager modes.

**Code to reproduce the issue**
```python
# Graph mode
import tensorflow as tf

x = tf.random.normal([5], seed=1)

with tf.Session() as sess:
    for i in range(3):
        print(sess.run(x))

# Output
# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]
# [-0.36332107 -0.07205155 -0.5527937   0.10289733 -0.39558855]
# [-0.666205   -0.416783    1.8211031   0.680353   -0.26143482]
```

```python
# Eager mode
import tensorflow as tf

tf.enable_eager_execution()
x = tf.random.normal([5], seed=1)

for i in range(3):
    print(x.numpy())

# Output
# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]
# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]
# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]
```"
28694,tf.data.Dataset.flat_map shape assertion bug on Dataset with single tensor ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes which I have added as an example
- Windows 7 Enterprise
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.5
- CUDA/cuDNN version: 10.0
- GPU model and memory:  GeForce GTX 1050, 4GB


**Describe the current behavior**
Applying tf.data.Dataset.flat_map() to a dataset that just contains one tensor (in contrast to a tupled pair of tensors) leads to an incorrect error (from the example below):
ValueError: Tensor's shape (2,) is not compatible with supplied shape (None, 2)
The supplied shape seems to be incorrectly derived. This works without problem when the dataset contains two tensors (see example.)

**Describe the expected behavior**
The tensor with Tensor's shape (2,) is the correct output when apllying flat_map to a dataset containing x sample, each consting of two values.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

mode = 1  # 1 for single tensor example, 0 for two tensor example


def get_sample(n_element):
    if mode == 1:
        samples = np.random.sample((n_element.numpy(), 2))
    else:
        samples = (np.random.sample((n_element.numpy(), 2)), np.random.sample((n_element.numpy(), 2)))

    tensors = []
    for t in samples:
        tensors.append(tf.convert_to_tensor(t, dtype=tf.float64))
    return tensors


def read_wrapper(number_ds):
    if mode == 1:
        dtypes = [tf.float64]
    else:
        dtypes = [tf.float64, tf.float64]

    ex = tf.py_function(get_sample, [number_ds], dtypes)

    tensors = []
    for t in ex:
        t.set_shape([None, 2])
        tensors.append(t)

    return tensors


def get_dataset(n_element_list):
    print('Initial List: ', n_element_list)
    ds = tf.data.Dataset.from_tensor_slices(n_element_list)
    print('After Slicing: ', ds)
    ds = ds.map(map_func=read_wrapper)
    print('After Mapping: ', ds)
    if mode == 1:
        ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))
    else:
        ds = ds.flat_map(lambda e, l: tf.data.Dataset.zip((
            tf.data.Dataset.from_tensor_slices(e),
            tf.data.Dataset.from_tensor_slices(l))))

    print('After Flattening: ', ds)
    return ds


if __name__ == '__main__':
    np.random.seed(42)
    n_e_list = np.random.randint(2, 4, size=3)
    n_e_list = tf.squeeze(tf.convert_to_tensor(n_e_list))
    data_set = get_dataset(n_e_list)
    data_set = data_set.prefetch(1)

    if mode == 1:
        for x_train in data_set:
            print('Finished Batch: ', x_train)
    else:
        for x_train, y_train in data_set:
            print('Finished Batch: ', x_train, y_train)
```

**Other info / logs**

with mode= 1  The erros occures. Output:

2019-05-14 10:37:23.354903: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-05-14 10:37:23.354903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-05-14 10:37:23.554905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.86GiB
2019-05-14 10:37:23.554905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3581 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Initial List:  tf.Tensor([2 3 2], shape=(3,), dtype=int32)
After Slicing:  <TensorSliceDataset shapes: (), types: tf.int32>
After Mapping:  <MapDataset shapes: ((None, 2),), types: (tf.float64,)>
Traceback (most recent call last):
  File ""DirectoryName/Documents/Repositories/LiverSegmentation/scripts/test_dataset.py"", line 60, in <module>
    for x_train in data_set:
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 557, in __next__
    return self.next()
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 586, in next
After Flattening:  <MapDataset shapes: ((None, 2),), types: (tf.float64,)>
[TensorShape([None, 2])]
    return self._next_internal()
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 580, in _next_internal
    return self._structure._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\data\util\structure.py"", line 393, in _from_compatible_tensor_list
    flat_ret.append(structure._from_compatible_tensor_list(sub_value))
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\data\util\structure.py"", line 473, in _from_compatible_tensor_list
    flat_value[0].set_shape(self._shape)
  File ""DirectoryName\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 928, in set_shape
    (self.shape, shape))
ValueError: Tensor's shape (2,) is not compatible with supplied shape (None, 2)

Process finished with exit code 1

with mode =0 everything runs correctly:

2019-05-14 10:36:57.849895: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-05-14 10:36:57.859895: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-05-14 10:36:58.041899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.86GiB
2019-05-14 10:36:58.041899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 
2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N 
2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3581 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Initial List:  tf.Tensor([2 3 2], shape=(3,), dtype=int32)
After Slicing:  <TensorSliceDataset shapes: (), types: tf.int32>
After Mapping:  <MapDataset shapes: ((None, 2), (None, 2)), types: (tf.float64, tf.float64)>
After Flattening:  <FlatMapDataset shapes: ((2,), (2,)), types: (tf.float64, tf.float64)>
[TensorShape([2]), TensorShape([2])]
Finished Batch:  tf.Tensor([0.18343479 0.779691  ], shape=(2,), dtype=float64) tf.Tensor([0.09997492 0.45924889], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([0.59685016 0.44583275], shape=(2,), dtype=float64) tf.Tensor([0.33370861 0.14286682], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([0.65088847 0.05641158], shape=(2,), dtype=float64) tf.Tensor([0.61748151 0.61165316], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([0.72199877 0.93855271], shape=(2,), dtype=float64) tf.Tensor([0.00706631 0.02306243], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([7.78765841e-04 9.92211559e-01], shape=(2,), dtype=float64) tf.Tensor([0.52477466 0.39986097], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([0.04666566 0.97375552], shape=(2,), dtype=float64) tf.Tensor([0.61838601 0.38246199], shape=(2,), dtype=float64)
Finished Batch:  tf.Tensor([0.23277134 0.09060643], shape=(2,), dtype=float64) tf.Tensor([0.98323089 0.46676289], shape=(2,), dtype=float64)

Process finished with exit code 0"
28693,[Train on official Docker image & custom TF build (no AVX)] failed to query event: CUDA_ERROR_LAUNCH_FAILED,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): used https://github.com/minimaxir/gpt-2-simple for finetuning
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & Docker
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): custom build TF (without AVX) from https://github.com/yaroslavvb/tensorflow-community-wheels/issues/109
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8' 1.13.1
- Python version: Python 3.5.2
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): Same as docker `tensorflow/tensorflow:latest-gpu-py3` & `tensorflow/tensorflow:devel-gpu-py3` (error reproduce in both)
- CUDA/cuDNN version: 10.0 / 7.4.1.5-1 ( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile )
- GPU model and memory: GeForce GTX 1080 Ti (11 Gb)

**Describe the current behavior**

> root@63f592f02a0e:/gpt2# python imdb_reviews.py
> ...
> 2019-05-14 07:05:22.669722: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
> 2019-05-14 07:05:22.669812: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
> Aborted (core dumped)


**Describe the expected behavior**
Train process (tested in google colab)...


**Code to reproduce the issue**
```bash
# tested on devel-gpu-py3 (where tf wheel builded) and latest-gpu-py3
docker pull tensorflow/tensorflow:latest-gpu-py3

# deep into docker image
docker run --runtime=nvidia -it  -v ~/projects/gpt2-simple:/gpt2 tensorflow/tensorflow:latest-gpu-py3 bash
```

In Docker:
```bash
pip uninstall tensorflow-gpu

cd /gpt2
# from https://github.com/yaroslavvb/tensorflow-community-wheels/issues/109
pip install tensorflow-1.13.1-cp35-cp35m-linux_x86_64.whl

# install gpt2-simple
pip install gpt_2_simple


# from https://gist.github.com/saippuakauppias/4f41ce1072a04588a2bab7dae00f9bb7
python imdb_reviews.py
```

**Other info / logs**
1. [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3176534/tf_env.txt) attached.
2. Its error looks like https://github.com/tensorflow/tensorflow/issues/22477 but I dont understand where solution in that issue.
3. docker run without `-u $(id -u):$(id -g)` because with this I cant uninstall tensorflow:

<details>
  <summary>Example</summary>

```
docker run --runtime=nvidia -u $(id -u):$(id -g) -v ~/projects/gpt2-simple:/gpt2 -it tensorflow/tensorflow:latest-gpu-py3 bash

You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

tf-docker / > pip uninstall tensorflow-gpu
WARNING: The directory '/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
Uninstalling tensorflow-gpu-1.13.1:
  Would remove:
    /usr/local/bin/freeze_graph
    /usr/local/bin/saved_model_cli
    /usr/local/bin/tensorboard
    /usr/local/bin/tf_upgrade_v2
    /usr/local/bin/tflite_convert
    /usr/local/bin/toco
    /usr/local/bin/toco_from_protos
    /usr/local/lib/python3.5/dist-packages/tensorflow/*
    /usr/local/lib/python3.5/dist-packages/tensorflow_gpu-1.13.1.dist-info/*
Proceed (y/n)? y
ERROR: Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/shutil.py"", line 538, in move
    os.rename(src, real_dst)
PermissionError: [Errno 13] Permission denied: '/usr/local/bin/freeze_graph' -> '/tmp/pip-uninstall-n9t_qerr/freeze_graph'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/cli/base_command.py"", line 178, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/commands/uninstall.py"", line 75, in run
    auto_confirm=options.yes, verbose=self.verbosity > 0,
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_install.py"", line 825, in uninstall
    uninstalled_pathset.remove(auto_confirm, verbose)
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_uninstall.py"", line 388, in remove
    moved.stash(path)
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_uninstall.py"", line 277, in stash
    renames(path, new_path)
  File ""/usr/local/lib/python3.5/dist-packages/pip/_internal/utils/misc.py"", line 305, in renames
    shutil.move(old, new)
  File ""/usr/lib/python3.5/shutil.py"", line 553, in move
    os.unlink(src)
PermissionError: [Errno 13] Permission denied: '/usr/local/bin/freeze_graph'
```
</details>

4. Tensorflow debugger with mnist works fine and use GPU (I see it in nvidia-smi).
<details>
  <summary>Log</summary>

```
root@4697d32838f4:/gpt2# python -m tensorflow.python.debug.examples.debug_mnist
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/debug/examples/debug_mnist.py:46: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
2019-05-14 17:07:28.376774: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz
2019-05-14 17:07:28.378926: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6115c40 executing computations on platform Host. Devices:
2019-05-14 17:07:28.378987: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-05-14 17:07:28.598760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-05-14 17:07:28.601785: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x61c1dd0 executing computations on platform CUDA. Devices:
2019-05-14 17:07:28.601829: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-05-14 17:07:28.602453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575
pciBusID: 0000:0b:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2019-05-14 17:07:28.602502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-05-14 17:07:28.605463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-14 17:07:28.605502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-05-14 17:07:28.605525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-05-14 17:07:28.605978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1)
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-05-14 17:07:29.756412: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
Accuracy at step 0: 0.1094
Accuracy at step 1: 0.098
Accuracy at step 2: 0.098
Accuracy at step 3: 0.098
Accuracy at step 4: 0.098
Accuracy at step 5: 0.098
Accuracy at step 6: 0.098
Accuracy at step 7: 0.098
Accuracy at step 8: 0.098
Accuracy at step 9: 0.098
```
</details>"
28692,empty array: [] on prediction (CMLE),"I have implemented a custom tfx example after working trough the tfx taxi example.

The model that is trained in the tfx pipeline is a canned estimator estimator: DNNRegressor.

inside model_fn of custom estimator
```python
tags = tf.feature_column.numeric_column(
        key=_transformed_name(_FEATURE_KEY), shape=[500], dtype=tf.int64)

    return tf.estimator.DNNRegressor(
        config=config,
        feature_columns=[tags],
        hidden_units=[25],
        activation_fn=tf.nn.relu
    )
```

definition of serving receiver

```python
def _example_serving_receiver_fn(transform_output, schema):
    """"""Build the serving in inputs.
    Args:
        transform_output: directory in which the tf-transform model was written
        during the preprocessing step.
        schema: the schema of the input data.
    Returns:
        Tensorflow graph which parses examples, applying tf-transform to them.
    """"""

    raw_feature_spec = _get_raw_feature_spec(schema)
    raw_feature_spec.pop(_LABEL_KEY)

  # if only this function wasn't 100% black-box
    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
        raw_feature_spec, default_batch_size=None)

    serving_input_receiver = raw_input_fn()

    _, transformed_features = (
        saved_transform_io.partially_apply_saved_transform(
            os.path.join(transform_output, transform_fn_io.TRANSFORM_FN_DIR),
            serving_input_receiver.features))

    return tf.estimator.export.ServingInputReceiver(
        transformed_features, serving_input_receiver.receiver_tensors)
```

actually calling serving receiver

```python
    def serving_receiver_fn(): return _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
            hparams.transform_output, schema)

    exporter = tf.estimator.FinalExporter(
        'stackoverflow-tfx', serving_receiver_fn)
    eval_spec = tf.estimator.EvalSpec(
        eval_input_fn,
        steps=hparams.eval_steps,
        exporters=[exporter],
        name='stackoverflow-tfx-eval')
```

The custom implementation of the tfx pipeline runs without problems all the way.
The ml engine model & version exist, the saved model.pb exists.

But when I try to make a prediction to the ml engine model it returns an empty array.
like: {'predictions': []}

I used to make predictions via code but found that the UI also works the same

![image](https://user-images.githubusercontent.com/26407191/57837718-a14cbe00-77c3-11e9-9b7f-3e0540348ac4.png)


Where do I start debugging or does anyone know what could be going on?

Training on ml engine with python version 2.7
ml engine runtimeVersion 1.12 for both training and serving (assuming that means tensrflow 1.12)"
28691,Error installing tf-nightly-2.0-preview with pip,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version: latest tf-nightly-2.0-preview
- Python version: Python 3.6.7
- Installed using virtualenv? pip? conda?: pip



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Official colab link for tensorboard 2.0 demo is able to reproduce the bug:

https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/r2/get_started.ipynb

```
$ pip install -q tf-nightly-2.0-preview

ERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.
```"
28690,tf.lite.TFLiteConverter.from_frozen_graph error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):1.13.1



```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.

```


"
28688,Why is the model file named as *00000-of-00001,"My variables model files is named as *00000-of-00002 or *00001-of-00002. My questions are:
1. What's the difference between them?
2. Why is the model file named as this pattern?
Thanks!"
28687,Using td.cond during training leads to reduced throughput,"In the process of using the resnet50 for imagenet training, we used LARS to update the learning rate and calculate LR at each step of the training. The throughput of the training is about 5500. For this we intend to optimize and calculate the LR operation every few steps to improve throughput. In the original code, we perform compute_lr calculation every step.
I modified the code as shown below：
gg is a tensor used to observe which step of training;
2 is a constant, indicating that lr is calculated every 2 steps.

The code:

def compute_lr()
coumpte_lr
...
stored_lr
...
return lr
def get_larsvalue()
get_stored_lr
...
return lr

tf.cond(tf.cast(tf.equal(tf.mod(gg,2),0),tf.bool),lambda:self.compute_lr(),lambda: self.get_larsvalue())

But after I modified the code, the throughput dropped. After analysis, I think it is because tf.cond is not a lazy operation, it will execute both branches, which is obviously not what I want. I don't know how to code to complete my thoughts now, ask you to help. Thanks"
28685,The cycle detection algorithm in the variable creation has bad performance,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Don’t know
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source): Not used
- GCC/Compiler version (if compiling from source): Not used
- CUDA/cuDNN version: Not related
- GPU model and memory: Not related

**Describe the current behavior**

Maybe related: #17439.

I noticed some slow variable creations. It happens when the initial value is a tensor with complex dependencies.  After some digging, I found that it may be caused by the algorithm used in the cycle detection code:

https://github.com/tensorflow/tensorflow/blob/d1022145203c1edb81a39010da3f61207533091d/tensorflow/python/ops/variables.py#L2505-L2517

**Describe the expected behavior**

Creating a variable should be completed within acceptable time.

**Code to reproduce the issue**


You can reproduce the problem using the following code:

```python
import time

import tensorflow as tf


def _build_tensor(depth):
    fibonacci = [tf.zeros(shape=()), tf.ones(shape=())]

    for _ in range(depth):
        fibonacci.append(fibonacci[-2] + fibonacci[-1])

    return fibonacci[-1]


def main():
    for depth in range(15, 25):
        with tf.Graph().as_default():
            tensor = _build_tensor(depth)

            start_time = time.perf_counter()
            tf.Variable(initial_value=tensor)
            end_time = time.perf_counter()

            print('Depth = {}, Time = {}'.format(depth, end_time - start_time))


if __name__ == '__main__':
    main()
```

**Other info / logs**

Here is my output from running the code:

```
Depth = 15, Time = 0.008952808999310946
Depth = 16, Time = 0.014324793000014324
Depth = 17, Time = 0.019670226999551232
Depth = 18, Time = 0.03077544999996462
Depth = 19, Time = 0.04785999100022309
Depth = 20, Time = 0.07621835800000554
Depth = 21, Time = 0.12341592400025547
Depth = 22, Time = 0.19620911800029717
Depth = 23, Time = 0.31814610099991114
Depth = 24, Time = 0.5071976489998633
```

Notice the time used for creating a variable grows exponentially.

----

The cycle detection algorithm could be optimized to have a linear time complexity. Also, the algorithm should avoid stack overflow if the initial value has a long dependency chain."
28684,"Error using tensorflow module, even appearing in the list shown by pip list","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version:
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- GPU model and memory: MX920



**Describe the problem**

I can't use tensorflow module, even appearing in the list shown by pip list

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I executed the 3 first commands: 
_python3 --version
pip3 --version
virtualenv --version_

But, the answer I get from the 1st command was: ""python3"" is not recognized as a intern or extern command, operational program or a batch file

Then I just keep with the installation, because the virtualenv and the pip3 were updated, and after that, I tried to do the command:
_virtualenv --system-site-packages -p python3 ./venv_
But the answer I get was:
_The path python3 (from --python=python3) does not exist_

so, I couldn't create the virtual environment.
After that, I wrote the command:
_pip3 install --user --upgrade tensorflow_

And the command:
_pip list_

In the answer of the last one, the list showed that the tensorflow was installed, but when I tried to run an example provided by the TensorFlow site, PyCharm couldn't find the module, and now I don't know what to do.
Also, when I write the command: 
_python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""_

I get this answer:
_2019-05-13 21:40:50.182555: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
tf.Tensor(1277.5322, shape=(), dtype=float32)_
"
28681,Keras model to tensorflow trainable pb file and checkpoint,"I tried to convert keras h5 model into pb and checkpoint files.
```python
from keras import backend as K
from keras.models import load_model
import tensorflow as tf

model = load_model('model/my_model.h5')

K.set_learning_phase(0) #0 : test, 1 : train

sess = K.get_session()

saver = tf.train.Saver()
saver.save(sess, 'keras/keras.ckpt')

sess.graph.as_default()
graph = sess.graph

with open('keras/keras.pb', 'wb') as f:
    f.write(graph.as_graph_def().SerializeToString())
```

I succeeded in bringing this pb and checkpoint to tensorflow and running the graph, obtaining an inference.
But the problem is, I have no idea how to train this thing.
Usually, tensorflow models have a training operation, and we just have to sess.run that operation, but I'm assuming keras is working in a different way somehow.
Is it possible to train keras models when they are converted to tensorflow models?"
28680,"[TF 2.0 API Docs] tf  - typo, extra symbols in pip install doc copy code","
## Existing URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf

## Description of issue (what needs changing):

Copyable code block has extra characters `"",` at the end:

```pip install tensorflow==2.0.0-alpha0"",```

### Clear description

Copyable code block should be changed to the following to make copying easier.

```pip install tensorflow==2.0.0-alpha0```

### Correct links

This is the overview for the TF 2.0 API, but I couldn't find the docstring in the referred source: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/__init__.py

### Submit a pull request?

Would love to, but couldn't find in source where this was a docstring or being generated :(
"
28679,Batch matrix-matrix product slower than Pytorch,"I'm doing a batch matrix multiplication using matmul but it seems to be slower than using [bmm](https://pytorch.org/docs/stable/torch.html#torch.bmm) function from Pytorch. Specifically,

```
sentence = tf.constant(tf.random.normal([10240, 32, 3]))
w = tf.constant(tf.random.normal([10240, 3, 1]))

%timeit tf.matmul(sentence, w)
725 µs ± 9.57 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

compared to 

```
sentence = torch.randn(10240, 32, 3).to(device)
w = torch.randn(10240, 3, 1).to(device)

%timeit torch.bmm(sentence, w)
78.5 µs ± 14 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

Is there anyway to speed up the batch matrix-matrix product computation?

Also, normal matrix multiplication seems to be slightly slower than Pytorch

```
sentence = tf.constant(tf.random.normal([10240, 32]))
w = tf.constant(tf.random.normal([32, 1]))

%timeit tf.matmul(sentence, w)
42.6 µs ± 807 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

```

compared to

```
sentence = torch.randn(10240, 32).to(device)
w = torch.randn(32, 1).to(device)

%timeit torch.mm(sentence, w)
19.2 µs ± 1.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```
but I guess that's between the acceptable limits.

Tested with eager mode both on 1.13 and 2.0-alpha.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): Both 1.13 and 2.0-alpha
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0/CuDNN 7.3.1
- GPU model and memory: NVidia Titan V

"
28678,atch matrix-matrix product,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28677,atch matrix-matrix product,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28676,atch matrix-matrix product,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28675,Build TF without AVX from docker devel-gpu-py3 failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): -
- TensorFlow version: -
- Python version: 3.5
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): same as docker image `devel-gpu-py3`
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080Ti



**Describe the problem**
I have old processors (2 x Xeon X5660) and GTX 1080 Ti and I want to play with TF, but installation tensorflow on my test Ubuntu 16.04 failed because my processors dont have AVX instructions.
I need a custiom build tensorflow for python 3.5, GPU and no AVX.
Therefore, I try to build tensorflow it myself.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
# get image from hub
docker pull tensorflow/tensorflow:devel-gpu-py3

# run image
docker run --runtime=nvidia -it  -v ~/projects/tf:/my-devel tensorflow/tensorflow:devel-gpu-py3 bash

# downgrade bazel (because tf 0.13.1 need bazel 0.21, not 0.24.1 as in docker image)
rm -rf /usr/local/bin/bazel
wget -O /bazel/installer.sh ""https://github.com/bazelbuild/bazel/releases/download/0.21.0/bazel-0.21.0-installer-linux-x86_64.sh""
chmod +x /bazel/installer.sh
/bazel/installer.sh


cd /tensorflow_src
git checkout v1.13.1

# create venv (I thought it would help to get around the error but did not help.)
virtualenv --python=/usr/local/bin/python venv
source venv/bin/activate

# install deps
pip --no-cache-dir install Pillow h5py keras_applications keras_preprocessing matplotlib mock numpy scipy sklearn pandas portpicker

export TF_ENABLE_XLA=1
export CC_OPT_FLAGS=""-march=native -mno-avx""


./configure

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```


**Any other info / logs**
After run last command I get:
```
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (364 packages loaded, 23677 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/double_conversion/BUILD.bazel:12:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 127)
/usr/bin/env: 'python': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 76.013s, Critical Path: 0.10s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

But:
```
(venv) root@6dae59a3faf7:/tensorflow_src# /usr/bin/env 'python'
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 
```


Why bazel or TF not found python? In configuration path to python is valid and it seems to be ok.

Its issue looks like very old issues with error message ""/usr/bin/env: 'python': No such file or directory"", but they not receive answers."
28673,Unsupported Operation (MEAN) while trying to apply GpuDelegate to tflite ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I am writing a custom Android app, built in Android Studio using the method mentioned [here](https://www.tensorflow.org/lite/performance/gpu#android_with_android_studio)
- Host system: Linux Ubuntu 16.04
- Target Device: Android s8+
- tflite installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite:0.0.0-nightly)
- tflite-gpu installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly)
- GPU model and memory: Adreno 540

**Describe the current behavior**
When I attempt to load my .tflite model on the Android (after applying the GpuDelegate as described [here](https://www.tensorflow.org/lite/performance/gpu#android)), it fails saying that 'MEAN' is an unsupported operation (see traceback below)

**Describe the expected behavior**
The net loads and runs fine if I do not attempt to apply the GPU delegate before the attempting to load it. The only difference in the code is the following lines:
```
c.delegate = new GpuDelegate();
c.tfliteOptions.addDelegate(c.delegate);
```

**Code to reproduce the issue**
Unfortunately I cannot provide my .tflite file, but it obviously contains a 'MEAN' operation. Is there a script anywhere in the tflite repo that prints out all the operations contained in a .tflite file? I looked briefly but was unable to find one.

**Other info / logs**
```
05-13 12:09:56.101 3348-3348/org.qus.viewqualTFLite E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.qus.viewqualTFLite, PID: 3348
    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.qus.viewqualTFLite/org.qus.viewqualTFLite.ViewQualActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:
    MEAN: Operation is not supported.
    First 43 operations will run on the GPU, and the remaining 7 on the CPU.TfLiteGpuDelegate Prepare: Tensor ref has unsupported number of dimensions: 5Node number 50 (TfLiteGpuDelegate) failed to prepare.
    
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2955)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3030)
        at android.app.ActivityThread.-wrap11(Unknown Source:0)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1696)
        at android.os.Handler.dispatchMessage(Handler.java:105)
        at android.os.Looper.loop(Looper.java:164)
        at android.app.ActivityThread.main(ActivityThread.java:6938)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)
     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:
    MEAN: Operation is not supported.
    First 43 operations will run on the GPU, and the remaining 7 on the CPU.TfLiteGpuDelegate Prepare: Tensor ref has unsupported number of dimensions: 5Node number 50 (TfLiteGpuDelegate) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at org.qus.viewqualTFLite.TensorFlowQUSRunner.create(TensorFlowQUSRunner.java:154)
        at org.qus.viewqualTFLite.ViewQualActivity.onCreate(ViewQualActivity.java:200)
        at android.app.Activity.performCreate(Activity.java:7183)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1220)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2908)
            ... 9 more
```"
28671,"Exception when import: DLL load failed (Windows 10, Tensorflow 1.12.0, python 3.6.7)","I have installed Python 3.6.7 on my Windows 10 machine. Afterwards I downloaded with the tensorflow module: pip install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.12.0-cp36-cp36m-win_amd64.whl

On the computer is Visual Studio 2019 installed with additional MSVC v141 (VS 2017) and MSVC v140 (VS 2015).

When I run import tensorflow as tf in python, I get the following error:

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\username\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

What I tried so far:

- checked if I am able to import dependencies (absl-py-0.7.1 astor-0.7.1. ...):
every module gets loaded except tensorflow_estimator and tensorflow

- reinstalled visual studio, python, tensorflow and tried multiple versions

Does anyone have another idea?
"
28669,AttributeError: '_OptionsDataset' object has no attribute 'output_shapes',"Downloading / extracting dataset imdb_reviews (?? GiB) to /home/lucifer/tensorflow_datasets/imdb_reviews/subwords8k/0.0.1...
Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]
Dl Size...:   0%|          | 0/80 [00:01<?, ? MiB/s]
Dl Completed...:   0%|          | 0/1 [00:41<?, ? url/s]MiB]
Dl Size...:   1%|▏         | 1/80 [00:41<54:16, 41.23s/ MiB]
Dl Completed...:   0%|          | 0/1 [01:52<?, ? url/s]/ MiB]
Dl Size...:   2%|▎         | 2/80 [01:52<1:05:24, 50.31s/ MiB]
Dl Completed...:   0%|          | 0/1 [02:12<?, ? url/s]MiB]  
Dl Size...:   4%|▍         | 3/80 [02:12<52:38, 41.02s/ MiB]
Dl Completed...:   0%|          | 0/1 [02:55<?, ? url/s]MiB]
Dl Size...:   5%|▌         | 4/80 [02:55<52:45, 41.65s/ MiB]
Dl Completed...:   0%|          | 0/1 [03:01<?, ? url/s]MiB]
Dl Size...:   6%|▋         | 5/80 [03:01<38:43, 30.98s/ MiB]
Dl Completed...:   0%|          | 0/1 [03:34<?, ? url/s]MiB]
Dl Size...:   8%|▊         | 6/80 [03:34<39:07, 31.72s/ MiB]
Dl Completed...:   0%|          | 0/1 [03:39<?, ? url/s]MiB]
Dl Size...:   9%|▉         | 7/80 [03:39<28:48, 23.68s/ MiB]

...........

Dl Size...:  99%|█████████▉| 79/80 [38:44<00:42, 42.73s/ MiB]
Dl Completed...:   0%|          | 0/1 [40:17<?, ? url/s] MiB]
Dl Size...: 100%|██████████| 80/80 [40:17<00:00, 57.89s/ MiB]
Dl Completed...:   0%|          | 0/1 [40:54<?, ? url/s]     
Dl Size...: 81 MiB [40:54, 51.50s/ MiB]
Dl Completed...:   0%|          | 0/1 [42:18<?, ? url/s]
Dl Size...: 82 MiB [42:18, 61.31s/ MiB]
Dl Completed...:   0%|          | 0/1 [42:50<?, ? url/s]
Dl Size...: 83 MiB [42:50, 52.68s/ MiB]
Dl Completed...:   0%|          | 0/1 [43:56<?, ? url/s]

........

Dl Size...: 154 MiB [1:22:41, 80.27s/ MiB]
Dl Completed...: 100%|██████████| 1/1 [1:23:56<00:00, 4554.05s/ url]
Dl Size...: 155 MiB [1:23:56, 78.58s/ MiB]
Dl Completed...: 100%|██████████| 1/1 [1:24:53<00:00, 4554.05s/ url]
Dl Completed...: 2 url [1:25:03, 3352.75s/ url]                     
Dl Size...: 156 MiB [1:25:03, 72.26s/ MiB]

25000 examples [00:28, 863.60 examples/s] 
Shuffling...:   0%|          | 0/10 [00:00<?, ? shard/s]WARNING: Logging before flag parsing goes to stderr.
W0513 21:39:05.540721 140294086346560 deprecation.py:323] From /home/lucifer/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/file_format_adapter.py:249: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`

Reading...: 0 examples [00:00, ? examples/s]
Reading...: 2500 examples [00:00, 230117.41 examples/s]
Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]
Writing...: 100%|██████████| 2500/2500 [00:00<00:00, 172112.14 examples/s]
Reading...: 0 examples [00:00, ? examples/s]
Reading...: 2500 examples [00:00, 198575.13 examples/s]
Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]
Writing...: 100%|██████████| 2500/2500 [00:00<00:00, 191566.22 examples/s]
Reading...: 0 examples [00:00, ? examples/s]
Reading...: 2500 examples [00:00, 203007.82 examples/s]
Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]

........

17342 examples [00:02, 5761.24 examples/s]
17994 examples [00:03, 5968.74 examples/s]
18644 examples [00:03, 6116.94 examples/s]
19303 examples [00:03, 6249.34 examples/s]
19966 examples [00:03, 6358.77 examples/s]
20624 examples [00:03, 6421.09 examples/s]
21278 examples [00:03, 6428.01 examples/s]
21935 examples [00:03, 6468.08 examples/s]
22588 examples [00:03, 6486.41 examples/s]
23241 examples [00:03, 6458.44 examples/s]
23890 examples [00:03, 6432.71 examples/s]
24551 examples [00:04, 6482.91 examples/s]
25000 examples [00:04, 5994.04 examples/s]
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-31d575d3ee06> in <module>
      1 dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
----> 2                           as_supervised=True)
      3 train_dataset, test_dataset = dataset['train'], dataset['test']

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py in load(name, split, data_dir, batch_size, download, as_supervised, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs)
    251   if download:
    252     download_and_prepare_kwargs = download_and_prepare_kwargs or {}
--> 253     dbuilder.download_and_prepare(**download_and_prepare_kwargs)
    254 
    255   if as_dataset_kwargs is None:

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    234         else:  # Mode is forced or stats do not exists yet
    235           logging.info(""Computing statistics."")
--> 236           self.info.compute_dynamic_properties()
    237         # Set checksums of downloaded (or cached) files, and size:
    238         self.info.download_checksums = dl_manager.recorded_download_checksums

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_info.py in compute_dynamic_properties(self)
    243 
    244   def compute_dynamic_properties(self):
--> 245     self._compute_dynamic_properties(self._builder)
    246     self._fully_initialized = True
    247 

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_info.py in _compute_dynamic_properties(self, builder)
    256         # Fill DatasetFeatureStatistics.
    257         dataset_feature_statistics, schema = get_dataset_feature_statistics(
--> 258             builder, split_name)
    259 
    260         # Add the statistics to this split.

~/anaconda3/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_info.py in get_dataset_feature_statistics(builder, split)
    488   # Start here, we've processed all examples.
    489 
--> 490   output_shapes_dict = dataset.output_shapes
    491   output_types_dict = dataset.output_types
    492 

AttributeError: '_OptionsDataset' object has no attribute 'output_shapes'"
28668,tf.keras.models.load_model fails on Sequential model,"**System information**
- TensorFlow installed from: `pip`
- TensorFlow version: `tf-nightly-2.0-preview-2.0.0.dev20190513`
- Python version: 3.6.7

**Describe the current behavior**

Attempting `tf.keras.models.load_model` on a `Sequential` model throws

```sh
ValueError: You are trying to load a weight file containing 2 layers into a model with 0 layers.
```

Might be caused by a `layers`/`_layers` mismatch as mentioned [here](https://github.com/keras-team/keras/issues/10417#issuecomment-418511814). Not sure if this is the problem but [`_clone_sequential_model`](https://github.com/tensorflow/tensorflow/blob/2c2d508aa2947ede05cfa195139b176d6cdc9056/tensorflow/python/keras/models.py#L218) uses `model._layers` whereas [`save_model_to_hdf5`](https://github.com/tensorflow/tensorflow/blob/30f682e776fbcff8d2da3c3cc0e8d12e1b3dde12/tensorflow/python/keras/saving/hdf5_format.py#L104) accesses `model.layers`.

**Minimal example**

```py
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.Input(3), tf.keras.layers.Dense(3), tf.keras.layers.Dense(1)]
)
model.compile(loss=""mse"", optimizer=""adam"")
model.fit(tf.constant([[1, 2, 3], [4, 5, 6]]), tf.constant([1, 2]))
model.save(""model.h5"")
restored_model = tf.keras.models.load_model(""model.h5"")
```"
28667,Can't Run code on ubuntu 18.04 ,"I tried to run the code in my ubuntu system but couldn't run the code
can u suggest a way to run the code without gpu support and it would be great if u say with few proper steps"
28666,I can not import tf successfully although it doesn't output any error,"<em> </em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.8.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.16.1
- GCC/Compiler version (if compiling from source): gcc 6.3
- CUDA/cuDNN version: CUDA_9.0; cuDNN_7.1
- GPU model and memory: GTX 1080ti



**Describe the problem**

I follow the guide : https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/
but when I used cmd or spyder to write&run tensorflow with 'import tensorflow as tf', the console will run a moment and output none


**Any other info / logs**

![spyder](https://user-images.githubusercontent.com/49911630/57619542-1d2ae880-75c1-11e9-97b8-c417790f4afb.png)
"
28664,tensorflow-gpu import error,"Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\shoba\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


Please help. I see that you asked to submit new ticket since its different for each system spec. 
kindly help what should i do to resolve this issue. i have installed python 3.6.8. cuda 9.0

thanks
shobana"
28663,meta_optimizer.cc layout failed on frozen_graph,"TF 2.0 preview gpu
Python 3.6.3

Importing a frozen graph def leads to an error: `... meta_optimizer.cc:486] layout failed: Invalid argument: Invalid graph: Frame ids for node ...`

**What I did**
Checkpoint: http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz

Run in `research/slim/` found here: https://github.com/tensorflow/models/tree/master/research/slim
1. GraphDef:
```python export_inference_graph.py --model_name=inception_v3 --output_file=MODELS/inception_v3_inf_graph.pb```
2. Obtain a copy of: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py and place it in `research/slim`
3. Create Frozen Graph:
```python freeze_graph.py --input_graph=MODELS/inception_v3_inf_graph.pb --input_checkpoint=MODELS/inception_v3.ckpt --input_binary=true --output_graph=MODELS/frozen_inception_v3.pb --output_node_names=InceptionV3/Predictions/Reshape```
4. Pack into `tar.gz`.

I then imported the frozen graph with
```python
import tensorflow as tf
import tarfile
import os

from tensorflow.core.framework import graph_pb2
from tensorflow.python.framework import importer

INCEPTION_V3_FILE_NAME = 'frozen_inception_v3.tar.gz'
INCEPTION_V3_FROZEN_GRAPH = 'frozen_inception_v3.pb'
# Shape[None x 299 x 299 x 3]
INCEPTION_V3_INPUT = 'input:0'
# Shape[None x 1 x 1 x 2048]
INCEPTION_V3_OUTPUT = 'InceptionV3/Logits/AvgPool_1a_8x8/AvgPool:0'


def graph_def_from_tarball(filename, tar_filename):
    with tarfile.open(tar_filename, 'r:gz') as tar:
        proto_str = tar.extractfile(filename).read()
    return graph_pb2.GraphDef.FromString(proto_str)


graph_def = graph_def_from_tarball(
    INCEPTION_V3_FROZEN_GRAPH,
    INCEPTION_V3_FILE_NAME
)


@tf.function
def run_classifier(images):

    def classifier_fn(images):
        input_map = {INCEPTION_V3_INPUT: images}
        output_tensor = [INCEPTION_V3_OUTPUT]
        classifier_outputs = importer.import_graph_def(
            graph_def,
            input_map,
            output_tensor,
            'InceptionV3'
        )
        return classifier_outputs[0]

    logits = tf.map_fn(
        fn=classifier_fn,
        elems=tf.expand_dims(images, 0),
        parallel_iterations=1,
        back_prop=False,
        swap_memory=True,
        dtype=tf.float32,
        name='RunClassifier')

    logits = tf.concat(tf.unstack(logits), 0)
    return logits


images = tf.random.normal([1, 299, 299, 3])

r = run_classifier(images)
print(r)

```

Error message (the node it errors on varies):
```
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:486] layout failed: Invalid argument: Invalid graph: Frame ids for node RunClassifier/while/body/_1/InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean does not match frame ids for it's fanout RunClassifier/while/body/_1/InceptionV3/InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/FusedBatchNorm/Mul2
```
Sometimes it is:
```
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Invalid graph: Frame ids for node RunClassifier/while/body/_1/InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta does not match frame ids for it's fanout RunClassifier/while/body/_1/InceptionV3/InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/FusedBatchNorm/Offset
```

**What I expect**
No error or some better description of what went wrong.
Note: The imported graph def works, it just throws this error and it is unclear to me if it does the right thing considering the error message."
28662,Unable to use custom problem for visualizing attention,"I am trying to run the code in the TransformerVisualization.ipynb notebook to visualize the attention on a user defined problem, however I am unable to do so. When running the following code

```
usr_dir.import_usr_dir('./transformer')
visualizer = visualization.AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)
```

I get the following error

```
INFO:tensorflow:Importing user module transformer from path /content/Deep-Learning
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-20-fd5211124aa0> in <module>()
      1 usr_dir.import_usr_dir('./transformer')
----> 2 visualizer = visualization.AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)

4 frames
/usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/registry.py in problem(name)
    256                   ] + all_problem_names
    257     error_msg = ""\n  * "".join(error_lines)
--> 258     raise LookupError(error_msg)
    259   return _PROBLEMS[base_name](was_reversed=was_reversed, was_copy=was_copy)
    260 

LookupError: code_transformer not in the set of supported problems:
  * algorithmic_addition_binary40
  * algorithmic_addition_decimal40
  * algorithmic_cipher_shift200
  * algorithmic_cipher_shift5
  * algorithmic_cipher_vigenere200
  * algorithmic_cipher_vigenere5
  * algorithmic_identity_binary40
  * algorithmic_identity_decimal40
  * algorithmic_multiplication_binary40
  * algorithmic_multiplication_decimal40
  * algorithmic_reverse_binary40
  * algorithmic_reverse_binary40_test
  * algorithmic_reverse_decimal40
  * algorithmic_reverse_nlplike32k
  * algorithmic_reverse_nlplike8k
  * algorithmic_shift_decimal40
  * algorithmic_sort_problem
  * audio_timit_characters_tune
  * audio_timit_tokens8k_test
  * audio_timit_tokens8k_tune
  * babi_qa_concat_all_tasks_10k
  * babi_qa_concat_all_tasks_1k
  * babi_qa_concat_task10_10k
  * babi_qa_concat_task10_1k
  * babi_qa_concat_task11_10k
  * babi_qa_concat_task11_1k
  * babi_qa_concat_task12_10k
  * babi_qa_concat_task12_1k
  * babi_qa_concat_task13_10k
  * babi_qa_concat_task13_1k
  * babi_qa_concat_task14_10k
  * babi_qa_concat_task14_1k
  * babi_qa_concat_task15_10k
  * babi_qa_concat_task15_1k
  * babi_qa_concat_task16_10k
  * babi_qa_concat_task16_1k
  * babi_qa_concat_task17_10k
  * babi_qa_concat_task17_1k
  * babi_qa_concat_task18_10k
  * babi_qa_concat_task18_1k
  * babi_qa_concat_task19_10k
  * babi_qa_concat_task19_1k
  * babi_qa_concat_task1_10k
  * babi_qa_concat_task1_1k
  * babi_qa_concat_task20_10k
  * babi_qa_concat_task20_1k
  * babi_qa_concat_task2_10k
  * babi_qa_concat_task2_1k
  * babi_qa_concat_task3_10k
  * babi_qa_concat_task3_1k
  * babi_qa_concat_task4_10k
  * babi_qa_concat_task4_1k
  * babi_qa_concat_task5_10k
  * babi_qa_concat_task5_1k
  * babi_qa_concat_task6_10k
  * babi_qa_concat_task6_1k
  * babi_qa_concat_task7_10k
  * babi_qa_concat_task7_1k
  * babi_qa_concat_task8_10k
  * babi_qa_concat_task8_1k
  * babi_qa_concat_task9_10k
  * babi_qa_concat_task9_1k
  * cola
  * cola_characters
  * common_voice
  * common_voice_clean
  * common_voice_noisy
  * common_voice_train_full_test_clean
  * genomics_expression_cage10
  * genomics_expression_gm12878
  * genomics_expression_l262k
  * github_function_docstring
  * gym_air_raid-v0_random
  * gym_air_raid-v4_random
  * gym_air_raid_deterministic-v0_random
  * gym_air_raid_deterministic-v4_random
  * gym_air_raid_no_frameskip-v0_random
  * gym_air_raid_no_frameskip-v4_random
  * gym_alien-v0_random
  * gym_alien-v4_random
  * gym_alien_deterministic-v0_random
  * gym_alien_deterministic-v4_random
  * gym_alien_no_frameskip-v0_random
  * gym_alien_no_frameskip-v4_random
  * gym_amidar-v0_random
  * gym_amidar-v4_random
  * gym_amidar_deterministic-v0_random
  * gym_amidar_deterministic-v4_random
  * gym_amidar_no_frameskip-v0_random
  * gym_amidar_no_frameskip-v4_random
  * gym_assault-v0_random
  * gym_assault-v4_random
  * gym_assault_deterministic-v0_random
  * gym_assault_deterministic-v4_random
  * gym_assault_no_frameskip-v0_random
  * gym_assault_no_frameskip-v4_random
  * gym_asterix-v0_random
  * gym_asterix-v4_random
  * gym_asterix_deterministic-v0_random
  * gym_asterix_deterministic-v4_random
  * gym_asterix_no_frameskip-v0_random
  * gym_asterix_no_frameskip-v4_random
  * gym_asteroids-v0_random
  * gym_asteroids-v4_random
  * gym_asteroids_deterministic-v0_random
  * gym_asteroids_deterministic-v4_random
  * gym_asteroids_no_frameskip-v0_random
  * gym_asteroids_no_frameskip-v4_random
  * gym_atlantis-v0_random
  * gym_atlantis-v4_random
  * gym_atlantis_deterministic-v0_random
  * gym_atlantis_deterministic-v4_random
  * gym_atlantis_no_frameskip-v0_random
  * gym_atlantis_no_frameskip-v4_random
  * gym_bank_heist-v0_random
  * gym_bank_heist-v4_random
  * gym_bank_heist_deterministic-v0_random
  * gym_bank_heist_deterministic-v4_random
  * gym_bank_heist_no_frameskip-v0_random
  * gym_bank_heist_no_frameskip-v4_random
  * gym_battle_zone-v0_random
  * gym_battle_zone-v4_random
  * gym_battle_zone_deterministic-v0_random
  * gym_battle_zone_deterministic-v4_random
  * gym_battle_zone_no_frameskip-v0_random
  * gym_battle_zone_no_frameskip-v4_random
  * gym_beam_rider-v0_random
  * gym_beam_rider-v4_random
  * gym_beam_rider_deterministic-v0_random
  * gym_beam_rider_deterministic-v4_random
  * gym_beam_rider_no_frameskip-v0_random
  * gym_beam_rider_no_frameskip-v4_random
  * gym_berzerk-v0_random
  * gym_berzerk-v4_random
  * gym_berzerk_deterministic-v0_random
  * gym_berzerk_deterministic-v4_random
  * gym_berzerk_no_frameskip-v0_random
  * gym_berzerk_no_frameskip-v4_random
  * gym_bowling-v0_random
  * gym_bowling-v4_random
  * gym_bowling_deterministic-v0_random
  * gym_bowling_deterministic-v4_random
  * gym_bowling_no_frameskip-v0_random
  * gym_bowling_no_frameskip-v4_random
  * gym_boxing-v0_random
  * gym_boxing-v4_random
  * gym_boxing_deterministic-v0_random
  * gym_boxing_deterministic-v4_random
  * gym_boxing_no_frameskip-v0_random
  * gym_boxing_no_frameskip-v4_random
  * gym_breakout-v0_random
  * gym_breakout-v4_random
  * gym_breakout_deterministic-v0_random
  * gym_breakout_deterministic-v4_random
  * gym_breakout_no_frameskip-v0_random
  * gym_breakout_no_frameskip-v4_random
  * gym_carnival-v0_random
  * gym_carnival-v4_random
  * gym_carnival_deterministic-v0_random
  * gym_carnival_deterministic-v4_random
  * gym_carnival_no_frameskip-v0_random
  * gym_carnival_no_frameskip-v4_random
  * gym_centipede-v0_random
  * gym_centipede-v4_random
  * gym_centipede_deterministic-v0_random
  * gym_centipede_deterministic-v4_random
  * gym_centipede_no_frameskip-v0_random
  * gym_centipede_no_frameskip-v4_random
  * gym_chopper_command-v0_random
  * gym_chopper_command-v4_random
  * gym_chopper_command_deterministic-v0_random
  * gym_chopper_command_deterministic-v4_random
  * gym_chopper_command_no_frameskip-v0_random
  * gym_chopper_command_no_frameskip-v4_random
  * gym_crazy_climber-v0_random
  * gym_crazy_climber-v4_random
  * gym_crazy_climber_deterministic-v0_random
  * gym_crazy_climber_deterministic-v4_random
  * gym_crazy_climber_no_frameskip-v0_random
  * gym_crazy_climber_no_frameskip-v4_random
  * gym_demon_attack-v0_random
  * gym_demon_attack-v4_random
  * gym_demon_attack_deterministic-v0_random
  * gym_demon_attack_deterministic-v4_random
  * gym_demon_attack_no_frameskip-v0_random
  * gym_demon_attack_no_frameskip-v4_random
  * gym_double_dunk-v0_random
  * gym_double_dunk-v4_random
  * gym_double_dunk_deterministic-v0_random
  * gym_double_dunk_deterministic-v4_random
  * gym_double_dunk_no_frameskip-v0_random
  * gym_double_dunk_no_frameskip-v4_random
  * gym_elevator_action-v0_random
  * gym_elevator_action-v4_random
  * gym_elevator_action_deterministic-v0_random
  * gym_elevator_action_deterministic-v4_random
  * gym_elevator_action_no_frameskip-v0_random
  * gym_elevator_action_no_frameskip-v4_random
  * gym_enduro-v0_random
  * gym_enduro-v4_random
  * gym_enduro_deterministic-v0_random
  * gym_enduro_deterministic-v4_random
  * gym_enduro_no_frameskip-v0_random
  * gym_enduro_no_frameskip-v4_random
  * gym_fishing_derby-v0_random
  * gym_fishing_derby-v4_random
  * gym_fishing_derby_deterministic-v0_random
  * gym_fishing_derby_deterministic-v4_random
  * gym_fishing_derby_no_frameskip-v0_random
  * gym_fishing_derby_no_frameskip-v4_random
  * gym_freeway-v0_random
  * gym_freeway-v4_random
  * gym_freeway_deterministic-v0_random
  * gym_freeway_deterministic-v4_random
  * gym_freeway_no_frameskip-v0_random
  * gym_freeway_no_frameskip-v4_random
  * gym_frostbite-v0_random
  * gym_frostbite-v4_random
  * gym_frostbite_deterministic-v0_random
  * gym_frostbite_deterministic-v4_random
  * gym_frostbite_no_frameskip-v0_random
  * gym_frostbite_no_frameskip-v4_random
  * gym_gopher-v0_random
  * gym_gopher-v4_random
  * gym_gopher_deterministic-v0_random
  * gym_gopher_deterministic-v4_random
  * gym_gopher_no_frameskip-v0_random
  * gym_gopher_no_frameskip-v4_random
  * gym_gravitar-v0_random
  * gym_gravitar-v4_random
  * gym_gravitar_deterministic-v0_random
  * gym_gravitar_deterministic-v4_random
  * gym_gravitar_no_frameskip-v0_random
  * gym_gravitar_no_frameskip-v4_random
  * gym_hero-v0_random
  * gym_hero-v4_random
  * gym_hero_deterministic-v0_random
  * gym_hero_deterministic-v4_random
  * gym_hero_no_frameskip-v0_random
  * gym_hero_no_frameskip-v4_random
  * gym_ice_hockey-v0_random
  * gym_ice_hockey-v4_random
  * gym_ice_hockey_deterministic-v0_random
  * gym_ice_hockey_deterministic-v4_random
  * gym_ice_hockey_no_frameskip-v0_random
  * gym_ice_hockey_no_frameskip-v4_random
  * gym_jamesbond-v0_random
  * gym_jamesbond-v4_random
  * gym_jamesbond_deterministic-v0_random
  * gym_jamesbond_deterministic-v4_random
  * gym_jamesbond_no_frameskip-v0_random
  * gym_jamesbond_no_frameskip-v4_random
  * gym_journey_escape-v0_random
  * gym_journey_escape-v4_random
  * gym_journey_escape_deterministic-v0_random
  * gym_journey_escape_deterministic-v4_random
  * gym_journey_escape_no_frameskip-v0_random
  * gym_journey_escape_no_frameskip-v4_random
  * gym_kangaroo-v0_random
  * gym_kangaroo-v4_random
  * gym_kangaroo_deterministic-v0_random
  * gym_kangaroo_deterministic-v4_random
  * gym_kangaroo_no_frameskip-v0_random
  * gym_kangaroo_no_frameskip-v4_random
  * gym_krull-v0_random
  * gym_krull-v4_random
  * gym_krull_deterministic-v0_random
  * gym_krull_deterministic-v4_random
  * gym_krull_no_frameskip-v0_random
  * gym_krull_no_frameskip-v4_random
  * gym_kung_fu_master-v0_random
  * gym_kung_fu_master-v4_random
  * gym_kung_fu_master_deterministic-v0_random
  * gym_kung_fu_master_deterministic-v4_random
  * gym_kung_fu_master_no_frameskip-v0_random
  * gym_kung_fu_master_no_frameskip-v4_random
  * gym_montezuma_revenge-v0_random
  * gym_montezuma_revenge-v4_random
  * gym_montezuma_revenge_deterministic-v0_random
  * gym_montezuma_revenge_deterministic-v4_random
  * gym_montezuma_revenge_no_frameskip-v0_random
  * gym_montezuma_revenge_no_frameskip-v4_random
  * gym_ms_pacman-v0_random
  * gym_ms_pacman-v4_random
  * gym_ms_pacman_deterministic-v0_random
  * gym_ms_pacman_deterministic-v4_random
  * gym_ms_pacman_no_frameskip-v0_random
  * gym_ms_pacman_no_frameskip-v4_random
  * gym_name_this_game-v0_random
  * gym_name_this_game-v4_random
  * gym_name_this_game_deterministic-v0_random
  * gym_name_this_game_deterministic-v4_random
  * gym_name_this_game_no_frameskip-v0_random
  * gym_name_this_game_no_frameskip-v4_random
  * gym_phoenix-v0_random
  * gym_phoenix-v4_random
  * gym_phoenix_deterministic-v0_random
  * gym_phoenix_deterministic-v4_random
  * gym_phoenix_no_frameskip-v0_random
  * gym_phoenix_no_frameskip-v4_random
  * gym_pitfall-v0_random
  * gym_pitfall-v4_random
  * gym_pitfall_deterministic-v0_random
  * gym_pitfall_deterministic-v4_random
  * gym_pitfall_no_frameskip-v0_random
  * gym_pitfall_no_frameskip-v4_random
  * gym_pong-v0_random
  * gym_pong-v4_random
  * gym_pong_deterministic-v0_random
  * gym_pong_deterministic-v4_random
  * gym_pong_no_frameskip-v0_random
  * gym_pong_no_frameskip-v4_random
  * gym_pooyan-v0_random
  * gym_pooyan-v4_random
  * gym_pooyan_deterministic-v0_random
  * gym_pooyan_deterministic-v4_random
  * gym_pooyan_no_frameskip-v0_random
  * gym_pooyan_no_frameskip-v4_random
  * gym_private_eye-v0_random
  * gym_private_eye-v4_random
  * gym_private_eye_deterministic-v0_random
  * gym_private_eye_deterministic-v4_random
  * gym_private_eye_no_frameskip-v0_random
  * gym_private_eye_no_frameskip-v4_random
  * gym_qbert-v0_random
  * gym_qbert-v4_random
  * gym_qbert_deterministic-v0_random
  * gym_qbert_deterministic-v4_random
  * gym_qbert_no_frameskip-v0_random
  * gym_qbert_no_frameskip-v4_random
  * gym_riverraid-v0_random
  * gym_riverraid-v4_random
  * gym_riverraid_deterministic-v0_random
  * gym_riverraid_deterministic-v4_random
  * gym_riverraid_no_frameskip-v0_random
  * gym_riverraid_no_frameskip-v4_random
  * gym_road_runner-v0_random
  * gym_road_runner-v4_random
  * gym_road_runner_deterministic-v0_random
  * gym_road_runner_deterministic-v4_random
  * gym_road_runner_no_frameskip-v0_random
  * gym_road_runner_no_frameskip-v4_random
  * gym_robotank-v0_random
  * gym_robotank-v4_random
  * gym_robotank_deterministic-v0_random
  * gym_robotank_deterministic-v4_random
  * gym_robotank_no_frameskip-v0_random
  * gym_robotank_no_frameskip-v4_random
  * gym_seaquest-v0_random
  * gym_seaquest-v4_random
  * gym_seaquest_deterministic-v0_random
  * gym_seaquest_deterministic-v4_random
  * gym_seaquest_no_frameskip-v0_random
  * gym_seaquest_no_frameskip-v4_random
  * gym_skiing-v0_random
  * gym_skiing-v4_random
  * gym_skiing_deterministic-v0_random
  * gym_skiing_deterministic-v4_random
  * gym_skiing_no_frameskip-v0_random
  * gym_skiing_no_frameskip-v4_random
  * gym_solaris-v0_random
  * gym_solaris-v4_random
  * gym_solaris_deterministic-v0_random
  * gym_solaris_deterministic-v4_random
  * gym_solaris_no_frameskip-v0_random
  * gym_solaris_no_frameskip-v4_random
  * gym_space_invaders-v0_random
  * gym_space_invaders-v4_random
  * gym_space_invaders_deterministic-v0_random
  * gym_space_invaders_deterministic-v4_random
  * gym_space_invaders_no_frameskip-v0_random
  * gym_space_invaders_no_frameskip-v4_random
  * gym_star_gunner-v0_random
  * gym_star_gunner-v4_random
  * gym_star_gunner_deterministic-v0_random
  * gym_star_gunner_deterministic-v4_random
  * gym_star_gunner_no_frameskip-v0_random
  * gym_star_gunner_no_frameskip-v4_random
  * gym_tennis-v0_random
  * gym_tennis-v4_random
  * gym_tennis_deterministic-v0_random
  * gym_tennis_deterministic-v4_random
  * gym_tennis_no_frameskip-v0_random
  * gym_tennis_no_frameskip-v4_random
  * gym_time_pilot-v0_random
  * gym_time_pilot-v4_random
  * gym_time_pilot_deterministic-v0_random
  * gym_time_pilot_deterministic-v4_random
  * gym_time_pilot_no_frameskip-v0_random
  * gym_time_pilot_no_frameskip-v4_random
  * gym_tutankham-v0_random
  * gym_tutankham-v4_random
  * gym_tutankham_deterministic-v0_random
  * gym_tutankham_deterministic-v4_random
  * gym_tutankham_no_frameskip-v0_random
  * gym_tutankham_no_frameskip-v4_random
  * gym_up_n_down-v0_random
  * gym_up_n_down-v4_random
  * gym_up_n_down_deterministic-v0_random
  * gym_up_n_down_deterministic-v4_random
  * gym_up_n_down_no_frameskip-v0_random
  * gym_up_n_down_no_frameskip-v4_random
  * gym_venture-v0_random
  * gym_venture-v4_random
  * gym_venture_deterministic-v0_random
  * gym_venture_deterministic-v4_random
  * gym_venture_no_frameskip-v0_random
  * gym_venture_no_frameskip-v4_random
  * gym_video_pinball-v0_random
  * gym_video_pinball-v4_random
  * gym_video_pinball_deterministic-v0_random
  * gym_video_pinball_deterministic-v4_random
  * gym_video_pinball_no_frameskip-v0_random
  * gym_video_pinball_no_frameskip-v4_random
  * gym_wizard_of_wor-v0_random
  * gym_wizard_of_wor-v4_random
  * gym_wizard_of_wor_deterministic-v0_random
  * gym_wizard_of_wor_deterministic-v4_random
  * gym_wizard_of_wor_no_frameskip-v0_random
  * gym_wizard_of_wor_no_frameskip-v4_random
  * gym_yars_revenge-v0_random
  * gym_yars_revenge-v4_random
  * gym_yars_revenge_deterministic-v0_random
  * gym_yars_revenge_deterministic-v4_random
  * gym_yars_revenge_no_frameskip-v0_random
  * gym_yars_revenge_no_frameskip-v4_random
  * gym_zaxxon-v0_random
  * gym_zaxxon-v4_random
  * gym_zaxxon_deterministic-v0_random
  * gym_zaxxon_deterministic-v4_random
  * gym_zaxxon_no_frameskip-v0_random
  * gym_zaxxon_no_frameskip-v4_random
  * image_celeba
  * image_celeba32
  * image_celeba64
  * image_celeba_multi_resolution
  * image_celebahq128
  * image_celebahq128_dmol
  * image_celebahq256
  * image_celebahq256_dmol
  * image_cifar10
  * image_cifar100
  * image_cifar100_plain
  * image_cifar100_plain8
  * image_cifar100_plain_gen
  * image_cifar100_tune
  * image_cifar10_plain
  * image_cifar10_plain8
  * image_cifar10_plain_gen
  * image_cifar10_plain_gen_dmol
  * image_cifar10_plain_random_shift
  * image_cifar10_tune
  * image_cifar20
  * image_cifar20_plain
  * image_cifar20_plain8
  * image_cifar20_plain_gen
  * image_cifar20_tune
  * image_fashion_mnist
  * image_fsns
  * image_imagenet
  * image_imagenet224
  * image_imagenet32
  * image_imagenet32_gen
  * image_imagenet32_small
  * image_imagenet64
  * image_imagenet64_gen
  * image_imagenet_multi_resolution_gen
  * image_lsun_bedrooms
  * image_mnist
  * image_mnist_tune
  * image_ms_coco_characters
  * image_ms_coco_tokens32k
  * image_text_ms_coco
  * image_text_ms_coco_multi_resolution
  * image_vqav2_rcnn_feature_tokens10k_labels3k
  * image_vqav2_tokens10k_labels3k
  * img2img_allen_brain
  * img2img_allen_brain_dim16to16_paint1
  * img2img_allen_brain_dim48to64
  * img2img_allen_brain_dim8to32
  * img2img_celeba
  * img2img_celeba64
  * img2img_cifar10
  * img2img_cifar100
  * img2img_imagenet
  * lambada_lm
  * lambada_lm_control
  * lambada_rc
  * lambada_rc_control
  * languagemodel_de_en_fr_ro_wiki64k
  * languagemodel_de_wiki32k
  * languagemodel_de_wiki64k
  * languagemodel_en_wiki32k
  * languagemodel_en_wiki64k
  * languagemodel_en_wiki64k_shorter
  * languagemodel_en_wiki_lm_multi_nli_subwords
  * languagemodel_en_wiki_lm_multi_nli_subwords64k
  * languagemodel_en_wiki_lm_short_multi_nli_subwords64k
  * languagemodel_en_wiki_lm_summarize_cnndm_subwords
  * languagemodel_en_wiki_lm_summarize_cnndm_subwords64k
  * languagemodel_fr_wiki32k
  * languagemodel_fr_wiki64k
  * languagemodel_lm1b32k
  * languagemodel_lm1b32k_packed
  * languagemodel_lm1b8k
  * languagemodel_lm1b8k_packed
  * languagemodel_lm1b_characters
  * languagemodel_lm1b_characters_packed
  * languagemodel_lm1b_multi_nli
  * languagemodel_lm1b_multi_nli_subwords
  * languagemodel_lm1b_sentiment_imdb
  * languagemodel_multi_wiki_translate_fr
  * languagemodel_ptb10k
  * languagemodel_ptb_characters
  * languagemodel_ro_wiki32k
  * languagemodel_ro_wiki64k
  * languagemodel_wiki_noref_v128k_l1k
  * languagemodel_wiki_noref_v32k_l16k
  * languagemodel_wiki_noref_v32k_l1k
  * languagemodel_wiki_noref_v8k_l16k
  * languagemodel_wiki_noref_v8k_l1k
  * languagemodel_wiki_scramble_l128
  * languagemodel_wiki_scramble_l1k
  * languagemodel_wiki_xml_v8k_l1k
  * languagemodel_wiki_xml_v8k_l4k
  * languagemodel_wikitext103
  * languagemodel_wikitext103_characters
  * librispeech
  * librispeech_clean
  * librispeech_clean_small
  * librispeech_noisy
  * librispeech_train_full_test_clean
  * msr_paraphrase_corpus
  * msr_paraphrase_corpus_characters
  * multi_nli
  * multi_nli_characters
  * multi_nli_shared_vocab
  * multi_nli_wiki_lm_shared_vocab
  * multi_nli_wiki_lm_shared_vocab64k
  * ocr_test
  * paraphrase_generation_ms_coco_problem1d
  * paraphrase_generation_ms_coco_problem1d_characters
  * paraphrase_generation_ms_coco_problem2d
  * paraphrase_generation_ms_coco_problem2d_characters
  * parsing_english_ptb16k
  * parsing_english_ptb8k
  * parsing_icelandic16k
  * program_search_algolisp
  * programming_desc2code_cpp
  * programming_desc2code_py
  * question_nli
  * question_nli_characters
  * quora_question_pairs
  * quora_question_pairs_characters
  * rte
  * rte_characters
  * sci_tail
  * sci_tail_characters
  * sci_tail_shared_vocab
  * sentiment_imdb
  * sentiment_imdb_characters
  * sentiment_sst_binary
  * sentiment_sst_binary_characters
  * squad
  * squad_concat
  * squad_concat_positioned
  * stanford_nli
  * stanford_nli_characters
  * stanford_nli_shared_vocab
  * stanford_nli_wiki_lm_shared_vocab
  * stanford_nli_wiki_lm_shared_vocab64k
  * style_transfer_modern_to_shakespeare
  * style_transfer_modern_to_shakespeare_characters
  * style_transfer_shakespeare_to_modern
  * style_transfer_shakespeare_to_modern_characters
  * summarize_cnn_dailymail32k
  * summarize_cnn_dailymail_wiki_lm_shared_vocab
  * summarize_cnn_dailymail_wiki_lm_shared_vocab64k
  * sva_language_modeling
  * sva_number_prediction
  * text2text_copyable_tokens
  * text2text_tmpdir
  * text2text_tmpdir_tokens
  * timeseries_synthetic_data_series10_samples100k
  * timeseries_toy_problem
  * timeseries_toy_problem_no_inputs
  * tiny_algo
  * translate_encs_wmt32k
  * translate_encs_wmt_characters
  * translate_ende_wmt32k
  * translate_ende_wmt32k_packed
  * translate_ende_wmt8k
  * translate_ende_wmt8k_packed
  * translate_ende_wmt_bpe32k
  * translate_ende_wmt_characters
  * translate_enet_wmt32k
  * translate_enet_wmt_characters
  * translate_enfr_wmt32k
  * translate_enfr_wmt32k_packed
  * translate_enfr_wmt32k_with_backtranslate_en
  * translate_enfr_wmt32k_with_backtranslate_fr
  * translate_enfr_wmt8k
  * translate_enfr_wmt_characters
  * translate_enfr_wmt_multi64k
  * translate_enfr_wmt_small32k
  * translate_enfr_wmt_small8k
  * translate_enfr_wmt_small_characters
  * translate_enid_iwslt32k
  * translate_enmk_setimes32k
  * translate_enmk_setimes_characters
  * translate_envi_iwslt32k
  * translate_enzh_wmt32k
  * translate_enzh_wmt8k
  * video_bair_robot_pushing
  * video_bair_robot_pushing_with_actions
  * video_google_robot_pushing
  * video_stochastic_shapes10k
  * video_twentybn
  * wiki_revision
  * wiki_revision_packed1k
  * wiki_revision_packed256
  * wikisum_commoncrawl
  * wikisum_commoncrawl_lead_section
  * wikisum_web
  * wikisum_web_lead_section
  * winograd_nli
  * winograd_nli_characters
  * wsj_parsing
```

Training the model using the custom problem worked just fine by running t2t-trainer with the t2t_usr_dir flag set to my directory with the custom problem, however I am unable to properly register the problem in the notebook.

Am I doing something wrong? Or is this not supported in the AttentionVIsualizer?"
28661,"TF2.0.0-alpha bad performance (compared to TF1.13.1) with model.fit() for 1-layer tf.keras.Sequential model, Linear Regression","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Nope, this only concerns performance of model.fit for a tf.keras.Sequential model, comparing versions 2.0.0-alpha to 1.13.1.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Windows 10, 64-bit (local machine), and on google colab with similar outcome
- TensorFlow installed from (source or binary): pip install tensorflow / pip install tensorflow==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha vs. 1.13.1
- Python version: Python 3.7.3


**Describe the current behavior**
Much higher (mse) loss with tensorflow 2.0.0-alpha, running 500 epochs of 'sgd' using model.fit() of a 1-layer tf.keras.Sequential model, implementing a simple Linear Regression (6 data points), when running the same code first with tensorflow 1.13.1 (performance well/as expected) and then with 2.0.0-alpha.

Results running on google colabs shown below (CPU only), similar performance difference on my local machine, on which I used the specified Python version (Windows 10, 64-bit, i5-7200U CPU) - https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb#scrollTo=btF2CSFH2iEX

1.13.1
...
Epoch 500/500
6/6 [==============================] - 0s 238us/sample - loss: 2.0710e-05

2.0.0-alpha
...
Epoch 500/500
6/6 [==============================] - 0s 542us/sample - loss: 0.2409

As you can see, the (mse) loss is several orders of magnitudes worse for 2.0.0-alpha.


**Describe the expected behavior**
With what I understand about tensorflow 2.0 is that this performance should be the same / similar for this code (see next item for the code).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb#scrollTo=btF2CSFH2iEX

( !pip install tensorflow==2.0.0-alpha0 )
import tensorflow as tf
import numpy as np
from tensorflow import keras
tf.version
model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss='mean_squared_error')
xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)
model.fit(xs, ys, epochs=500)
print(model.predict([10.0]))



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28660,ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /home/xsx/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so),"- centos7_DVD(The latest version)
- python: 3.7.3
- tensorflow:  1.13.1
- anaconda: Anaconda3-2019.03-Linux-x86_64

After the installation of tensorflow(cpu version) via pip command, I just used import tensorflow as tf which leads to the following error message；

```
### >>> import tensorflow
Traceback (most recent call last):
  File ""/home/xsx/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/xsx/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/xsx/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/xsx/.conda/envs/tensorflow/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/xsx/.conda/envs/tensorflow/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /home/xsx/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
```

I try to update GLIBC_2.23,But the whole system crashed；

This problem has been bothering me for a week and my system has crashed three times.

**Is it a version support issue for tensorflow?**"
28659,model.fit,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28658,tf.cond？？？？,"
How to achieve in tensorflow: meet the condition, execute 1 (do not execute 2); do not satisfy the condition, execute 2 (do not execute 1)"
28657,custom tflite model run failed with Gpudelegate  (android P),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.12.2
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
build GPUdelegate failed on android P   java demo
**Describe the expected behavior**
build pass
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I convert my model like this:
freeze_graph --input_graph=eval.pb --input_checkpoint=./model_quant_self/model.ckpt-19-19 --output_graph=frozen_eval_graph.pb --output_node_names=Softmax
tflite_convert --output_file=poolnet_gzq.tflite --graph_def_file=./model_gzq.pb --inference_type=FLOAT --input_arrays=Placeholder --input_shapes=1,224,224,3 --output_arrays=oup

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


here is the log:

2019-05-13 16:25:04.870 20456-22983/? W/System.err: java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: fuse_auto_input failedNode number 133 (GpuDelegate) failed to prepare.

2019-05-13 16:25:04.870 20456-22983/? W/System.err: Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: fuse_auto_input failedNode number 133 (GpuDelegate) failed to prepare.
failedNode133 is my outputNode Sigmiod/Softmax ,I had tried ,both are failed
"
28655,train.py,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): git clone
- TensorFlow version: newest
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

./train.py
./train.py: line 68: --wanted_words: command not found
./train.py: line 68: rSimple speech recognition to spot a limited number of keywords.
why is that?how to solve it?"
28654,NotFoundError: Resource worker/embedding_layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- GPU model and memory: TPU


**Describe the current behavior**
When i try to call fit on a character level LSTM based model I get following error `NotFoundError: Resource worker/embedding_layer/embeddings/N10tensorflow3VarE does not exist.
	 [[{{node embedding_layer/embedding_lookup}}]]`

**Describe the expected behavior**
If I run the exact same model on a GPU, everything works as expected (eg. the model trains).

**Code to reproduce the issue**
I am following the official google guides as to how train a keras model on tpu using the tensorflow contrib keras to tpu model function

**Other info / logs**
```Train on 501147 samples, validate on 55684 samples
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-32-fff2e852201f> in <module>()
----> 1 model.fit(X,y,epochs=1,validation_split=0.1,batch_size=batch_size)#,callbacks=callbacks)

4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

NotFoundError: Resource worker/embedding_layer/embeddings/N10tensorflow3VarE does not exist.
	 [[{{node embedding_layer/embedding_lookup}}]]```
"
28653,Multi-GPU performance degradation in custom built TF,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): tensorflow/benchmarks
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 33141833284a81a5ab3300047d2845c7124f4a66
- Python version: Python 3.5.3
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: V100-PCIE-32GB

**Describe the current behavior**

Use the following command to compile (without running of `./configure`):

```
bazel build \
    -c opt \
    --jobs=32 \
    --action_env=PYTHON_BIN_PATH=""/usr/bin/python3"" \
    --action_env=PYTHON_LIB_PATH=""/usr/local/lib/python3.5/dist-packages"" \
    --python_path=""/usr/bin/python3"" \
    --define with_xla_support=true \
    --copt=-Wno-sign-compare \
    --copt=-march=broadwell \
    --host_copt=-march=broadwell \
    --define=with_default_optimizations=true \
    --config=cuda \
    --config=gdr \
    --config=mkl \
    //tensorflow/tools/pip_package:build_pip_package
```

Use the following command for benchmarking:

```
python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
    --model=resnet50 \
    --batch_size=256 \
    --use_fp16 \
    --variable_update=parameter_server \
    --local_parameter_device=cpu \
    --num_gpus=$NUM_GPUS
```

| TF Version | 1-GPU | 2-GPU | 4-GPU | 8-GPU |
|:-:|:-:|:-:|:-:|:-:|
|1.13.1|786.63|1557.89|2982.52|5741.76|
|1.14.1.dev20190512|779.81|1547.23|2872.75|5671.7|
| 33141833284a81a5ab3300047d2845c7124f4a66|776.75|387.99|713.77|1249.80|

**Describe the expected behavior**

Related to #28628. I am currently not sure how to reproduce a custom build with the same GIT_VERSION as of nightly. I will try to reproduce this issue with the next nightly package."
28651,mode.fit() Error: ValueError: TypeError: len() of unsized object,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): No
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0
- TensorFlow version (use command below):2.0.0-alpha0
- Python version: 3.6.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
2.0.0-alpha0
**Describe the current behavior**
I run the code on my local machine : the TF 2.0 Alpha Train your first neural network: basic classification.
There is all right with the code before i call : model.fit(train_images, train_labels, epochs=5).
The errors are:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-2e1aa44bfe09> in <module>
      1 # 训练
----> 2 model.fit(x_train, y_train, batch_size=50, epochs=50, validation_split=0.1, verbose=1)

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    237     # Setup work for each epoch
    238     epoch_logs = {}
--> 239     model.reset_metrics()
    240     if mode == ModeKeys.TRAIN:
    241       callbacks.on_epoch_begin(epoch, epoch_logs)

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in reset_metrics(self)
   1171     if hasattr(self, 'metrics'):
   1172       for m in self.metrics:
-> 1173         m.reset_states()
   1174 
   1175     # Reset the state of loss metric wrappers.

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\keras\metrics.py in reset_states(self)
    197     when a metric is evaluated during training.
    198     """"""
--> 199     K.batch_set_value([(v, 0) for v in self.variables])
    200 
    201   @abc.abstractmethod

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py in batch_set_value(tuples)
   2878   if ops.executing_eagerly_outside_functions():
   2879     for x, value in tuples:
-> 2880       x.assign(np.asarray(value, dtype=dtype(x)))
   2881   else:
   2882     with get_graph().as_default():

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in assign(self, value, use_locking, name, read_value)
   1051     # initialize the variable.
   1052     with _handle_graph(self.handle):
-> 1053       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)
   1054       self._shape.assert_is_compatible_with(value_tensor.shape)
   1055       assign_op = gen_resource_variable_ops.assign_variable_op(

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1048   preferred_dtype = deprecation.deprecated_argument_lookup(
   1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1051 
   1052 

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1106       name=name,
   1107       preferred_dtype=dtype_hint,
-> 1108       as_ref=False)
   1109 
   1110 

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1184 
   1185     if ret is None:
-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1187 
   1188     if ret is NotImplemented:

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    302                                          as_ref=False):
    303   _ = as_ref
--> 304   return constant(v, dtype=dtype, name=name)
    305 
    306 

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)
    243   """"""
    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 245                         allow_broadcast=True)
    246 
    247 

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    251   ctx = context.context()
    252   if ctx.executing_eagerly():
--> 253     t = convert_to_eager_tensor(value, ctx, dtype)
    254     if shape is None:
    255       return t

D:\Softwares\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    112     return t
    113   else:
--> 114     return ops.EagerTensor(value, handle, device, dtype)
    115 
    116 

ValueError: TypeError: len() of unsized object
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28650,"Getting error message while converting data to TFrecords. TypeError: 'Self-emp-not-inc' has type str, but expected one of: bytes","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64 bit 
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```Code. import pandas
import tensorflow as tf


DATA_FILE = ""C:/Users/Harika Reddy/AppData/Local/Programs/Python/Python37-32/data-linter/adult.data""
OUTPUT_FILE = ""C:/Users/Harika Reddy/AppData/Local/Programs/Python/Python37-32/data-linter/adult.tfrecords""

column_names = [
    'age', 'workclass', 'fnlwgt', 'education', 'education-num',
    'marital-status', 'occupation', 'relationship', 'race', 'sex',
    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',
    'label'
]

numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain',
                      'capital-loss', 'hours-per-week']

df = pandas.read_csv(DATA_FILE).values
with tf.python_io.TFRecordWriter(OUTPUT_FILE) as writer:
  for row in df:
    example = tf.train.Example()
    for col_name, val in zip(column_names, row):
      if col_name in numerical_features:
        example.features.feature[col_name].float_list.value.append(val)
      else:
        example.features.feature[col_name].bytes_list.value.append(val.strip())
    writer.write(example.SerializeToString())
#

 Getting Error message: 

(C:\Anaconda) C:\Users\Harika Reddy\AppData\Local\Programs\Python\Python37-32\d
ta-linter\demo>python convert_to_tfrecord.py
Traceback (most recent call last):
  File ""convert_to_tfrecord.py"", line 51, in <module>
    example.features.feature[col_name].bytes_list.value.append(val.strip())
TypeError: 'Self-emp-not-inc' has type str, but expected one of: bytes"
28649,tf.lite.TFLiteConverter.from_saved_model error,"**System information**
- OS Platform and Distribution (MacOS 10.14.4):
- TensorFlow installed from (binary):
- TensorFlow version (tensorflow==1.13.1):


**Provide the text output from tflite_convert**


Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: LSTMBlockCell, Merge, Switch.
Traceback (most recent call last):
  File ""/Users/jim/src/ai/xiayu/deepiano/venv/bin/toco_from_protos"", line 11, in <module>
    load_entry_point('tensorflow', 'console_scripts', 'toco_from_protos')()
  File ""/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/jim/src/ai/xiayu/deepiano/venv/lib/python3.7/site-packages/tensorflow-1.13.1-py3.7-macosx-10.14-x86_64.egg/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, PACK, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: LSTMBlockCell, Merge, Switch.

Also, please include a link to a GraphDef or the model if possible.



Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28648,Keras fit_generator using validation does not respect verbose argument,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13 gpu
- Python version: 3.6

**Bug**
When using `fit_generator` with validation the progress bar is printed regardless of the `verbose` setting.

**Cause**
Line 216 of tensorflow/python/keras/engine/training_generator.py calls `model_iterator` to do validation, but does not pass the `verbose` parameter, meaning that the default value of `verbose=1` is used.

**Solution**
Add in the missing parameter, e.g.

```
    # Run the test loop every epoch during training.
    if do_validation and not callbacks.model.stop_training:
      val_results = model_iteration(
          model,
          validation_data,
          steps_per_epoch=validation_steps,
          batch_size=batch_size,
          class_weight=class_weight,
          workers=workers,
          use_multiprocessing=use_multiprocessing,
          max_queue_size=max_queue_size,
          verbose=verbose,
          mode='test')
```
"
28647,switch statement,"I am a beginner of tensorflow。When I use tensorflow, I want to do this:
If (global_step%2==0):
      Compute and store
elif(global_step%2!=0):
      Get the stored date
After analysis, I found that tensorflow builds a graph using tf.cond instead of if。。。 else。。。.So I modified the code：tf.cond(tf.cast(global_step%2==0),tf.bool,Compute and store,  Get the stored date).However, there are still problems with such modifications after testing：Because the use of the tf.cond function will run both branches at the time of the feed, so add extra store and get overhead.
The purpose I want to accomplish is If (global_step%2==0)，Compute and store；else Get the stored date。This can reduce the computational overhead。
So I want to ask everyone how to achieve this in tensorflow?

"
28646,[TF 2.0] keras model.train_on_batch allocates extremely large CPU memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190504
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: Geforce GTX 1070 8GB

**Describe the current behavior**
When I try to train_on_batch, TF 2.0 allcaotes memory endless and finally crashes. Using `tf.keras.backend.set_learning_phase(1)` can be temporary solution, but this trick doesn work with lateset build (dev20190511).

**Describe the expected behavior**
The model can be trained regardless of `tf.keras.backend.set_learning_phase(1)` because I used model.train_on_batch(). It should handle learning phase internally.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np


def conv(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal'):
    c = tf.keras.layers.Conv2D(
        filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=initializer, use_bias=False)(x)

    return c


def conv_bn(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c = conv(x, filters=filters, kernel_size=kernel_size,
             strides=strides, padding=padding, initializer=initializer)

    c_bn = tf.keras.layers.BatchNormalization(
        gamma_initializer=bn_gamma_initializer)(c)

    return c_bn


def conv_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):
    c_bn = conv_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,
                   initializer=initializer, bn_gamma_initializer=bn_gamma_initializer)

    return tf.keras.layers.Activation('relu')(c_bn)


def conv_gap(x, output_filters, kernel_size=(1, 1)):
    x = conv(x, filters=output_filters, kernel_size=kernel_size)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)

    return x


def my_block(x, output_filters, inter_filters):
    c1 = conv_bn_relu(x, inter_filters, (1, 1))
    c2 = conv_bn_relu(c1, inter_filters, (3, 3))
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    p = tf.keras.layers.add([c3, x])

    return tf.keras.layers.Activation('relu')(p)


def my_block_inc(x, output_filters, inter_filters, strides1x1=(1, 1), strides2x2=(2, 2)):
    c1 = conv_bn_relu(
        x, inter_filters, (1, 1), strides=strides1x1)
    c2 = conv_bn_relu(
        c1, inter_filters, (3, 3), strides=strides2x2)
    c3 = conv_bn(
        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')

    strides = np.multiply(strides1x1, strides2x2)
    s = conv_bn(
        x, output_filters, (1, 1), strides=strides)  # shortcut

    p = tf.keras.layers.add([c3, s])

    return tf.keras.layers.Activation('relu')(p)


def repeat_blocks(x, block_delegate, count, **kwargs):
    assert count >= 0

    for _ in range(count):
        x = block_delegate(x, **kwargs)
    return x


# This line makes trick!
# tf.keras.backend.set_learning_phase(1)
shape = (299, 299, 3)

inputs = tf.keras.Input(shape, dtype='float32')
outputs = conv_bn_relu(inputs, 256 // 4, (7, 7), strides=(2, 2))
outputs = tf.keras.layers.MaxPool2D(
    (3, 3), strides=(2, 2), padding='same')(outputs)
outputs = my_block_inc(outputs, 256, 256 // 4, strides2x2=(1, 1))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=2,
                        output_filters=256,
                        inter_filters=256 // 4)

outputs = my_block_inc(outputs, 512, 512 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=7,
                        output_filters=512,
                        inter_filters=512 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=40,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=16,
                        output_filters=1024,
                        inter_filters=1024 // 4)

outputs = my_block_inc(outputs, 2048, 2048 // 4, strides2x2=(2, 2))
outputs = repeat_blocks(outputs, block_delegate=my_block,
                        count=6,
                        output_filters=2048,
                        inter_filters=2048 // 4)

outputs = conv_gap(outputs, 1024)
outputs = tf.keras.layers.Activation('sigmoid')(outputs)
model = tf.keras.models.Model(
    inputs=inputs, outputs=outputs, name='resnet_custom_v2')

optimizer = tf.optimizers.Adam(0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy',
              metrics=['mean_absolute_error'])


def make_batch(_):
    x = np.ones((shape[0], shape[1], shape[2]), dtype='float32')
    y = np.ones((1024), dtype='float32')

    return (x, y)


dataset = tf.data.Dataset.from_tensor_slices(
    ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])
dataset = dataset.map(lambda x: tf.py_function(
    make_batch, (x,), (tf.float32, tf.float32)))
dataset = dataset.batch(10)

for x, y in dataset:
    step_results = model.train_on_batch(x=x, y=y)
    print(f'loss={step_results[0]}')

```

If you uncomment `tf.keras.backend.set_learning_phase(1)`, the model may be successully trained. But with TF 2.0 dev20190511, the traininig is always failed regardless of `tf.keras.backend.set_learning_phase(1)`.
"
28645,AttributeError: module 'tensorflow.python.saved_model.signature_constants' has no attribute 'DEFAULT_TRAIN_SIGNATURE_DEF_KEY',"```
Successfully installed tensorflow-1.13.1 tensorflow-probability-0.6.0

```
```
import tensorflow as tf
```
```
/home/maksim/dev_projects/rl/venv/bin/python /home/maksim/dev_projects/rl/racetrack.py
Traceback (most recent call last):
  File ""/home/maksim/dev_projects/rl/racetrack.py"", line 3, in <module>
    import tensorflow as tf
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/__init__.py"", line 29, in <module>
    from tensorflow._api.v1 import compat
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py"", line 21, in <module>
    from tensorflow._api.v1.compat import v1
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py"", line 626, in <module>
    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py"", line 56, in package_hook
    child_pkg = importlib.import_module(child_package_str)
  File ""/home/maksim/anaconda3/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py"", line 64, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 66, in <module>
    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/model_fn.py"", line 36, in <module>
    from tensorflow_estimator.python.estimator.export import export_lib
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/export/export_lib.py"", line 25, in <module>
    from tensorflow.python.saved_model.model_utils import build_all_signature_defs
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/model_utils/__init__.py"", line 22, in <module>
    from tensorflow.python.saved_model.model_utils.export_utils import build_all_signature_defs
  File ""/home/maksim/dev_projects/rl/venv/lib/python3.7/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py"", line 51, in <module>
    ModeKeys.TRAIN: signature_constants.DEFAULT_TRAIN_SIGNATURE_DEF_KEY,
AttributeError: module 'tensorflow.python.saved_model.signature_constants' has no attribute 'DEFAULT_TRAIN_SIGNATURE_DEF_KEY'

Process finished with exit code 1
```"
28644,GPU not available after installation of TF 2.0 alpha,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-alpha0
- Python version: Python 3.6.8 Anaconda, Inc.
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 10, cuDNN release 9.2, V9.2.148
- GPU model and memory: GTX1050M, 4GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I installed the tensorflow following the instructions:
```pip install tensorflow-gpu==2.0.0-alpha0 tensorflow==2.0.0-alpha0```
and then instruction on installing CUDA on Ubuntu 18.04
```
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-418
# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-0 \
    libcudnn7=7.4.1.5-1+cuda10.0  \
    libcudnn7-dev=7.4.1.5-1+cuda10.0
```
After the above steps, I checked the installation with
```
$ nvidia-smi
Sun May 12 21:43:20 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    On   | 00000000:01:00.0 Off |                  N/A |
| N/A   50C    P0    N/A /  N/A |    356MiB /  4042MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1313      G   /usr/lib/xorg/Xorg                           207MiB |
|    0      1819      G   kwin_x11                                      19MiB |
|    0      1823      G   /usr/bin/krunner                              11MiB |
|    0      1825      G   /usr/bin/plasmashell                          50MiB |
|    0      2287      G   ...Charm-P/ch-0/191.6605.12/jre64/bin/java     2MiB |
|    0      2355      G   ...uest-channel-token=13980915512808857744    63MiB |
+-----------------------------------------------------------------------------+

$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
```

When I test, however, whether TF runs on GPU, I get:

```
import tensorflow as tf
Limited tf.compat.v2.summary API due to missing TensorBoard installation
Limited tf.summary API due to missing TensorBoard installation
tf.debugging.set_log_device_placement(True)
# Creates some tensors
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
print(c)
2019-05-12 21:45:27.263715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-12 21:45:27.277136: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-05-12 21:45:27.277588: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x561bde54b680 executing computations on platform Host. Devices:
2019-05-12 21:45:27.277607: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0
2019-05-12 21:45:27.278422: I tensorflow/core/common_runtime/eager/execute.cc:394] Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0
2019-05-12 21:45:27.278804: I tensorflow/core/common_runtime/eager/execute.cc:394] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
```
while previously (TF 1.13 with CUDA 9) it worked fine."
28643,Keras fit_generator fails in graph mode when input is dict,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): simple keras model combined from examples
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS X 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 2.7
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no, CPU-version
- GPU model and memory: no, CPU-version

**Describe the current behavior**
When running provided code it fails only when fit_generator executed in graph mode.
In other cases fit() and fit_generator() work well.
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-11180391ba3c> in <module>()
     61 # Fails
     62 model.run_eagerly = False
---> 63 model.fit_generator(input_fn())

/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1513         shuffle=shuffle,
   1514         initial_epoch=initial_epoch,
-> 1515         steps_name='steps_per_epoch')
   1516 
   1517   def evaluate_generator(self,

/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_generator.pyc in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    255 
    256       is_deferred = not model._is_compiled
--> 257       batch_outs = batch_function(*batch_data)
    258       if not isinstance(batch_outs, list):
    259         batch_outs = [batch_outs]

/usr/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.pyc in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1248     else:
   1249       if not isinstance(K.symbolic_learning_phase(), int):
-> 1250         ins = x + y + sample_weights + [True]
   1251       else:
   1252         ins = x + y + sample_weights

TypeError: unsupported operand type(s) for +: 'dict' and 'list'
```

**Describe the expected behavior**
tf.keras.Model().fit_generator() should work properly with input of type dict.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf


def input_fn():
    x = np.random.random((1024, 10))
    y = np.random.randint(2, size=(1024, 1))
    x = tf.cast(x, tf.float32)
    
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.shuffle(100)
    dataset = dataset.batch(32)
    dataset = dataset.repeat(10)
    
    def _extract_features(_x, _y):
        features = {
            'x': _x,
            'z': tf.zeros_like(_x)
        }
        
        return features, _y

    dataset = dataset.map(_extract_features)

    return dataset


class MyModel0(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        
        self.features = tf.keras.layers.DenseFeatures([
            tf.feature_column.numeric_column('x', shape=(10,))
        ])
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')
        
    def call(self, inputs, training=None, mask=None):
        outputs = self.features(inputs)
        outputs = self.dense1(outputs)
        outputs = self.dense2(outputs)

        return outputs


model = MyModel()
model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.05))

# Works
model.run_eagerly = True
model.fit(input_fn())

# Works
model.run_eagerly = False
model.fit(input_fn())

# Works
model.run_eagerly = True
model.fit_generator(input_fn())

# Fails
model.run_eagerly = False
model.fit_generator(input_fn())
```"
28642,Is it possible to train custom models for Tensorflow Android?,"Hello,

Currently, I'm working on one projects and I need to train my models. I want to train models using YoloV2 and my question is that will my custom trained models work on Android Tensorflow if I add .pb model into the assets folder?

**System information**
- TensorFlow version 1.5.0
- Are you willing to contribute it (YES)


**It will not change the API .**

**I would like to benefit from this feature.**

"
28639,tf 1.8.0 with horovod hang at the middle of training,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
`yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
`Linux Ubuntu 16.04`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
`No`
- TensorFlow installed from (source or binary):
`pip install tensorflow_gpu==1.8.0`
- TensorFlow version (use command below):
```
work@job1b-pub-v100-5wh5z:~$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.8.0-0-g93bc2e2072 1.8.0
```
- Python version:
`Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)` 
- Bazel version (if compiling from source):
None
- GCC/Compiler version (if compiling from source):
None
- CUDA/cuDNN version:
`cuda 9.0.176-1`
- GPU model and memory:
`V100` and `32G` mem

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

i run the webvision train code in tf 1.8.0 with horovod (nccl), in 4 (nodes) * 8 Nvidia V100 GPU cluster, but the trainning job rang at the middle of training. and one of the nodes processes info are

```
work       1955      0  0 May09 ?        00:00:00 /bin/sh -c     PATH=/usr/local/openmpi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/usr/local/openmpi/lib:$L
work       1961   1955  0 May09 ?        00:00:08 /usr/local/openmpi/bin/orted -mca ess env -mca ess_base_jobid 2660237312 -mca ess_base_vpid 1 -mca ess_bas
work       1965   1961 99 May09 ?        3-20:29:04 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1966   1961 99 May09 ?        3-18:25:41 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1967   1961 99 May09 ?        5-18:09:20 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1968   1961 99 May09 ?        5-12:52:59 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1969   1961 99 May09 ?        3-18:07:41 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1970   1961 99 May09 ?        3-18:04:06 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1971   1961 99 May09 ?        3-18:19:37 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       1972   1961 99 May09 ?        3-17:27:18 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3
work       2665   1965  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2759   1972  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2813   1971  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2839   1966  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2853   1969  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2861   1967  0 May09 ?        00:00:12 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2876   1968  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work       2882   1970  0 May09 ?        00:00:14 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
work     165333   1965  0 May10 ?        00:00:10 /home/work/anaconda3/bin/python webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py --data_url=s3:/
```

[ps-ef-info.txt](https://github.com/tensorflow/tensorflow/files/3170040/ps-ef-info.txt)

and i print the `1965` process python call stack it shows

```
Thread 0x7f50cdffb700
  File ""/home/work/anaconda3/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/home/work/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/work/anaconda3/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py"", line 391, in _run
    enqueue_callable()
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1244, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)

Thread 0x7f5a3f48c700
  File ""webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py"", line 1065, in <module>
    tf.app.run(main=main)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""webvision-train-code/train_all_data_fixed_lr_64_gpu_5-8.py"", line 1036, in main
    save_model_secs=flags.save_model_secs)
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_builder.py"", line 473, in run
    save_model_secs, export_model, use_trt, fetch_strategy_fn, save_model_steps)
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_wrapper.py"", line 270, in run
    self._run()
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning_wrapper.py"", line 540, in _run
    self._save_model_steps)
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py"", line 878, in run
    self.training()
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py"", line 1530, in training
    self.train_step(sess)
  File ""/home/work/user-job-dir/webvision-train-code/moxing/tensorflow/executor/learning.py"", line 1274, in train_step
    feed_dict=feed_dict)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 567, in run
    run_metadata=run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1043, in run
    run_metadata=run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1119, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1191, in run
    run_metadata=run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 971, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
```

[python-call-stack.txt](https://github.com/tensorflow/tensorflow/files/3170036/python-call-stack.txt)

and gdb bt of this process are

```
(gdb) bt full
#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
No locals.
#1  0x00007f5971f7da54 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#2  0x00007f5971f7d221 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#3  0x00007f5971f7a764 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#4  0x00007f5971f7ac85 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#5  0x00007f5971f80f2b in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long)
    () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#6  0x00007f5971f85010 in tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#7  0x00007f5971f8e3d5 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#8  0x00007f596f424c8a in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#9  0x00007f596f425886 in TF_SessionRun () from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#10 0x00007f596f0d4186 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Outp---Type <return> to continue, or q <return> to quit---
ut> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#11 0x00007f596f0d42ca in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
No symbol table info available.
#12 0x00007f596f090a6e in _wrap_TF_SessionRun_wrapper ()
   from /home/work/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```

[gdb-backtrace.txt](https://github.com/tensorflow/tensorflow/files/3170038/gdb-backtrace.txt)

and it seems it wait at the `direct_session.cc`, L552

```
  WaitForNotification(&run_state, &step_cancellation_manager,
                      run_options.timeout_in_ms() > 0
                          ? run_options.timeout_in_ms()
                          : operation_timeout_in_ms_);
```

but i never set the `run_options.timeout_in_ms` and `config.operation_timeout_in_ms`, and it seems it wait for the deadline in gdb bt, so the question is how can this forever hang happen when the `nsync::nsync_cv_wait_with_deadline` be called ? (at least for now, it runs almost three days ...)

oh, i know why ... the gdb lack of some output (as it lack of symbols ...) ref [https://github.com/tensorflow/tensorflow/issues/26559](https://github.com/tensorflow/tensorflow/issues/26559)

`nsync::nsync_cv_wait` -> `nsync::nsync_cv_wait_with_deadline`

**Describe the expected behavior**

Not hang at the middle of training, the final train log are

```
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.464     ent_loss: 4.997 top-1: 0.375    top-5: 0.547    reg_loss: 0.309 total_loss: 5.306
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.328     ent_loss: 4.942 top-1: 0.344    top-5: 0.531    reg_loss: 0.309 total_loss: 5.251
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 368.118     ent_loss: 4.036 top-1: 0.578    top-5: 0.672    reg_loss: 0.309 total_loss: 4.344
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 367.937     ent_loss: 4.518 top-1: 0.453    top-5: 0.578    reg_loss: 0.309 total_loss: 4.827
INFO:tensorflow:global_step/sec: 1.46428
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 367.743     ent_loss: 4.083 top-1: 0.500    top-5: 0.703    reg_loss: 0.309 total_loss: 4.392
INFO:tensorflow:global_step/sec: 1.46428
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 286.646     ent_loss: 3.888 top-1: 0.484    top-5: 0.641    reg_loss: 0.309 total_loss: 4.197
INFO:tensorflow:step: 162300(global step: 162300)       sample/sec: 286.646     ent_loss: 3.888 top-1: 0.484    top-5: 0.641    reg_loss: 0.309 total_loss: 4.197
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

not sure ...

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

all nodes GPU utils is 100%

```
[root@job1b-pub-v100-5wh5z ~]# /var/paas/nvidia/bin/nvidia-smi 
Sun May 12 20:19:41 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:2D:00.0 Off |                    0 |
| N/A   41C    P0    65W / 300W |  29952MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:32:00.0 Off |                    0 |
| N/A   40C    P0    71W / 300W |  29954MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:5B:00.0 Off |                    0 |
| N/A   42C    P0    69W / 300W |  29942MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:5F:00.0 Off |                    0 |
| N/A   37C    P0    64W / 300W |  29942MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM2...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   40C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM2...  Off  | 00000000:BE:00.0 Off |                    0 |
| N/A   39C    P0    66W / 300W |  29954MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM2...  Off  | 00000000:E1:00.0 Off |                    0 |
| N/A   41C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM2...  Off  | 00000000:E9:00.0 Off |                    0 |
| N/A   41C    P0    67W / 300W |  29954MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     71807      C   /home/work/anaconda3/bin/python            29939MiB |
|    1     71808      C   /home/work/anaconda3/bin/python            29939MiB |
|    2     71809      C   /home/work/anaconda3/bin/python            29927MiB |
|    3     71810      C   /home/work/anaconda3/bin/python            29927MiB |
|    4     71811      C   /home/work/anaconda3/bin/python            29939MiB |
|    5     71812      C   /home/work/anaconda3/bin/python            29939MiB |
|    6     71813      C   /home/work/anaconda3/bin/python            29939MiB |
|    7     71814      C   /home/work/anaconda3/bin/python            29939MiB |
+-----------------------------------------------------------------------------+
```

and dmesg output the segment fault info

```
[Sun May 12 19:53:37 2019] sh[429661]: segfault at 7fff7f09f428 ip 00007fff7f09f428 sp 00007fff7f09e078 error 15
[Sun May 12 19:54:37 2019] sh[433888]: segfault at 7ffe30bcccd8 ip 00007ffe30bcccd8 sp 00007ffe30bcb928 error 15
[Sun May 12 19:55:37 2019] sh[437936]: segfault at 7ffc5e4a0958 ip 00007ffc5e4a0958 sp 00007ffc5e49f5a8 error 15
[Sun May 12 19:56:37 2019] sh[442090]: segfault at 7ffc4ad375b8 ip 00007ffc4ad375b8 sp 00007ffc4ad36208 error 15
[Sun May 12 19:57:37 2019] sh[446313]: segfault at 7ffcba547148 ip 00007ffcba547148 sp 00007ffcba545d98 error 15
[Sun May 12 19:57:37 2019] sh[446409]: segfault at 7ffca4e2dc18 ip 00007ffca4e2dc18 sp 00007ffca4e2c868 error 15
[Sun May 12 19:58:37 2019] sh[450425]: segfault at 7ffd09ab86d8 ip 00007ffd09ab86d8 sp 00007ffd09ab7328 error 15
[Sun May 12 19:59:37 2019] sh[454603]: segfault at 7ffca20adf08 ip 00007ffca20adf08 sp 00007ffca20acb58 error 15
[Sun May 12 20:00:37 2019] sh[499]: segfault at 7ffc588dd7e8 ip 00007ffc588dd7e8 sp 00007ffc588dc438 error 15
[Sun May 12 20:01:37 2019] sh[4810]: segfault at 7ffe67cdfbd8 ip 00007ffe67cdfbd8 sp 00007ffe67cde828 error 15
[Sun May 12 20:02:37 2019] sh[8913]: segfault at 7ffc6fd6a3b8 ip 00007ffc6fd6a3b8 sp 00007ffc6fd69008 error 15
[Sun May 12 20:02:37 2019] sh[8973]: segfault at 7ffe613b4388 ip 00007ffe613b4388 sp 00007ffe613b2fd8 error 15
[Sun May 12 20:03:37 2019] sh[13161]: segfault at 7fff17ac1bd8 ip 00007fff17ac1bd8 sp 00007fff17ac0828 error 15
[Sun May 12 20:04:37 2019] sh[17308]: segfault at 7fff5fe44448 ip 00007fff5fe44448 sp 00007fff5fe43098 error 15
[Sun May 12 20:05:37 2019] sh[21475]: segfault at 7ffeca86ff58 ip 00007ffeca86ff58 sp 00007ffeca86eba8 error 15
[Sun May 12 20:06:37 2019] sh[25887]: segfault at 7ffea93cd2a8 ip 00007ffea93cd2a8 sp 00007ffea93cbef8 error 15
[Sun May 12 20:07:37 2019] sh[30090]: segfault at 7ffe9a32ba78 ip 00007ffe9a32ba78 sp 00007ffe9a32a6c8 error 15
[Sun May 12 20:07:37 2019] sh[30151]: segfault at 7ffe0e8b00e8 ip 00007ffe0e8b00e8 sp 00007ffe0e8aed38 error 15
[Sun May 12 20:08:37 2019] sh[34257]: segfault at 7fff46231e38 ip 00007fff46231e38 sp 00007fff46230a88 error 15
[Sun May 12 20:09:37 2019] sh[38356]: segfault at 7ffc8b9d2ff8 ip 00007ffc8b9d2ff8 sp 00007ffc8b9d1c48 error 15
[Sun May 12 20:10:37 2019] sh[42685]: segfault at 7ffd19c3c068 ip 00007ffd19c3c068 sp 00007ffd19c3acb8 error 15
[Sun May 12 20:11:37 2019] sh[46730]: segfault at 7ffc11dcc218 ip 00007ffc11dcc218 sp 00007ffc11dcae68 error 15
[Sun May 12 20:12:37 2019] sh[50885]: segfault at 7ffde26e73c8 ip 00007ffde26e73c8 sp 00007ffde26e6018 error 15
[Sun May 12 20:12:37 2019] sh[51091]: segfault at 7ffcaaf42788 ip 00007ffcaaf42788 sp 00007ffcaaf413d8 error 15
[Sun May 12 20:13:37 2019] sh[55116]: segfault at 7ffc9faf70a8 ip 00007ffc9faf70a8 sp 00007ffc9faf5cf8 error 15
[Sun May 12 20:14:37 2019] sh[59159]: segfault at 7fff7ed38518 ip 00007fff7ed38518 sp 00007fff7ed37168 error 15
[Sun May 12 20:15:37 2019] sh[63389]: segfault at 7ffcf8b8d068 ip 00007ffcf8b8d068 sp 00007ffcf8b8bcb8 error 15
[Sun May 12 20:16:37 2019] sh[67564]: segfault at 7fffc0e890d8 ip 00007fffc0e890d8 sp 00007fffc0e87d28 error 15
[Sun May 12 20:17:37 2019] sh[71615]: segfault at 7ffd22222168 ip 00007ffd22222168 sp 00007ffd22220db8 error 15
[Sun May 12 20:17:37 2019] sh[71666]: segfault at 7ffc62aff5e8 ip 00007ffc62aff5e8 sp 00007ffc62afe238 error 15
[Sun May 12 20:18:37 2019] sh[75987]: segfault at 7ffdf614bd88 ip 00007ffdf614bd88 sp 00007ffdf614a9d8 error 15
[Sun May 12 20:19:37 2019] sh[80035]: segfault at 7ffffcbe7c38 ip 00007ffffcbe7c38 sp 00007ffffcbe6888 error 15
```
"
28635,`ConcreteFunction` does not raise errors when inputs and `tf.TensorSpec` are not compatible with each other,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): osx
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.1-dev20190509
- Python version: 3.7.1
- Bazel version (if compiling from source):None
- GCC/Compiler version (if compiling from source):None
- CUDA/cuDNN version:None
- GPU model and memory:None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
import tensorflow as tf
tf.enable_v2_behavior()


@tf.function
def test_rank(x):
    return x


test_rank_cf = test_rank.get_concrete_function(tf.TensorSpec([None, None], tf.float32))
# run smoothly, should raise errors here?
test_rank_cf(tf.random.normal((2, 3, 4)))
```
**Describe the expected behavior**
Errors should be rasied if inputs and `tf.TensorSpec` are not compatible with each other
**Code to reproduce the issue**
See the above
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28632,AttributeError: module 'tensorflow' has no attribute 'gfile',"Hi,

I am trying to load a file using the following code below. However, whenever I run my code I get an error stating that `AttributeError: module 'tensorflow' has no attribute 'gfile'` . I have successfully installed gin using `pip install gin-config`.

```
import tensorflow as tf

def load_directory_data(directory):
  data = {}
  data[""sentence""] = []
  data[""sentiment""] = []
  for file_path in os.listdir(directory):
    with tf.io.gfile.GFile(os.path.join(directory, file_path), ""r"") as f:
      data[""sentence""].append(f.read())
      data[""sentiment""].append(re.match(""\d+_(\d+)\.txt"", file_path).group(1))
  return pd.DataFrame.from_dict(data)
```

I've even done `import gin.tf` , but when I run my code I then get...

`
  File ""C:\Users\WTC\Anaconda3\lib\site-packages\bert\tokenization.py"", line 125, in load_vocab
    with tf.gfile.GFile(vocab_file, ""r"") as reader:

AttributeError: module 'tensorflow' has no attribute 'gfile'`


Any thoughts as to why?"
28630,TensorFlow install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I tried to install Tensorflow, but it wont import (in either python 3.6 or 3.7). I think the problem is that I used pip, and that I don't have ""pywrap_tensorflow_internal""

Here is the Error Message:
ImportError                               Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: dlopen(/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.13)
  Expected in: /usr/lib/libSystem.B.dylib
 in /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-8-c542f92e050d> in <module>
      6 import numpy as np
      7 import pandas as pd
----> 8 import tensorflow as tf
      9 from sklearn import metrics
     10 from tensorflow.python.data import Dataset

/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
     25 import sys as _sys
     26 
---> 27 from tensorflow._api.v2 import audio
     28 from tensorflow._api.v2 import autograph
     29 from tensorflow._api.v2 import bitwise

/anaconda3/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py in <module>
      6 from __future__ import print_function as _print_function
      7 
----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav
      9 from tensorflow.python.ops.gen_audio_ops import encode_wav
     10 

/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime
  Referenced from: /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.13)
  Expected in: /usr/lib/libSystem.B.dylib
 in /anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so

Failed to load the native TensorFlow runtime.



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28628,Unexpected tf.version.GIT_VERSION for nightly builds,"In colab, use the following commands to reproduce:

```
!pip uninstall tensorflow -yq && pip install tf-nightly-gpu==1.14.1.dev20190510 -q
import tensorflow as tf
tf.version.GIT_VERSION
```

Which shows ``v1.12.1-1705-g978532afa9`` (last hex is not any git commit btw)

Shouldn't it be the git version used to build that package instead?

Gentle ping @gunan and @yifeif "
28626,TF 2.0: Invalid argument: Unsupported type: 21,"Hello guys,

Did any of you encounter this weird warning when training any network:

`E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`

<img width=""1175"" alt=""warning_error"" src=""https://user-images.githubusercontent.com/3697692/57572249-f4b3ba80-7417-11e9-90c0-8c6bdc223da1.png"">

I am using Adam optimizer, however, I tried other optimizers but nothing changes!

And, this warning appeared just after I set-up the Tensorboard using callbacks:
`tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)`

To set-up the Tensorboard I used: `pip install -q tf-nightly-2.0-preview` 


Thanks

"
28625,"tensorflow, does this do LSTM","Hello,

what deos this do ?

LSTM shows the same photo:
0.38, 0.57, 0.31, -0.2

i don't know what this do !

can some one help me with this?

```
Epoch 20/20
24946/24946 [==============================] - 2s 73us/step - loss: 0.7507 - acc: 0.5095
1/1 [==============================] - 0s 343ms/step
[0.5433492]
{'loss': [5.713657855987549, 3.161858320236206, 1.936337947845459, 1.414483904838562, 1.1403812170028687, 1.0629992485046387, 1.071506142616272, 1.062917947769165, 1.036919116973877, 1.0067180395126343, 0.9582799673080444, 0.9101015329360962, 0.88093501329422, 0.8427869081497192, 0.8161187767982483, 0.7948179244995117, 0.7787773013114929, 0.7691016793251038, 0.7594917416572571, 0.7506719827651978], 'acc': [0.4476068317890167, 0.4788743555545807, 0.49587106704711914, 0.4971538484096527, 0.49302494525909424, 0.4872123897075653, 0.4887356758117676, 0.49174216389656067, 0.4925839900970459, 0.49286457896232605, 0.4937865734100342, 0.4937865734100342, 0.495269775390625, 0.4943878650665283, 0.4947887361049652, 0.4935460686683655, 0.4955103099346161, 0.4991982579231262, 0.49703359603881836, 0.50946044921875]}
```"
28624,TypeError: '<' not supported between instances of 'list' and 'tuple'.,"
I **git pull http directly** and **not git fork any more**...
So, I do **NOT** provide **pull request** for now.

A bug:
```
TypeError: '<' not supported between instances of 'list' and 'tuple'.
```

File **third_party/gpus/find_cuda_config.py**

Line 447:
```
    if cuda_version.split(""."") < (10, 1):
```
to
```
    ver = cuda_version.split(""."")
    ver1 = int(ver[0])
    ver2 = int(ver[1])
    if (ver1, ver2) < (10, 1):
```

BTW, I'm using **CUDA 10.1** 


Cheers
Pei"
28622,"expected conv2d_input to have 4 dimensions with shape(1, 1)","after i fit my machinelerning i trying to predict a new one. as image named demo1.jpg

what i expected get new feature. into my library:

My detailts:
RTX 2080
Tensorflow  1.13.1
Cuda 10.0

Can some one help me? Thanks !!

I'm using tf.keras and I'm getting following error:

> ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 1)

I got new error then i mode.predict a new (feature):

```
import tensorflow as tf
import numpy as np
import pickle
import cv2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D

IMG_SIZE = 50

def prepare(file):
    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)
    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))

    predictdata = tf.reshape(new_array, (1, 50, 50))
    predictdata = np.expand_dims(predictdata, -1)
    return predictdata


pickle_ind = open(""x.pickle"", ""rb"")
x = pickle.load(pickle_ind)
x = np.array(x, dtype=float)
x = np.expand_dims(x, -1)

pickle_ind = open(""y.pickle"", ""rb"")
y = pickle.load(pickle_ind)

n_batch = len(x)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(1, activation='softmax'))

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(x, y, epochs=1, batch_size=n_batch)
prediction = model.predict([prepare('demo1.jpg')], batch_size=n_batch, steps=1, verbose=1)

print(prediction)
```"
28621,About transformer,"transformer:https://www.tensorflow.org/alpha/tutorials/text/transformer#top_of_page

Has anyone run this experiment, and the results of my run have not reached the official results. I posted my code and helped me find the reason.

```python
import tensorflow_datasets as tfds
import tensorflow as tf

import time
import numpy as np
import matplotlib.pyplot as plt

from tqdm.auto import tqdm
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

print(tf.__version__)

if tf.test.is_gpu_available():
    device = ""/gpu:0""
else:
    device = ""/cpu:0""


print(""(1):Reading dataset and token......"")
examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (en.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)

tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    (pt.numpy() for pt, en in train_examples), target_vocab_size=2 ** 13)


BUFFER_SIZE = 20000
BATCH_SIZE = 64

""""""Add a start and end token to the input and target.""""""


def encode(lang1, lang2):
    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(
        lang1.numpy()) + [tokenizer_pt.vocab_size + 1]

    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(
        lang2.numpy()) + [tokenizer_en.vocab_size + 1]

    return lang1, lang2


""""""Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens.""""""

MAX_LENGTH = 40


def filter_max_length(x, y, max_length=MAX_LENGTH):
    return tf.logical_and(tf.size(x) <= max_length,
                          tf.size(y) <= max_length)


""""""Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value.""""""


def tf_encode(pt, en):
    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])

print(""(2):Encode and padded batch......"")
train_dataset = train_examples.map(tf_encode)
train_dataset = train_dataset.filter(filter_max_length)
# cache the dataset to memory to get a speedup while reading from it.
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(
    BATCH_SIZE, padded_shapes=([-1], [-1]))
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

val_dataset = val_examples.map(tf_encode)
val_dataset = val_dataset.filter(filter_max_length).padded_batch(
    BATCH_SIZE, padded_shapes=([-1], [-1]))



def get_angles(pos, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angle_rates


def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)

    # apply sin to even indices in the array; 2i
    sines = np.sin(angle_rads[:, 0::2])

    # apply cos to odd indices in the array; 2i+1
    cosines = np.cos(angle_rads[:, 1::2])

    pos_encoding = np.concatenate([sines, cosines], axis=-1)

    pos_encoding = pos_encoding[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)


def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)

    # add extra dimensions so that we can add the padding
    # to the attention logits.
    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)



def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask  # (seq_len, seq_len)



def scaled_dot_product_attention(q, k, v, mask):
    """"""Calculate the attention weights.
    q, k, v must have matching leading dimensions.
    The mask has different shapes depending on its type(padding or look ahead)
    but it must be broadcastable for addition.

    Args:
      q: query shape == (..., seq_len_q, depth)
      k: key shape == (..., seq_len_k, depth)
      v: value shape == (..., seq_len_v, depth)
      mask: Float tensor with shape broadcastable
            to (..., seq_len_q, seq_len_k). Defaults to None.

    Returns:
      output, attention_weights
    """"""

    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)

    # scale matmul_qk
    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    # add the mask to the scaled tensor.
    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    # softmax is normalized on the last axis (seq_len_k) so that the scores
    # add up to 1.
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)

    output = tf.matmul(attention_weights, v)  # (..., seq_len_v, depth)

    return output, attention_weights

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        """"""Split the last dimension into (num_heads, depth).
        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)
        """"""
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)  # (batch_size, seq_len, d_model)
        k = self.wk(k)  # (batch_size, seq_len, d_model)
        v = self.wv(v)  # (batch_size, seq_len, d_model)

        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)
        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)

        scaled_attention = tf.transpose(scaled_attention,
                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_v, num_heads, depth)

        concat_attention = tf.reshape(scaled_attention,
                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_v, d_model)

        output = self.dense(concat_attention)  # (batch_size, seq_len_v, d_model)

        return output, attention_weights


def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)
        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)
    ])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(EncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)

        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)

        return out2


class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(DecoderLayer, self).__init__()

        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)

        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)
        self.dropout3 = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training,
             look_ahead_mask, padding_mask):
        # enc_output.shape == (batch_size, input_seq_len, d_model)

        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)

        attn2, attn_weights_block2 = self.mha2(
            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)

        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)

        return out3, attn_weights_block1, attn_weights_block2


class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 rate=0.1):
        super(Encoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)
                           for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        # adding embedding and position encoding.
        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)

        return x  # (batch_size, input_seq_len, d_model)


class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,
                 rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)

        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)
                           for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)

    def call(self, x, enc_output, training,
             look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1]
        attention_weights = {}

        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x, block1, block2 = self.dec_layers[i](x, enc_output, training,
                                                   look_ahead_mask, padding_mask)

            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1
            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2

        # x.shape == (batch_size, target_seq_len, d_model)
        return x, attention_weights


""""""## Create the Transformer

Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.
""""""


class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 target_vocab_size, rate=0.1):
        super(Transformer, self).__init__()

        self.encoder = Encoder(num_layers, d_model, num_heads, dff,
                               input_vocab_size, rate)

        self.decoder = Decoder(num_layers, d_model, num_heads, dff,
                               target_vocab_size, rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training, enc_padding_mask,
             look_ahead_mask, dec_padding_mask):
        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)

        # dec_output.shape == (batch_size, tar_seq_len, d_model)
        dec_output, attention_weights = self.decoder(
            tar, enc_output, training, look_ahead_mask, dec_padding_mask)

        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)

        return final_output, attention_weights


num_layers = 4
d_model = 128
dff = 512
num_heads = 8

input_vocab_size = tokenizer_pt.vocab_size + 2
target_vocab_size = tokenizer_en.vocab_size + 2
dropout_rate = 0.1


class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)


learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

temp_learning_rate_schedule = CustomSchedule(d_model)

plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
plt.ylabel(""Learning Rate"")
plt.xlabel(""Train Step"")

""""""## Loss and metrics

Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.
""""""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')


def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)


train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
val_loss = tf.keras.metrics.Mean(name='val_loss')
val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')

""""""## Training and checkpointing""""""

transformer = Transformer(num_layers, d_model, num_heads, dff,
                          input_vocab_size, target_vocab_size, dropout_rate)


def create_masks(inp, tar):
    # Encoder padding mask
    enc_padding_mask = create_padding_mask(inp)

    # Used in the 2nd attention block in the decoder.
    # This padding mask is used to mask the encoder outputs.
    dec_padding_mask = create_padding_mask(inp)

    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by
    # the decoder.
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask


""""""Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs.""""""

checkpoint_path = ""./checkpoints/train""

ckpt = tf.train.Checkpoint(transformer=transformer,
                           optimizer=optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# if a checkpoint exists, restore the latest checkpoint.
if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print('Latest checkpoint restored!!')


EPOCHS = 200


train_num = len([1 for _, _ in train_dataset])

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]

    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)

    with tf.GradientTape() as tape:
        predictions, _ = transformer(inp, tar_inp,
                                     True,
                                     enc_padding_mask,
                                     combined_mask,
                                     dec_padding_mask)
        loss = loss_function(tar_real, predictions)

    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

    train_loss(loss)
    train_accuracy(tar_real, predictions)

@tf.function
def val_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
    predictions, _ = transformer(inp,tar_inp,
                                enc_padding_mask=enc_padding_mask,
                                look_ahead_mask=combined_mask,
                                dec_padding_mask=dec_padding_mask,
                                training=False)
    loss = loss_function(tar_real, predictions)
    ppl = tf.exp(loss)
    val_loss(ppl)
    val_accuracy(tar_real, predictions)

print(""(3):Traning model......"")
""""""Portuguese is used as the input language and English is the target language.""""""

for epoch in range(EPOCHS):
    train_loss.reset_states()
    train_accuracy.reset_states()
    val_loss.reset_states()
    val_accuracy.reset_states()

    print('Epoch {}'.format(epoch + 1))
    start = time.time()
    # inp -> portuguese, tar -> english
    with tqdm(total=train_num * BATCH_SIZE) as pbar:
        for inp, tar in train_dataset:
            train_step(inp, tar)
            pbar.update(BATCH_SIZE)

    for inp, tar in val_dataset:
        val_step(inp, tar)

    end = time.time()
    print('train_loss {:.4f}\ttrain_acc {:.2f}\t'
          'val_loss {:.4f}\tval_acc {:.2f}\t'
          'time {:.2f}s'.format(train_loss.result(),
                                train_accuracy.result() * 100,
                                val_loss.result(),
                                val_accuracy.result() * 100,
                                end - start,
                                ))


def evaluate(inp_sentence):
    start_token = [tokenizer_pt.vocab_size]
    end_token = [tokenizer_pt.vocab_size + 1]

    # inp sentence is portuguese, hence adding the start and end token
    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token
    encoder_input = tf.expand_dims(inp_sentence, 0)

    # as the target is english, the first word to the transformer should be the
    # english start token.
    decoder_input = [tokenizer_en.vocab_size]
    output = tf.expand_dims(decoder_input, 0)

    for i in range(MAX_LENGTH):
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(
            encoder_input, output)

        # predictions.shape == (batch_size, seq_len, vocab_size)
        predictions, attention_weights = transformer(encoder_input,
                                                     output,
                                                     False,
                                                     enc_padding_mask,
                                                     combined_mask,
                                                     dec_padding_mask)

        # select the last word from the seq_len dimension
        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)

        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

        # return the result if the predicted_id is equal to the end token
        if tf.equal(predicted_id, tokenizer_en.vocab_size + 1):
            return tf.squeeze(output, axis=0), attention_weights

        # concatentate the predicted_id to the output which is given to the decoder
        # as its input.
        output = tf.concat([output, predicted_id], axis=-1)

    return tf.squeeze(output, axis=0), attention_weights


def plot_attention_weights(attention, sentence, result, layer):
    fig = plt.figure(figsize=(16, 8))

    sentence = tokenizer_pt.encode(sentence)

    attention = tf.squeeze(attention[layer], axis=0)

    for head in range(attention.shape[0]):
        ax = fig.add_subplot(2, 4, head + 1)

        # plot the attention weights
        ax.matshow(attention[head][:-1, :], cmap='viridis')

        fontdict = {'fontsize': 10}

        ax.set_xticks(range(len(sentence) + 2))
        ax.set_yticks(range(len(result)))

        ax.set_ylim(len(result) - 1.5, -0.5)

        ax.set_xticklabels(
            ['<start>'] + [tokenizer_pt.decode([i]) for i in sentence] + ['<end>'],
            fontdict=fontdict, rotation=90)

        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result
                            if i < tokenizer_en.vocab_size],
                           fontdict=fontdict)

        ax.set_xlabel('Head {}'.format(head + 1))

    plt.tight_layout()
    plt.show()


def translate(sentence, plot=''):
    result, attention_weights = evaluate(sentence)

    predicted_sentence = tokenizer_en.decode([i for i in result
                                              if i < tokenizer_en.vocab_size])

    print('Input: {}'.format(sentence))
    print('Predicted translation: {}'.format(predicted_sentence))

    if plot:
        plot_attention_weights(attention_weights, sentence, result, plot)

print(""(4):Evaluate model......"")
translate(""este é um problema que temos que resolver."")
print(""Real translation: this is a problem we have to solve ."")

translate(""os meus vizinhos ouviram sobre esta ideia."")
print(""Real translation: and my neighboring homes heard about this idea ."")

translate(""vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram."")
print(
    ""Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened ."")

""""""You can pass different layers and attention blocks of the decoder to the `plot` parameter.""""""

translate(""este é o primeiro livro que eu fiz."", plot='decoder_layer4_block2')
print(""Real translation: this is the first book i've ever done."")

```"
28619,pass sample weight into py_func,"I tried to use the sklearn roc_auc_score function in a customized metric and would like to pass the sample weight. Here is my code:

```
from sklearn.metrics import roc_auc_score
from keras.layers import Input, Dense
from keras.models import Model
from keras import backend as K
import tensorflow as tf
import numpy as np
from functools import partial

def auc(weight):
    def metric(y_true, y_pred):
        score = tf.py_func(partial(roc_auc_score, sample_weight=weight), (y_true, y_pred), tf.float32)
        K.get_session().run(tf.local_variables_initializer())
        return score
    return metric

x=Input(shape=(10, ))
weights = Input(shape=(1,))
hidden = Dense(10, activation='relu')(x)
result = Dense(1, activation='sigmoid')(hidden)
model = Model(inputs=[x, weights], outputs=result)
model.compile('adam', 'binary_crossentropy', metrics=[auc(weights)])

X = np.random.rand(10000, 10)
y = np.random.randint(2, size=(10000, 1))
w = np.random.rand(10000, 1)
X_val = np.random.rand(100, 10)
y_val = np.random.randint(2, size=(100, 1))
w_val = np.random.rand(100, 1)
model.fit([X, w], y, epochs=20, sample_weight=w.flatten(), validation_data=([X_val, w_val], y_val), verbose=2)

```
But I got the error message:
TypeError: object of type 'Tensor' has no len()
	 [[{{node metrics_7/metric/PyFunc}} = PyFunc[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_FLOAT], token=""pyfunc_2"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_dense_4_target_5_0_2, dense_4/Sigmoid)]]

"
28614,Keras RNN example from docs does not support statefulness when multilayer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.7 (Anaconda)
- CUDA/cuDNN version: 9.2/7.3.1
- GPU model and memory: GTX 1070 Ti

**Describe the current behavior**
Modifying the example code given [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) to have `stateful=True` leads to the following error:
```
Traceback (most recent call last):
  File ""tmp.py"", line 6, in <module>
    y = layer(x)
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 701, in __call__                                                                                                                                
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 538, in __call__                                                                                                                               
    self._maybe_build(inputs)
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1603, in _maybe_build                                                                                                                          
    self.build(input_shapes)
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 636, in build                                                                                                                                   
    self.reset_states()
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 906, in reset_states                                                                                                                            
    tensor_shape.as_shape(dim).as_list()))
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 2833, in set_value
    value = np.asarray(value, dtype=dtype(x))
  File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 1015, in dtype
    return x.dtype.base_dtype.name
AttributeError: 'list' object has no attribute 'dtype'
```

**Describe the expected behavior**
Code should run with no error

**Code to reproduce the issue**
```
cells = [tf.keras.layers.LSTMCell(32), tf.keras.layers.LSTMCell(64)]
x = tf.keras.Input(batch_shape=(42, None, 5)) 
layer = tf.keras.layers.RNN(cells, stateful=True)
y = layer(x)
```"
28613,Issue with transformer guide,"## URL(s) with the issue:
https://www.tensorflow.org/alpha/tutorials/text/transformer
## Description of issue (what needs changing):
format and normalization layer.

### Clear description
1. when describe the multi-head attention, there is this text in one line:  ""Multi-head attention consists of four parts: * Linear layers and split into heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer. ""
  I guess the * mean bullet items. It is not formatted correctly.
2. The EncoderLayer and DecoderLayer use LayerNormalization. There is no LayerNormalization in keras.layers. There is only BatchNormalization.
3. Evaluate step creates mask. No mask is needed since a) there is no padding for a single sentence. b) there is no look_ahead since we are trying to predict next words. 
"
28610,tf.cast() throws an error using DEVICE_PLACEMENT_EXPLICIT that contradicts the argument's .device,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubunut 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 10
- GPU model and memory: GeForce GTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
After getting tf.shape() on a tensor that's on the GPU, the resulting tensor says it's on the GPU. Calling tf.cast() has an error saying it's on the CPU. Explicitly calling .gpu() makes tf.cast() work, even though .device tells me it's on the gpu.

**Describe the expected behavior**
Either tf.cast() gives me a tensor that tells me it's on the CPU, or else tf.cast() should work correctly. Either the tensor's .device is wrong, or tf.cast() is wrong.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from __future__ import print_function
import tensorflow as tf
tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)
import numpy as np

# Create an arbitrary tensor on the GPU.
a = tf.convert_to_tensor(np.zeros((5, 5)), dtype=tf.float32).gpu()
print(a.device)  # says it's on the GPU, correctly

b = tf.shape(a)
print(b.device)  # says it's on the GPU

c = tf.cast(b, tf.float32) # error, says first argument is on the CPU
c = tf.cast(b.gpu(), tf.float32)  # no error

print(b.device == b.gpu().device)  # True
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28608,Tensorflow gives incorrect results for simple example,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.1-2-g09e3b09e69 1.13.1
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GeForce GTX 1080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The actual output is [0.7853982 1.1071488]

**Describe the expected behavior**
The expected output is [0.7853982 0.7853982]

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```

from __future__ import print_function
import tensorflow as tf

with tf.device('cpu:0'):
    val0 = tf.ones((1,), dtype=tf.float32)
    val0 = tf.Print(val0, [])
    a = tf.atan2(val0, val0)
    b = tf.atan2(val0 + 1, val0 + 1)
    c = tf.concat([a, b], axis=0)
    c = tf.identity(c)

with tf.Session() as sess:
    print(sess.run(c))
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



"
28607,Unable to install tensorflow-gpu (python 3.6 windows),"Where did I go wrong?

**System information**
- OS Platform and Distribution: Windows 10 Pro 10.0.17134 Build 17134
- CUDA Version: 10.1
- cuDNN version: 7.5.1
- TensorFlow version: tensorflow-gpu 1.13.1
- Python version: python-3.6.8-amd64 (64-bit version)
- Installed using pip in virtualenv
- GPU model and memory: NVIDIA GeForce GTX 980 4GB VRAM
(checked versions here: https://www.tensorflow.org/install/gpu)
(and step by step followed this: https://www.tensorflow.org/install/pip)

**Describe the problem**
TensorFlow GPU does not import correctly in Python 3.6 on Windows.
Cannot find any 


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed cuda_10.1.105_418.96_win10.exe
Downloaded and copied the 3 folders from cudnn-10.1-windows10-x64-v7.5.1.10 into C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1 (replaced files)
Download and install the Microsoft Visual C++ 2015 Redistributable Update 3 from provided link
Clean install of Python 3.6.8 (added to path) (C:\Python36 & C:\Python36\Scripts)

```
C:\> pip --version
pip 19.1.1 from c:\python36\lib\site-packages\pip (python 3.6)
C:\> pip install -U pip virtualenv
C:\> virtualenv --system-site-package ./tfgpu
C:\>.\tfgpu\Scripts\activate

(tfgpu) C:\>pip list
Package    Version
---------- -------
pip        19.1.1
setuptools 41.0.1
virtualenv 16.5.0
wheel      0.33.3

(tfgpu) C:\> pip install --upgrade tensorflow-gpu
(tfgpu) C:\> pip list
Package              Version
-------------------- -------
absl-py              0.7.1
astor                0.7.1
gast                 0.2.2
grpcio               1.20.1
h5py                 2.9.0
Keras-Applications   1.0.7
Keras-Preprocessing  1.0.9
Markdown             3.1
mock                 3.0.5
numpy                1.16.3
pip                  19.1.1
protobuf             3.7.1
setuptools           41.0.1
six                  1.12.0
tensorboard          1.13.1
tensorflow-estimator 1.13.0
tensorflow-gpu       1.13.1
termcolor            1.1.0
virtualenv           16.5.0
Werkzeug             0.15.2
wheel                0.33.3

(tfgpu) C:\>python -c ""import tensorflow as tf""
Traceback (most recent call last):
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\tfgpu\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\tfgpu\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\tfgpu\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\tfgpu\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\tfgpu\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\tfgpu\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```



**Any other info / logs**
Also created a new project in PyCharm (project name: tfgpu) with the existing interpreter which successfully finds tensorflow after typing ""import t"" and lists it under Settings->Project: tfgpu -> Project Interpreter.

Run gives the exact same error as above."
28606,tflite GPU Delegate sub operator not supported,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (But the model should be running)
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus3 (Android 8.0)
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.13
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: NIL
- GPU model and memory: NIL

**Describe the current behavior**
The TFLite GPU Delegate benchmark tool provides support for sub operator  to run on the GPU of the mobile. 
**sub operator which was included to the model, is not running on the GPU as part of GPU delegate, but falls back to CPU.** 


**Describe the expected behavior**
Sub operator should be running on the GPU as per the documentation provided. 

**Code to reproduce the issue**
Attached with this, is the models and error logs of the models. The model is a modified version of the Deeplab GPU delegate model provided by google. (Input size is 197)

**Graph Appending Code**

**trial.tflite(sub model)**
```
output1 = tf.reshape(tf.strided_slice(tf.get_default_graph().get_tensor_by_name(""ResizeBilinear_2:0""), begin=[0,0,0,0], end=[1,197,197,1], strides=[1,1,1,1]), shape=[1,-1])
output2 = tf.reshape(tf.strided_slice(tf.get_default_graph().get_tensor_by_name(""ResizeBilinear_2:0""), begin=[0,0,0,1], end=[1,197,197,2], strides=[1,1,1,1]), shape=[1,-1])
output3 = tf.subtract(output2, output1)

```

**Benchmark Tool Log**

**trial.tflite(sub model)**


`adb shell /data/local/tmp/benchmark_model_gpu --graph=/data/local/tmp/trial.tflite --use_gpu=true`

Loaded model /data/local/tmp/trial.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
SUB: Incorrect operation type passed
First 74 operations will run on the GPU, and the remaining 1 on the CPU.
Applied GPU delegate.
Initialized session in 744.972ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=11 first=91729 curr=37009 min=36876 max=91729 avg=46306.5 std=16106

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=37205 curr=37165 min=36706 max=37530 avg=37075.8 std=158

============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                DELEGATE	        1	    37.034	    99.906%	    99.906%	     0.000	        1
	                     SUB	        1	     0.035	     0.094%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=37200 curr=37161 min=36700 max=37520 avg=37070.4 std=158
Memory (bytes): count=0
2 nodes observed


Average inference timings in us: Warmup: 46306.5, Init: 744972, no stats: 37075.8

**TFLITE File**
The tflite file is attached below:
[trial.tflite](https://github.com/tensorflow/tensorflow/files/3167392/trial.tflite.zip)

**Screenshot of modified part**
![image](https://user-images.githubusercontent.com/41156980/57543244-7a305f80-7371-11e9-960c-efd3262ca719.png)

"
28605,FailedPreconditionError (see above for traceback): Attempting to use uninitialized value ,"FailedPreconditionError (see above for traceback): Attempting to use uninitialized value con1/bias
         [[node con1/bias/read (defined at c:/users/tran thi diem/documents/diem/research/reinforcementlearning/code/codeforlearningreinforcement/cnnandrl/object-recognition-cifar-10-master/code/codechuan/10_5_2019/2_5_2019_cifar_modify_bangchay_testingwithrl.py:60) ]]

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10

- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
**Describe the current behavior**
I have this bug but I don't know how to fix this
**Describe the expected behavior**

**Code to reproduce the issue**
import tensorflow as tf
from keras.datasets import cifar10
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
#from tesnsorlow import keras


#import random
import numpy as np
from scipy.stats import spearmanr

from sklearn.utils import shuffle

def lr_schedule(epoch):
    lrate = 0.001
    if epoch > 75:
        lrate = 0.0005
    elif epoch > 100:
        lrate = 0.0003        
    return lrate
class DQNetwork:
    
    def __init__(self):
#        self.state_size = state_size
 #       self.action_size = action_size
        self.action = tf.placeholder(tf.float32)
        if (self.action == 0):
            test = 5
        else:
            test = 7
            
        sess = tf.Session()
        sess.run(tf.global_variables_initializer())
        self.learning_rate = tf.placeholder(tf.float32)

        self.inputs_ = tf.placeholder(tf.float32, [None, *state_size])       

        self.label = tf.placeholder(tf.float32, [None,10])
        

        self.filter_size_layer1 = tf.placeholder(tf.float32) 
        self.filter_size_layer2 = tf.placeholder(tf.float32) 
        self.filter_size_layer3 = tf.placeholder(tf.float32) 
        self.filter_size_layer4 = tf.placeholder(tf.float32) 
        self.filter_size_layer5 = tf.placeholder(tf.float32) 
        self.filter_size_layer6 = tf.placeholder(tf.float32)
        ###############
#layer 1
        # Input is 84x84x4 
        self.conv1 = tf.layers.conv2d(inputs=self.inputs_,
                                      filters=1,
                                      kernel_size=[test,test],
                                     
                                      #kernel_size=[3, 3],
                                      #kernel_size=[self.filter_size_layer1,self.filter_size_layer1],
                                      strides=[1,1],
                                      padding=""same"",  
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      name = ""con1"",
                                      reuse=tf.AUTO_REUSE
                                      #kernel_regularizer=regularizers.l2(1e-4)
                                      )
       
       
        self.weight_conv1 = tf.all_variables()
        
        
        self.conv1_activation = tf.nn.elu(self.conv1)

        self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
        
       
      
#layer2

        self.conv2 = tf.layers.conv2d(inputs=self.conv1_batchnorm,
                                      filters=32,
                                      kernel_size=[3, 3],
                                      strides=[1,1],
                                      padding=""same"",
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      name = ""con2"",
                                      reuse=tf.AUTO_REUSE
                                      #kernel_regularizer=regularizers.l2(1e-4)
                                      )
        self.conv2_activation = tf.nn.elu(self.conv2)
        self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
        self.conv2_Maxpool = tf.layers.max_pooling2d(self.conv2_batchnorm, 1, 2)
        #self.conv2_Maxpool = tf.layers.MaxPooling2D(self.conv2_batchnorm, pool_size =(2,2)) 
        self.conv2_Dropout = tf.layers.dropout (self.conv2_Maxpool,rate = 0.2)
        

#layer3
        self.conv3 = tf.layers.conv2d(inputs=self.conv2_Dropout,
                                      filters=64,
                                      kernel_size=[3,3],
                                      strides=[1,1],
                                      padding=""same"",
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      name = ""con3"",
                                      reuse=tf.AUTO_REUSE
                                      
                                      #kernel_regularizer=regularizers.l2(1e-4)
                                      )
        self.conv3_activation = tf.nn.elu(self.conv3)

        self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
#layer 4
       


        self.conv4 = tf.layers.conv2d(inputs=self.conv3_batchnorm,
                                      filters=64,
                                      kernel_size=[3, 3],
                                      strides=[1,1],
                                      padding=""same"",
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      reuse=tf.AUTO_REUSE,
                                      name = ""con4""
                                      
                                      #kernel_regularizer=regularizers.l2(1e-4)
                                      )
        
        self.conv4_activation = tf.nn.elu(self.conv4)
        self.conv4_batchnorm = tf.layers.batch_normalization(self.conv4_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
        self.conv4_Maxpool = tf.layers.max_pooling2d(self.conv4_batchnorm, 1, 2)

        #self.conv4_Maxpool = tf.layers.MaxPooling2D(self.conv4_batchnorm, pool_size =(2,2)) 
        self.conv4_Dropout = tf.layers.dropout (self.conv4_Maxpool,rate = 0.3)
        
        
#layer5
        self.conv5 = tf.layers.conv2d(inputs=self.conv4_Dropout,
                                      filters=128,
                                      kernel_size=[3,3],
                                      strides=[1,1],
                                      padding=""same"",
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      reuse=tf.AUTO_REUSE,
                                      name = ""con5""
                                     # kernel_regularizer=regularizers.l2(1e-4)
                                      )
        self.conv5_activation = tf.nn.elu(self.conv5)

        self.conv5_batchnorm = tf.layers.batch_normalization(self.conv5_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
#layer 6
       


        self.conv6 = tf.layers.conv2d(inputs=self.conv5_batchnorm,
                                      filters=128,
                                      kernel_size=[3, 3],
                                      strides=[1,1],
                                      padding=""same"",
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      reuse=tf.AUTO_REUSE,
                                      name = ""con6""
                                     # kernel_regularizer=regularizers.l2(1e-4)
                                      )
        self.conv6_activation = tf.nn.elu(self.conv6)
        self.conv6_batchnorm = tf.layers.batch_normalization(self.conv6_activation,
                                                             training=True,
                                                             epsilon=1e-5
                                                             )
        self.conv6_Maxpool = tf.layers.max_pooling2d(self.conv6_batchnorm, 1, 2)

       # self.conv6_Maxpool = tf.layers.MaxPooling2D(self.conv6_batchnorm, pool_size =(2,2)) 
        self.conv6_Dropout = tf.layers.dropout (self.conv6_Maxpool,rate = 0.4)   
      
          
#fully connected        
        
           

        self.flatten = tf.layers.flatten(self.conv6_Dropout)
        ## --> [43,264]

        self.output = tf.layers.dense(inputs=self.flatten,
                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      #kernel_regularizer=regularizers.l2(1e-4),
                                      units=10,
                                      activation='softmax')
        
        self.targets1 = tf.squeeze(tf.cast(self.label, tf.int32))   # Get predicted values by finding which logit is the greatest
        self.targets = tf.cast(tf.argmax(self.targets1, 1), tf.int32)        
        self.batch_predictions = tf.cast(tf.argmax(self.output, 1), tf.int32)
        self.predicted_correctly = tf.equal(self.batch_predictions, self.targets)
    # Average the 1's and 0's (True's and False's) across the batch size
        self.accuracy = tf.reduce_mean(tf.cast(self.predicted_correctly, tf.float32))
        
        
        self.cross_entropy =  tf.nn.softmax_cross_entropy_with_logits_v2(logits= self.output, labels=self.label)
        self.gradient = tf.gradients (self.cross_entropy,self.weight_conv1)  
        self.loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits= self.output, labels=self.label) )
          
        #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits = self.output,label = self.label)
       # self.loss = tf.reduce_mean(cross_entropy, name='cross_entropy')
        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate,decay=1e-6).minimize(self.loss)
        
        sess = tf.Session()
        sess.run(tf.global_variables_initializer())
        
        
       
        #self.optimizer = tf.train.AdamOptimizer(self.learning_rate,decay=1e-6).minimize(self.loss)


(X_train, y_train), (X_test, y_test) = cifar10.load_data()
#X_train = X_train.astype('float32')
#X_test = X_test.astype('float32')
#X_train = normalize(X_train)
#X_test = normalize(X_test)
#y_train = np_utils.to_categorical(y_train)
#y_test = np_utils.to_categorical(y_test)
#num_classes = y_test.shape[1]
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

#z-score
mean = np.mean(X_train,axis=(0,1,2,3))
std = np.std(X_train,axis=(0,1,2,3))
X_train = (X_train-mean)/(std+1e-7)
X_test = (X_test-mean)/(std+1e-7)

num_classes = 10
y_train = np_utils.to_categorical(y_train,num_classes) # convert the numbers to onehot vector
y_test = np_utils.to_categorical(y_test,num_classes)
#DQNetwork

action_size = 3
#learning_rate = 0.00001

state_size = [32,32,3]

# Exploration parameters for epsilon greedy strategy
explore_start = 1.0            # exploration probability at start
explore_stop = 0.01            # minimum exploration probability 
           # exponential decay rate for exploration prob

# Q learning hyperparameters
gamma = 0.95  
alpha = 0.5

 
sess = tf.Session()
sess.run(tf.global_variables_initializer())

Qtable_before =np.zeros((12500,3))
np.random.seed(100)
exp_exp_tradeoff = 0.25




generations = 20
batch = 64


def train_process (X_train,y_train,batch,before_loss_train,learnig_rate):
  #  gradient_descent_test = []
    number_image = 0   
    
    while (number_image <len(X_train[0:64])):
        before_state = X_train[number_image: number_image +batch]
        before_Y_label = y_train [number_image: number_image +batch]
   #while i<20:
               
        _,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],
                                                   feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate})
     
 
        number_image = number_image + batch   
            
               
        
       
    return current_loss,current_accuracy
        
        #print(""step {}: eps {} : loss {} "".format (i,eps, current_loss))
train_loss = []
train_accuracy = []
test_loss = []
test_accuracy = []
before_loss_train = 100
eps =0


datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    )
datagen.fit(X_train)


number_image = 0   
learning_rate = 0.1

gradient_total = []
shape = []

i =0
action = 1
dqn = DQNetwork()
    
while (number_image <len(X_train[0:640])):
    before_state = X_train[number_image: number_image +batch]
    before_Y_label = y_train [number_image: number_image +batch]
   # test = sess.run(dqn.test,feed_dict = {dqn.action:action})
    conv1 = sess.run([dqn.conv1],feed_dict = {dqn.inputs_: before_state,dqn.action:action})
   #while i<20:
    weight = sess.run(dqn.weight_conv1,feed_dict = {dqn.inputs_: before_state,dqn.action:action})       
    gradient = sess.run (dqn.gradient,feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate,dqn.action:action})
    _,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],
                                               feed_dict={dqn.inputs_:before_state,dqn.label:before_Y_label,dqn.learning_rate:learning_rate,dqn.action:action})
    number_image = number_image + batch
    gradient = np.mean (gradient [0])
    gradient_total.append(gradient)
    shape.append (np.array (weight[0]).shape)
    i = i+1
#    print (""\n weigth"",weight [0])
#    print (""\n bias"",weight [1])
#    print (""\n gradient weight"", np.mean (gradient[0]))
#    print (""\n gradient bias"", gradient[1])
    print(""step {}: loss {} : accuracy {} "".format (i, current_loss,current_accuracy))
    
    tf.reset_default_graph()
print (gradient_total)   
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28602,Custom layer build should fail if it includes tensor computations,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0
- Python version: Any

**Describe the current behavior**
Since https://github.com/tensorflow/tensorflow/commit/9df99aafc3480638cd8d2bf6a0caeab111b68302 it is required that there not be tensor computations in a layer's `build` method. We noticed this [because our layer serialization failed](https://github.com/tensorflow/addons/issues/203 ) after this commit. The fix was to move tensor computation into `call` method, though it was a silent fail and  would have been very difficult to troubleshoot if it didn't fail on a nightly. 

**Describe the expected behavior**
The build method should raise some kind of error saying that tensor computation is not allowed in build. It should also probably be mentioned in the documentation of TF or Keras:
https://www.tensorflow.org/tutorials/eager/custom_layers
https://keras.io/layers/writing-your-own-keras-layers/

It's sort of suggested but not well enough imo, especially with the hard to find fail it produces

**Code to reproduce the issue**
https://github.com/tensorflow/addons/pull/208

"
28601,Request for public APIs or alternatives: TF-Addons,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): TF 2.0

As part of our effort to make [tensorflow/addons](https://github.com/tensorflow/addons) independent of internal TensorFlow core APIs we've identified several things that we cannot find public API alternatives for; or would like to discuss other options. One such option is to copy the code statically into addons, but for anything that is likely to change in the future this isn't great.

cc @karmel 

## Testing
We heavily depend on testing decorators that are not exposed in the public API.

* [tensorflow.python.framework.test_util.run_all_in_graph_and_eager_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L893)
* [tensorflow.python.framework.test_util.run_in_graph_and_eager_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L957)
* [tensorflow.python.keras.keras_parameterized.run_all_keras_modes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/keras_parameterized.py#L176)
* [tensorflow.python.keras.testing_utils.layer_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/testing_utils.py#L71)
    * `layer_test` has been very helpful for catching serialization issues and other problems with our custom layers.
* [tensorflow.python.framework.test_util.run_deprecated_v1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1128)
     * `run_deprecated_v1` can probably be statically copied over? We should be looking to move away from this anyhow.

## Keras
* [tensorflow.python.keras.losses.LossFunctionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L177)
     * `LossFunctionWrapper` is used to convert our loss functions to a Keras Loss. It seems extensively used in core repository, but there is no API. Should we be refactoring our functions to inherit from base Loss? And if so why not do that internally as well?
* [tensorflow.python.keras.engine.base_layer_utils.mark_checked](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer_utils.py#L414)
     * This looks like we could easily re-create it using tf.nest. Just want to confirm that this isn't expected to used outside of TF often.
 

## RNN Ops
* [tensorflow.python.ops. rnn._transpose_batch_time](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L43)
* [tensorflow.python.ops. rnn_cell_impl.assert_like_rnncell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L75)
* [tensorflow.python.ops. rnn_cell_impl._zero_state_tensors](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L171)

## Control Flow
* [tensorflow.python.ops.control_flow_util.GetContainingXLAContext](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py#L200)
* [tensorflow.python.ops.control_flow_util.GetContainingWhileContext](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_util.py#L178)
"
28600,Tensorflow Lite GPU Delegate - Argmax Op Request,"<em>Tensorflow Lite GPU Delegate - Argmax Op Request</em>


**System information**
- TensorFlow version (you are using): Tensorflow 1.13
Tensorflow Lite GPU Delegate 0.0.0-nightly
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The current GPU delegate does not support Argmax op to run on the mobile GPU. It is one of the major stuff needed to be done for effective deeplab processing on the GPU, which could also be helpful in running the entire model on the GPU in the later stage. So, it would be useful if you could provide support for Argmax functionality on TFLite-GPU Delegate. In the current context, deeplab execution falls back from GPU to CPU on introduction of Argmax op. If that could not be done, can there be any other alternative to Argmax that can be used to effect a 2 class segmentation, or can a combination of ops can be added unto the base Model to effect a complete TFLite GPU Delegate model. If so, what has to be done?

**Will this change the current api? How?**
No. It is one more tflite op request. So, it must not affect other ops and functionality

**Who will benefit with this feature?**
Edge learning and Deeplab developers
"
28599,Restore from SavedModel not compatible with tf.distribute,"**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.0

**Describe the current behavior**

I'm using Tensorflow Hub to restore a model from a SavedModel, I do expect the restore to work even if it executed inside a distribution strategy scope, but instead it raises an exception.

**Describe the expected behavior**

It should load the model correctly even it the restore from the savedmodel is performed inside a distribution strategy context.

**Code to reproduce the issue**

```python
import tensorflow as tf
import tensorflow_hub as hub

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():

    model = tf.keras.Sequential(
        [
            hub.KerasLayer(
                ""https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2"",
                output_shape=[2048],
                trainable=True,
            )
        ]
    )
```

**Other info / logs**

The exception:

```
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py"", line 98, in __init__
    self._func = module_v2.load(handle)
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow_hub/module_v2.py"", line 80, in load
    return tf_v1.saved_model.load_v2(module_handle)
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 324, in load
    export_dir)
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 63, in __init__
    self._setup_functions_captures()
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 100, in _setup_functions_captures
    for node_id in proto.bound_inputs]
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 100, in <listcomp>
    for node_id in proto.bound_inputs]
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 117, in _get_tensor_from_node
    return obj.handle
  File ""/data/pgaleone/env/lib/python3.6/site-packages/tensorflow/python/distribute/values.py"", line 317, in __getattr__
    return getattr(self.get(), name)
AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'handle'
```"
28598,Tensorflow 2.0 rejection resample not producing the proper target distribution,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): from binary pip3
- TensorFlow version (use command below): 2.0-alpha
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX Titan V 12GB

**Issues**
I am trying to test rejection sampling with the following code. 
This is the result I got:

    target_dist [0.5, 0.5] 
    initial distribution [0.8333333333333334, 0.16666666666666666]
    result counts [1500, 600] 
    final dist 0.7142857142857143 0.2857142857142857

**Expectation**
The final distribution does not reflect the target distribution I set. I feel like this is a bug in the original algorithm because I can't seem to find what I did wrong in my code as shown below:

```
import tensorflow as tf
import numpy as np
# everything is based on tensorflow 2.0
tf.random.set_seed(2342)


def map2label(sample):
    return tf.cast(tf.math.equal(sample, 2), tf.int32)

np_data = np.array([0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2])
target_dist = [0.5, 0.5]
init_dist = [(np_data.shape[0]-3)/np_data.shape[0], 3/np_data.shape[0]]

dataset = tf.data.Dataset.from_tensor_slices(np_data)
rej = tf.data.experimental.rejection_resample(map2label, target_dist, init_dist, 2342)   # set seed explicitly
dataset = dataset.apply(rej)

bucket_counts = [0, 0]
for i in range(100):
    for data in dataset:
        class_id, data_content = data
        bucket_counts[class_id.numpy()] += 1

print(""This is your target_dist"", target_dist, ""This is your initial distribution"", init_dist)
print(""This is your result counts"", bucket_counts,
      ""This is your final dist"", bucket_counts[0] / np.sum(bucket_counts), bucket_counts[1] / np.sum(bucket_counts))

```"
28597,tensorflow 2.0 rejection resample printing proportion of examples ... message,"I am using tensorflow 2.0 CUDA 10.

The message comes from rejection sampling. I receive multiple lines similar to this:
> Proportion of examples rejected by sampler is high: [0.935080409][0.935080409 0.0649196059][0 1]

Turn the logging level to ERROR does not help. Is there a way to turn this off or direct it to a disk file so that it won't mess up my other progress messages? You can use the following snippet to reproduce:

```
import tensorflow as tf
import numpy as np
# everything is based on tensorflow 2.0

# this is important or rejection sample won't work
# I set this here so that all random process keeps the same order but you can
# also set it at rejection_resample function
tf.random.set_seed(2342)


def map2label(sample):
    return tf.cast(tf.math.equal(sample, 2), tf.int32)

np_data = np.array([0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2])
target_dist = [0.5, 0.5]
init_dist = [(np_data.shape[0]-3)/np_data.shape[0], 3/np_data.shape[0]]

dataset = tf.data.Dataset.from_tensor_slices(np_data)
rej = tf.data.experimental.rejection_resample(map2label, target_dist, init_dist, 20)
dataset = dataset.apply(rej)

bucket_counts = [0, 0]
for i in range(100):
    for data in dataset:
        class_id, data_content = data
        bucket_counts[class_id.numpy()] += 1
print(""This is your target_dist"", target_dist, ""This is your initial distribution"", init_dist)
print(""This is your result counts"", bucket_counts,
      ""This is your final dist"", bucket_counts[0] / np.sum(bucket_counts), bucket_counts[1] / np.sum(bucket_counts))
```
"
28596,Keras code in colab from tensorflow.org is not showing tensorflow version details,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28595, I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0 ,"Hello , 
I need a help , when i try to execute program with tensorflow it frozen on this line:

>  I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0

I run this script :  
`import tensorflow as tf
tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)`

**System information**
- Windows 10 x64
- TensorFlow installed from (binary):
- TensorFlow version: 1.13.1
- Python version: 3.6
- Installed using pip
- CUDA/cuDNN version:10.0/7.5
- GPU model and memory : Nvidia Geforce 840m 



**Describe the problem**

As you can see this is a screenshot of the bug :  
![image](https://user-images.githubusercontent.com/19480228/57528912-d6748e80-7333-11e9-896b-b8840339303a.png)

"
28592,About slim.nets.inception_v3,"I have read the function Inception_v3_base, I have questions about the code of Mixed_5b, 5c, 5d. The paper replaces the 5x5 convolution with 1x1 3x3 3x3, but why replace the original 1x1 3x3 combination with 1x1 5x5 in the code."
28590,python3.exe crush after running python script with tensorflow,"Hello ,
I'm setting and cloned the this project :
https://github.com/yinguobing/cnn-facial-landmark
But when i run: 

> python3 landmark.py 


I get this bug : 

![ce](https://user-images.githubusercontent.com/19480228/57519953-c8ffda00-731c-11e9-9e8e-21c9db0e0387.PNG)

when i debug , i get this : 

![des](https://user-images.githubusercontent.com/19480228/57520070-13815680-731d-11e9-8b58-73e93587da41.PNG)


Note : I test tensorflow with code mentionned in the readme and it work perfectly . 

![image](https://user-images.githubusercontent.com/19480228/57520025-fa78a580-731c-11e9-8432-a4171c6b956a.png)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform: Windows 10 x64
- TensorFlow installed from (binary):
- TensorFlow version (1.13)
- Python version:(3.6)
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: Nvidia Geforce 840m 4.00 Go


How can i solve this error ?"
28589,Build TensorFlow Lite for ARM64 boards failed,"**System information**
- Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.14



**Describe the problem**
I got build error: undefined reference to `NnApiImplementation()'

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed https://www.tensorflow.org/lite/guide/build_arm64 . Here are the exact sequence of commands:
`git clone https://github.com/tensorflow/tensorflow && cd tensorflow`
`git checkout -b r1.14 origin/r1.14`
`./tensorflow/lite/tools/make/download_dependencies.sh` `./tensorflow/lite/tools/make/build_aarch64_lib.sh`

**Any other info / logs**
I got these error info when I execute `build_aarch64_lib.sh` :

/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::~NNAPIAllocation()':
nnapi_delegate.cc:(.text+0x28): undefined reference to `NnApiImplementation()'
/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::NNAPIAllocation(char const*, tflite::ErrorReporter*)':
nnapi_delegate.cc:(.text+0x1a4): undefined reference to `NnApiImplementation()'
/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIDelegate::~NNAPIDelegate()':
nnapi_delegate.cc:(.text+0x218): undefined reference to `NnApiImplementation()'
nnapi_delegate.cc:(.text+0x234): undefined reference to `NnApiImplementation()'
/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::addTensorOperands(tflite::Subgraph*, ANeuralNetworksModel*, unsigned int*, std::vector<long, std::allocator<long> >*)':
nnapi_delegate.cc:(.text+0x2b8): undefined reference to `NnApiImplementation()'
/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o):nnapi_delegate.cc:(.text+0x578): more undefined references to `NnApiImplementation()' follow
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:227: recipe for target '/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed
make: *** [/home/administrator/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1
make: *** Waiting for unfinished jobs....

"
28587,Erratum 843419 found and fixed at **,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  aarch64-linux-gnu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no 
- TensorFlow installed from (source or binary): source
- TensorFlow version:  tensorflow-1.13.0-rc2
- Python version: python 2.7
- Installed using virtualenv? pip? conda?: native python 2.7
- Bazel version (if compiling from source):  0.19.2- (@non-git)
- GCC/Compiler version (if compiling from source):  gcc version 5.3.1 20160413 (Ubuntu/Linaro 5.3.1-14kord4)
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**
the detail info  is here [https://github.com/bazelbuild/bazel/issues/8285](https://github.com/bazelbuild/bazel/issues/8285)

could you help me? thanks."
28586,build error on aarch64,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  aarch64-linux-gnu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.0-rc2
- Python version: 2.7
- Installed using virtualenv? pip? conda?: native python 2.7
- Bazel version (if compiling from source): compile from source   `bazel version` output:
Build label: 0.19.2- (@non-git)
Build target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
...

- GCC/Compiler version (if compiling from source): `gcc -v`

COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/5/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 5.3.1-14kord4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-arm64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-arm64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-arm64 --with-arch-directory=aarch64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
**gcc version 5.3.1 20160413 (Ubuntu/Linaro 5.3.1-14kord4)**

- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
**first  have successfuly build bazel ,and can see bazel version info by `bazel version`,ad output :
Build label: 0.19.2- (@non-git)
...**

**then  run `bazel build --config=opt  //tensorflow:libtensorflow_cc.so`
there is all have compiled but when linking get an error below**

ERROR: /home/greatwall/recog_vote/tensorflow/tensorflow-1.13.0-rc2/tensorflow/BUILD:489:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1)
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_master_service_impl/grpc_master_service_impl.pic.o"", section 581, offset 0x00000180.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/eager/_objs/grpc_eager_service_impl/grpc_eager_service_impl.pic.o"", section 444, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc/init.pic.o"", section 24, offset 0x00000050.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_client_channel/client_channel.pic.o"", section 192, offset 0x000002e8.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.pic.o"", section 92, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/profiler/internal/_objs/tfprof_tensor/tfprof_tensor.pic.o"", section 72, offset 0x00000f60.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops/spacetodepth_op.pic.o"", section 168, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/one_hot_op/one_hot_op.pic.o"", section 789, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/unpack_op/unpack_op.pic.o"", section 204, offset 0x0000003c.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op/dynamic_partition_op.pic.o"", section 261, offset 0x0000003c.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/matrix_inverse_op/matrix_inverse_op.pic.o"", section 112, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/summary_op/summary_op.pic.o"", section 203, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.pic.o"", section 2108, offset 0x00000398.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_bool.pic.o"", section 274, offset 0x000000dc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex128.pic.o"", section 299, offset 0x00000318.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o"", section 246, offset 0x000000dc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/reduction_ops/reduction_ops_min.pic.o"", section 1426, offset 0x00000d78.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/set_kernels/set_kernels.pic.o"", section 997, offset 0x00000100.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/scatter_nd_op_cpu_impl_1.pic.o"", section 857, offset 0x000001a0.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_add_1.pic.o"", section 1800, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_pow.pic.o"", section 1347, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_real.pic.o"", section 123, offset 0x00000360.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_tan.pic.o"", section 167, offset 0x00000588.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/grappler/clusters/_objs/virtual_cluster/virtual_cluster.pic.o"", section 150, offset 0x00000cfc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/boringssl/_objs/crypto/bcm.pic.o"", section 1350, offset 0x0000026c.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_master_service_impl/grpc_master_service_impl.pic.o"", section 581, offset 0x00000180.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/eager/_objs/grpc_eager_service_impl/grpc_eager_service_impl.pic.o"", section 444, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc/init.pic.o"", section 24, offset 0x00000050.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_client_channel/client_channel.pic.o"", section 192, offset 0x000002e8.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/grpc/_objs/grpc_transport_chttp2/hpack_parser.pic.o"", section 92, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/profiler/internal/_objs/tfprof_tensor/tfprof_tensor.pic.o"", section 72, offset 0x00000f60.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops/spacetodepth_op.pic.o"", section 168, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/one_hot_op/one_hot_op.pic.o"", section 789, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/unpack_op/unpack_op.pic.o"", section 204, offset 0x0000003c.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op/dynamic_partition_op.pic.o"", section 261, offset 0x0000003c.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/matrix_inverse_op/matrix_inverse_op.pic.o"", section 112, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/summary_op/summary_op.pic.o"", section 203, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/batch_matmul_op/batch_matmul_op_real.pic.o"", section 2108, offset 0x00000398.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_bool.pic.o"", section 274, offset 0x000000dc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex128.pic.o"", section 299, offset 0x00000318.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o"", section 246, offset 0x000000dc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/reduction_ops/reduction_ops_min.pic.o"", section 1426, offset 0x00000d78.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/set_kernels/set_kernels.pic.o"", section 997, offset 0x00000100.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/scatter_nd_op_cpu_impl_1.pic.o"", section 857, offset 0x000001a0.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_add_1.pic.o"", section 1800, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_pow.pic.o"", section 1347, offset 0x00000000.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_real.pic.o"", section 123, offset 0x00000360.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_tan.pic.o"", section 167, offset 0x00000588.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/tensorflow/core/grappler/clusters/_objs/virtual_cluster/virtual_cluster.pic.o"", section 150, offset 0x00000cfc.
Erratum 843419 found and fixed at ""bazel-out/aarch64-opt/bin/external/boringssl/_objs/crypto/bcm.pic.o"", section 1350, offset 0x0000026c.
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::EnvironmentAWSCredentialsProvider::GetAWSCredentials(): error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetConfigProfileFilenameabi:cxx11: error: undefined reference to 'Aws::FileSystem::GetHomeDirectoryabi:cxx11'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilenameabi:cxx11: error: undefined reference to 'Aws::Environment::GetEnv[abi:cxx11](char const*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/AWSCredentialsProvider.pic.o:AWSCredentialsProvider.cpp:function Aws::Auth::ProfileConfigFileAWSCredentialsProvider::GetCredentialsProfileFilenameabi:cxx11: error: undefined reference to 'Aws::FileSystem::GetHomeDirectoryabi:cxx11'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/ClientConfiguration.pic.o:ClientConfiguration.cpp:function Aws::Client::ComputeUserAgentString(): error: undefined reference to 'Aws::OSVersionInfo::ComputeOSVersionStringabi:cxx11'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampStringToTimePoint(char const*, Aws::Utils::DateFormat): error: undefined reference to 'Aws::Time::TimeGM(tm*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToLocalTimeStruct() const: error: undefined reference to 'Aws::Time::LocalTime(tm*, long)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/DateTimeCommon.pic.o:DateTimeCommon.cpp:function Aws::Utils::DateTime::ConvertTimestampToGmtStruct() const: error: undefined reference to 'Aws::Time::GMTime(tm*, long)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function Aws::Utils::TempFile::~TempFile(): error: undefined reference to 'Aws::FileSystem::RemoveFileIfExists(char const*)'
bazel-out/aarch64-opt/bin/external/aws/_objs/aws/TempFile.pic.o:TempFile.cpp:function Aws::Utils::ComputeTempFileName(char const*, char const*): error: undefined reference to 'Aws::FileSystem::CreateTempFilePathabi:cxx11'

**and the last output**

collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 6534.318s, Critical Path: 428.70s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 1956 processes: 1956 local.
FAILED: Build did NOT complete successfully

there is all of the error about
Erratum 843419 found and fixed at ***

and bazel is compile all but the error for the last step of linking

**here is some info:**

- --enable-fix-cortex-a53-843419
    https://github.com/gcc-mirror/gcc/blob/master/gcc/doc/install.texi#L3439 in this web , tell me add this option
    could be ok, but i don't know how to add to bazel when building...
- arm64: errata: Check for --fix-cortex-a53-843419 and --fix-cortex-a53
    https://patchwork.kernel.org/patch/9406341/
    about the last of this web, here look like some info for help.


could you help me? thanks."
28585,The package org.tensorflow.lite.nnapi  does not exist,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution：Ubuntu 16.04:
- Mobile device：Pixel 2
- TensorFlow installed from：source
- TensorFlow version:1.13.1
- Python version:3.6
- Bazel version :0.24.1


**Describe the current behavior**
when i run the demo which in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo
so ,I found the bug,don't have  org.tensorflow.lite.nnapi 



"
28582,tf-nightly-gpu 1.14.1 failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error,"My Python version is 3.6.8, the operating system is Windows 10, the CUDA version is cuda_10.0.130,the cudnn version is cudnn-10.0-windows10-x64-v7.5.1.10, the GPU is Nvidia GTX965M, and the tf-nightly-gpu-1.14.1is installed.
When I run  ""import tensorflow as tf""
there is no problem.
When I run  ""python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""

An error was reported：
2019-05-10 12:33:26.728437: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error

How to solve this problem？Thanks！！"
28581,absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'h',"@jmhodges @ry @bmabey @djones I want to look at my own parameters in command line. I used `'python main.py -h '`  . But this error occurred 'absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'h''. I use tensorflow-gpu == 1.10.0 . Can you give me some advice ?

*Here is my code :*
```python
Flags = tf.app.flags

Flags.DEFINE_integer('rand_seed', 1 , 'random seed' )

# Directories
Flags.DEFINE_string('input_dir_LR', None, 'The directory of the input resolution input data, for inference mode')
Flags.DEFINE_string('input_dir_HR', None, 'The directory of the input resolution input data, for inference mode')
Flags.DEFINE_string('mode', 'inference', 'train, or inference')
Flags.DEFINE_string('output_dir', None, 'The output directory of the checkpoint')
Flags.DEFINE_string('output_pre', 'images', 'The output pre of the images')
Flags.DEFINE_string('output_ext', 'jpg', 'The format of the output when evaluating')
Flags.DEFINE_string('summary_dir', None, 'The dirctory to output the summary')

# Models
Flags.DEFINE_string('checkpoint', None, 'If provided, the weight will be restored from the provided checkpoint')
Flags.DEFINE_integer('num_resblock', 16, 'How many residual blocks are there in the generator')

# Machine resources
Flags.DEFINE_string('cudaID', '0', 'CUDA devices')

FLAGS = Flags.FLAGS
```
"
28580,keras.layers.BatchNormalization.call() with default training param does not reflect K.learning_phase() Tensor input when fed into feed_dict,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 update 1809
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.12.0
- **Python version**: 3.6.8
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDAv9.0.176 / cuDNNv7.4.2
- **GPU model and memory**: NVIDIA GeForce GTX 1060 6GB RAM

## Describe the problem
When using keras.layers.BatchNormalization, K.learning_phase() does not seem to have any affect during training when BatchNormalization.\_\_call__ is called with training as default param. However, if K.learning_phase() is explicitly passed in as BatchNormalization.\_\_call__(training=K.learning_phase()), the model is able to train batch norm's moving mean and moving variance ops.

#### Expected Behavior
Keras.layers.BatchNormalization is expected to fill in K.learning_phase() if training=None and ops are updated during training.

## Exact command to reproduce
Running below code snippet for BatchNormalization.\_\_call__(input) with non-supplied training param:

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input
from tensorflow.train import AdamOptimizer
import numpy as np
from keras import backend as K

shape = (100, 4)
data = np.random.random(size=shape)

tf.reset_default_graph()
graph = tf.Graph()

with graph.as_default():
  is_training = K.learning_phase()
  input_tensor = tf.placeholder(tf.float32, shape=shape)
  x = Input(tensor=input_tensor)
  layer = Dense(units=128, activation='relu')(x)
  layer = Dense(units=32, activation='relu')(layer)
  layer = Dense(units=4, activation='linear')(layer)
  bn = BatchNormalization()
  # line of interest
  layer = bn(layer)
  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)
  logits = Activation('relu')(layer)
  loss = tf.losses.mean_squared_error(logits, x)
  opt = AdamOptimizer()
  grads_and_vars = opt.compute_gradients(loss)
  opt = opt.apply_gradients(grads_and_vars)
  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                          scope='batch_normalization')

print([var.name for var in variables])
with tf.Session(graph=graph) as sess:
  sess.run(tf.global_variables_initializer())
  for idx in range(50):
    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})
    if not idx % 5: 
      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]
      outs = np.linalg.norm(outs, axis=1)
      print(f'Running {idx}: {outs}')
```
Produces with the following output (notice that moving_mean=0 and moving_variance=2 throughout) :
```
['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']
Running 0: [2.0005007e+00 1.7320185e-03 0.0000000e+00 2.0000000e+00 6.5435906e-04
 3.3886202e-08 1.7219670e-02 2.0985259e-05]
Running 5: [2.0065410e+00 8.4475391e-03 0.0000000e+00 2.0000000e+00 1.4017796e-02
 3.9132146e-06 1.1373939e-01 2.7749091e-04]
Running 10: [2.01387644e+00 1.56976394e-02 0.00000000e+00 2.00000000e+00
 2.52754204e-02 1.10197625e-05 1.15519769e-01 3.48794798e-04]
Running 15: [2.01952314e+00 2.05438156e-02 0.00000000e+00 2.00000000e+00
 1.88015848e-02 1.29885275e-05 7.03223869e-02 3.55401076e-04]
Running 20: [2.0201447e+00 2.0984445e-02 0.0000000e+00 2.0000000e+00 1.1408665e-02
 1.6400263e-05 3.1892959e-02 3.6953186e-04]
Running 25: [2.0176027e+00 1.9309264e-02 0.0000000e+00 2.0000000e+00 1.3336688e-02
 1.8967023e-05 3.1269908e-02 3.8181755e-04]
Running 30: [2.0159297e+00 1.7820496e-02 0.0000000e+00 2.0000000e+00 6.4963060e-03
 1.9346602e-05 2.3151563e-02 3.8251214e-04]
Running 35: [2.0171123e+00 1.7269025e-02 0.0000000e+00 2.0000000e+00 6.5930812e-03
 2.0723272e-05 6.8678367e-03 3.8147590e-04]
Running 40: [2.0202377e+00 1.7495051e-02 0.0000000e+00 2.0000000e+00 1.2473603e-02
 2.2621665e-05 7.9890825e-03 3.8188352e-04]
Running 45: [2.0230916e+00 1.8221466e-02 0.0000000e+00 2.0000000e+00 1.3377106e-02
 2.3886721e-05 1.5546208e-02 3.8239476e-04]
```

However, after explicitly adding training parameter to \_\_call__ with K.learning_phase(), training seems to be updating batch norm ops: 

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Input
from tensorflow.train import AdamOptimizer
import numpy as np
from keras import backend as K

shape = (100, 4)
data = np.random.random(size=shape)

tf.reset_default_graph()
graph = tf.Graph()

with graph.as_default():
  is_training = K.learning_phase()
  input_tensor = tf.placeholder(tf.float32, shape=shape)
  x = Input(tensor=input_tensor)
  layer = Dense(units=128, activation='relu')(x)
  layer = Dense(units=32, activation='relu')(layer)
  layer = Dense(units=4, activation='linear')(layer)
  bn = BatchNormalization()
  # line of interest
  layer = bn(layer, training=is_training)
  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, bn.updates)
  logits = Activation('relu')(layer)
  loss = tf.losses.mean_squared_error(logits, x)
  opt = AdamOptimizer()
  grads_and_vars = opt.compute_gradients(loss)
  opt = opt.apply_gradients(grads_and_vars)
  variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                          scope='batch_normalization')

print([var.name for var in variables])
with tf.Session(graph=graph) as sess:
  sess.run(tf.global_variables_initializer())
  for idx in range(50):
    out = sess.run([opt] + bn.updates, feed_dict={input_tensor:np.array(data), is_training:1})
    if not idx % 5: 
      outs = [sess.run(var, feed_dict={input_tensor:np.array(data), is_training:0}) for var in variables]
      outs = np.linalg.norm(outs, axis=1)
      print(f'Running {idx}: {outs}')
```
Now moving_mean and moving_variance both seem to be updating during training as expected.
```
['batch_normalization_v1/gamma:0', 'batch_normalization_v1/beta:0', 'batch_normalization_v1/moving_mean:0', 'batch_normalization_v1/moving_variance:0', 'batch_normalization_v1/gamma/Adam:0', 'batch_normalization_v1/gamma/Adam_1:0', 'batch_normalization_v1/beta/Adam:0', 'batch_normalization_v1/beta/Adam_1:0']
Running 0: [1.9980000e+00 1.9999903e-03 2.0461062e-03 1.9801079e+00 2.8676972e-02
 4.9131999e-05 1.5716424e-02 1.4908865e-05]
Running 5: [1.9882594e+00 1.1786965e-02 1.1019228e-02 1.8835746e+00 9.9898919e-02
 1.8135854e-04 6.1359003e-02 7.0034890e-05]
Running 10: [1.9793329e+00 2.0299187e-02 2.0462982e-02 1.7917910e+00 1.1684086e-01
 2.4303599e-04 6.9242075e-02 9.5857220e-05]
Running 15: [1.9711298e+00 2.7165966e-02 2.9484417e-02 1.7045155e+00 1.1539625e-01
 2.8652389e-04 6.3280165e-02 1.0698218e-04]
Running 20: [1.9639944e+00 3.2249879e-02 3.6651909e-02 1.6214770e+00 9.9982716e-02
 3.0927311e-04 5.2701045e-02 1.1211485e-04]
Running 25: [1.95816207e+00 3.52966003e-02 4.23905924e-02 1.54244149e+00
 8.03107098e-02 3.20362276e-04 3.72414254e-02 1.13687325e-04]
Running 30: [1.9536946e+00 3.6224511e-02 4.7054645e-02 1.4672419e+00 6.2270045e-02
 3.2455754e-04 3.2511890e-02 1.1524249e-04]
Running 35: [1.9503421e+00 3.6185008e-02 5.0670151e-02 1.3957475e+00 5.0311577e-02
 3.2821088e-04 3.6015689e-02 1.1753518e-04]
Running 40: [1.9479088e+00 3.5928987e-02 5.3476196e-02 1.3277912e+00 4.1749101e-02
 3.3096061e-04 3.5941143e-02 1.1917475e-04]
Running 45: [1.9461093e+00 3.5778154e-02 5.5816881e-02 1.2631813e+00 3.3145458e-02
 3.3147351e-04 3.4038719e-02 1.2038982e-04]
```

"
28579,[XLA] AlgebraicSimplifierTest::DotContractingReorder_NoChangeInContractingDimsOrder is broken (reads out-of-bounds in an array),"https://github.com/tensorflow/tensorflow/commit/3eb4a44f0536888881d49df60c18f528b7122d25 fixed a bug in `AlgebraicSimplifierTest::DotContractingReorder_NoChangeInContractingDimsOrder` that was causing it not to run.

With that bug fixed, the test now crashes XLA!  STR, get rid of ""DISABLED_"" from the front of the test and run it.  It should crash with an out-of-bounds array access in `ComposePermutation`.

@BinFan would you be willing to have a look at this?  I can't assign the bug to you because you're not in the github TF organization, so I've assigned it to myself."
28578,tensorflow 2.0 from_generator,"While using from_generator, tensorflow is giving me this message. Is there a way to turn this off?

> W0509 19:06:38.972821 139709270456064 deprecation.py:323] From /home/celine/.environments/tensorflow_new/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> tf.py_func is deprecated in TF V2. Instead, there are two
>     options available in V2.
>     - tf.py_function takes a python function which manipulates tf eager
>     tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
>     an ndarray (just call tensor.numpy()) but having access to eager tensors
>     means `tf.py_function`s can use accelerators such as GPUs as well as
>     being differentiable using a gradient tape.
>     - tf.numpy_function maintains the semantics of the deprecated tf.py_func
>     (it is not differentiable, and manipulates numpy arrays). It drops the
>     stateful argument making all functions stateful.
> "
28577,tensorflow 2.0 giving tf.print message while using rejection_resample,"I am using rejection_resample from tensorflow v2.0, this is giving me the following message. While this is no big deal, it is kind of annoying. I don't think this is an issue on my end, I didn't use any tf.print. It will be great to turn this off. Thank you.

> 
> Instructions for updating:
> Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:
> ```python
>     sess = tf.Session()
>     with sess.as_default():
>         tensor = tf.range(10)
>         print_op = tf.print(tensor)
>         with tf.control_dependencies([print_op]):
>           out = tf.add(tensor, tensor)
>         sess.run(out)
>     ```
> Additionally, to use tf.print in python 2.7, users must make sure to import
> the following:
> 
>   `from __future__ import print_function`
> "
28572,Does the order matter for importing tensorflow and keras?,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04;
- I create a conda virtual environment:
conda create -n tf_gpu tensorflow-gpu
conda activate tf_gpu
- TensorFlow installed in conda virtual environment: 
conda install -c conda-forge/label/cf201901 
- TensorFlow version: 1.14.1-dev20190424;
- Python version: 3.6.7;

**Problem (1)**
I am running python in linux terminal and get errors if I change the order of importing keras and tensorflow. Is there any explanation for why the following happens? What is the dependency relation between tensorflow and keras?

No error if I import keras first and then tensorflow:
![image](https://user-images.githubusercontent.com/13983910/57488299-50dfd880-7278-11e9-8b60-78ac24a38f63.png)

Got errors if I import tensorflow first and then keras (error occurs for both importing tensorflow and keras):
![image](https://user-images.githubusercontent.com/13983910/57488524-f2672a00-7278-11e9-9ca9-b69d54778af3.png)

![image](https://user-images.githubusercontent.com/13983910/57488548-03b03680-7279-11e9-96fd-c71d4200fe7c.png)

**Problem (2)**
- I add the virtual environment to my jupyter kernel by:
```
source activate tf_gpu
python -m ipykernel install --user --name tf_gpu
```
- I create a jupyter notebook and run the commands:
```
import keras 
import tensorflow
```
The order of importing keras and tensorflow doesn't matter in jupyter notebook.

**My question is:** 
Why does the order of importing keras and tensorflow matter when I am running python script, but the order doesn't matter when I am running a jupyter notebook?
"
28570,TF 1.13.1 SequenceExample & Dataset/Estimator Saved Model: No attr named 'Ncontext_sparse' in NodeDef,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
In TF 1.13.1, using a Dataset API pipeline that parses a [SequenceExample](https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample) with [parse_single_sequence_example](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_sequence_example) exported as a SavedModel with `strip_default_attrs` set to `True` (which is the default behavior for Estimators) fails with the following error:
```
2019-05-09 16:55:22.618817: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at optimize_dataset_op.cc:67 : Not found: No attr named 'Ncontext_sparse' in NodeDef:
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
```
See the full stacktrace below.

**Describe the expected behavior**
The above error does not occur in TF 1.12, perhaps due to `DatasetV2` changes with `OptimizeDataset`. After some experimentation, I found the error does not occur when `strip_default_attrs` is set to `False` when exporting the SavedModel. Nor does it occur when using [Example](https://www.tensorflow.org/api_docs/python/tf/train/Example) in a similar capacity.

Though the example below is intentionally mundane, I've found the  Dataset API functions like `padded_batch` to be extremely nice and was disappointed when I found I couldn't immediately use 1.13.1 saved models as I had been in previous versions.

**Code to reproduce the issue**
```python
import tempfile

import tensorflow as tf

# Set to `False`, and the below script produces 'Hello, World!'
strip_default_attrs = True


# function to deserialize SequenceExample proto
def _parse_proto(proto):
    return tf.parse_single_sequence_example(
        serialized=proto,
        context_features={'f': tf.FixedLenFeature([], tf.dtypes.string)})[0]


# create a minimal SequenceExample
feature = tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[bytes('Hello, World!', encoding='utf-8')]))
example = tf.train.SequenceExample(
    context=tf.train.Features(feature={'f': feature})).SerializeToString()

with tempfile.TemporaryDirectory() as tmp:
    export_dir = tmp + '/export'

    # build and export saved model to a temporary directory
    with tf.Session(graph=tf.Graph()) as sess:
        dataset = tf.data.Dataset.from_tensor_slices(tf.constant([example]))
        dataset = dataset.map(_parse_proto)
        dataset.make_one_shot_iterator().get_next(name='hello')

        builder = tf.saved_model.builder.SavedModelBuilder(export_dir)

        builder.add_meta_graph_and_variables(
            sess,
            tags=[tf.saved_model.tag_constants.SERVING],
            strip_default_attrs=strip_default_attrs)

        builder.save()

    # load and apply saved model
    with tf.Session(graph=tf.Graph()) as sess:
        tf.saved_model.loader.load(sess,
                                   tags=[tf.saved_model.tag_constants.SERVING],
                                   export_dir=export_dir)
        # get element from dataset
        result = sess.graph.get_tensor_by_name('hello:0')

        # if strip_default_attrs is True, fails with ""NotFoundError: No attr named 'Ncontext_sparse' in NodeDef..."" error
        print(str(sess.run(result), 'utf-8'))  # otherwise, will print 'Hello, World!'
```

**Other info / logs**
Full stacktrace:
```
2019-05-09 16:55:22.618817: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at optimize_dataset_op.cc:67 : Not found: No attr named 'Ncontext_sparse' in NodeDef:
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
2019-05-09 16:55:22.618916: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at iterator_ops.cc:1022 : Not found: No attr named 'Ncontext_sparse' in NodeDef:
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[{{node OptimizeDataset}}]]
Traceback (most recent call last):
  File ""/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/Users/jgung/.venv/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'Ncontext_sparse' in NodeDef:
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[{{node OptimizeDataset}}]]
         [[{{node OneShotIterator}}]]
```
"
28563,TFLite GPU delegate produces very different results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 16.04)
- Mobile device if the issue happens on mobile device: LG V30 Android 8.0.0
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 0.0.0-gpu-experimental
- Python version: 3.6.5

**Describe the current behavior**

When DeepLab [mobilenetv2_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz) is run on a sample image using GPU delegate, the result is significantly different to that generated using CPU. The TFLite model was converted using:

```
tflite_convert \
--output_file= mobilenetv2_coco_voc_trainaug.tflite \
--graph_def_file=$FROZEN_GRAPH \
--input_arrays=sub_7 \
--output_arrays=ResizeBilinear_3 \
--inference_type=FLOAT \
--inference_input_type=FLOAT
```
And, here's the [TFLite model](https://drive.google.com/file/d/1rlD4uBxKegUuWVPBzllFVTQXlJTNb1g0/view?usp=sharing) used.

CPU gives the following (using random colours):

![Screenshot_2019-05-10-13-30-19](https://user-images.githubusercontent.com/14197204/57501503-2438c900-732b-11e9-930d-0f9ba51451fe.png)


GPU delegate gives the following (using random colours):

![Screenshot_2019-05-10-13-29-13](https://user-images.githubusercontent.com/14197204/57501509-2733b980-732b-11e9-824c-bb9c360926ee.png)

Basically, all pixels here are (wrongly) identified to be of the same class (at index 0) because the scores for class 0 are always the highest.

Similarly, if [mobilenetv2_ade20k_train](http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz) is used, the two also give very different results:

CPU gives the following (using random colours):

![Screenshot_2019-05-10-04-10-59](https://user-images.githubusercontent.com/14197204/57530071-c4b5da00-7379-11e9-94f1-2d87282eb9eb.png)


GPU delegate gives the following (using random colours):

![Screenshot_2019-05-10-04-09-41](https://user-images.githubusercontent.com/14197204/57530083-cc757e80-7379-11e9-859e-41804569b851.png)

Here's the [TFLite model](https://drive.google.com/file/d/1RdHs85WPxL1u0TUlPHwCqdY6KOn5XuYe/view?usp=sharing) used.

Indeed, the models use operations like `SPACE_TO_BATCH`, but as far as I know these operations should only affect speed performance.

**Describe the expected behavior**

CPU and GPU delegate should produce the same masks."
28558,Tensorflow v2 Variable name uniquification for Keras Layers in eager is inconsistent,"Tensorflow v2.0a

When creating e.g. keras models I would assume, that when I run `make_generator_model` twice in eager mode that the `trainable_variable` names are identical.

**Why would I assume this?**
Because the `tf.train.Checkpoint` and `Checkpointable` api makes you believe that variables are coupled with their corresponding object/class and uniquification of variables would be no longer necessary. And indeed, this is the case when creating a variable with the same name twice (as can be seen at the end of the code)

**What do I get instead?**
In the below example the variables of the second `make_generator_model()` call will be `uniquified`.
```
# First call
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Second
['dense_1/kernel:0', 'batch_normalization_v2_3/gamma:0', 'batch_normalization_v2_3/beta:0', 'conv2d_transpose_3/kernel:0', 'batch_normalization_v2_4/gamma:0', 'batch_normalization_v2_4/beta:0', 'conv2d_transpose_4/kernel:0', 'batch_normalization_v2_5/gamma:0', 'batch_normalization_v2_5/beta:0', 'conv2d_transpose_5/kernel:0']

# Third
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Fourth
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Manual Creation
<tf.Variable 'test:0' shape=() dtype=int32, numpy=1>
<tf.Variable 'test:0' shape=() dtype=int32, numpy=1>
```

```python
import tensorflow as tf
from tensorflow.keras import layers

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model


m1 = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = m1(noise, training=False)
print([v.name for v in m1.trainable_variables])

m2 = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = m2(noise, training=False)
print([v.name for v in m2.trainable_variables])

with tf.Graph().as_default():
    m1 = make_generator_model()
    noise = tf.random.normal([1, 100])
    generated_image = m1(noise, training=False)
    print([v.name for v in m1.trainable_variables])

with tf.Graph().as_default():
    m2 = make_generator_model()
    noise = tf.random.normal([1, 100])
    generated_image = m2(noise, training=False)
    print([v.name for v in m2.trainable_variables])

a = tf.Variable(1, name='test')
b = tf.Variable(1, name='test')
print(a)
print(b)
```"
28557,tensorflow 1.13.1 and RTX 2080 Ti,"tensorflow 1.13.1 can only  identify the max compute capability is 72, but RTX 2080 ti is 75,how can they are used together ?"
28556,AttributeError: module 'tensorflow_hub.tf_v1' has no attribute 'estimator',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.3 (18D109)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version:  3.7.2
- Installed using virtualenv? pip? conda?: virtualenv 
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 
- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I just installed Tensorflow-hub and Tensorflow as described in the Tensorflow installation instructions. When I run the following code:

`import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
import numpy as np
import os, sys
from sklearn.metrics.pairwise import cosine_similarity

# get cosine similairty matrix
def cos_sim(input_vectors):
    similarity = cosine_similarity(input_vectors)
    return similarity

# get topN similar sentences
def get_top_similar(sentence, sentence_list, similarity_matrix, topN):
    # find the index of sentence in list
    index = sentence_list.index(sentence)
    # get the corresponding row in similarity matrix
    similarity_row = np.array(similarity_matrix[index, :])
    # get the indices of top similar
    indices = similarity_row.argsort()[-topN:][::-1]
    return [sentence_list[i] for i in indices]


module_url = ""https://tfhub.dev/google/universal-sentence-encoder/2"" #@param [""https://tfhub.dev/google/universal-sentence-encoder/2"", ""https://tfhub.dev/google/universal-sentence-encoder-large/3""]

# Import the Universal Sentence Encoder's TF Hub module
embed = hub.Module(module_url)

# Reduce logging output.
tf.logging.set_verbosity(tf.logging.ERROR)

sentences_list = [
    # phone related
    'My phone is slow',
    'My phone is not good',
    'I need to change my phone. It does not work well',
    'How is your phone?',

    # age related
    'What is your age?',
    'How old are you?',
    'I am 10 years old',

    # weather related
    'It is raining today',
    'Would it be sunny tomorrow?',
    'The summers are here.'
]

with tf.Session() as session:

  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  sentences_embeddings = session.run(embed(sentences_list))

similarity_matrix = cos_sim(np.array(sentences_embeddings))

sentence = ""It is raining today""
top_similar = get_top_similar(sentence, sentences_list, similarity_matrix, 3)

# printing the list using loop
for x in range(len(top_similar)):
    print(top_similar[x])


Then I get the following issue after running it:

Traceback (most recent call last):
  File ""/Users/svea/PycharmProjects/untitled/main.py"", line 3, in <module>
    import tensorflow_hub as hub
  File ""/Users/svea/PycharmProjects/untitled/venv/lib/python3.7/site-packages/tensorflow_hub/__init__.py"", line 30, in <module>
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/Users/svea/PycharmProjects/untitled/venv/lib/python3.7/site-packages/tensorflow_hub/estimator.py"", line 63, in <module>
    class LatestModuleExporter(tf_v1.estimator.Exporter):
AttributeError: module 'tensorflow_hub.tf_v1' has no attribute 'estimator'

This is my first work with Tensorflow and I could not find any solutions. Thank you for any help!

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28555,Building TensorFlow Lite Micro for TARGET=bluepill fails,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: b02f70947d
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 0.25.1
- GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 7.3.1
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the problem**
When building Tensorflow Lite Micro for the bluepill target the build fails on the file tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc with the following message:

In file included from ./tensorflow/lite/kernels/internal/common.h:49:0,
        from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In constructor 'gemmlowp::Mutex::Mutex()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:70:13: error: 'pthread_mutex_init' was not declared in this scope
    Mutex() { pthread_mutex_init(&m, NULL); }
                   ^~~~~~~~~~~~~~~~~~
             pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In destructor 'gemmlowp::Mutex::~Mutex()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: error: 'pthread_mutex_destroy' was not declared in this scope
   ~Mutex() { pthread_mutex_destroy(&m); }
              ^~~~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: note: suggested alternative: 'pthread_mutexattr_t'
   ~Mutex() { pthread_mutex_destroy(&m); }
              ^~~~~~~~~~~~~~~~~~~~~
              pthread_mutexattr_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Lock()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: error: 'pthread_mutex_lock' was not declared in this scope
   void Lock() { pthread_mutex_lock(&m); }
                 ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: note: suggested alternative: 'pthread_mutex_t'
   void Lock() { pthread_mutex_lock(&m); }
                 ^~~~~~~~~~~~~~~~~~
                 pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Unlock()':
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: error: 'pthread_mutex_unlock' was not declared in this scope
   void Unlock() { pthread_mutex_unlock(&m); }
                   ^~~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: note: suggested alternative: 'pthread_mutex_t'
   void Unlock() { pthread_mutex_unlock(&m); }
                   ^~~~~~~~~~~~~~~~~~~~
                   pthread_mutex_t
tensorflow/lite/experimental/micro/tools/make/Makefile:209: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o' failed
make: *** [tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o] Error 1


**Provide the exact sequence of commands / steps that you executed before running into the problem**
make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=bluepill test


**Any other info / logs**
Reverting commit a52f5b54e8 fixes the build issue."
28554,"""The tensorboard module is not an IPython extension."" in Google Colab","I am running the `image_summaries.ipynb` notebook from this [tutorial](https://www.tensorflow.org/tensorboard/r2/image_summaries) on Colab. Even on Colab, `%load_ext tensorboard` is not working and the message is - 

`The tensorboard module is not an IPython extension.`

TensorFlow version:  2.0.0-dev20190509
"
28553,TensorFlow test execution fails with Bazel v0.24.1 on s390x,"**System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 s390x**
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **NA**
TensorFlow installed from (source or binary): **Source**
TensorFlow version: **current master** 
Python version: **2.7.x**
Installed using virtualenv? pip? conda?: **Building from source**
Bazel version (if compiling from source): **v0.24.1**
GCC/Compiler version (if compiling from source): **7.4.0 (Ubuntu 18.04)**
JAVA : **openjdk version ""11.0.2"" 2019-01-15**
**OpenJDK Runtime Environment (build 11.0.2+9-Ubuntu-3ubuntu118.04.3)**
CUDA/cuDNN version: **NA**
GPU model and memory: **NA**

**Describe the problem**
Tensorflow test command fails with Bazel v0.24.1 

**Details:** 

I have built Bazel v0.24.1 from source using steps mentioned below: 

```
mkdir bazel && cd bazel
wget https://github.com/bazelbuild/bazel/releases/download/0.24.1/bazel-0.24.1-dist.zip
unzip bazel-0.24.1-dist.zip 
chmod -R +w .
env EXTRA_BAZEL_ARGS=""--host_javabase=@local_jdk//:jdk"" bash ./compile.sh
export PATH=$PATH:/bazel/output/
```

Then built TensorFlow master :
```
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-s390x
export PATH=$JAVA_HOME/bin:$PATH
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_CUDA=0
export TF_NEED_MKL=0
export TF_NEED_OPENCL=0
export TF_NEED_VERBS=0
export TF_NEED_S3=0
export TF_NEED_KAFKA=0
export TF_NEED_AWS=0
export TF_ENABLE_XLA=0
yes """" | ./configure

# Build
bazel --host_jvm_args=""-Xms512m"" --host_jvm_args=""-Xmx1024m"" build  --define=tensorflow_mkldnn_contraction_kernel=0 --config=opt //tensorflow/tools/pip_package:build_pip_package

```
Tried to execute tests:

```
# Run bazel test command. Double test timeouts to avoid flakes.
 bazel  --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test \
    --test_tag_filters=-gpu,-benchmark-test -k \
  --test_timeout 300,450,1200,3600 --build_tests_only \
  --test_output=errors -- \
  //tensorflow/... -//tensorflow/compiler/...  
```
  
At this step, test execution failed with below error:
`ERROR: /external/bazel_tools/tools/jdk/BUILD:443:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.``

Tried to provide `--host_javabase` to test command however `--host_javabase` option not getting recognized.
`[bazel FATAL src/main/cpp/blaze.cc:1329] Unknown startup option: '--host_javabase=@local_jdk//:jdk'`

Could you please provide some suggestions/inputs on this? How to execute TensorFlow tests? 

"
28552,W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.,"This is running on tensorflow v2 alpha on a GPU (NVIDIA RTX 2080ti)

This message keeps showing up in the logs, and I'm not sure how to dig further. Training has been very slow, and I'm guessing that it's because of this:

    W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.

This error occurs when I call the model.fit function:

    def get_model():
        model_input = Input(shape=input_shape[1:], name=""input_layer"")
        x = Conv2D(32, (3, 10), padding=""same"", name=""layer1a"")(model_input)
        x = BatchNormalization()(x)
        x = Activation(""relu"")(x)
        x = MaxPool2D()(x)

        x = Conv2D(32, (3, 10), padding=""same"")(x)
        x = BatchNormalization()(x)
        x = Activation(""relu"")(x)
        x = MaxPool2D()(x)

        x = Conv2D(32, (3, 10), padding=""same"")(x)
        x = BatchNormalization()(x)
        x = Activation(""relu"")(x)
        x = MaxPool2D()(x)

        x = Conv2D(32, (3, 10), padding=""same"")(x)
        x = BatchNormalization()(x)
        x = Activation(""relu"")(x)
        x = MaxPool2D()(x)

        x = Flatten()(x)
        x = Dense(64)(x)
        x = BatchNormalization()(x)
        x = Activation(""relu"")(x)
        model_output = Dense(number_classes, activation='softmax')(x)
        model = Model(inputs=model_input, outputs=model_output)
        model.compile(loss=""categorical_crossentropy"", optimizer=""sgd"")
        return model

    model = get_model()

    model.fit(train_dataset, steps_per_epoch=total_size // args.batch_size, epochs=args.epochs, validation_data=validation_dataset, validation_steps=500)

where both train_dataset and validation_dataset are instances of tf.data:

    def preprocess(self, x):
    	image, target = _parse_function_train(x)
    	image = tf.image.convert_image_dtype(image, tf.float32)
    	image = tf.image.resize_with_pad(image, target_height=self.image_height, target_width=self.image_width)
    	return image, target

    def get_datasets(self):
    	self.data[""train_curated""] = tf.data.TFRecordDataset(""train_curated.tfrecords"")
    	dataset = self.data[""train_curated""].shuffle(args.train_curated_count)
    	validation_size = int(args.validation_size_split_ratio * args.train_curated_count)

    	validation_dataset = dataset.take(validation_size)
    	validation_dataset = validation_dataset.map(self.preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    	validation_dataset = validation_dataset.shuffle(validation_size)
    	validation_dataset = validation_dataset.cache()
    	validation_dataset = validation_dataset.repeat()
    	validation_dataset = validation_dataset.batch(args.batch_size)
    	validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    	train_dataset = dataset.skip(validation_size)
    	train_dataset = train_dataset.map(self.preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    	train_dataset = train_dataset.shuffle(args.train_curated_count - validation_size)
    	train_dataset = train_dataset.repeat()
    	train_dataset = train_dataset.batch(args.batch_size)
    	train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    	return train_dataset, validation_dataset


"
28551,Tensorflow build issue on Windows,"**System information**

- OS Platform and Distribution: Windows 10 1809 Build
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest (get with `git clone`)
- Python version: 2.7.16
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.23.0
- CUDA/cuDNN version: not installed

**Problem**

I need to build Tensorflow on Windows for further use in my C++ VS project. There are appear errors when trying to build from source with Bazel. I tried to follow the installation instructions set out on the official website: https://www.tensorflow.org/install/source_windows

Build command (CPU only):
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

**Logs**

> Starting local Bazel server and connecting to it...
> ERROR: C:/users/lenovo/desktop/tensorflow/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
>         File ""C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl"", line 344
>                 _create_local_python_repository(repository_ctx)
>         File ""C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
>                 _get_numpy_include(repository_ctx, python_bin)
>         File ""C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
>                 _execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
>         File ""C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl"", line 56, in _execute
>                 _fail(""\n"".join([error_msg.strip() if ... """"]))
>         File ""C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
>                 fail((""%sPython Configuration Error:%...)))
> Python Configuration Error: Problem getting numpy include path.
> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
> ImportError: No module named numpy
> Is numpy installed?
>  and referenced by '//third_party/python_runtime:headers'
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
> INFO: Elapsed time: 31.933s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (195 packages loaded, 3695 targets configured)
>     Fetching @grpc; fetching 11s
>     Fetching @llvm; fetching 9s
>     Fetching @boringssl; fetching
>     Fetching @local_config_python;"
28547,ssd_mobilenet_v2 model convert tflite errors,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow 1.12, python 3.6.5
I use tensorflow object_detection train ssd_mobilenet_v2 model on my own dataset, and this code(https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) convert trained ckpt model to pb, and use this .pb file predict on my dataset, the result is very good. but I use tf.contrib.lite.TFLiteConverte convert this .pb file to .tflite file, outputs some erros :
:RuntimeError: TOCO failed see console for info. Converting unsupporteOp node missing output type attribute,   NonMaxSuppressionV3,  and so on...
this is my convert code:
graph_def_file = 'graph.pb'
input_arrays = ['image_tensor']
output_arrays = ['num_detections', 'detection_boxes', 'detecction_scores', 'detection_class']
converter = tf.contrib.lite.TFFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes={'image_tensor':[1,300,300,3]})
#converter.post_training_quantize=True
#converter.default_ranges_stats=(0,6)
tflite_model = converter.convert()
open('detect.tflite', 'wb').write(tflite_model)
does anyone meet thsi problems, thanks a lot
"
28545,problem with installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC Yosemite 10.10.5 (14F27)
- TensorFlow installed from (source or binary):source
- TensorFlow version: 1.13.1
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: pip


- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 4 GB 1600 MHz DDR3

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Although tensorflow is installed and pip install tensorflow shows that it is already satisfied, but I cannot import tensorflow library

File ""/Users/botaduisenbay/Desktop/tester.py"", line 9, in <module>
    import tensorflow as tf

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): no suitable image found.  Did find:
	/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: truncated mach-o error: segment __LINKEDIT extends to 365926664 which is past end of file 154042368


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
28544,problem with installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC Yosemite 10.10.5 (14F27)
- TensorFlow installed from (source or binary):source
- TensorFlow version: 1.13.1
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: pip


- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 4 GB 1600 MHz DDR3

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Although tensorflow is installed and pip install tensorflow shows that it is already satisfied, but I cannot import tensorflow library

File ""/Users/botaduisenbay/Desktop/tester.py"", line 9, in <module>
    import tensorflow as tf

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): no suitable image found.  Did find:
	/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: truncated mach-o error: segment __LINKEDIT extends to 365926664 which is past end of file 154042368


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
28541,tf.data.Dataset.padded_batch does not actually pad data when working on multiple GPUs.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
X
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
X
- **GCC/Compiler version (if compiling from source)**:
X
- **CUDA/cuDNN version**:
10.0
- **GPU model and memory**:
Tesla K80 / 12GB RAM
- **Exact command to reproduce**:
See test below

### Describe the problem
The problem appears when one wants to use a multi-gpu-compiled model with variable-shaped data, fed from a tf.data pipeline. 
Batching data using padded_batch (compulsory here, since the data is of variable shape) does not seem to really pad the tensors comprised in a batch. Calling `fit`or `predict `after doing so raises:
`ValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica`.

See below a MVCE:
- A python generator is used to generate tensors of variable shape: (4, 4, 1), then (5, 5, 1), then (4, 4, 1), then (5, 5, 1), etc...
- A tf.data Dataset is built using `tf.data.Dataset.from_generator`, including a consistency check.
- A dummy model is built using the tf.keras functional API and a `tf.distribute.MirroredStrategy` on several GPUS.

### Source code / logs
MVCE:

```
import tensorflow as tf
import numpy as np


def data_generator():
    """"""
    Creates a generator of variable_sized tensors.
        tensor 1: (4, 4, 1)
        tensor 2: (5, 5, 1)
        tensor 3: (4, 4, 1)
        tensor 4: (5, 5, 1)
        ...
    """"""
    i = 0

    while True:
        size = (4 + i % 2, 4 + i % 2, 1)
        x = tf.random.normal(shape=size)
        yield x
        i += 1


def build_data_pipeline():
    """"""
    Builds a tf.data pipeline yielding variably-sized tensors.
    """"""
    dataset = tf.data.Dataset.from_generator(
        generator=data_generator,
        output_types=tf.float32,
        output_shapes=[None, None, 1])

    # Padded batch, should output (batch_size, 5, 5, 1) - shaped tensors.
    dataset = dataset.padded_batch(
        batch_size=2,
        padded_shapes=(None, None, 1),
        padding_values=0.)

    # Juste making sure that the tf.data pipeline is as expected
    it = iter(dataset)
    batch_1 = next(it)  #~= tf.data.experimental.get_single_element(dataset)
    batch_2 = next(it)  #~= tf.data.experimental.get_single_element(dataset)

    np.testing.assert_allclose(batch_1.shape, batch_2.shape)
    np.testing.assert_allclose(batch_1.shape, (2, 5, 5, 1))

    return dataset


def run_on_multi_gpus():
    """"""
    Building multi-gpu model using MirroredStrategy,
    And trying to predict
    """"""
    strat = tf.distribute.MirroredStrategy()

    with strat.scope():

        i_ = tf.keras.layers.Input((None, None, 1))
        model = tf.keras.models.Model(i_, i_)

        model.compile(optimizer='adam',
                      loss='binary_crossentropy')

    dataset = build_data_pipeline()
    model.predict(dataset, steps=1, verbose=1)


if __name__ == '__main__':
    run_on_multi_gpus()
```
"
28540,Crashing on TF Detect Android sample with Yolov3-tiny model,"I converted Yolov3-tiny model (Trained from scratch) with [DW2TF](https://github.com/jinyu121/DW2TF). Then I put .pb file to assets and changed MODE == DetectorMode.YOLO in **[DetectorActivity.java](https://gist.github.com/androuino/2b67d88d00bc52ac181740fa475828c4)**.

When I start TF Detect app I have a crash:
```
--------- beginning of crash
05-09 17:07:39.959 13973-13999/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.demo, PID: 13973
    java.lang.IllegalArgumentException: No Operation named [input] in the Graph
        at org.tensorflow.Session$Runner.operationByName(Session.java:372)
        at org.tensorflow.Session$Runner.feed(Session.java:142)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.addFeed(TensorFlowInferenceInterface.java:577)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.feed(TensorFlowInferenceInterface.java:318)
        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:154)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:297)
        at android.os.Handler.handleCallback(Handler.java:815)
        at android.os.Handler.dispatchMessage(Handler.java:104)
        at android.os.Looper.loop(Looper.java:207)
        at android.os.HandlerThread.run(HandlerThread.java:61)
```

Thank you in advance for the help."
28539,Tutorials next_step page is giving 404,"

## URL(s) with the issue:

https://www.tensorflow.org/tutorials/next_steps

## Description of the issue (what needs changing):
This page is not opening

### Clear description

Link is getting 404



### Request visuals, if applicable

![404     Page Not Found     TensorFlow](https://user-images.githubusercontent.com/1620769/57435595-65fa3e80-725b-11e9-81da-8394e7147ca7.png)


"
28538,IDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.,"

**System information**
- OS Platform and Distribution Linux Ubuntu 16.04

- TensorFlow installed from pip:
- TensorFlow version 1.13:
- Python version 3.5:
- Installed using created virtualenvironment and then used ubuntu command to install :

- GPU model and memory   NVIDIA Corporation GT218 [GeForce 210] (rev a2)
 



# Install NVIDIA driver
apt-get install --no-install-recommends nvidia-410 .

to check this nvidia installation i used  
nvidia-smi 
command it ended up with above error message

https://www.tensorflow.org/install/gpu followedsame link

Is this issue is because the GPU version is not supported??
If so please to share the GPU compatible version and memory size"
28537,Dataset train terminate and Logging error,"`python retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--how_many_training_steps 500 \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/data_set`

Ths is my training code

> --- Logging error ---
Traceback (most recent call last):
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 992, in emit
    msg = self.format(record)
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 838, in format
    return fmt.format(record)
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 575, in format
    record.message = record.getMessage()
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File ""retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""retrain.py"", line 982, in main
    maybe_download_and_extract(model_info['data_url'])
  File ""retrain.py"", line 339, in maybe_download_and_extract
    'bytes.')
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 116, in info
    _get_logger().info(msg, *args, **kwargs)
Message: 'Successfully downloaded'
Arguments: ('inception-2015-12-05.tgz', 88931400, 'bytes.')
ERROR:tensorflow:Image directory '/tf_files/thuru_care_data_set' not found.
Traceback (most recent call last):
  File ""retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""retrain.py"", line 989, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'"
28536,Dataset train terminate and Logging error,"`python retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--how_many_training_steps 500 \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/data_set`

Ths is my training code

> --- Logging error ---
Traceback (most recent call last):
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 992, in emit
    msg = self.format(record)
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 838, in format
    return fmt.format(record)
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 575, in format
    record.message = record.getMessage()
  File ""/root/anaconda3/lib/python3.6/logging/__init__.py"", line 338, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File ""retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""retrain.py"", line 982, in main
    maybe_download_and_extract(model_info['data_url'])
  File ""retrain.py"", line 339, in maybe_download_and_extract
    'bytes.')
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 116, in info
    _get_logger().info(msg, *args, **kwargs)
Message: 'Successfully downloaded'
Arguments: ('inception-2015-12-05.tgz', 88931400, 'bytes.')
ERROR:tensorflow:Image directory '/tf_files/thuru_care_data_set' not found.
Traceback (most recent call last):
  File ""retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""retrain.py"", line 989, in main
    class_count = len(image_lists.keys())
AttributeError: 'NoneType' object has no attribute 'keys'"
28532,ValueError when reading a frozen graph,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Linux Fedora 30
- TensorFlow installed from: Binary
- TensorFlow version: 1.13.1
- Python version: 3.7.3 (64-bit)
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: Nvidia GeForce GTX 1050M [4GB]

**Current behavior**

```
Traceback (most recent call last):
  File ""/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 426, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bn_data/cond/ones_like/Shape}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/fuzzybatman/Workspace/ET4399-Extra_Project_[Okotech]/09 - UNet/frozen_predict.py"", line 30, in <module>
    tf.import_graph_def(trt_graph, name='')
  File ""/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 430, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node bn_data/cond/ones_like/Shape}}
```

**Describe the expected behavior**

Load the graph so it can be used for inference.

**Code to reproduce the issue**

```
import tensorflow as tf

output_names = ['sigmod/Sigmoid']
input_names = ['data']

def get_frozen_graph(graph_file):
    """"""Read Frozen Graph file from disk.""""""
    with tf.gfile.FastGFile(graph_file, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    return graph_def

trt_graph = get_frozen_graph('<PATH-TO-MODEL>')

# Create session and load graph
tf_config = tf.ConfigProto()
tf_config.gpu_options.allow_growth = True

tf_sess = tf.Session(config=tf_config)
tf_sess.run(tf.global_variables_initializer())

tf.import_graph_def(trt_graph, name='')
```

**Other info / logs**
The original model was built in keras
"
28530,Python Flow-based Programming UI (graphical designer IDE) similar to FlowHub or Node-red,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13 and 2.0
- Are you willing to contribute it (Yes/No):yes



**Describe the feature and the current behavior/state.**
Python Flow-based Programming UI (graphical designer IDE) similar to NoFLo or others but using Python API  which is more complete . Design the entire model on a canvas by drag-and-drop components on the canvas, then generate the python code and that can be run externally or on the system as a python code. This will make it easy for new users to design new models without coding but benefit from the knowledge 

**Will this change the current api? How?** Not really 

**Who will benefit with this feature?** Tons of new users to Tensorflow AI  without indepth AI low-level programming 

**Any Other info.**
"
28528,TFLite calibration-and-quantization I/O min/max mismatch on some operators,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux 5.0.10

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A

- TensorFlow installed from (source or binary):
source

- TensorFlow version (use command below):
Running: https://github.com/ajarthurs/tensorflow/tree/q_leakyrelu
Base commit: 3ea8756ce6d08a473d78347fb7b876ad5c1be973
Related PR: https://github.com/tensorflow/tensorflow/pull/27028
Related issue: https://github.com/tensorflow/tensorflow/issues/28268

- Python version:
3.7.3

- Bazel version (if compiling from source):
0.25.0

- GCC/Compiler version (if compiling from source):
8.3.0

- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
During calibration-and-quantization (see `tensorflow/lite/tools/optimize/calibration`), the tensor feeding a `RELU` operator shows that its minimum value is 0 instead of a negative number. This may lead to a quantized model that produces erroneous results; however, I currently cannot confirm if the said behavior is causing erroneous quantization.

**Describe the expected behavior**
I expect `RELU`'s input minimum value (for quantization) to match the input tensor's actual minimum value (a negative real number).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

An MSCOCO-trained floating-point (32bit) model that I am attempting to calibrate-and-quantize is available at https://www.dropbox.com/s/hnkmqzcasb8lu8n/retinanet-float32.tflite?dl=0

[calibrate_and_quantize.py](https://github.com/tensorflow/tensorflow/files/3154074/calibrate_and_quantize.py.zip) is an example script that runs TF Lite's calibration-and-quantization tool. It can be executed as such:
```
# Adjust paths as needed.
$ PYTHONPATH=path/to/tensorflow/tpu/models/official/retinanet:$PYTHONPATH python calibrate_and_quantize.py --input_tflite_file=retinanet-float32.tflite --output_tflite_file=retinanet-int8.tflite --train_file_pattern=path/to/mscoco-tfrecords/train*
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

While running the attached Python script, it reports the following warnings:
```
Note the output min/max is different from the input min/max for op MAX_POOL_2D at index 2 in subgraph 0. This is legal but should happens rarely.
Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 77 in subgraph 0. This is legal but should happens rarely.
Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 80 in subgraph 0. This is legal but should happens rarely.
```

And during one my `gdb` sessions, I check the I/O min/max values of a `MAX_POOL_2D` operator highlighted by one of the warnings above. The output minimum value of the `MAX_POOL_2D` operator is 0 instead of -19.3, which would throw the said warning.
```
Thread 1 ""calibrate_and_q"" hit Breakpoint 2, tflite::optimize::(anonymous namespace)::QuantizeOpOutput (
    model=0x55822760c890, subgraph_idx=0, op_idx=2, property=..., output_idx=0, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:504
504             printf(
(gdb) p min
$5 = -19.2778091
(gdb) p max
$6 = 22.6024609
...
(gdb) p output_tensor->quantization->min[0]
$10 = 0
(gdb) p op_code
$11 = tflite::BuiltinOperator_MAX_POOL_2D
(gdb) p output_tensor->quantization->max[0]
$12 = 22.6024609
(gdb) bt
#0  tflite::optimize::(anonymous namespace)::QuantizeOpOutput (model=0x55822760c890, subgraph_idx=0, op_idx=2,
    property=..., output_idx=0, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:504
#1  0x00007f78cc28497c in tflite::optimize::(anonymous namespace)::QuantizeWeightsInputOutput (
    builder=0x7fff304b4860, model=0x55822760c890, allow_float=false, error_reporter=0x558223944030)
    at tensorflow/lite/tools/optimize/quantize_model.cc:570
#2  0x00007f78cc284faf in tflite::optimize::QuantizeModel (builder=0x7fff304b4860, model=0x55822760c890,
    input_type=@0x7fff304b4844: tflite::TensorType_INT8, output_type=@0x7fff304b4848: tflite::TensorType_INT8,
    allow_float=false, error_reporter=0x558223944030) at tensorflow/lite/tools/optimize/quantize_model.cc:638
#3  0x00007f78cc26a8c1 in tflite::calibration_wrapper::CalibrationWrapper::QuantizeModel (this=0x5582274c7790,
    input_py_type=1, output_py_type=1, allow_float=false)
    at tensorflow/lite/python/optimize/calibration_wrapper.cc:201
#4  0x00007f78cc268f88 in _wrap_CalibrationWrapper_QuantizeModel (args=0x7f78cc697458)
    at bazel-out/k8-dbg/bin/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.cc:3418
#5  0x00007f795c552e68 in _PyMethodDef_RawFastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#6  0x00007f795c553101 in _PyCFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#7  0x00007f795c5c3d19 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#8  0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#9  0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#10 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#11 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#12 0x00007f795c5bff9c in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#13 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#14 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#15 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#16 0x00007f795c5526db in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#17 0x00007f795c5bf22d in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#18 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#19 0x00007f795c552882 in _PyFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#20 0x00007f795c5c36ea in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#21 0x00007f795c50bd09 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
--Type <RET> for more, q to quit, c to continue without paging--q
```"
28527,tfdbg and tf.data API:  Inconsistency with handling dataset iterators' OutOfRangeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS Mojave v10.14.4
- TensorFlow installed from (source or binary):  Binary (PIP)
- TensorFlow version (use command below):  pip install tensorflow==1.13.1
- Python version:  3.6.2
- CUDA/cuDNN version:  N/A (found bug on CPU version)

print(tf.GIT_VERSION, tf.VERSION):  ==>  'v1.13.0-rc2-5-g6612da8951' 1.13.1


**Describe the current behavior**
Different behavior while using tfbdg responding to catching OutOfRangeError required for iteratively looping on dataset iterators from the tf.data API.  Using tfdbg command  'run -t 100', we get execution as normally expected.  Using tfbdg command 'run -f has_inf_or_nan', we get a cascade of errors.

**Describe the expected behavior**
Running tfbdg with a filter vs running without a filter produces the same code errors when dealing with catching OutOfRangeErrors.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug

dataset = tf.data.Dataset.range(3)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()
result = tf.add(next_element, next_element)

with tf.Session() as sess:

  sess = tf_debug.LocalCLIDebugWrapperSession(sess)

  sess.run(iterator.initializer)
  while(True):
    try:
      output = sess.run(result)
      print(output)
    except tf.errors.OutOfRangeError:
      print(""End of dataset"")
      break
```

**Other info / logs**
FYI: This code snippit is a condensed version of a similarly-structured larger project I am working with; in that larger project (which I am unable to condensely reproduce at this time) the difference between running with tfbdg with a filter produces error 'OutOfRangeError is not iterable' and halts execution whereas tfdbg running without a filter does not and executes normally as intended (ie. slightly different than the discrepency displayed in the code snippet above)."
28526,build_pip_package broken because of 58b53e19c3d06cea5909c66379d073aea153651,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution ( Linux Ubuntu 16.04):
- TensorFlow version: master at 66b193faeecda4e6bc7e2767c2d927eecd199a34
- Python version: 2
- Installed using virtualenv? pip? conda?: docker container build
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.3

**Describe the problem**
build_pip_package is failing: 
```
[0m[91mTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
[0m[91mINFO: Elapsed time: 559.703s, Critical Path: 225.66s
INFO: 12822 processes: 12822 local.
[0m[91mINFO: Build completed successfully, 13671 total actions
[0m[91mINFO: Build completed successfully, 13671 total actions
[0mWed May 8 17:39:25 UTC 2019 : === Preparing sources in dir: /tmp/tmp.kL5rPiZPYi
/tensorflow /tensorflow
/tensorflow
[91mcp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external/local_config_cuda/cuda/cuda/cuda_config.h': No such file or directory
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel --bazelrc=/root/.bazelrc build -c opt     tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/whl

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28524,Using subword information in OOV token from fasttext in word embedding layer (keras/tensorflow),"Serious, there is nothing on the internet I found in this way to handle OOV tokens with custom word vectors! There are some post, but all are saying using eg. fasttext (https://stackoverflow.com/questions/48395570/how-to-initialize-word-embeddings-for-out-of-vocabulary-word?rq=1 ) wih no explanation how to do it.
It should be somehow possibleto  bypass/hack around to assign a vector for OOVs?
But I struggle to find the proper position of code like in tf.nn.embedding_lookup where I can do it in a way like `if not in vocab get.fasttext.vector`

I have my own Fasttext model and trained with it a tensorflow classification model with a word embedding layer.

```
with tf.variable_scope('embeddings'):
            word_embeddings = tf.constant(self.embedding_mat, dtype=tf.float32, name=""embedding"")
            self.embedded_x1 = tf.nn.embedding_lookup(word_embeddings, self.x1)
            self.embedded_x2 = tf.nn.embedding_lookup(word_embeddings, self.x2)

```
But, I wonder how I can make use of the subword information of my model for OOV words? Since the word embedding layer operated via indices to look up word vectors and OOV words have no index. Even if a OOV token has a index how would I assign it the proper word vector to this OOV on the fly for an already trained model?

One understanding question: If I have a UNK token in my matrix but I did not used this UNK in training, can I assign a custom vector (eg. from fasttext) during prediction? So, for this UNK word which might be similar to a trained word such that the text is classified as same class? Then you could have also more UNK token like UNK1 and UNK2. 

I found two interesting posts, but I do't know if I understand that solution and if that is the solution...
https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo

https://stackoverflow.com/questions/47022591/use-tensorflow-and-pre-trained-fasttext-to-get-embeddings-of-unseen-words?rq=1

Thanks in advance!"
28523,Tensorflow v2 cannot save with CheckpointManager in tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0a
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0

**Describe the current behavior**

I am trying to save a `tf.train.Checkpoint` within a `tf.function` annotated function.
I use the code below:
```python
import tensorflow as tf

sw = tf.summary.create_file_writer('models/test/')

data = tf.random.normal((1000, 10, 10, 1))
dataset = tf.data.Dataset.from_tensors(data).batch(10)


step = tf.Variable(0)

ckpt = tf.train.Checkpoint(step=step)
mgr = tf.train.CheckpointManager(ckpt, 'models/ckpt/', max_to_keep=20)


@tf.function
def train_step(x):
    return x * x * x


def train(dataset):
    for i in range(10):
        for x in dataset:
            step.assign_add(1)
            print(step)
            print(ckpt.save_counter)
            if tf.equal(step % 2, 0):
                print('save')
                mgr.save()

train = tf.function(train)
train(dataset)
```

<details><summary>Console output</summary>

```
2019-05-08 17:52:33.083828: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
<tf.Variable 'Variable:0' shape=() dtype=int32>
<tf.Variable 'save_counter:0' shape=() dtype=int64>
save
Traceback (most recent call last):
  File "".\test.py"", line 49, in <module>
    train(dataset)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\def_function.py"", line 426, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1580, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1512, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\def_function.py"", line 317, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 686, in wrapper
    ), args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""C:\Users\user\AppData\Local\Temp\tmpw8xyj0j5.py"", line 25, in tf__train
    ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (10,), {}), None, loop_body_1, ())
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 81, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, init_state)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 90, in _py_for_stmt
    state = body(target, *state)
  File ""C:\Users\user\AppData\Local\Temp\tmpw8xyj0j5.py"", line 23, in loop_body_1
    ag__.for_stmt(dataset, None, loop_body, ())
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 79, in for_stmt
    return _dataset_for_stmt(iter_, extra_test, body, init_state)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 152, in _dataset_for_stmt
    ds.reduce((constant_op.constant(0),), reduce_body_with_dummy_state)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1276, in reduce
    add_to_graph=False)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2388, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1319, in _get_concrete_function_internal
    *args, **kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1580, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\eager\function.py"", line 1512, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2381, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2326, in _wrapper_helper
    ret = func(*nested_args)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 150, in reduce_body_with_dummy_state
    reduce_body((), iterate)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 141, in reduce_body
    new_state = body(iterate, *state)
  File ""C:\Users\user\AppData\Local\Temp\tmpw8xyj0j5.py"", line 21, in loop_body
    ag__.if_stmt(cond, if_true, if_false)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 245, in if_stmt
    return tf_if_stmt(cond, body, orelse)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 256, in tf_if_stmt
    return control_flow_ops.cond(cond, protected_body, protected_orelse)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 1918, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\ops\cond_v2.py"", line 74, in cond_v2
    op_return_value=pred)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py"", line 263, in protected_func
    results = func()
  File ""C:\Users\user\AppData\Local\Temp\tmpw8xyj0j5.py"", line 16, in if_true
    ag__.converted_call('save', mgr, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {})
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 267, in converted_call
    return _call_unconverted(f, args, kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 188, in _call_unconverted
    return f(*args, **kwargs)
  File ""C:\Progams\Miniconda\envs\tf2-cpu\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 687, in
save
    session.run(self._save_counter_assign)
AttributeError: 'NoneType' object has no attribute 'run'
```
</details>

It is able to save once and then crashes.

Proabably a topic for @allenlavoie ?"
28521,"LSTM vs Conv2D, tf-nightly-gpu (CUDNN_STATUS_INTERNAL_ERROR)","Hello,

info:
tf-nightly-gpu (1.14.1.dev20190508)
cuda 10.0
cuDNN v7.4.1
RTX 2080
ubuntu 16.04

I'm trying to execute some code. (ML)

i'm useing LSTM and i get no error !

i'm useing Conv2D and i get this error here:
`2019-05-08 14:45:09.120400: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-05-08 14:45:09.724859: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-05-08 14:45:10.964612: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-08 14:45:10.967118: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""test.py"", line 55, in <module>
    model.fit(x, y, epochs=1, batch_size=n_batch)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 802, in fit
    steps_name='steps_per_epoch')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py"", line 357, in model_iteration
    batch_outs = f(ins_batch)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 3180, in __call__
    run_metadata=self.run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1456, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d/Conv2D}}]]
	 [[loss/mul/_93]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d/Conv2D}}]]
0 successful operations.
0 derived errors ignored.`

i think there are some problem with Conv2D on GPU.

Can some one help me ?"
28520,RTX 2080ti bazel build: nvcc fatal: Unsupported gpu architecture 'compute_75' ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.4.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.5.4
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: CUDA 8.0 cuDNN 6.0
- GPU model and memory: RTX2080Ti 11G



**Describe the problem**
command
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

Error info
```
ERROR: /home/googlecamp/.cache/bazel/_bazel_googlecamp/fe4eeee4bfb5545ecaf6693290f5b5f2/external/nccl_archive/BUILD:33:1:
 error while parsing .d file: /home/googlecamp/.cache/bazel/_bazel_googlecamp/fe4eeee4bfb5545ecaf6693290f5b5f2/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_gather.cu.pic.d(No such file or directory).
nvcc fatal   : Unsupported gpu architecture 'compute_75'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

I wonder whether it's due to the new architecture of RTX2018Ti."
28519,Bug in Keras Guide,"## URL(s) with the issue:

https://www.tensorflow.org/guide/keras#multiple_gpus

## Description of issue (what needs changing):

The last line of this guide produces a bug when ran on the associated colab notebook (maybe everywhere, I haven't checked).

### Raises listed and defined

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

19 frames
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/TFOptimizer/NcclAllReduce}}with these attrs: [reduction=""sum"", shared_name=""c0"", T=DT_FLOAT, num_devices=1]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  <no registered kernels>

	 [[{{node training/TFOptimizer/NcclAllReduce}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by node training/TFOptimizer/NcclAllReduce (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1254) with these attrs: [reduction=""sum"", shared_name=""c0"", T=DT_FLOAT, num_devices=1]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  <no registered kernels>
```
Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

No, I'm only beginning in TF/Keras :) "
28518,Tensorflow v2 tensorflow.python.ops.linalg_ops.svd hangs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0a-gpu
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2060

**Describe the current behavior**
```python
import tensorflow as tf
from tensorflow.python.ops import linalg_ops

mat = tf.random.uniform([2048, 2048])

s, u, v = linalg_ops.svd(mat)
```
When running this example with my tensorflow-gpu installation it hangs (until I cancel with ctrl+c, which takes very long):
```
2019-05-08 13:20:05.994717: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-05-08 13:20:06.002398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2019-05-08 13:20:06.250271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:1f:00.0
totalMemory: 6.00GiB freeMemory: 4.89GiB
2019-05-08 13:20:06.256846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-05-08 13:20:06.803341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-08 13:20:06.806295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
2019-05-08 13:20:06.810187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
2019-05-08 13:20:06.812341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4618 MB memory) -> physical GPU (device: 0,
name: GeForce RTX 2060, pci bus id: 0000:1f:00.0, compute capability: 7.5)
2019-05-08 13:20:07.023250: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles
for stream 000001C2BFA888A0
Traceback (most recent call last):
  File "".\svd_check.py"", line 6, in <module>
    s, u, v = linalg_ops.svd(mat)
  File ""C:\Progams\Miniconda\envs\tf2-gpu\lib\site-packages\tensorflow\python\ops\linalg_ops.py"", line 418, in svd
    tensor, compute_uv=compute_uv, full_matrices=full_matrices, name=name)
  File ""C:\Progams\Miniconda\envs\tf2-gpu\lib\site-packages\tensorflow\python\ops\gen_linalg_ops.py"", line 2409, in svd
    ""full_matrices"", full_matrices)
KeyboardInterrupt
```
On tensorflow most recent v1 version it just works.

Also, when running the following it works as well:
```python
import tensorflow as tf
from tensorflow.python.ops import linalg_ops

mat = tf.random.uniform([2048, 2048])

with tf.device('/device:CPU:0'):
    s, u, v = linalg_ops.svd(mat)
```"
28517,Tensorflow failed to CMake due to Python module not found: tensorflow/contrib/tpu/ops on Windows server 2016,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Windows server 2016
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
  Source
- TensorFlow version:
  [d58b53e](https://github.com/tensorflow/tensorflow/commit/d58b53e19c3d06cea5909c66379d073aea153651) 
- Python version:
Anaconda 4.1.1 (Python 2.7.14 64-bit)
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
VS2017 15.7.2
- CUDA/cuDNN version:
NVidia CUDA Toolkit 8.0
 NVidia CUDNN 5.1
- GPU model and memory:
N/A

Describe the problem:
Tensorflow failed to CMake due to Python module not found: tensorflow/contrib/tpu/ops. It can be reproduced on latest revision, could you please help take a look at this? Thanks!

Note: 
We did not find the ops in the D:\Tensorflow\src\tensorflow\contrib\tpu path according to the error message. However, the ops path is at D:\Tensorflow\src\tensorflow\contrib\tpu\python\ops.

Repro steps:
1. git clone https://github.com/tensorflow/tensorflow D:\Tensorflow\src
2. pushd D:\Tensorflow
3. set PreferredToolArchitecture=x64
4. set rel=Release
5. set cmake_dir=D:\Tensorflow\src\tensorflow\contrib\cmake
6. set CUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\cuda""
7. set PY=C:\ProgramData\Anaconda3
7. set _CL_=/FS
8. mkdir build_x64   && pushd build_x64
9. cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON

[log_x64_cmake.log](https://github.com/tensorflow/tensorflow/files/3156985/log_x64_cmake.log)

Error Message:
[executing command] cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON 
-- Building for: Visual Studio 15 2017
CMake Warning at CMakeLists.txt:9 (message):
  Your current cmake generator is set to use 32 bit toolset architecture.
  This may cause ""compiler out of heap space"" errors when building.  Consider
  using the flag -Thost=x64 when running cmake.  Ignore this if you are on
  CMake GUI.
-- Selecting Windows SDK version 10.0.17134.0 to target Windows 10.0.14393.
-- The C compiler identification is MSVC 19.22.27706.96
-- The CXX compiler identification is MSVC 19.22.27706.96
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED
-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed
-- Performing Test MSVC_OPENMP_SUPPORT
-- Performing Test MSVC_OPENMP_SUPPORT - Success
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED
-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success
-- D:/Tensorflow/build_x64/abseil_cpp/src/abseil_cpp_build
-- Found PythonInterp: C:/ProgramData/Anaconda3/python.exe (found version ""3.6.5"") 
-- Found PythonLibs: C:/ProgramData/Anaconda3/libs/python36.lib (found version ""3.6.5"") 
CMake Error at tf_python.cmake:217 (message):
  Python module not found: tensorflow/contrib/tpu/ops
Call Stack (most recent call first):
  CMakeLists.txt:612 (include)


-- Found SWIG: D:/Tensorflow/swigwin-3.0.12/swig.exe (found version ""3.0.12"") 
-- Configuring incomplete, errors occurred!
See also ""D:/Tensorflow/build_x64/CMakeFiles/CMakeOutput.log"".
See also ""D:/Tensorflow/build_x64/CMakeFiles/CMakeError.log"".
[command took 31 seconds] 


"
28516,Issue recognize_commands micro_speech demo,"**System information**
run on SparkfunEdge Board Apollo3 operating in Burst Mode (96MHz)

Bin build on the following linux env:
Linux edh-VirtualBox 4.18.0-17-generic #18.04.1-Ubuntu SMP Fri Mar 15 15:27:12 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
TensorFlow installed from (source or binary): Git
TensorFlow version: 1.13.1
Python version:Python 2.7.15rc1
Installed using virtualenv? pip? conda?: pip
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)
CUDA/cuDNN version: No
GPU model and memory: No

**Description**
The micro_speech example does not work.
The 'Yes' or 'No' are not regognized.
It seems a logic issue in the source 

> tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.cc

The TF model returns a good score > 200 following call to` interpreter.Invoke() `and the vector output reurned by `interpreter.output(0)` is most of the time correct in the source 

> main.cc

 **BUT there is a average computation**  that breaks the results in the source 

> recognize_commands.cc



With the default configuration (`average_window_duration_ms = 1000`) , the average is computed on about 10 model outputs. 
The issue is that when I say 'Yes', only 1or 2 model ouputs detect 'yes with a score > 200. So the average computed on 10 outputs provides a average score always smaller than the threshold equal to 200.
I have tried to reduce the param `average_window_duration_ms = 300`, then the average is computed on 3 or 4 outputs but the result is still < 200.
So to solve the issue, **I have removed the average and then it works**.

I don't understand the purpose of the average of the outputs. When I say 'yes', the model detects only one occurence of 'yes', There is no stuttering in the input data?

Could you please confirm my analyse and my fix.

Thanks in advance



"
28515,min/max data missing for FusedBatchNorm op ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.13
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: GeForce 2080Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm trying to use contrib/quantize module to perform quantize aware training and convert the trained model to quantize TFlite model. When run `converter.convert()`, the following error will be raised:
```
tensorflow/lite/toco/tooling_util.cc:1702] Array conv0/batch_normalization_v1/FusedBatchNorm, which is an input to the Conv operator producing the output array conv1/re_lu/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Aborted (core dumped)
```

When adding ReLU op after the BN, it will be converted without the error massage. But in the tensorboard graph, I cannot see any differences on the FusedBatchNorm part with and without ReLU. 
**Describe the expected behavior**
Convert to TFLite without error

**Code to reproduce the issue**
```python
import tensorflow as tf
import os
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ['CUDA_VISIBLE_DEVICES'] = ''


def build_model(input):
    '''build the model'''
    with tf.name_scope('conv0'):
        x = tf.keras.layers.Conv2D(24, kernel_size=(3, 3), padding='same', use_bias=False)(input)
        x = tf.keras.layers.BatchNormalization(fused=True)(x)
        #x = tf.keras.layers.ReLU()(x)
    with tf.name_scope('conv1'):
        x = tf.keras.layers.Conv2D(48, kernel_size=(3, 3), padding='same', use_bias=False)(x)
        x = tf.keras.layers.BatchNormalization(fused=True)(x)
        x = tf.keras.layers.ReLU()(x)
    x = tf.concat((input, x), axis=-1)
    with tf.name_scope('conv_out'):
        x = tf.keras.layers.Conv2D(10, kernel_size=(1, 1), padding='same', use_bias=True)(x)
        x = tf.keras.layers.ReLU()(x)
    return x


# build model
tf.keras.backend.set_learning_phase(1)
input = tf.placeholder(tf.float32, shape=(None, None, None, 3))
x = build_model(input)

# quantize
graph = tf.get_default_graph()
tf.contrib.quantize.create_training_graph(input_graph=graph, quant_delay=0)

# save
saver = tf.train.Saver()
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    saver.save(sess, './tmp/simple/model.ckpt')
    writer = tf.summary.FileWriter('./tmp/simple/train/', sess.graph)
    writer.flush()
    writer.close()


# eval
tf.reset_default_graph()
tf.keras.backend.set_learning_phase(0)
input = tf.placeholder(tf.float32, shape=(1, 32, 32, 3), name='input')
x = build_model(input)
x = tf.identity(x, 'output')

# quantize
graph = tf.get_default_graph()
init_min=-6, init_max=6)
tf.contrib.quantize.create_eval_graph(graph)
saver = tf.train.Saver()
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    saver.restore(sess, tf.train.latest_checkpoint('./tmp/simple'))
    writer = tf.summary.FileWriter('./tmp/simple/eval/', graph)
    writer.flush()
    writer.close()

    # freeze graph
    graph_def = graph.as_graph_def()
    froze_graph = tf.graph_util.convert_variables_to_constants(sess, graph_def, ['output'])
    tf.io.write_graph(froze_graph, './tmp/simple/', 'freeze_graph.pb')

# convert to TFLite
graph_def_file = './tmp/simple/freeze_graph.pb'
input_array = [""input""]
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_array, ['output'],
                                                      input_shapes={""input"": [1, 32, 32, 3]})
converter.allow_custom_ops = True
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
converter.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
converter.quantized_input_stats = {""input"": (0., 255.)}
tfmodel = converter.convert()
open(""./tmp/simple/converted_model.tflite"", ""wb"").write(tfmodel)

# load the TFLite
interpreter = tf.lite.Interpreter(model_path=""./tmp/simple/converted_model.tflite"")
interpreter.allocate_tensors()
print(""TFLite model loadded!!"")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28514,GoDoc link error,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/doc.go

## Description of issue (what needs changing):

The link doesn't work. It strips the trailing `.md` off the URL when it is redirected through `tensorflow.org/code`


### Correct links

it is correct, I think the redirect is broken. for example, if you change 
`https://www.tensorflow.org/code/tensorflow/go/README.md` 
to
`https://www.tensorflow.org/code/tensorflow/go/README.md.md` 

it works.

### Parameters defined

NA

### Returns defined

NA

### Raises listed and defined

NA
### Usage example

NA

### Request visuals, if applicable

NA

### Submit a pull request?

i don't think a PR is the solution here. if there is a better URL tho i'm happy to submit a PR.
"
28513,"bazel test fails ""C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): python.exe failed: error executing command""","Env:
OS: windows 10
bazel version: 0.25
tensorflow version:r1.14
python:3.6.4

I use  command to build .whl file is ok:
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

before action:
I modify third_party/gpus/crosstool/BUILD.tpl.  
cc_toolchain(
    name = ""cc-compiler-local"",
    all_files = ""%{linker_files}"",
    compiler_files = "":empty"",
    #cpu = ""local"",    **#remove cpu attribute**
    dwp_files = "":empty"",
    linker_files = ""%{linker_files}"",
    objcopy_files = "":empty"",
    strip_files = "":empty"",
    # To support linker flags that need to go to the start of command line
    # we need the toolchain to support parameter files. Parameter files are
    # last on the command line and contain all shared libraries to link, so all
    # regular options will be left of them.
    supports_param_files = 1,
    toolchain_identifier = ""local_linux"",
)

cc_toolchain(
    name = ""cc-compiler-darwin"",
    all_files = ""%{linker_files}"",
    compiler_files = "":empty"",
    #cpu = ""darwin"", **#remove cpu attribute**
    dwp_files = "":empty"",
    linker_files = ""%{linker_files}"",
    objcopy_files = "":empty"",
    strip_files = "":empty"",
    supports_param_files = 0,
    toolchain_identifier = ""local_darwin"",
)
because there will make a exception to [#7075](https://github.com/bazelbuild/bazel/issues/7075) 
 so remove the cpu attribute. 

**But**   when i run bazel test behind command occur exception:
bazel test --noincompatible_disable_crosstool_file tensorflow/contrib/tensor_forest:model_ops_test

error:
$ bazel test --noincompatible_disable_crosstool_file tensorflow/contrib/tensor_forest:model_ops_test
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Loading: 0 packages loaded
    currently loading: tensorflow/contrib/tensor_forest
Analyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (10 packages loaded, 34 targets configured)
Analyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (31 packages loaded, 847 targets configured)
Analyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (83 packages loaded, 3373 targets configured)
INFO: Analyzed target //tensorflow/contrib/tensor_forest:model_ops_test (84 packages loaded, 4238 targets configured).
INFO: Found 1 test target...
[0 / 15] [-----] BazelWorkspaceStatusAction stable-status.txt ... (4 actions, 0 running)
[0 / 15] [-----] BazelWorkspaceStatusAction stable-status.txt ... (4 actions, 0 running)
ERROR: C:/users/.../7hwgx76p/external/protobuf_archive/BUILD:277:1: C++ compilation of rule '@protobuf_archive//:protoc_lib' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/.../_bazel_.../7hwgx76p/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\8.1\include\shared;C:\Program Files (x86)\Windows Kits\8.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Windows\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\...\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
    SET TMP=C:\Users\...\AppData\Local\Temp
  C:/Python/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /DHAVE_PTHREAD /wd4018 /wd4065 /wd4146 /wd4244 /wd4251 /wd4267 /wd4305 /wd4307 /wd4309 /wd4334 /wd4355 /wd4506 /wd4514 /wd4800 /wd4996 /Fobazel-out/x64_windows-opt/bin/external/protobuf_archive/_objs/protoc_lib/java_doc_comment.o /c external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc
Execution platform: @bazel_tools//platforms:host_platform
external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc(35): fatal error C1083: Cannot open include file: 'google/protobuf/compiler/java/java_doc_comment.h': No such file or directory
Target //tensorflow/contrib/tensor_forest:model_ops_test failed to build
INFO: Elapsed time: 24.179s, Critical Path: 0.55s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

==============================
how to fix it?
"
28512,AttributeError: module 'tensorflow' has no attribute 'placeholder',"Hello, dear guys.

I build the tensorflow(CPU) version in Windows 10.

I install tensorflow-2.0-beta.

I want to run some tensorflow old-version code:
```
#
#  ColorHandPose3DNetwork - Network for estimating 3D Hand Pose from a single RGB Image
#  Copyright (C) 2017  Christian Zimmermann
#  
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 2 of the License, or
#  (at your option) any later version.
#  
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import print_function, unicode_literals

import tensorflow as tf
import numpy as np
import scipy.misc
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from nets.ColorHandPose3DNetwork import ColorHandPose3DNetwork
from utils.general import detect_keypoints, trafo_coords, plot_hand, plot_hand_3d
import datetime

if __name__ == '__main__':
    # images to be shown
    image_list = list()
    '''
    image_list.append('./data_new/img.png')
    image_list.append('./data_new/img2.png')
    image_list.append('./data_new/img3.png')
    image_list.append('./data_new/img4.png')
    image_list.append('./data_new/img5.png')
    '''
    starttime = datetime.datetime.now()
    image_list.append('./data_new/demo1.jpg')
    # network input
    image_tf = tf.placeholder(tf.float32, shape=(1, 240, 320, 3))
    hand_side_tf = tf.constant([[1.0, 0.0]])  # left hand (true for all samples provided)
    evaluation = tf.placeholder_with_default(True, shape=())

    # build network
    net = ColorHandPose3DNetwork()
    hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,\
    keypoints_scoremap_tf, keypoint_coord3d_tf = net.inference(image_tf, hand_side_tf, evaluation)

    # Start TF
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)
    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

    # initialize network
    net.init(sess)

    # Feed image list through network
    for img_name in image_list:
        starttime = datetime.datetime.now()
        image_raw = scipy.misc.imread(img_name)
        image_raw = scipy.misc.imresize(image_raw, (240, 320))
        image_v = np.expand_dims((image_raw.astype('float') / 255.0) - 0.5, 0)

        hand_scoremap_v, image_crop_v, scale_v, center_v,\
        keypoints_scoremap_v, keypoint_coord3d_v = sess.run([hand_scoremap_tf, image_crop_tf, scale_tf, center_tf,
                                                             keypoints_scoremap_tf, keypoint_coord3d_tf],
                                                            feed_dict={image_tf: image_v})

        hand_scoremap_v = np.squeeze(hand_scoremap_v)
        image_crop_v = np.squeeze(image_crop_v)
        keypoints_scoremap_v = np.squeeze(keypoints_scoremap_v)
        keypoint_coord3d_v = np.squeeze(keypoint_coord3d_v)

        # post processing
        image_crop_v = ((image_crop_v + 0.5) * 255).astype('uint8')
        coord_hw_crop = detect_keypoints(np.squeeze(keypoints_scoremap_v))
        coord_hw = trafo_coords(coord_hw_crop, center_v, scale_v, 256)

        # visualize
        fig = plt.figure(1)
        ax1 = fig.add_subplot(221)
        ax2 = fig.add_subplot(222)
        ax3 = fig.add_subplot(223)
        ax4 = fig.add_subplot(224, projection='3d')
        ax1.imshow(image_raw)
        plot_hand(coord_hw, ax1)
        ax2.imshow(image_crop_v)
        plot_hand(coord_hw_crop, ax2)
        ax3.imshow(np.argmax(hand_scoremap_v, 2))
        plot_hand_3d(keypoint_coord3d_v, ax4)
        ax4.view_init(azim=-90.0, elev=-90.0)  # aligns the 3d coord with the camera view
        ax4.set_xlim([-3, 3])
        ax4.set_ylim([-3, 1])
        ax4.set_zlim([-3, 3])
        endtime2 = datetime.datetime.now()
        print(endtime2-starttime)
        plt.show()

```

However, it reports the error:

> Traceback (most recent call last):
>   File ""run.py"", line 43, in <module>
>     image_tf = tf.placeholder(tf.float32, shape=(1, 240, 320, 3))
> AttributeError: module 'tensorflow' has no attribute 'placeholder'

I wonder how can I  make my old code compatible with new version software?

Thanks & Regards!"
28511,Not found: Op type not registered 'StatefulPartitionedCall',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9.9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: - 
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): clang from ./configure
- CUDA/cuDNN version: -
- GPU model and memory: -



I built libtensorflow_cc.so and libtensorflow_framework.so from source using bazel, CPU only. Using the resulting libraries I can compile my small c++ project just fine with cmake (requires some work with dependencies and headers).

However when I load a model via LoadSavedModel into a SavedModelBundle I get the following output:

2019-05-07 15:21:15.497043: I tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: /.../.../model_dir/autoencoder
2019-05-07 15:21:15.511093: I tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: fail. Took 13982 microseconds.
Not found: Op type not registered 'StatefulPartitionedCall' in binary running on ---. Make sure the Op and Kernel are registered in the binary running in this process.

The model has been trained in python and is a functional keras model. It was exported via keras.experimental.export_saved_model. My guess is that the 'StatefulPartitionedCall' is added by the export.

When I compile my cpp using bazel inside the tensorflow repo everything goes fine. So is the build of  the shared libraries not up to date or am I doing something wrong?

"
28509,【XLA on Centos】poor performance improvement on Centos than  on Ubuntu,"Linking to XLA libraries has been a default option since version 1.12 .It's a really great job!!
I found training speed was faster, double to speed of tf 1.10, on Ubuntu system(48CPU,0 GPU).

Then I tried to test this improvement on Centos where I found there is no improvement. I also tried to compile source code with XLA, but no improvement was found either.

Does XLA only support Ubuntu system? Or do I have configure anything special before compiling the source code?

Additional info:
source_code: tensorflow-1.12.0-rc2"
28508,gfile.Copy does not overwrite dest file properly on posix filesystems,"**Describe the current behavior**
`gfile.Copy(overwrite=True)` does not truncate the destination file before overwriting. That means if the src file is shorter than the dest file, the resulting dest file contains the mix of the two.


**Describe the expected behavior**
`gfile.Copy(overwrite=True)` results in having the exact same content of src file in the dest file.

**Code to reproduce the issue**
```
$ echo 'aaa' > a.txt
$ echo 'bbbbbb' > b.txt
$ python3 -c ""from tensorflow import gfile; gfile.Copy('a.txt', 'b.txt', overwrite=True)""
$ cat b.txt
aaa
bb
```

Tested with `pip3 install tensorflow==1.13.1`, python 3.5.2
`b.txt` should have `aaa` as  the content, not `aaa\nbb`.

Ref. https://www.tensorflow.org/api_docs/python/tf/gfile/Copy
"
28507,"when I run my model in gpudelegate in android , it build crash","here is my bug message:

TfLiteGpuDelegate Prepare: Suitable node shader is not found: Output size is less than input size.Node number 129 (TfLiteGpuDelegate) failed to prepare.

my aar is tensorflow-lite-0.0.0-gpu-experimental.aar"
28506,Tensorflow build does not support bazel 0.25???,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Tensorflow source doesn't support bazel 0.25 ??

WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.25.0- (@non-git) installed.
Please downgrade your bazel installation to version 0.24.1 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.

This seems stupid."
28504,Error while converting .pb model into tensorflowlite model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.7.3
- Bazel version (if compiling from source): 0.25

**Describe the current behavior**

Converting Deeplab .pb model into TFLite model gives the following error:

```
F tensorflow/lite/toco/model_cmdline_flags.cc:289] Check failed: uses_single_input_flags
```

**Code to reproduce the issue**

```
bazel-bin/tensorflow/lite/toco/toco \
--input_file=flatten.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--output_file=""lite.tflite"" \
--inference_input_type=FLOAT \
--input_data_type=FLOAT \
--input_arrays=sub_7 \
--output_arrays=ResizeBilinear_3 \
--input_shapes=1,513,513,3
```

**Other info / logs**
Here is the flatten graph `flatten.pb`: https://drive.google.com/open?id=1u6crZ9fBv-Xz88AJcqRyu17p9vFIiNPf"
28503,"set_visible_device_list("""") in c++ not work ","I use tensorflow 1.12 cuda 9.0 build from source.
My code have follow lines, but session still use gpu. I not use CUDA_VISIBLE_DEVICES='' because my program have two session.
```
auto options = SessionOptions();
options.config.mutable_gpu_options()->set_visible_device_list("""");
```"
28501,"tf2.0, tf.keras Embedding layer behave different than tf1.13","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7

```python
from tensorflow import keras
import numpy as np

L = keras.layers

embedding_matrix = np.random.random((10, 5))

model = keras.Sequential([
    L.Embedding(input_dim=10, 
                output_dim=5,
                weights=[embedding_matrix],
                trainable=False)
])

model.compile('rmsprop', 'mse')

embedding_matrix[2] == model.predict([2])[0][0]
```

**Describe the current behavior**

`embedding_matrix[2]` is not equal to  `model.predict([2])[0][0]`.

**Describe the expected behavior**

`embedding_matrix[2]` should equal to  `model.predict([2])[0][0]`, those two equals on tf 1.13.1 version."
28500,Bazel build error,"
Follow the instructions listed in [https://www.tensorflow.org/install/source](url), but encountered errors when building 

```
 bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

The errors are:
```
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (320 packages loaded).
INFO: Found 1 target...
ERROR: /home/ww/.cache/bazel/_bazel_ww/7b32ff1c0ac316bf710c70621fc9fbba/external/double_conversion/BUILD.bazel:12:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 1)
/tmp/ccv70MBq.s: Assembler messages:
/tmp/ccv70MBq.s:1068: Error: operand type mismatch for `vxorps'
/tmp/ccv70MBq.s:1076: Error: operand type mismatch for `vxorps'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.214s, Critical Path: 0.43s
INFO: 6 processes: 6 local.
FAILED: Build did NOT complete successfully
```
Tensorflow: 1.11
bazel: 0.15.0"
28499,tensorflow debugging tools not support eager execution ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Can't enter debugging mode with enable eager execution
**Describe the expected behavior**
Enter debugging tools
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` python
if hasattr(tf,'enable_eager_execution'):
    tf.enable_eager_execution()
tf.keras.backend.set_session(tf_debug.LocalCLIDebugWrapperSession(tf.Session()))
```
``` bash
 python -m train --debug
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28498,cyclic graph is not supported?,"Please correct me if I am wrong.
I am building cyclic graph. However cyclic graph is not supported in tensorflow. 

**System information**
- TensorFlow version: 2.0.0-alpha0
- Are you willing to contribute it (Yes/No): Yes

**Who will benefit with this feature?**
tf is more expressive with cyclic graph. tf.while can be build with cyclic graph with conditional node.  Recursion, such as https://stackoverflow.com/questions/43129225/python-recursive-tensorflow-fibonacci-number-calculation, can be written easily.
"
28497,cudnn.so could not be found when using bazel-0.21.0 in r1.13,"**System information**
- OS Platform and Distribution : Linux 3.10.0-CentOS 7
- TensorFlow installed from: source
- TensorFlow version: **r1.13**
- Python version: 2.7
- Bazel version : **0.21.0**
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: cuda9.0/cudnn7.3.1
- GPU model and memory: V-100



**Describe the problem**
I encountered below error using **bazel-0.21.0**. It has gone when replaced with bazel-0.19.2
ERROR: /apsarapangu/disk2/xinan.jxn/my_git/tensorflow/tensorflow/python/BUILD:1847:1: Executing genrule //tensorflow/python:sdca_ops_pygenrule failed (Exit 127): bash failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/xxxxxx/execroot/org_tensorflow && \
  exec env - \
    PATH=/bin:/usr/bin \
  /bin/bash bazel-out/host/genfiles/tensorflow/python/sdca_ops_pygenrule.genrule_script.sh)
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/host/bin/tensorflow/python/gen_sdca_ops_py_wrappers_cc: error while loading shared libraries: libcudnn.so.7: cannot open shared object file: No such file or directory


I have added right LD_LIBRARY_PATH ""/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64""
and libcudnn.so.7 is just in /usr/local/cuda/lib64


"
28496,TF_LoadLibrary in c++ when loading resampler.so,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below):1.7.0
- Python version: 3.5
- Bazel version (if compiling from source): 1.10.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I freeze a ckpt file into a pb file in python and want to load it in C++. Since the pb file includes tf.contrib.resampler, it fails loading the model saying resampler op is not registered.
I would like to load _resampler_ops.so generated by bazel to fix the bug ( I am not sure am I doing the right thing or not, if not, what should I do to fix this bug?). TF_LoadLibrary(""tensorflow-1.7.0/bazel-bin/tensorflow/contrib/resampler/python/ops/_resampler_ops.so
"")
the return status is False.
I also tried the path:/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/resampler/python/ops/_resampler_ops.so
the error is the same: load library fails.
Then I tried to rebuild tensorflow using --config=monolithic， and load /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/resampler/python/ops/_resampler_ops.so. 
the error is: segmentation fault; core dumped.

**Describe the expected behavior**
I suppose the TF_LoadLibrary should work and fix the non-registration error.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
string graph_name = argv[1];// name.pb
	Session *session;
	Status status = NewSession(SessionOptions(), &session);
	if (!status.ok())
	{
		cout << ""create session wrong: ""<< status.ToString() << endl;
		return -1;
	}
	GraphDef graph_def;
	//read model:
	if (!ReadBinaryProto(Env::Default(), graph_name, &graph_def).ok())
	{
		cout << ""Read model(pb) failed!"" << endl;
		return -1;
	}
TF_Status* status_1 = TF_NewStatus();
	TF_LoadLibrary(""tensorflow-1.7.0/bazel-bin/tensorflow/contrib/resampler/python/ops/_resampler_ops.so"", status_1);
	if (TF_GetCode(status_1) != TF_OK) {
		cout << ""load lib wrong"" << endl;
	}
	TF_DeleteStatus(status_1);
	status = session->Create(graph_def);
	if (!status.ok())
	{
		cout << ""create graph wrong: "" << status.ToString() << endl;
		return -1;
	}
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28495,Move the Dockerfiles to ubuntu-18.04,"[Current Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile#L22) we have is based out of 16.04, its better if we can move to 18.04.

The corresponding version of TF Serving is already using 18.04 based ubuntu in their Dockerfile."
28494,Unable to install tensorflow in python 3.6.8 with pip,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version:1.8.0
- Python version:3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA



**Describe the problem**
I am unable to import tensorflow on windows 10 and python 3.6.8 without error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow

**Any other info / logs**

I get this error:
Traceback (most recent call last):
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Owen\Documents\neural-network-master\TFMusicAudioEncoder-master\autoe.py"", line 1, in <module>
    from keras.layers import Input, Dense
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Owen\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow_internal
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.
"
28491,Valgrind showing memory leak in c++ with libtensorflow.so file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: using c++
- Bazel version (if compiling from source): bazel release 0.17.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0

- CUDA/cuDNN version: Not using GPU
- GPU model and memory: Not using GPU 



**Describe the current behavior**

Tensorflow c++ code compiled using libtensorflow.so file is showing memory leaks using Valgrind.
```
==13544== Memcheck, a memory error detector
==13544== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==13544== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==13544== Command: ./bin/valmain_tfexample_cpu.out
==13544== 
Hello TF..
==13544== 
==13544== HEAP SUMMARY:
==13544==     in use at exit: 5,609,733 bytes in 85,138 blocks
==13544==   total heap usage: 323,684 allocs, 238,546 frees, 19,116,320 bytes allocated
==13544== 
==13544== LEAK SUMMARY:
==13544==    definitely lost: 0 bytes in 0 blocks
==13544==    indirectly lost: 0 bytes in 0 blocks
==13544==      possibly lost: 0 bytes in 0 blocks
==13544==    still reachable: 5,609,733 bytes in 85,138 blocks
==13544==                       of which reachable via heuristic:
==13544==                         newarray           : 2,976 bytes in 5 blocks
==13544==         suppressed: 0 bytes in 0 blocks
==13544== Rerun with --leak-check=full to see details of leaked memory
==13544== 
==13544== For counts of detected and suppressed errors, rerun with: -v
==13544== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```

You can see there is 5,609,733 bytes of memory leak occurred.

Also tried TCMalloc and found the same issues:
$$$ HEAPCHECK=normal ./bin/main.out
``` 
WARNING: Perftools heap leak checker is active -- Performance may suffer
Starting tracking the heap
Hello TF..
Dumping heap profile to /tmp/heapprof.0001.heap (Exiting, 1 kB in use)
Have memory regions w/o callers: might report false leaks
No leaks found for check ""_main_"" (but no 100% guarantee that there aren't any): found 85224 reachable heap objects of 5740540 bytes
```

I compiled the tensorflow code and got the .so file using the following commands:
```
$$ git clone https://github.com/tensorflow/tensorflow.git
$$ git checkout r1.12
$$ ./configure
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/tumu/.cache/bazel/_bazel_tumu/install/792a28b07894763eaa2bd870f8776b23/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.17.2 installed.
Please specify the location of python. [Default is /usr/bin/python]: 


Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
Configuration finished

**$$ bazel build --config=monolithic  //tensorflow:libtensorflow_cc.so** (want to use cpu only)


```
**Describe the expected behavior**

**Code to reproduce the issue**
Sample Code: main.cc
```
#include <fstream>
#include <utility>
#include <vector>

#include ""tensorflow/cc/ops/const_op.h""
#include ""tensorflow/cc/ops/image_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/str_util.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""

// These are all common classes it's handy to reference with no namespace.
using tensorflow::Flag;
using tensorflow::Tensor;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::int32;


int main(int argc, char* argv[]) {
  std::cout << ""Hello TF..\n"";
}
```
Compilation script:
```
\g++ -O4 -std=c++11 -o ./bin/main.out \
-I ./include \
-I ./include/third_party \
-I ./include/third_party/eigen3 \
./src/main_tfexample.cc \
-L ./libs \
-lpthread \
$PWD/libs/libtensorflow_cc.so
```
$$$  valgrind ./bin/main.out 
```
==13544== Memcheck, a memory error detector
==13544== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==13544== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==13544== Command: ./bin/valmain_tfexample_cpu.out
==13544== 
Hello TF..
==13544== 
==13544== HEAP SUMMARY:
==13544==     in use at exit: 5,609,733 bytes in 85,138 blocks
==13544==   total heap usage: 323,684 allocs, 238,546 frees, 19,116,320 bytes allocated
==13544== 
==13544== LEAK SUMMARY:
==13544==    definitely lost: 0 bytes in 0 blocks
==13544==    indirectly lost: 0 bytes in 0 blocks
==13544==      possibly lost: 0 bytes in 0 blocks
==13544==    still reachable: 5,609,733 bytes in 85,138 blocks
==13544==                       of which reachable via heuristic:
==13544==                         newarray           : 2,976 bytes in 5 blocks
==13544==         suppressed: 0 bytes in 0 blocks
==13544== Rerun with --leak-check=full to see details of leaked memory
==13544== 
==13544== For counts of detected and suppressed errors, rerun with: -v
==13544== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```

$$$ valgrind --leak-check=full ./bin/main.out
```
==13549== Memcheck, a memory error detector
==13549== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==13549== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==13549== Command: ./bin/valmain_tfexample_cpu.out
==13549== 
Hello TF..
==13549== 
==13549== HEAP SUMMARY:
==13549==     in use at exit: 5,609,733 bytes in 85,138 blocks
==13549==   total heap usage: 323,684 allocs, 238,546 frees, 19,116,320 bytes allocated
==13549== 
==13549== LEAK SUMMARY:
==13549==    definitely lost: 0 bytes in 0 blocks
==13549==    indirectly lost: 0 bytes in 0 blocks
==13549==      possibly lost: 0 bytes in 0 blocks
==13549==    still reachable: 5,609,733 bytes in 85,138 blocks
==13549==                       of which reachable via heuristic:
==13549==                         newarray           : 2,976 bytes in 5 blocks
==13549==         suppressed: 0 bytes in 0 blocks
==13549== Reachable blocks (those to which a pointer was found) are not shown.
==13549== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==13549== 
==13549== For counts of detected and suppressed errors, rerun with: -v
==13549== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28487,Adam not learning when an embedding is initialized to zero ,"I am working on  Tensorflow b'v1.13.1-0-g6612da8951' 1.13.1

**Describe the current behavior**
Adam optimizer does not update embeddings when they are initialized to zero

**Describe the expected behavior**
Embeddings should be optimized


**Code to reproduce the issue**
```
from tensorflow.keras.layers import Input, Embedding
from tensorflow.keras import Model
import numpy as np
import tensorflow.keras as keras

my_input = Input(batch_shape=(None,1))
my_embed = (Embedding(input_dim=5, output_dim=1, weights=[ np.ones((5, 1))  * 0.0  ])(my_input))
keras_model = Model(my_input, my_embed)


keras_model.compile( loss='binary_crossentropy', 
                    metrics=['binary_accuracy'], 
                    optimizer=keras.optimizers.Adam() )

batch = np.ones(10000)
batch
keras_model.fit(x=batch, y=np.ones(10000), epochs=900)
```


![image](https://user-images.githubusercontent.com/1832193/57323946-a7ec7e80-70bb-11e9-96f8-aab696204c93.png)
"
28485,Supporting control flow in TensorFlow Lite,"This is a tracking ticket for supporting generalized control flow in TensorFlow Lite. See also the [RFC](https://github.com/tensorflow/community/pull/83). 

At this moment, if you see missing ops like ""Switch"", ""Merge"", ""Enter"", ""Exit"", ""NextIteration"" when converting a TensorFlow model to TensorFlow Lite, it means the graph contains control flows, and there's no way to convert it. 

We're working hard to enable this feature. Updates will be posted here. "
28484,Always-on dropout layer,"**Describe the feature and the current behavior/state.**

Add the ability to create always-on dropout layers.

**Will this change the current api? How?**

Yes, adds a boolean option `always_on` to `tf.keras.layers.Dropout`.

Currently, implementing this functionality requires writing a custom dropout layer:

```py
class Dropout(tf.keras.layers.Layer):
    """"""Always-on dropout layer, i.e. does not respect the training flag
    set to true by model.fit but false by model.predict.
    Unlike tf.keras.layers.Dropout, this layer does not return input
    unchanged if training=true, but always randomly drops a fraction specified
    by self.size of the input nodes.
    """"""

    def __init__(self, rate, **kwargs):
        super().__init__(**kwargs)
        self.rate = rate

    def call(self, inputs):
        return tf.nn.dropout(inputs, self.rate)

    def get_config(self):
        """"""enables model.save and restoration through tf.keras.models.load_model""""""
        config = super().get_config()
        config[""rate""] = self.rate
        return config
```

**Who will benefit from this feature?**

Everyone using dropout to create Bayesian neural networks ([see this paper](https://arxiv.org/abs/1703.04977)). In particular, users of `tfp` may be interested."
28482,checkpoint.save failure on colab when using MirroredStrategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab GPU execution
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  1.14.1-dev20190507
- Python version: 3.6.7
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: 10.0/7.5.0
- GPU model and memory: Tesla K80

**Describe the current behavior**
The tutorial *tf.distribute.Strategy with Training Loops* fails on the `checkpoint.save(checkpoint_prefix)` call when I run it on Colab, as does any other attempt on my part to use checkpoints with Mirrored strategy in Colab with custom loop. The VM still has lots of disk memory so no problem on this part. I also tried to make it save on a mounted Google drive with no more success.

**Describe the expected behavior**
Checkpoint saving without error.

**Code to reproduce the issue**
Run this in Google Colab : https://www.tensorflow.org/alpha/tutorials/distribute/training_loops
For the code to execute without error (until the save), you'll need to replace
```python
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      reduction=tf.keras.losses.Reduction.NONE)
```
with

```python
  from tensorflow.python.keras.utils import losses_utils
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      reduction=losses_utils.ReductionV2.NONE)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

<ipython-input-55-2d2ddd674439> in <module>()
     26 
     27     if epoch % 2 == 0:
---> 28       checkpoint.save(checkpoint_prefix)
     29 
     30     template = (""Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, ""

6 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py in handle(self)
    578       device = distribute_lib.get_update_device()
    579       if device is None:
--> 580         raise ValueError(""`handle` is not available outside the replica context""
    581                          "" or a `tf.distribute.Strategy.update()` call."")
    582     return self.get(device=device).handle

ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.
```"
28481,Cloud ml engine + tfhub universal sentence encoder prediction issue,"
Hello,
I trained a text classification model using TF-hub universal sentence encoder.
The model trained successfully and hosted on google cloud ML engine as well.

When i will try to prediction on model using
gcloud ml-engine predict --model $MODEL_NAME --version v1 --json-instances ./request.json
This is error i am getting:

{""error"": ""Prediction failed: Error during model execution:
AbortionError(code=StatusCode.UNAVAILABLE, details=""Connect Failed"")""
}"
28480,Enable Slack Github Integration and Notifications,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): NA
- Are you willing to contribute it (Yes/No): NA



**Describe the feature and the current behavior/state.**

I want to the Slack Github integration to notify my team of activity in `tensorflow/tensorflow`, but I received an error: 
```
Either the app isn't installed on your repository or the repository does not exist. Install it to proceed.
```

To reproduce:

1. Add GitHub app to your Slack workspace
2. In a channel, run `/github subscribe tensorflow/tensorflow`
3. See error 

**Will this change the current api? How?**
NA

**Who will benefit with this feature?**
Everyone who uses Slack for GitHub notifications! 

**Any Other info.**
Only an organization owner can enable this integration 
"
28477,Tensorflow 2.0 Bus error 10 when creating a tf.data.Dataset from numpy arrays and eigen-decomposing them,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (yes, to generate data only):
- OS Platform and Distribution (e.g., MacOs):
- TensorFlow installed from (pip):
- TensorFlow version (2.0.0a0):
- Python version: 3.6.6

**Describe the current behavior**
When executing `test.py` the process dies without any exceptions with `Bus Error 10`.

**Describe the expected behavior**
Print the value of tensors in the dataset.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np 
from tensorflow.keras.applications import VGG19
from tensorflow.keras.layers import Input, UpSampling2D, Conv2D
from tensorflow.keras.models import Model

dg = DataGen()

output_types=(
    tf.float32
 )
output_shapes=(
    tf.TensorShape([None, None])
)

ds = tf.data.Dataset.from_generator(dg.gen,
                                   output_types=output_types, 
                                   output_shapes=output_shapes)

for v in ds.take(10):
    print(v)
```

And the generator:
```
class DataGen:

    def __init__(self):

        self.im_shape = (256, 256, 3)
        self.vgg = self.build_vgg_loss()


    def build_vgg_loss(self):
        vgg = VGG19(weights=""imagenet"", include_top=False, input_shape=self.im_shape)
        vgg.outputs = [
            vgg.get_layer('block1_conv1').output,
            vgg.get_layer('block2_conv1').output,
            vgg.get_layer('block3_conv1').output,
            vgg.get_layer('block4_conv1').output
        ]

        model = Model(inputs=vgg.inputs, outputs=vgg.outputs)
        model.trainable = False

        return model

    def gen(self):
        while True:
            A = np.random.randn(1,256,256,3).astype(np.float32)
            A = np.array(self.vgg(A)[3]).reshape((-1,512))
            A = A - np.mean(A, axis=0)
            cov_matrix = np.matmul(A, A.T)
            print(cov_matrix.shape)
            values, vectors = np.linalg.eig(cov_matrix)
            A = np.diag(values)
            yield A
```"
28474,Seek help ,"I use tensorflow to train a end-to-end scene text recognition system. The training stage seems good. total_loss gradually declined until convergence. But when I load pretrained weight and use the same network to predict a picture. it reminds me:
###
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value recognition_lstm/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/lstm_cell/bias
###
Meanwhile  I run my demo.py and print(tf.global_variables()).
I found that some variables related to recognition part are not in tf.global_variables()
Why? Any help will be appreciated.

Here is my demo.py
#####################################################################
![2019-05-07 20-39-22屏幕截图](https://user-images.githubusercontent.com/34428575/57300316-2ae7f600-7109-11e9-87de-89123c56a123.png)
![2019-05-07 20-39-35屏幕截图](https://user-images.githubusercontent.com/34428575/57300329-320f0400-7109-11e9-912c-a434dcb658b0.png)
![2019-05-07 20-39-44屏幕截图](https://user-images.githubusercontent.com/34428575/57300338-363b2180-7109-11e9-93c6-57a2fbb196c6.png)
"
28473,suppress logging messages in tf2.0,"We have tf.logging.set_verbesity() in tf1.x.

but tf2.0 there is not such module, what should we do?"
28472,Low GPU utilization ,"I am running windows 10, core i7-8700 cpu, gtx geforce 1660 ti GPU.
When training models, gpu utilization is very low (5-10% at max, sometimes lower).
Even is network is five layers. CPU utilization on the other hand is 30% and above."
28470,[nGraph] mkldnn::relu_forward and mkldnn::relu_backward error: expected type-specifier,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: dc77be8f92fd7b7ac57d69e35c4d0dbd994bf1a0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz


**Describe the problem**

```
bazel build --config=opt --config=mkl --config=numa --config=ngraph //tensorflow/tools/pip_package:build_pip_package
```


**Any other info / logs**

```
ERROR: /root/.cache/bazel/_bazel_root/5f3660b20527c5657e75cad53b138917/external/ngraph/BUILD.bazel:11:1: C++ compilation of rule '@ngraph//:ngraph_cpu_backend' failed (Exit 1) 
external/ngraph/src/ngraph/runtime/cpu/mkldnn_emitter.cpp: In member function 'size_t ngraph::runtime::cpu::MKLDNNEmitter::build_relu_forward(const mkldnn::memory::desc&, const mkldnn::memory::desc&)': 
external/ngraph/src/ngraph/runtime/cpu/mkldnn_emitter.cpp:810:51: error: expected type-specifier
      size_t primitive_index = insert_primitive(new mkldnn::relu_forward(
                                                    ^~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_emitter.cpp: In member function 'size_t ngraph::runtime::cpu::MKLDNNEmitter::build_relu_backward(const mkldnn::memory::desc&, const mkldnn::memory::desc&, const mkldnn::memory::desc&)':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_emitter.cpp:828:51: error: expected type-specifier
      size_t primitive_index = insert_primitive(new mkldnn::relu_backward(
                                                    ^~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build 
Use --verbose_failures to see the command lines of failed build steps. 
INFO: Elapsed time: 95.354s, Critical Path: 46.01s 
INFO: 1419 processes: 1419 local. 
FAILED: Build did NOT complete successfully
```

r1.14 branch:

```
ERROR: /root/.cache/bazel/_bazel_root/5f3660b20527c5657e75cad53b138917/external/ngraph/BUILD.bazel:11:1: C++ compilation of rule '@ngraph//:ngraph_cpu_backend' failed (Exit 1)
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp: In function 'std::map<mkldnn::memory::format, const std::__cxx11::basic_string<char> >& ngraph::runtime::cpu::mkldnn_utils::get_mkldnn_format_string_map()':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:162:10: error: 'ldigo_p' is not a member of 'mkldnn::memory::format'
         {memory::format::ldigo_p, ""memory::format::ldigo_p""},
          ^~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:164:10: error: 'ldgoi_p' is not a member of 'mkldnn::memory::format'
         {memory::format::ldgoi_p, ""memory::format::ldgoi_p""},
          ^~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:168:5: error: no matching function for call to 'std::map<mkldnn::memory::format, const std::__cxx11::basic_string<char> >::map(<brace-enclosed initializer list>)'
     };
     ^
In file included from /usr/include/c++/6/map:61:0,
                 from external/ngraph/src/ngraph/autodiff/adjoints.hpp:19,
                 from external/ngraph/src/ngraph/node.hpp:32,
                 from external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:22:
/usr/include/c++/6/bits/stl_map.h:273:9: note: candidate: template<class _InputIterator> std::map<_Key, _Tp, _Compare, _Alloc>::map(_InputIterator, _InputIterator, const _Compare&, const allocator_type&)
         map(_InputIterator __first, _InputIterator __last,
         ^~~
/usr/include/c++/6/bits/stl_map.h:273:9: note:   template argument deduction/substitution failed:
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:168:5: note:   candidate expects 4 arguments, 82 provided
     };
     ^
In file included from /usr/include/c++/6/map:61:0,
                 from external/ngraph/src/ngraph/autodiff/adjoints.hpp:19,
                 from external/ngraph/src/ngraph/node.hpp:32,
                 from external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:22:
/usr/include/c++/6/bits/stl_map.h:256:9: note: candidate: template<class _InputIterator> std::map<_Key, _Tp, _Compare, _Alloc>::map(_InputIterator, _InputIterator)
         map(_InputIterator __first, _InputIterator __last)
         ^~~
/usr/include/c++/6/bits/stl_map.h:256:9: note:   template argument deduction/substitution failed:
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:168:5: note:   candidate expects 2 arguments, 82 provided
     };
     ^
In file included from /usr/include/c++/6/map:61:0,
                 from external/ngraph/src/ngraph/autodiff/adjoints.hpp:19,
                 from external/ngraph/src/ngraph/node.hpp:32,
                 from external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:22:
/usr/include/c++/6/bits/stl_map.h:239:9: note: candidate: template<class _InputIterator> std::map<_Key, _Tp, _Compare, _Alloc>::map(_InputIterator, _InputIterator, const allocator_type&)
         map(_InputIterator __first, _InputIterator __last,
         ^~~
/usr/include/c++/6/bits/stl_map.h:239:9: note:   template argument deduction/substitution failed:
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:168:5: note:   candidate expects 3 arguments, 82 provided
     };
     ^
In file included from /usr/include/c++/6/map:61:0,
                 from external/ngraph/src/ngraph/autodiff/adjoints.hpp:19,
                 from external/ngraph/src/ngraph/node.hpp:32,
                 from external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:22:
/usr/include/c++/6/bits/stl_map.h:233:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(std::initializer_list<std::pair<const _Key, _Tp> >, const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(initializer_list<value_type> __l, const allocator_type& __a)
       ^~~
/usr/include/c++/6/bits/stl_map.h:233:7: note:   candidate expects 2 arguments, 82 provided
/usr/include/c++/6/bits/stl_map.h:227:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(std::map<_Key, _Tp, _Compare, _Alloc>&&, const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(map&& __m, const allocator_type& __a)
       ^~~
/usr/include/c++/6/bits/stl_map.h:227:7: note:   candidate expects 2 arguments, 82 provided
/usr/include/c++/6/bits/stl_map.h:223:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(const std::map<_Key, _Tp, _Compare, _Alloc>&, const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(const map& __m, const allocator_type& __a)
       ^~~
/usr/include/c++/6/bits/stl_map.h:223:7: note:   candidate expects 2 arguments, 82 provided
/usr/include/c++/6/bits/stl_map.h:219:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(const allocator_type& __a)
       ^~~
/usr/include/c++/6/bits/stl_map.h:219:7: note:   candidate expects 1 argument, 82 provided
/usr/include/c++/6/bits/stl_map.h:211:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(std::initializer_list<std::pair<const _Key, _Tp> >, const _Compare&, const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(initializer_list<value_type> __l,
       ^~~
/usr/include/c++/6/bits/stl_map.h:211:7: note:   candidate expects 3 arguments, 82 provided
/usr/include/c++/6/bits/stl_map.h:196:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(std::map<_Key, _Tp, _Compare, _Alloc>&&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(map&& __x)
       ^~~
/usr/include/c++/6/bits/stl_map.h:196:7: note:   candidate expects 1 argument, 82 provided
/usr/include/c++/6/bits/stl_map.h:185:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(const std::map<_Key, _Tp, _Compare, _Alloc>&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(const map& __x)
       ^~~
/usr/include/c++/6/bits/stl_map.h:185:7: note:   candidate expects 1 argument, 82 provided
/usr/include/c++/6/bits/stl_map.h:174:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map(const _Compare&, const allocator_type&) [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >; std::map<_Key, _Tp, _Compare, _Alloc>::allocator_type = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map(const _Compare& __comp,
       ^~~
/usr/include/c++/6/bits/stl_map.h:174:7: note:   candidate expects 2 arguments, 82 provided
/usr/include/c++/6/bits/stl_map.h:162:7: note: candidate: std::map<_Key, _Tp, _Compare, _Alloc>::map() [with _Key = mkldnn::memory::format; _Tp = const std::__cxx11::basic_string<char>; _Compare = std::less<mkldnn::memory::format>; _Alloc = std::allocator<std::pair<const mkldnn::memory::format, const std::__cxx11::basic_string<char> > >]
       map()
       ^~~
/usr/include/c++/6/bits/stl_map.h:162:7: note:   candidate expects 0 arguments, 82 provided
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp: In function 'mkldnn::memory::desc ngraph::runtime::cpu::mkldnn_utils::rotate_blocked_md(const mkldnn::memory::desc&, const ngraph::AxisVector&)':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:469:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (size_t i = 0; i < in.data.ndims; i++)
                        ~~^~~~~~~~~~~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp: In function 'mkldnn::memory::desc ngraph::runtime::cpu::mkldnn_utils::squeeze_blocked_md(const mkldnn::memory::desc&, ngraph::AxisVector&)':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:492:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (in.data.ndims <= axis_list.size())
         ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:513:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (size_t i = 0, j = 0; i < in.data.ndims; i++)
                               ~~^~~~~~~~~~~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp: In function 'mkldnn::memory::desc ngraph::runtime::cpu::mkldnn_utils::expand_blocked_md(const mkldnn::memory::desc&, ngraph::AxisVector&)':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:546:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (size_t i = 0, j = 0; j < md.ndims; j++)
                               ~~^~~~~~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:567:42: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                 for (size_t idx = 0; idx < in.data.ndims; idx++)
                                      ~~~~^~~~~~~~~~~~~~~
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp: In function 'bool ngraph::runtime::cpu::mkldnn_utils::is_mkldnn_padded_layout(const mkldnn::memory::desc&, const ngraph::AxisVector&)':
external/ngraph/src/ngraph/runtime/cpu/mkldnn_utils.cpp:630:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (size_t i = 0; i < in.data.ndims; i++)
                        ~~^~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 108.907s, Critical Path: 44.36s
INFO: 3128 processes: 3128 local.
FAILED: Build did NOT complete successfully
```"
28469,TFLite: please don't force copy outputs,"**System information**
- TensorFlow version (you are using): **1.3.1**
- Are you willing to contribute it (Yes/No): **Yes**

**Describe the feature and the current behavior/state.**

Currently, TFLite insistes that the [outputs](https://github.com/alexcohn/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L123) must not be empty, and copies the output tensor after each run to the array or ByteBuffer we provide in the *outputs* Map. This copy may come with a significant performance price, and may easily be avoided. 

Instead of providing the output buffer, the app can simply call

    interpreter.runForMultipleInputsOutputs(arrayOfInputs, null);
    drawResults(interpreter.getOutputTensor(0).buffer());

**Will this change the current api? How?**

1. second parameter of [**Interpreter.runForMultipleInputsOutputs()**](https://github.com/alexcohn/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L273) will become **Nullable**.
2. **NativeInterpreterWrapper.run()** will not [throw](https://github.com/alexcohn/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L123) when it gets **null** or empty **output**.
3. The [copy cycle](https://github.com/alexcohn/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L160) in the epilogue of **NativeInterpreterWrapper.run()** will not be invoked if **outputs == null**.
4. [Tensor.buffer()](https://github.com/alexcohn/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L303) will become **public**.

**Who will benefit with this feature?**

When I don't need a persistent copy of inference outputs, and only have some quick things to do with these buffers, this approach can save memory and CPU cycles.

**Any Other info.**

I have run tests with mobileNet_v1 on Android arm64 device from the **master** branch with the above patches, and have encountered no problems."
28468,Issue in using Tensorflow lite GPU delegate on iOS device,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone X
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
I am trying to benchmark a custom TF lite model on Android and iOS devices with the GPU delegate option. On the Android device(Google Pixel 3), I am able to successfully benchmark the model using the Tensorflow provided benchmark utility (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark).

Running the same model on iOS device (iPhone X) without GPU delegate option works fine. However when I try to run it with the GPU delegate option I observe the following error:

**tflite_camera_example[572:134773] [DYMTLInitPlatform] platform initialization successful
Loaded model 1resolved reporter
tflite_camera_example[572:134606] Metal GPU Frame Capture Enabled
tflite_camera_example[572:134606] Metal API Validation Enabled
**depth_multiplier 16 != weights output channels 1**
Node number 53 (MetalGpuDelegate) failed to prepare.
Node number 53 (MetalGpuDelegate) failed to prepare.
Failed to allocate tensors!
tflite_camera_example[572:134606] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles
tflite_camera_example[572:134606] [MC] Reading from public effective user settings.
(lldb)** 

I am modifying the iOS Tensorflowlite sample camera app(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/ios/camera) as per the instructions in this video from Tensorflow : https://www.youtube.com/watch?v=a5H4Zwjp49c&feature=youtu.be to evaluate the GPU delegate option.

**Describe the expected behavior**
Should be able to run the inference using the GPU delegate on iOS with the same model that worked on android

**Code to reproduce the issue**

**Other info / logs**
"
28466,tf.distribute behave inconsistent when using custom loss(BUG?),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0 alpha gpu
- Python version: 3.7

**Describe the current behavior**
UPDATE:
I tried the latest nightly building, still no luck and now more error info showed:
```
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run DeleteIterator: No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::IteratorResource::Deleter [Op:DeleteIterator]
```



**Below is on alpha 2.0 gpu**
I'm trying to transfer my tf1.0 code to 2.0 these days. And want to make use of the distribution strategy to optimize the multi-gpu training scheme. Simply description here:

> My goal is adopting a complex loss that computed by args more than just (y_true, y_pred) in a distribution manner

My older version implementation of the custom-loss is following  [@fchollet suggestion](https://github.com/tensorflow/addons/issues/26#issuecomment-473139603) by using add_loss/add_metric function.However if i want to use tf.distribution, the add_loss way is not allowed:
```
ValueError: We currently do not support compiling the model with distribution strategy if `model.add_loss(tensor)` or `model.add_metric(tensor)` has been called.
```
So, I am using another working-around from [here](https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618)

A working simplified code is:
```
import tensorflow as tf
from tensorflow.python import keras
from tensorflow.python.keras import layers as KL
from tensorflow.python.keras import models as KM
import numpy as np
print(tf.__version__)

def my_custom_loss_wrapper(input_weight):
    def real_loss(y_true, y_pred):
        # expand the out put of binary_crossentropy(64,64) to (64,64,1) to match the shape of input_weight
        bce_loss = tf.expand_dims(keras.losses.binary_crossentropy(y_true, y_pred), axis=-1)
        return bce_loss * input_weight

    return real_loss

fake_img = np.ones((64, 64, 3))
fake_label = np.ones((64, 64, 1))
fake_weight = np.ones((64, 64, 1)) * 5
dataset = tf.data.Dataset.from_tensors(((fake_img, fake_weight), fake_label)).repeat(100).batch(10)

img_input = KL.Input(shape=(64, 64, 3))
weight_input = KL.Input(shape=(64, 64, 1))
x = KL.Conv2D(32, (3, 3), strides=2, padding=""same"")(img_input)
x = KL.Conv2DTranspose(32, (3, 3), strides=2, padding=""same"")(x)
mask = KL.Conv2D(1, (1, 1), strides=1, activation=""sigmoid"", name=""mask"")(x)
model = KM.Model(inputs=[img_input, weight_input], outputs=mask)
model.compile(
    loss=my_custom_loss_wrapper(weight_input),  # return a function real_loss(y_true, y_pred)
    optimizer='sgd'
)
model.fit(dataset, epochs=1000)
```
Since above code is working good, let's enable the distribution strategy:
```
strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()
)
with strategy.scope():
    img_input = KL.Input(shape=(64, 64, 3))
    weight_input = KL.Input(shape=(64, 64, 1))
    x = KL.Conv2D(32, (3, 3), strides=2, padding=""same"")(img_input)
    x = KL.Conv2DTranspose(32, (3, 3), strides=2, padding=""same"")(x)
    mask = KL.Conv2D(1, (1, 1), strides=1, activation=""sigmoid"", name=""mask"")(x)
    model = KM.Model(inputs=[img_input, weight_input], outputs=mask)
    model.compile(
        # loss=""mse"",
        loss=my_custom_loss_wrapper(weight_input),  # return a function real_loss(y_true, y_pred)
        optimizer='sgd'
    )
model.fit(dataset, epochs=1000)
```
No luck this time:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,64,64,1]
	 [[{{node input_2}}]]
	 [[mask_sample_weights_1/_11]] [Op:__inference_keras_scratch_graph_1609]
```
I was trying to debug this by trace the _**actual_inputs**_ values in this [file ](https://github.com/tensorflow/tensorflow/blob/f7e3da0d6a284399ef2d6e79f4dbe61b32ae70f5/tensorflow/python/keras/engine/training_arrays.py), however it's exactly same with the non-distributed version. And the training engine just cannot get the ""input_2"". Which I double checked do have values match the type and shape in  _**actual_inputs**_ .

My very personal guess is that in the strategy scope, the context somehow nested and the feeding data process therefore failed when custom loss involved with another layer of the model like in my example ""input_weight"" except y_true and y_pred. If i don use the input_weight values in the function, the other code can still work.
```
def my_custom_loss_wrapper(input_weight):
    def real_loss(y_true, y_pred):
        # expand the out put of binary_crossentropy(64,64) to (64,64,1) to match the shape of input_weight
        bce_loss = tf.expand_dims(keras.losses.binary_crossentropy(y_true, y_pred), axis=-1)
        return bce_loss
    return real_loss
```
I dont know if this the reason why add_loss not supported by tf.distribution now. Any thought is appreciated!


**Describe the expected behavior**
Custom loss with multiple input layer works consistent in both distribute and non-distribute env.




"
28465,Poor performance while writing custom layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- Windows 10
- Tensorflow installed from pre-built binary
- TensorFlow version: 1.13.1
- Python: 3.6.6
- CUDA/cuDNN version: 10.0/7.1
- GPU model and memory: GTX 960M/4GB
- CPU: Intel i7 6700HQ

**Describe the current behavior**
I recently had the requirement to code a slightly different version of the convolution layer. The first choice for me was to build a custom layer using your article [here](https://www.tensorflow.org/tutorials/eager/custom_layers). I coded a naive version of 2D Convolution using a O(n^4) code just for basic prototyping. I used a lot of low-level operations like tf.ones, tf.concat, tf.tile, tf.reduce_sum and matrix slicing. To measure inference time I decided to call the object manually by supplying a (256, 256, 3) image as input to the overridden call() method. However, before calling I  included a tf.enable_eager_execution() call at the starting of the code by mistake. The code takes 50 seconds to execute the code on GPU(I monitored the GPU usage so I know it is using the GPU indeed). The peculiar part is that when I omit the tf.enable_eager_execution() it seems to run on the CPU rather inefficiently, in about 980 seconds. To have a yardstick for CPU execution, I re-wrote the same O(n^4) code using numpy exclusively and it takes just 210 seconds.

I also included the custom layer as part of a tf.keras.model and simply printed the model summary. The issue in this scenario is that it seems to execute the entire call routine to print the summary which essentially means that it takes 980 seconds to just print the summary!

**Describe the expected behavior**
There are a couple of questions I have:

1. While I realize that numpy and tensorflow as two completely different libraries, should the disparity in performance be this much? I am overriding the __init__, build and call functions while making the appropriate super calls as per your article. Am I doing something wrong?
2. Do custom layers use the CPU by default? I also tried forcing it by including a with tf.device(""/gpu:0"") call but that doesn't seem to help at all.

I realize that it might be more efficient to code the convolution by adding a custom op but I want to avoid that in the prototyping stage as much as possible.

**Code to reproduce the issue**
I cannot reveal the code for proprietary reasons but I could furnish a similar version if need be.

Any help in this regard would be much appreciated. 
"
28464, Where is the entry point for Receiver to receive Tensor using GRPC?,"I'm a student trying to understand how distributed TensorFlow transmits Tensor through GRPC. I have now found that the Sender is issuing a generic asynchronous request by calling the `IssueRequest` tool function(as listed below), but how does the Receiver respond to this request? Where is the entry point?

>   // file path: \tensorflow-r1.12\tensorflow\core\distributed_runtime\rpc\grpc_remote_worker.cc
  // Utility method for issuing a generic asynchronous request. The
  // given callback, `done`, will be called when the RPC completes.
  void IssueRequest(const protobuf::Message* request, TensorResponse* response,
                    const ::grpc::string& method, StatusCallback done,
                    CallOptions* call_opts = nullptr) {
        new RPCState<TensorResponse>(&stub_, cq_, method, *request, response,
                                     std::move(done), call_opts);
  }

Thanks!
"
28463,Why does TF run ops in the build phase?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information** 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source 
- TensorFlow version:  tf-nightly-1.13.1
- Python version: python 3
- Installed using virtualenv? pip? conda?:  pip
- Bazel version (if compiling from source):  0.23.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7
- GPU model and memory: 1080ti



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Add logging to the code:

```cpp
...
std::clog << ""Terminated \n"";
ec_.Notify(true);
return false;
...
```

Bazel build output:

```bash
INFO: From Executing genrule //tensorflow/python:cudnn_rnn_ops_pygenrule:
Terminated  # my std::clog output
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
INFO: From Executing genrule //tensorflow/python:candidate_sampling_ops_pygenrule:
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
Terminated 
```

I use local eigen instead of the remote repo. I try to tracing / logging eigen library, like `WaitForWork`. 
I find that when I build TF from source, it seems like TF runs the code. Worse, it will segmentation fault / core dump if I add logging to some part code of eigen, like loop.
What is the design behind it? "
28462,Poor performance on debug version,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- Bazel version (if compiling from source): 0.19
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I build tensorflow from source code for debug with the options **""-c dbg""** 
I also remove the build options: **""-c opt --copt=-O3""**
With this debug version, I only get 1/10 performance comparing with the release version.
I don't know it's normal in your aspect?
Which one(add ""-c dbg"" or remove ""-c opt --copt=-O3"") may majorly cause this problem.

**Describe the expected behavior**
Debug version and release version has similar performance.

"
28460,adjust_hsv_nhwc doesn't process all items,"CUDA kernel  ""adjust_hsv_nhwc"" in [adjust_hsv_gpu.cu.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/adjust_hsv_gpu.cu.h) doesn't fully iterate through all the items in the input array when the number of threads spawned is larger than the total number of items. A simple fix would be to have each thread iterate through all the items inside a for-loop in order to check that all the items in the input array has been processed. I have implemented a fix for this kernel, and would like to send a pull request after consulting with a dev from TF. ( I can attach my test code if needed )"
28459,How to export a eager mode checkpoint for tfserving,"I have train a model by tf1.13 and eager mode, then save it to checkpoint.

now I need to export the model to pb file and deploy on tfserving ?

Is there any one help me?"
28458,map_fn() missing GPU implementation,"It appears that `map_fn()` does not have a GPU implementation for integer tensors, which seems to be causing performance issues for some networks I'm designing (e.g. excessive data copying).

This may be related to #5965, but `map_fn()` isn't explicitly mentioned there.

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04 & Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Ubuntu apt-get repo, WinPython
- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4.1.5
- GPU model and memory: nVidia K80 & nVidia GTX 1050 Ti

**Describe the current behavior**
`tf.map_fn()` is located on the CPU when calling with integer tensors. Forcing GPU location results in an error message.

**Describe the expected behavior**
`tf.map_fn()` should have a GPU implementation to avoid excessive data copying.

**Code to reproduce the issue**
```
import tensorflow as tf

initial_integers = tf.zeros((10,), dtype=tf.int32)
initial_floats = tf.zeros((10,), dtype=tf.float32)
                
with tf.Session(config=tf.ConfigProto(allow_soft_placement=False)) as sess:
    with tf.device('/device:GPU:0'):

        # This works on GPU:
        results1 = tf.map_fn(tf.square, initial_floats)
        output1 = sess.run(results1)

        # This fails to locate on GPU, and results in an error.
        results2 = tf.map_fn(tf.square, initial_integers)
        output2 = sess.run(results2)
```"
28457,Tensor flow 2.0 and OpenCV usage,"Hi,
       I have already opened this issue in [Stack Overflow ](https://stackoverflow.com/questions/55986982/what-is-the-way-to-use-tensor-flow-2-0-object-in-open-cv2-python-and-why-is-it-s)but without much notice, I decided to add it here too. My question is related to usage of OpenCv operations like for instance in a train_step lets say I want to use an opencv function of remap - say undistort the output from a training model, then how do I do this in tensorflow ? Is this impossible as tensor flow object resides on GPU and calling an open cv operation requires a CPU copy and then a GPU push ? Is there already exists a provision through py_function  (I tried it) or something fancier ? Please advice.  "
28449,Typo in TensorFlow For Poets,"## Existing URLs containing the issue:
https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3

## Description of issue (what needs changing):
Typo

### Clear Description
```
MobileNet is a a small efficient convolutional neural network.
```
should be
```
MobileNet is a small efficient convolutional neural network.
```

### Submit PR?
I couldn't find the source for this codelab to fix the error. Is this tutorial obsolete or has it been updated for TensorFlow 2.0?

The issue has also been reported [here](https://github.com/googlecodelabs/tensorflow-for-poets-2/issues/67)."
28446,Return better error message when using RefVariables with v2 control flow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):v1.12.0-12395-g32b84a4 1.14.1-dev20190412  (this is the nightly at 20190412).
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

The following code:
```python
import os
os.environ['TF_ENABLE_COND_V2'] = '1'
import tensorflow as tf

a = tf.get_variable(""x"", dtype=tf.float32, shape=[3])
loss = tf.reduce_sum(a)
opt = tf.train.GradientDescentOptimizer(0.1)
def f():
    train_op = opt.minimize(loss)
    return train_op
op = tf.cond(tf.greater(a[0], 0), f, tf.no_op)
print(op)
```
throws:
```
WARNING: Logging before flag parsing goes to stderr.
W0506 12:29:26.078263 140364116391744 deprecation.py:506] From /home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Tensor(""x/Initializer/random_uniform/max:0"", shape=(), dtype=float32) Tensor(""x/Initializer/random_uniform/min:0"", shape=(), dtype=float32)
Tensor(""x/Initializer/random_uniform/RandomUniform:0"", shape=(3,), dtype=float32) Tensor(""x/Initializer/random_uniform/sub:0"", shape=(), dtype=float32)
Traceback (most recent call last):
  File ""a.py"", line 20, in <module>
    op = tf.cond(tf.greater(a[0], 0), f, tf.no_op)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1921, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py"", line 74, in cond_v2
    op_return_value=pred)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 710, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""a.py"", line 16, in f
    train_op = opt.minimize(loss)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 412, in minimize
    name=name)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 594, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 118, in update_op
    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/training/gradient_descent.py"", line 60, in _apply_dense
    use_locking=self._use_locking).op
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/training/gen_training_ops.py"", line 639, in apply_gradient_descent
    use_locking=use_locking, name=name)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 461, in create_op
    compute_device=compute_device)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3579, in create_op
    op_def=op_def)
  File ""/home/XXX/environments/tf1-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1969, in __init__
    input_types))
TypeError: In op 'GradientDescent/update_x/ApplyGradientDescent', input types ([tf.float32, tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32, tf.float32])
```

But it works without COND_V2. Similar to #24517."
28445,Load Images,"There is some problem in loading images 

for n in range(3):
  image_path = random.choice(all_image_paths)
  display.display(display.Image(image_path))
  print(caption_image(image_path))
  print()

Error:
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-9-dd286b0cbeda> in <module>
      2   image_path = random.choice(all_image_paths)
      3   display.display(display.Image(image_path))
----> 4   print(caption_image(image_path))
      5   print()

<ipython-input-8-a5b6e935786e> in caption_image(image_path)
      3 def caption_image(image_path):
      4     image_rel = pathlib.Path(image_path).relative_to(data_root)
----> 5     return ""Image (CC BY 2.0) "" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])
      6 

KeyError: 'daisy\\3764116502_f394428ee0_n.jpg'

Please what is the problem?


<em>Thanks so much for taking the time to file a documentation issue and even
more thanks if you intend to contribute to updating it! Please do introduce
yourself on our mailing list with
[Google Groups](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs)
or [email](mailto:docs@tensorflow.org) and let us know if you have any
questions. We also encourage you to review our
[Documentation Contributor Guide](https://www.tensorflow.org/community/contribute/docs).
As a side note, per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:doc_template</em>

## Existing URLs containing the issue:

Link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Correct Links

Is the link pointing to the source code correct? To find the source code, use
`git grep my_method` from the git command line in your locally checked out
repository.

### Clear Description

Why should someone use this method? How is it useful?

### Usage Example

Is there a usage example?

### Parameters Defined

Are all arguments that can be passed in defined and formatted correctly?

### Returns Defined

Are return values defined?

### Raises Listed and Defined

Are errors defined?
[Example](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises)

### Request Visuals, if Applicable

Are there currently visuals? If not, would they make the content clearer?

### Submit PR?

Are you planning to also submit a
[Pull Request](https://help.github.com/en/articles/about-pull-requests) to fix
this issue? See the
[Documentation Contributor Guide](https://www.tensorflow.org/community/contribute/docs)
the
[Documentation Style Guide](https://www.tensorflow.org/community/contribute/docs_style).
"
28444,tf.signal.inverse_stft AttributeError: 'int' object has no attribute 'value' in 2.0.0-alpha0,"There appears to be a bug in tf.signal.inverse_stft, when testing for `real_frames.shape[-1].value is None`, where `real_frames.shape[-1]` is an integer, and does not have a `value`. 

Reproducible code:
```
print(tf.__version__)
frame_length = 512
frame_step = 256
signal = tf.random.uniform(shape=(1000,))
x = tf.signal.stft(signal, frame_length, frame_step)
y = tf.signal.inverse_stft(x, frame_length, frame_step)
```

```
2.0.0-alpha0
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-261-d4526d62007f> in <module>
----> 1 tf.signal.inverse_stft(x, frame_length, frame_step)

/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/ops/signal/spectral_ops.py in inverse_stft(stfts, frame_length, frame_step, fft_length, window_fn, name)
    242     if (frame_length_static is None or
    243         real_frames.shape.ndims is None or
--> 244         real_frames.shape[-1].value is None):
    245       real_frames = real_frames[..., :frame_length]
    246       real_frames_rank = array_ops.rank(real_frames)

AttributeError: 'int' object has no attribute 'value'
```"
28443,2nd python can't run tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):tensorflow.org
- TensorFlow version:1.13.1
- Python version:3.5
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:1070

**Describe the problem**
While import tensorflow as tf in jupyter notebook. It's my second python in my system. The first one can run tf ok.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`import tensorflow as tf`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\RQPro\rqalpha\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

C:\RQPro\rqalpha\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-41389fad42b5> in <module>()
----> 1 import tensorflow as tf

C:\RQPro\rqalpha\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\RQPro\rqalpha\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\RQPro\rqalpha\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\RQPro\rqalpha\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


​

​`
"
28442,expected conv2d_input to have 4 dimensions,"Hello

I'm on tensorflow and i'm trying to Machine Learning but i got this error here:
ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (24946, 50, 50)

```
Image_Size is: 50x50

import tensorflow as tf
import numpy as np
import pickle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D


pickle_ind = open(""x.pickle"", ""rb"")
x = pickle.load(pickle_ind)
x = np.array(x, dtype=float)
# x = x/255.0

pickle_ind = open(""y.pickle"", ""rb"")
y = pickle.load(pickle_ind)

n_batch = len(x)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(x, y, epochs=20, batch_size=n_batch)
```"
28441,Shape errors occur after compiling keras model with tf.keras.losses.CategoricalCrossentropy,"**System information**
- OS Platform and Distribution:  Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 3.6.3

**Describe the current behavior**
If I want to specify, for example, a reduction method of the loss function, I will need to explicitly create an instance of `tf.keras.losses.CategoricalCrossentropy` and pass it to `model.compile()` instead of passing the `""categorical_crossentropy""` keyword. However, compiling a model with an instance of `tf.keras.losses.CategoricalCrossentropy` results in the following shape error when calling `model.fit()` afterwards:
```
InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [1000,10] and labels shape [10000]
	 [[{{node loss_18/dense_37_loss/CategoricalCrossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]


```
**Describe the expected behavior**
I was expecting similar behavior for both `""categorical_crossentropy""` and `tf.keras.losses.CategoricalCrossentropy`.

**Code to reproduce the issue**
A short MNIST example. I realized that removing the `to_categorical` transformation resolves the shape errors but the model does not learn properly anymore.
```
from tensorflow.python.keras.datasets import mnist
from tensorflow.python.keras.utils import to_categorical

import tensorflow as tf
import numpy as np

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train, X_test = X_train.astype(float)/255, X_test.astype(float)/255
X_train, X_test = X_train.reshape(len(X_train),28,28,1), X_test.reshape(len(X_test),28,28,1)
y_train, y_test = to_categorical(y_train), to_categorical(y_test)

# model definition
model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(16, 8,
                             strides=2,
                             padding='same',
                             activation='relu',
                             input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPool2D(2, 1),
      tf.keras.layers.Conv2D(32, 4,
                             strides=2,
                             padding='valid',
                             activation='relu'),
      tf.keras.layers.MaxPool2D(2, 1),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(10, activation=""softmax"")
  ])

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)

# this causes errors
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)

# using the keyword, everything works
# loss = ""categorical_crossentropy"" 

# Compile model with Keras
model.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy'])

# Train model with Keras
model.fit(X_train, y_train, epochs=5, batch_size=1000,
          validation_data=(X_test, y_test), verbose=2)

```
Thank you!"
28440,Improper output shape of DenseFeatures layer with numeric_column ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
- TensorFlow installed from (source or binary): from pip I think, I don't remember and I don't know the difference
- TensorFlow version: v1.12.0-10066-g5cbe8af8ed 2.0.0-dev20190313
- Python version: 3.6.7

**Describe the problem**

I am using the `DenseFeatures` layer feeded with a feature `numeric_column` with a shape argument of `shape=(3,1)`. According to [the documentation of `numeric_column`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/numeric_column?hl=en), the output tensor has shape [batch_size] + `shape`. My batch size is 2 and instead of having shape `(2,3,1)` as indicated by the documentation, my output tensor only has output shape of `(2,3)`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Here is a minimal working example reproduccing the issue:

```
from tensorflow.data import Dataset
from tensorflow.feature_column import numeric_column
from tensorflow.keras.layers import DenseFeatures

raw_dataset = {'feature1': [[3., 4., 6.], [2., 12., 7.]]}
dataset = Dataset.from_tensor_slices(raw_dataset)
batch_size = 2
batched = dataset.batch(batch_size)
feature_column = numeric_column('feature1', shape=(3,1))
layer = DenseFeatures(feature_column)
for y in batched:
    pass
layer(y)
```

the output of which is

```
<tf.Tensor: id=19, shape=(2, 3), dtype=float32, numpy=
array([[ 3.,  4.,  6.],
       [ 2., 12.,  7.]], dtype=float32)>
```"
28439,Tensorflow 1.12.2 a typo for 1.13.2?,"I note there was 1.12.0, but no 1.12.1.

https://github.com/tensorflow/tensorflow/releases has 1.12.2."
28438,TensorFlow Lite conversion of frozen graph error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2f9cc84ba3f5d59753d843f167adee2e2534c143


**Provide the text output from tflite_convert**
While converting [faster_rcnn_nas](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_coco_2018_01_28.tar.gz) from tensoflow trained model to tensorflow lite found some error.
```
Unsupported Control Flow Ops:
1. Enter
2. Exit
3. Merge
4. Switch
5. LoopCond

Unsupported Lite Ops:
1. NonMaxSuppressionV2
2. TensorArrayGatherV3
3. TensorArrayReadV3
4. TensorArrayScatterV3
5. TensorArraySizeV3
6. TensorArrayV3
7. TensorArrayWriteV3

Unsupported Quantize Ops:
1. Div
2. Exp
3. Cast
4. Fill
5. Range
6. Size
7. Tile
8. ZerosLike
9. Unpack 

```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[faster_rcnn_nas_lite_error.txt](https://github.com/tensorflow/tensorflow/files/3148187/faster_rcnn_nas_lite_error.txt)
"
28437,How to enable INTEL_MKL_ML?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):  source
- TensorFlow version: 1.10
- Python version: 2.7.13
- Installed using virtualenv? pip? conda?:  none
- GCC/Compiler version (if compiling from source): 5.4
- Intel CPU



**Describe the problem**
I want build tensorflow with mkl that use the command below:
bazel build tensorflow/tools/benchmark:benchmark_model --verbose_failures -c opt --copt=-march=native --config=mkl --copt=""-mfpmath=both"" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2
but when I gdb with file tensorflow/core/kernels/mkl_conv_ops.cc, I find INTEL_MKL_ML is disabled and use the #else branch code.
you can read the file tensorflow/core/kernels/mkl_conv_ops.cc that
#ifdef INTEL_MKL_ML
class MklConv2DOp : public OpKernel {
  Path1
}
#else 
class MklConv2DOp : public OpKernel {
  Path2
}
#endif
WHY tensorflow execute Path2 not Path1? what the difference between these two paths?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28436,libcudart.so.7.5: cannot open shared object file: No such file or directory,"Hello,

I try to install tensorflow-gpu on my ubuntu machine.

i got this error here:
ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory

some one can help me ?

inside my ~/.bashrc
export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64""
export CUDA_HOME=/usr/local/cuda

Installed:
Cuda 10.0
cuDNN 7.5.0"
28435,TypeError: unhashable type: 'memmap',"When I want to use `tf.as_dtype` to convert a `numpy.memmap` data , the error was given. Is tensorflow not support this dtype? Or I shoulde use other function to convert my data to a type that tensorflow can recognize?"
28434,NPM Install failing,"I do hope this is the right place for this; there's no `issues` enabled on tensorflow/tfjs-node...

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   - Windows 10

**Describe the problem**


downloading and unzipping is failing; apparently the unzip utility is running out of memory.  Should add an additional argument 
installing with node 11.9.0  x86 version.  Produces ths output at the end.

```
M:\javascript\tensorflow>npm install @tensorflow/tfjs-node-gpu

> @tensorflow/tfjs-node-gpu@1.1.2 install M:\javascript\tensorflow\node_modules\@tensorflow\tfjs-node-gpu
> node scripts/install.js gpu download

* Downloading libtensorflow
[==============================] 21249429/bps 100% 0.0s
(node:17992) UnhandledPromiseRejectionWarning: RangeError: Array buffer allocation failed
    at new ArrayBuffer (<anonymous>)
    at new Uint8Array (<anonymous>)
    at new FastBuffer (internal/buffer.js:788:1)
    at Function.alloc (buffer.js:280:10)
    at decompress (M:\javascript\tensorflow\node_modules\adm-zip\zipEntry.js:56:27)
    at Object.getData (M:\javascript\tensorflow\node_modules\adm-zip\zipEntry.js:242:12)
    at M:\javascript\tensorflow\node_modules\adm-zip\adm-zip.js:438:25
    at Array.forEach (<anonymous>)
    at Object.extractAllTo (M:\javascript\tensorflow\node_modules\adm-zip\adm-zip.js:432:17)
    at WriteStream.response.on.pipe.on (M:\javascript\tensorflow\node_modules\@tensorflow\tfjs-node-gpu\scripts\resources.js:70:21)
(node:17992) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by
ing a promise which was not handled with .catch(). (rejection id: 1)
(node:17992) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js proc
th a non-zero exit code.
npm WARN rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.
npm WARN tensorflow@1.0.0 No description
npm WARN tensorflow@1.0.0 No repository field.

+ @tensorflow/tfjs-node-gpu@1.1.2
removed 649 packages, updated 2 packages and audited 62 packages in 57.866s
found 0 vulnerabilities

```



As I was writing this, decided to try x64 version; and it got past that point.
(node-gyp couldn't find msbuild.exe; but that's a different issue)

When I fix those issues... I still get

```
290 verbose stack Error: @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`
290 verbose stack Exit status 1
290 verbose stack     at EventEmitter.<anonymous> (C:\Users\Panther\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\index.js:301:16)
290 verbose stack     at EventEmitter.emit (events.js:197:13)
290 verbose stack     at ChildProcess.<anonymous> (C:\Users\Panther\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\lib\spawn.js:55:14)
290 verbose stack     at ChildProcess.emit (events.js:197:13)
290 verbose stack     at maybeClose (internal/child_process.js:978:16)
290 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)
291 verbose pkgid @tensorflow/tfjs-node-gpu@1.1.2
292 verbose cwd M:\javascript\tensorflow
293 verbose Windows_NT 10.0.17134
294 verbose argv ""h:\\dev2\\nodejs\\node.exe"" ""C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js"" ""install"" ""@tensorflow/tfjs-node-gpu""
```

Redirecting output to a log seems to make the download fail too...  So THIS is what I get now; and don't understand what's missing.

```

* Downloading libtensorflow
[==============================] 22424948/bps 100% 0.0s
[==============================] 2118839/bps 100% 0.0s
* Building TensorFlow Node.js bindings
M:\javascript\tensorflow\node_modules\@tensorflow\tfjs-node-gpu\scripts\install.js:165
      throw new Error('node-gyp rebuild failed with: ' + err);
      ^

Error: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuild
gyp ERR! build error
gyp ERR! stack Error: `msbuild` failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\Panther\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\build.js:262:23)
gyp ERR! stack     at ChildProcess.emit (events.js:197:13)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)
gyp ERR! System Windows_NT 10.0.17134
gyp ERR! command ""h:\\dev2\\nodejs\\node.exe"" ""C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""rebuild""
gyp ERR! cwd M:\javascript\tensorflow\node_modules\@tensorflow\tfjs-node-gpu
gyp ERR! node -v v11.9.0
gyp ERR! node-gyp -v v3.8.0
gyp ERR! not ok

    at cp.exec (M:\javascript\tensorflow\node_modules\@tensorflow\tfjs-node-gpu\scripts\install.js:165:13)
    at ChildProcess.exithandler (child_process.js:304:5)
    at ChildProcess.emit (events.js:197:13)
    at maybeClose (internal/child_process.js:978:16)
    at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)
npm WARN rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.
npm WARN tensorflow@1.0.0 No description
npm WARN tensorflow@1.0.0 No repository field.

npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the @tensorflow/tfjs-node-gpu@1.1.2 install script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.

npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\Panther\AppData\Roaming\npm-cache\_logs\2019-05-06T10_57_50_660Z-debug.log
```

(and the mentioned debug.log)

```
279 warn rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.
280 warn tensorflow@1.0.0 No description
281 warn tensorflow@1.0.0 No repository field.
282 verbose stack Error: @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`
282 verbose stack Exit status 1
282 verbose stack     at EventEmitter.<anonymous> (C:\Users\Panther\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\index.js:301:16)
282 verbose stack     at EventEmitter.emit (events.js:197:13)
282 verbose stack     at ChildProcess.<anonymous> (C:\Users\Panther\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\lib\spawn.js:55:14)
282 verbose stack     at ChildProcess.emit (events.js:197:13)
282 verbose stack     at maybeClose (internal/child_process.js:978:16)
282 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)
283 verbose pkgid @tensorflow/tfjs-node-gpu@1.1.2
284 verbose cwd M:\javascript\tensorflow
285 verbose Windows_NT 10.0.17134
286 verbose argv ""h:\\dev2\\nodejs\\node.exe"" ""C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js"" ""install"" ""@tensorflow/tfjs-node-gpu""
287 verbose node v11.9.0
288 verbose npm  v6.4.1
289 error code ELIFECYCLE
290 error errno 1
291 error @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`
291 error Exit status 1
292 error Failed at the @tensorflow/tfjs-node-gpu@1.1.2 install script.
292 error This is probably not a problem with npm. There is likely additional logging output above.
293 verbose exit [ 1, true ]
```


"
28432,Build failure while installing TensorFlow from source following TF documentation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 
- TensorFlow installed from (source or binary): Source 
- TensorFlow version: Tensorflow 2.0 
- Python version: Python 3.7
- Installed using virtualenv? pip? conda?: Created a Conda virtual environment 
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc version 6.5.0 
- CUDA/cuDNN version: CUDA 9.0/ cuDNN 7.4.2
- GPU model and memory: Geoforce GTX 1050ti



**Describe the problem**

**Provide the exact sequence of commands/steps that you executed before running into the problem**

I followed the steps listed here for building from source (https://www.tensorflow.org/install/source). I keep getting stuck at the step wherein I invoke ""bazel build"" which is the penultimate step in the installation process. Now, these errors aren't consistent. I have run the command ""bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures"" multiple times and each time the build fails with a different error. However this is the error I got for the last two times I tried the installation process


ERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: output 'tensorflow/contrib/resampler/_objs/python/ops/_resampler_ops_gpu/resampler_ops_gpu.cu.pic.o' was not created
ERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 234.229s, Critical Path: 40.57s
INFO: 648 processes: 648 local.
FAILED: Build did NOT complete successfully

I copy pasted the entire stack of messages printed out to the console here https://pastebin.com/kJFxdHgA"
28431,sparse_image_warp don't accept the tensor with dynamic shape.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04)
- TensorFlow installed from (binary)
- TensorFlow version (1.13.1)
- Python version

**Describe the current behavior**
Traceback (most recent call last):
  File ""<stdin>"", line 6, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py"", line 179, in sparse_image_warp
    grid_locations = _get_grid_locations(image_height, image_width)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py"", line 34, in _get_grid_locations
    y_range = np.linspace(0, image_height - 1, image_height)
TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'

**Describe the expected behavior**
No error: should return the warped image

**Code to reproduce the issue**
image_placeholder = tf.placeholder(dtype=tf.float32, shape=[1, None, None, 1])
src_pts = tf.constant([10, 10])
dest_pts = tf.constant([100, 100])
tf.contrib.image.sparse_image_warp(image_placeholder,
                                                        source_control_point_locations = src_pts,
                                                         dest_control_point_locations = dest_pts,
                                                         interpolation_order = 2,
                                                         regularization_weight = 0,
                                                         num_boundary_points = 1
                                                         ) 
**Other info / logs**
None.
"
28430,TF2.0 MirroredStrategy Example failed on windows if GPU > 1,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, Using the sample code in tensorflow docs
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0 
- Python version: 3.7
- CUDA/cuDNN version: cuda10/cudnn7.5
- GPU model and memory: 2*1080ti

**Describe the current behavior**
```
import tensorflow_datasets as tfds
import tensorflow as tf

import os
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets['train'], datasets['test']
strategy = tf.distribute.MirroredStrategy()
print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))
# You can also do info.splits.total_num_examples to get the total
# number of examples in the dataset.

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255

  return image, label

# train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
train_dataset = mnist_train.repeat(2).map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

with strategy.scope():
    model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(loss='sparse_categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])
model.fit(train_dataset, epochs=10)

```
The code above is what i copy from [tf docs](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb)
Which will result an Error like:
```
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=""sum"", T=DT_FLOAT, num_devices=1, shared_name=""c0""]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>
	 [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_1495]
```
At first I thought I had a wrong build of tensorflow without GPU support. However, when i re-installed the whole env exactly as the guide in the official site. Still the same result. And I found that if I uncomment the mirror strategy part, the code is working good
```
# with strategy.scope():
model = tf.keras.Sequential([
  tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy',
            optimizer=tf.keras.optimizers.Adam(),
            metrics=['accuracy'])
model.fit(train_dataset, epochs=10)
```
And another found is that I myself only have 2 gpus however, when tensorflow init, it prints ""adding devices"" 3 times which i thought should be 2:
```
2019-05-06 16:38:55.635182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1
2019-05-06 16:39:01.691770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-06 16:39:01.694145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 
2019-05-06 16:39:01.694243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N 
2019-05-06 16:39:01.704826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N 
2019-05-06 16:39:01.705405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-05-06 16:39:01.706830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-05-06 16:39:02.008523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1
2019-05-06 16:39:02.008834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-06 16:39:02.009000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 
2019-05-06 16:39:02.009103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N 
2019-05-06 16:39:02.009207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N 
2019-05-06 16:39:02.009398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-05-06 16:39:02.021524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-05-06 16:39:04.813463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1
2019-05-06 16:39:04.813706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-06 16:39:04.813870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 
2019-05-06 16:39:04.813975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N 
2019-05-06 16:39:04.814077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N 
2019-05-06 16:39:04.814274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-05-06 16:39:04.815618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
```

**Describe the expected behavior**
The mirror strategy should work well on machine that have more than one GPU otherwise it is useless right? I found other related issues like: https://github.com/tensorflow/tensorflow/issues/28372 and https://github.com/tensorflow/tensorflow/issues/28334
They all reported that the scripts can run for 1 gpu and failed on gpu>=2
Any thought is appreciated!
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""D:/ziyi/EastAI2/models/test.py"", line 82, in <module>
    model.fit(train_data, epochs=2, steps_per_epoch=100)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 746, in fit
    validation_freq=validation_freq)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_distributed.py"", line 131, in fit_distributed
    steps_name='steps_per_epoch')
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 263, in model_iteration
    batch_outs = f(actual_inputs)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\keras\backend.py"", line 3217, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 558, in __call__
    return self._call_flat(args)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 627, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 415, in call
    ctx=ctx)
  File ""C:\Users\jerry\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=""sum"", T=DT_FLOAT, num_devices=2, shared_name=""c0""]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_2200]

```
"
28429,"Profile tab in tensorboard keeps saying ""Processing datasets"" ","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1, 7.5
- GPU model and memory: 3x GTX 1080Ti, 11Gb

**Describe the current behavior**
I was training a tf.keras and had a tensorboard callback, but i don't seem to get the new profiling feature to work.



**Code to reproduce the issue**
```
train_dataset = tf.data.Dataset.from_tensor_slices(
    (train_images, train_labels))
train_dataset = train_dataset.shuffle(buffer_size=256)
train_dataset = train_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data,
                                                                       batch_size=batch_size,
                                                                       num_parallel_calls=tf.data.experimental.AUTOTUNE,
                                                                       drop_remainder=True))
train_dataset = train_dataset.repeat()
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

val_dataset = tf.data.Dataset.from_tensor_slices(
    (val_images, val_labels))
val_dataset = val_dataset.shuffle(buffer_size=256)
val_dataset = val_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data,
                                                                   batch_size=batch_size,
                                                                   num_parallel_calls=tf.data.experimental.AUTOTUNE,
                                                                   drop_remainder=True))
val_dataset = train_dataset.repeat()
val_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

callbacks = [tf.keras.callbacks.TensorBoard(update_freq='batch'),
             tf.keras.callbacks.ModelCheckpoint('model/weights.h5', 
                                                save_best_only=True, 
                                                save_weights_only=True)]
model.fit(train_dataset,
          steps_per_epoch=len(train_images) // batch_size,
          epochs=300,
          validation_data=val_dataset,
          validation_steps=len(val_images) // batch_size,
          callbacks=callbacks)
```
**Other info / logs**
There are the logs from tensorboard
```
I0506 13:36:28.764028 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] ""GET /data/plugin/profile/tools HTTP/1.1"" 200 -
I0506 13:36:28.786284 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] ""GET /data/plugin/profile/hosts?run=2019-05-05_18-45-33&tag=trace_viewer HTTP/1.1"" 200 -
I0506 13:36:28.829900 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] ""GET /data/plugin/profile/hosts?run=2019-05-05_18-45-33&tag=trace_viewer HTTP/1.1"" 200 -
```
"
28428,Reset sequence of samples generate by tf.random.uniform without restarting the session,"As of the rules I have raised this issue on StackOverflow but it's dying a slow death over there, therefore I ask it here: https://stackoverflow.com/questions/55951644/reset-sequence-of-samples-generate-by-tf-random-uniform-without-restarting-the-s

I can set the seed of the random.uniform operation, but this seed is only used as ""starting-point"" of the session. I cannot instruct the tensor to ""re-initialize"" from the set seed.

I want to train my model on random samples generated by random.uniform and every batch I want the same samples to be used, without having to store these samples.

I could create one big array to write these uniform samples into and use that to train upon, but using random.uniform tensor with a ""reset"" of the seed every batch seems more elegant and more memory friendly. In some use cases I have to create millions and millions of random uniform samples. Using tf.data with a random.uniform tensor should be way more elegant."
28427,FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_93/bias,"I run my code, but I don't know what is the below error. Please help me to fix this
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_93/bias 	 [[node dense_93/bias/read (defined at C:/Users/TRAN THI DIEM/Documents/diem/research/ReinforcementLearning/code/Depression/Code detecting depression using multi layer with survey data_ da chay duoc/depression_test_reinforcement.py:126) ]]


import tensorflow as tf
import pandas as pd
import numpy as np

from sklearn.utils import shuffle

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
#train set
df_train = pd.read_csv('data/train.csv')
# test set
df_test = pd.read_csv('data/test.csv')

#show the shape of the train dataframe
df_train.shape
def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()  # sum the number of each colum that has no value
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df) #len(df); return number of row in df_train
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print (""The dataset has "" + str(df.shape[1]) + "" columns.\n""      
            ""There are "" + str(mis_val_table_ren_columns.shape[0]) +
              "" columns that have missing values."")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns
    # Get the columns with > 50% missing
missing_df = missing_values_table(df_train);
missing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)
print('\n','%d columns will be deleted.' % len(missing_columns))

# Drop the columns with 50% missing data
df_train = df_train.drop(columns = list(missing_columns)) # delete some column have miss information more than 50%
# shape for df_train now is (1143,71)compare with the first (1143,75)

# Create a label (category) encoder object
#ncoder = LabelEncoder()
encoder = preprocessing.LabelEncoder()

# fitting the encoder to the ""survey_date"" column
encoder.fit(df_train['survey_date'])

# Apply the fitted encoder to the ""survey_date"" to transform categories into integers
encoded_train = encoder.transform(df_train['survey_date'])
# encoded_test = encoder.transform(df_test['survey_date'])

#assign the tranformed column back to the dataframe
df_train['survey_date'] = encoded_train
# split data into train and test sets
X = df_train.drop([""depressed""], axis=1)

# fill missing values with mean column values

imputer = preprocessing.Imputer()
transformed_X = imputer.fit_transform(X)

y_temp = df_train.depressed
y= df_train['depressed']
y_matrix = y.reset_index().values
y =y_matrix[:,1]

seed = 5
test_size = 0.33

X_train, X_test, y_train, y_test =train_test_split(transformed_X, y, test_size=test_size, random_state=seed)

input_size = [70]
class DQNetwork:
    def __init__(self):
#        self.state_size = state_size
 #       self.action_size = action_size
    
        self.learning_rate = tf.placeholder(tf.float32)
        #self.weight_decay = tf.constant(1e-4)

#        self.action = tf.placeholder(tf.uint16, [None,3])  
        # We create the placeholders
        # *state_size means that we take each elements of state_size in tuple hence is like if we wrote
        
        # [None, 84, 84, 4]
        self.inputs_ = tf.placeholder(tf.float64, [None,*input_size])
        
#        self.actions_ = tf.placeholder(tf.float32, [None, 3])

        # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')
        self.label = tf.placeholder(tf.int64,[None])    
#fully connected        
        
           

       # self.flatten = tf.layers.flatten(self.conv6_Dropout)
        ## --> [43,264]

        self.output_layer1 = tf.layers.dense(inputs=self.inputs_,
                                     #    bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),
                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),
                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      units=36,
                                     activation=tf.nn.elu)
        self.output_layer2 = tf.layers.dense(inputs=self.output_layer1,
                                      #       bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),
                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),
                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      units=36,
                                    activation=tf.nn.elu)
        self.output = tf.layers.dense(inputs=self.output_layer2,
                                     #bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),
                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),
                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),
                                      units=1,
                                     activation=tf.nn.sigmoid)
        
        self.targets1 = tf.squeeze(tf.cast(self.label, tf.int32))   # Get predicted values by finding which logit is the greatest
        self.targets = tf.cast(tf.argmax(self.targets1, 1), tf.int32)        
        self.batch_predictions = tf.cast(tf.argmax(self.output, 1), tf.int32)
        self.predicted_correctly = tf.equal(self.batch_predictions, self.targets)
    # Average the 1's and 0's (True's and False's) across the batch size
        self.accuracy = tf.reduce_mean(tf.cast(self.predicted_correctly, tf.float32))
        
        
        
        self.loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits= self.output, labels=self.label) )
        #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits = self.output,label = self.label)
       # self.loss = tf.reduce_mean(cross_entropy, name='cross_entropy')
        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)
        #self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)
# create model
#model = Sequential()
#model.add(Dense(36, input_dim=70, activation='elu'))
#model.add(Dense(36, activation='elu'))
#model.add(Dense(1, activation='sigmoid'))
## Compile model
#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
## Fit the model
#model.fit(X_train, y_train, epochs=100, batch_size=10)
#scores = model.evaluate(X_test, y_test)
#print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
learning_rate = 0.0001
sess = tf.Session()
sess.run(tf.global_variables_initializer())
dqn = DQNetwork ()
_,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],
                                                  feed_dict={dqn.inputs_:X_train,dqn.label:y_train,dqn.learning_rate:learning_rate})"
28426,Makefile build fails on MacOS 10.13,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacPro 5.1 XCode 9.4 OSX 10.13
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.13.1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): XCode 9.4.1 clang
- CUDA/cuDNN version: CUDA 10, cuDNN 7.4
- GPU model and memory: GTX 1060 6Gb



**Describe the problem**
Typing make in the contrib Makefile locks up the machine and it becomes unreponsive

**Provide the exact sequence of commands / steps that you executed before running into the problem**
git clone tensorflow
git checkout v1.13.1
cd tensorflow/tensorflow/contrib/makefile
make

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Gets past absl compilation with 8 warnings, mouse stops moving machine is still pingable with port 22 open according to nmap but machine is not responsive.

Would be happy with a CPU only build for now to statically link against."
28425,An CNN model error: You must feed a value for placeholder tensor 'y' with dtype int32 and shape [?] ,"import tensorflow as tf
import os
from skimage import io, transform
import glob

import numpy as np


#dataset parameter
mode = 'folder'
train_dataset_path = '/Users/resun/ML_data/train_data_png/'   #change here
test_dataset_path = '/Users/resun/ML_data/test_data_png/'
tag_dataset_path = '/Users/resun/ML_data/tag_image/'


'''
ROOT_FOLDER
       |-------- SUBFOLDER (CLASS 0)
       |             |
       |             | ----- image1.jpg
       |             | ----- image2.jpg
       |             | ----- etc...
       |             
       |-------- SUBFOLDER (CLASS 1)
       |             |
       |             | ----- image1.jpg
       |             | ----- image2.jpg
       |             | ----- etc...
'''
# image parameters
num_classes = 2     #change here
image_height = 128    #change here
image_width = 128    #change here
channels = 3     #the 3 color channels, change to 1 if grayscale
'''
0: Use the number of channels in the PNG-encoded image.
1: output a grayscale image.
3: output an RGB image.
4: output an RGBA image.
'''


#########################################
########## reading the dataset ##########
#########################################
#mode = folder
'''

def read_images(dataset_path, mode, batch_size):
    imagepaths, labels = list(), list()
    if mode == 'folder':
        # an ID will be affected to each sub-folders by alphabetical order
        label = 0
        #list the directory
        try:   #python2     #two paths in the dataset_path
            classes = sorted(os.walk(dataset_path).next()[1])
            #walk: For each directory in the directory tree rooted at top, yields a 3-tuple (dirpath, dirnames, filenames)
        except Exception:  #python3
            classes = sorted(os.walk(dataset_path).__next__()[1])

        #list each sub-directory (the classes)
        for c in classes:    #img inside the folder
            c_dir = os.path.join(dataset_path, c)
            try:   #python2
                walk = os.walk(c_dir).next()
            except Exception:   #python3
                walk = os.walk(c_dir).__next__()

            #add each image to the training set
            for sample in walk[2]:    #walk[2] in sub-directory
                #only keeps jpeg images
                if sample.endswith('.png') or sample.endswith('.jpeg'):
                    imagepaths.append(os.path.join(c_dir, sample))
                    labels.append(label)
            label += 1
    else:
        raise Exception(""unknown mode"")

    #img data convert to a tensor
    imagepaths = tf.convert_to_tensor(imagepaths, dtype = tf.string)
    labels = tf.convert_to_tensor(labels, dtype=tf.int32)
    #build a tf queue, shuffle data
    image, label = tf.train.slice_input_producer([imagepaths, labels], shuffle=True)   #Produces a slice of each `Tensor` in `tensor_list
    #read images from disk
    image = tf.read_file(image)    #Reads and outputs the entire contents of the input filename
    image = tf.image.decode_png(image, channels=channels)    #Decode a PNG-encoded image to a uint8 or uint16 tensor
    #resize images from disk
    image = tf.image.resize_images(image, [image_height, image_width])
    #normalize
    image = image * 1.0/127.5 - 1.0

    #create batches
    X, Y = tf.train.batch([image, label], batch_size = batch_size, capacity = batch_size*8, num_threads = 4)
    return X, Y
'''


def read_img(path):
    cate = [train_dataset_path+x for x in os.listdir(train_dataset_path) if os.path.isdir(train_dataset_path+x)]
    imgs = []
    labels = []
    for idx, folder in enumerate(cate):
        for im in glob.glob(folder+'\*.png'):
            #print('reading the images: %s' %(im))
            img = io.imread(im)
            img = transform.resize(img, (image_width, image_height, channels))
            imgs.append(img)
            labels.append(idx)
    X=np.asarray(imgs, np.float32)
    Y=np.asarray(labels, np.int32)
    return X, Y

#######################################
############# classic CNN #############
#######################################

#parameters
learning_rate = 0.001
num_steps = 100
batch_size = 50
display_steps = 50
dropout = 0.75    #probability to keep units

X_train, Y_train = read_img(train_dataset_path)
#X_test, Y_test = read_images(test_dataset_path, mode, batch_size)
#X_tag, Y_tag = read_images(tag_dataset_path, mode, batch_size)
print(X_train)
print(Y_train)
print(type(X_train))
print(type(Y_train))


#create model
x = tf.placeholder(tf.float32, shape=[None, image_width, image_height, channels], name = 'x')
y = tf.placeholder(tf.int32, shape=[None, ], name = 'y')
def conv_net(input, n_classes, dropout, reuse, training):
    #define a scope for reusing the variables
    with tf.variable_scope('ConvNet', reuse=reuse):
        conv1 = tf.layers.conv2d(input, 32, 5, activation=tf.nn.relu)   #convolution layer with 32 filters and a kernel size of 5
        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)     #max pooling (down-sampling) with pool_size of 2 and strides(步长) of 2
        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)    #convolution layer with 64 filters and a kernel size of 3
        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)     #max pooling (down-sampling) with pool-size of 2 and strides of 2
        fc1 = tf.contrib.layers.flatten(conv2)    #flatten the data to a 1D vector for the fully connected layer
        fc1 = tf.layers.dense(fc1, 1024)     #fully connected layer (in contrib folder for now)
        fc1 = tf.layers.dropout(fc1, rate = dropout, training = training)    #apply dropout (if training is false, dropout is not applied)
        out = tf.layers.dense(fc1, n_classes)     #output layer, class prediction
        out = tf.nn.softmax(out) if not training else out
    return out

constant = tf.constant(value=1, dtype=tf.float32)

logits_train = conv_net(x, num_classes, dropout, reuse=False, training=True)    #create a graph for training
logits_eval = tf.multiply(logits_train, constant, name='logits_eval')
#logits_test = conv_net(X_test, num_classes, dropout, reuse=True, training=False)    #create another graph for testing that reuse the same weights
#logits_tag = conv_net(X_tag, 2, dropout, reuse=True, training=False)

loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)
#evulate model
correct_pred = tf.equal(tf.argmax(tf.nn.softmax(logits_train), 1), tf.cast(y, tf.int64))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
init = tf.global_variables_initializer()
#save object
saver = tf.train.Saver()   #The constructor adds ops to save and restore variables.


#########################################
############ start training #############
#########################################

with tf.Session() as sess:

    sess.run(init)


    #start data queue
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(coord=coord)

    for step in range(1, num_steps+1):
        if step % display_steps == 0 or step == 1:    #run optimization and calculate batch loss and accuracy
            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={x: X_train, y: Y_train})
            print(""Step "" + str(step) + "", Minibatch Loss= "" + ""{:.4f}"".format(loss) + "", Training Accuracy= "" + ""{:.3f}"".format(acc))
        else:
            sess.run(train_op)   #only run the optimization op (backprop)
    print(""Optimization Finished!"")
"
28424,Multiplication of two-dimensional and three-dimensional tensors,"There is a two dimensional tensor a[m,n], and a three dimensional tensor b[k,n,h]. What API should I use to multiply two dimensional tensors by three dimensional tensors to get a three dimensional tensor c[k,m,h] ?
Actually I can got it by:

import tensorflow as tf
import tensorly as tl
x=tf.constant([[[1,2],[3,7],[8,9]],
[[4,5],[6,10],[11,12]]],tf.float32)
a=tf.constant([[-0.70711,0.57735],
[0.0000,0.57735],
[0.70711,0.57735]])
reshape_A = tf.reshape(x, [2,6])

re = tf.reshape( tf.matmul(a, reshape_A), [3, 3, 2])

with tf.Session() as sess:
print(sess.run(re))
re=re.eval()

But is there an easier way?"
28423,tf.contrib.summary.create_file_writer 'name' parameter is not recognized,"While creating summary writer in eager execution, name parameter is not recognized properly, hence all the graphs are getting associated with single (default) name.
```
logdir = 'logs'
model_name = 'mnist-dense-128'
summary_writer = tf.contrib.summary.create_file_writer(logdir, flush_millis=10000,
name=model_name)
summary_writer.set_as_default()
```
Attaching [screenshot](https://drive.google.com/open?id=1bwOFOA631vtBvejoXGu9Xf6oVTXtqnVI) of Tensorboard for reference

Tensorflow version: 1.13"
28421,Why we cannot define two networks in the same graph when they are trained independently,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.4.1
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Why we cannot define two networks in the same graph when they are trained independently.

It can be done only using two different grpahs.

**Will this change the current api? How?**
No, just allow to define two networks in the same graph that have to be trained independently.

**Who will benefit with this feature?**

Developers and production code. Because it's time and memory consuming to create two graphs.

**Any Other info.**
"
28420,ImportError: DLL load failed: The specified module could not be found. (Windows),"System:

OS Name	Microsoft Windows 10 Pro
Version	10.0.17763 Build 17763

CUDA Version 10.0

NOT RUNNING A CONDA ENVIRONMENT

python version: 3.6.8

Installed via the documentation @ https://www.tensorflow.org/install/gpu

pip install tensorflow-gpu

Tried to test if tensorflow was loaded

python

>> import tensorflow as tf

Error (entire shell output):

Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
28419,Cannot import tensorflow on yarn,"When I tried to upload all tensorflow and google.protocol pkgs to yarn, I come across a problem like this

```
Traceback (most recent call last):
  File ""/data05/yarn/nmdata/usercache/tiger/appcache/application_1557054435006_18114/container_e117_1557054435006_18114_01_000009/./mapreduce_text.py"", line 34, in <module>
    from extract_features_tmp import predict
  File ""./bert/bin/extract_features_tmp.py"", line 15, in <module>
    import tensorflow as tf
  File ""./tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""./tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""./tensorflow/core/framework/graph_pb2.py"", line 8, in <module>
    from google.protobuf import reflection as _reflection
  File ""/data01/yarn/nmdata/usercache/tiger/filecache/88784/google.tar.gz/protobuf/reflection.py"", line 62, in <module>
    GeneratedProtocolMessageType = message_impl.GeneratedProtocolMessageType
AttributeError: 'module' object has no attribute 'GeneratedProtocolMessageType'
```
Why this happened? I used `tar -czvf` to compress tensorflow and protobuf, used -archives to upload these pkgs.tar.gz to yarn."
28418,"[2.0 alpha] tf.keras callbacks - Invalid argument ""callback"" passed to K.function with TensorFlow backend","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Manjaro Linux
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below):

- Python version:
- CUDA/cuDNN version: 2.0 alpha

**Describe the current behavior**

```ValueError: Invalid argument ""callback"" passed to K.function with TensorFlow backend```

**Describe the expected behavior**

- Run as normal like in previous version's of tf/keras.

**Code to reproduce the issue**
```python

model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=10))
model.add(tf.compat.v1.keras.layers.CuDNNLSTM(50))
model.add(tf.keras.layers.Dense(50, activation='relu'))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dropout(0.2))


model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

es = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=10)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'],callback=[es])

model.fit(train_padded_docs, train_labels,validation_data=(val_padded_docs,val_labels), epochs=15, verbose=1,batch_size=10000)

```

**Other info / logs**

```python
Train on 200000 samples, validate on 20000 samples

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-14-7ae2a63ebc51> in <module>
----> 1 model.fit(train_padded_docs, train_labels,validation_data=(val_padded_docs,val_labels), epochs=15, verbose=1,batch_size=10000)

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    149 
    150   # Get step function and loop type.
--> 151   f = _make_execution_function(model, mode)
    152   use_steps = is_dataset or steps_per_epoch is not None
    153   do_validation = val_inputs is not None

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)
    519   if model._distribution_strategy:
    520     return distributed_training_utils._make_execution_function(model, mode)
--> 521   return model._make_execution_function(mode)
    522 
    523 

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_execution_function(self, mode)
   2229   def _make_execution_function(self, mode):
   2230     if mode == ModeKeys.TRAIN:
-> 2231       self._make_fit_function()
   2232       return self._fit_function
   2233     if mode == ModeKeys.TEST:

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_fit_function(self)
   2175     ]
   2176     self._make_train_function_helper(
-> 2177         '_fit_function', [self.total_loss] + metrics_tensors)
   2178 
   2179   def _make_test_function_helper(self, fn_name, outputs):

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_train_function_helper(self, fn_name, outputs)
   2160             updates=updates,
   2161             name='train_function',
-> 2162             **self._function_kwargs)
   2163         setattr(self, fn_name, fn)
   2164 

~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)
   3249         msg = ('Invalid argument ""%s"" passed to K.function with TensorFlow '
   3250                'backend') % key
-> 3251         raise ValueError(msg)
   3252   return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)
   3253 

ValueError: Invalid argument ""callback"" passed to K.function with TensorFlow backend

```
"
28417,Reloaded replicated model did not work  for distributed training,"I am running distributed tutorial as following. I can use the unsaved 'model' in replicated mode to evaluate but the reloaded replicated_model didn't work.
I am using CentOS7.5+Python3.7.1. I am not sure what's wrong with this.
In [45]: with strategy.scope():
    ...:     replicated_model=tf.keras.experimental.load_from_saved_model(path)
    ...:     replicated_model.compile(loss='sparse_categorical_crossentropy',
    ...:             optimizer=tf.keras.optimizers.Adam(),
    ...:             metrics=['accuracy'])
    ...:     eval_loss,eval_acc=replicated_model.evaluate(eval_dataset)
    ...:
2019-05-06 00:59:35.889966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1
2019-05-06 00:59:35.890048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-06 00:59:35.890110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1
2019-05-06 00:59:35.890119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N Y
2019-05-06 00:59:35.890126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   Y N
2019-05-06 00:59:35.890631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21625 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:3b:00.0, compute capability: 6.1)
2019-05-06 00:59:35.890865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21625 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:d8:00.0, compute capability: 6.1)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-45-9147a4df43ec> in <module>
      3     replicated_model.compile(loss='sparse_categorical_crossentropy',
      4             optimizer=tf.keras.optimizers.Adam(),
----> 5             metrics=['accuracy'])
      6     eval_loss,eval_acc=replicated_model.evaluate(eval_dataset)
      7

/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    459                 'with strategy.scope():\n'
    460                 '  model=_create_model()\n'
--> 461                 '  model.compile(...)'% (v, strategy))
    462
    463   @property

ValueError: Variable (<tf.Variable 'conv2d_1_10/kernel:0' shape=(3, 3, 1, 32) dtype=float32>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f4a5a7814a8>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.
with strategy.scope():
  model=_create_model()
  model.compile(...)"
28415,Can't Reproduce Results With tf.keras.layers.Conv2D,"Putting the code below in the beginning of my script, I can consistently reproduce the result 100%. However this is only true if I only use Dense layer.

```
import numpy as np
import random as rn
import tensorflow as tf
import os
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(1)
rn.seed(2)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
from tensorflow.keras import backend as K
tf.set_random_seed(3)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
```

If I insert this one line ""model.add(Conv2D(32, 3, activation='relu'))"" before ""model.add(Flatten())"", it produces different results every time I rerun.

Input> flatten > dense produces consistent result, but input > conv2d > flatten > dense produces different result every time I run the code.

It looks like a bug to me, but I apologize if isn't.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Debian GNU/Linux 9.8 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64)
- TensorFlow installed from (source or binary): conda install -c anaconda tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.1
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla P4 7611MiB
"
28414,[C API] support for CudnnCompatibleLSTMCell in native code,"
**System information**
- Have I written custom code : Yes
- OS Platform and Distribution : Ubuntu 16.04
- TensorFlow installed from : binary
- TensorFlow version : 1.12
- Python version: 3.6
- CUDA/cuDNN version: CUDA 9.0 /  CuDNN 7,1
- GPU model and memory: NVidia p100, 16 gb

**Bug description**
After training the model in Python with CudnnCompatibleLSTMCell used and running the graph-freeze procedure I then try to load this model in my custom inference app that uses C-API. I use pre-compiled version of TF 1.12 for Linux. At the call to TF_GraphImportGraphDef(...) I get error TF_NOT_FOUND in the returned status. For more details please see code bellow. 

**Question**
After short look through the code of TF 1.12 I cannot find clear evidence of 'CudnnCompatibleLSTMCell' being supported at all at native level. Could someone confirm that this cell can be used at all from C-API?

**Code to reproduce the issue**
```c++
TF_Graph* LoadGraphDef(const char* file) {
  TF_Buffer* buffer = ReadBufferFromFile(file);

  TF_Graph* graph = TF_NewGraph();
  TF_Status* status = TF_NewStatus();
  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();

  TF_GraphImportGraphDef(graph, buffer, opts, status);
  TF_DeleteImportGraphDefOptions(opts);
  TF_DeleteBuffer(buffer);

  printf(""STATUS : %d\n"", TF_GetCode(status));

  TF_DeleteStatus(status);

  return graph;
}

static TF_Buffer* ReadBufferFromFile(const char* file) {
  const auto f = std::fopen(file, ""rb"");
  if (f == nullptr) {
    return nullptr;
  }

  std::fseek(f, 0, SEEK_END);
  const auto fsize = ftell(f);
  std::fseek(f, 0, SEEK_SET);

  if (fsize < 1) {
    std::fclose(f);
    return nullptr;
  }

  const auto data = std::malloc(fsize);
  std::fread(data, fsize, 1, f);
  std::fclose(f);

  TF_Buffer* buf = TF_NewBuffer();
  buf->data = data;
  buf->length = fsize;
  buf->data_deallocator = DeallocateBuffer;

  return buf;
}

static void DeallocateBuffer(void* data, size_t) {
  std::free(data);
}
```"
28413,tensorflow installation error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution :windows 10
- TensorFlow installed from :tensorflow.org
- TensorFlow version:1.12.0
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- CUDA/cuDNN version:10.1
- GPU model and memory: nVidia GeForce GTX 1060 6 GB 



**Describe the problem**
While checking the installation of tensorflow in python idle i found the importerror?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
>>> import tensorflow as tf



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\parek\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\parek\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\parek\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\parek\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\parek\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found."
28412,Threadpool created by SingleThreadedCpuDevice ,"Here we have a question about the threadpool created by SingleThreadedCpuDevice which could make sense on performance.
Each time the constructor of SingleThreadedCpuDevice is called, a threadpool with only one thread will be created.
 
  (single_threaded_cpu_device.cc:44)
```
  explicit SingleThreadedCpuDevice(Env* env)
      : Device(env, Device::BuildDeviceAttributes(""/device:CPU:0"", DEVICE_CPU,
                                                  Bytes(256 << 20),
                                                  DeviceLocality())) {
    eigen_worker_threads_.num_threads = kNumThreads;
    eigen_worker_threads_.workers = GraphRunnerThreadPool();
    eigen_device_.reset(new Eigen::ThreadPoolDevice(
        eigen_worker_threads_.workers->AsEigenThreadPool(),
        eigen_worker_threads_.num_threads));
    set_tensorflow_cpu_worker_threads(&eigen_worker_threads_);
    set_eigen_cpu_device(eigen_device_.get());
  }
```
 
As far as we know, a threadpool cost more than a thread, so why a SingleThreadedCpuDevice create a threadpoll instead of a thread?"
28411,Eigen version bump breaks nightly AVX512 build with gcc 6.3,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: e432bf03931f4062f7c5e3a1553aff61a7294751
- Python version: 2.7.13
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

In a GCE VM with AVX512 supported CPU:

```
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

```
In file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:59:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/ConfigureVectorization.h:318,
                 from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:22,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/tensor_shape.h:21,
                 from ./tensorflow/core/kernels/conv_grad_ops.h:164,
                 from tensorflow/core/kernels/conv_grad_filter_ops.cc:21:
/usr/lib/gcc/x86_64-linux-gnu/6/include/avx512vlbwintrin.h: In function 'typename Eigen::internal::enable_if<Eigen::internal::unpacket_traits<T>::masked_load_available, Packet>::type Eigen::internal::ploadu(const typename Eigen::internal::unpacket_traits<Packet>::type*, typename Eigen::internal::unpacket_traits<Packet>::mask_t) [with Packet = Eigen::internal::Packet16h]':
/usr/lib/gcc/x86_64-linux-gnu/6/include/avx512vlbwintrin.h:105:1: error: inlining failed in call to always_inline '__m256i _mm256_maskz_loadu_epi16(__mmask16, const void*)': target specific option mismatch
 _mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)
 ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:202:0,
                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/core/framework/tensor_shape.h:21,
                 from ./tensorflow/core/kernels/conv_grad_ops.h:164,
                 from tensorflow/core/kernels/conv_grad_filter_ops.cc:21:
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/GPU/PacketMathHalf.h:598:38: note: called from here
   result.x = _mm256_maskz_loadu_epi16(mask, from);
              ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 387.379s, Critical Path: 200.24s
INFO: 763 processes: 763 local.
FAILED: Build did NOT complete successfully
```

Bisect to b53fd7648b5ca7eaeceb602617433be6e9a4abec reports the following error:

```
In file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:59:0,
                 from external/eigen_archive/Eigen/src/Core/util/ConfigureVectorization.h:318,
                 from external/eigen_archive/Eigen/Core:22,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc:16:
/usr/lib/gcc/x86_64-linux-gnu/6/include/avx512vlbwintrin.h: In function 'typename Eigen::internal::enable_if<Eigen::internal::unpacket_traits<T>::masked_load_available, Packet>::type Eigen::internal::ploadu(const typename Eigen::internal::unpacket_traits<Packet>::type*, typename Eigen::internal::unpacket_traits<Packet>::mask_t) [with Packet = Eigen::internal::Packet16h]':
/usr/lib/gcc/x86_64-linux-gnu/6/include/avx512vlbwintrin.h:105:1: error: inlining failed in call to always_inline '__m256i _mm256_maskz_loadu_epi16(__mmask16, const void*)': target specific option mismatch
 _mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)
 ^~~~~~~~~~~~~~~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:202:0,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d.h:19,
                 from tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/arch/GPU/PacketMathHalf.h:598:38: note: called from here
   result.x = _mm256_maskz_loadu_epi16(mask, from);
              ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 639.500s, Critical Path: 90.25s
INFO: 4690 processes: 4690 local.
FAILED: Build did NOT complete successfully
```

Reverting b53fd7648b5ca7eaeceb602617433be6e9a4abec resolves the issue."
28409,Mac os tensorflow website gets wrong,"when i open the website with my macbook air, the website gets wrong results like following:
![Screen Shot 2019-05-05 at 3 54 53 PM](https://user-images.githubusercontent.com/25492462/57190612-ab162c00-6f4e-11e9-82f4-835830952fb8.png)
![Screen Shot 2019-05-05 at 3 54 53 PM](https://user-images.githubusercontent.com/25492462/57190645-12cc7700-6f4f-11e9-9df3-b05ccb829555.png)
"
28407,Performance issue with the C API,"I'm currently working on a project that requires deep learning inference with Tensorflow's C API. I have a trained neural net (format: frozen graph) to do this. We use the inference for Computational Fluid Dynamics, which makes performance a key aspect for me. For example, one single simulation includes thousands of timesteps. In each timestep, the inference must be carried out for thousands of sets of input data. In my current case, I have a computational domain including 33400 cells and 880 boundary patches. That means, for each single of these thousands of timesteps I have to do the inference 34280 times. We use 3 input and 15 output values.

The whole inference process (from providing the input values to receiving the output values) requires a total of 91 milliseconds on my GPU. The actual inference step: TF_SessionRun(...) makes up for 98% of the computation time.

`TF_CAPI_EXPORT extern void TF_SessionRun(TF_Session* session, const TF_Buffer* run_options, const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs, const TF_Output* outputs, TF_Tensor** output_values, int noutputs, const TF_Operation* const* target_opers, int ntargets, TF_Buffer* run_metadata, TF_Status*);`

The problem now is that I need to do the inference 34280 times in every timestep, which then takes approximately 52 minutes. That means for thousands of timesteps, the computation time is extensive.

Surprisingly, if I convert the frozen graph to a uff-model and do the inference using TensorRT, it only takes me 90 milliseconds for all 34280 input sets. That means the speed-up of TensorRT vs. the C API would be about 35000. As we want to do the inference on a CPU-only architecture, later on, TensorRT is no option for me.

My question: do you know a way to use Tensorflow's C API in a way, that drastically reduces the computation time for multiple inferences? The bottleneck definitely is the TF_SessionRun(...) command, but I can not see a way to run 34280 inferences by only calling the command once. Moreover, the command provides several options (run options, run metadata, target operations, number of targets - see code above) that aren't used in a single example of those I found on the internet. Maybe these can be used to improve the performance?"
28406,[tflite doc] CONV_2D_TRANSPOSE -> TRANSPOSE_CONV,"## Existing URLs containing the issue:

https://www.tensorflow.org/lite/guide/ops_compatibility

## Description of issue (what needs changing):

TensorFlow `r1.13`.

`CONV_2D_TRANSPOSE` op is not present in TensorFlow Lite schema.
After glimpsed `toco` source code,  `tf.nn.conv2d_transpose`(`Conv2DBackpropInput`) is converted to `TRANSPOSE_CONV`.

https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/toco/import_tensorflow.cc#L1778 

So updating tflite documentation(replace `CONV_2D_TRANSPOSE` with `TRANSPOSE_CONV`  ) would be nice.

"
28405, GPU utilization of these two Epoch is very low,"I run https://www.tensorflow.org/alpha/tutorials/text/transformer in Calab with GPU.
The result is a strange phenomenon. The first two Epoch are very slow. I tried to run on my local machine and the result was the same. According to observations, the GPU utilization of these two Epoch is very low, almost zero. What happened to these two Epoch?

```
Epoch 1 Batch 0 Loss 4.4497 Accuracy 0.0000
Epoch 1 Batch 500 Loss 3.6058 Accuracy 0.0401
Epoch 1 Loss 3.3683 Accuracy 0.0538
Time taken for 1 epoch: 1175.7783224582672 secs

Epoch 2 Batch 0 Loss 2.8402 Accuracy 0.0966
Epoch 2 Batch 500 Loss 2.4239 Accuracy 0.1189
Epoch 2 Loss 2.3728 Accuracy 0.1240
Time taken for 1 epoch: 1116.1639959812164 secs

Epoch 3 Batch 0 Loss 2.4180 Accuracy 0.1410
Epoch 3 Batch 500 Loss 2.1358 Accuracy 0.1484
Epoch 3 Loss 2.1007 Accuracy 0.1518
Time taken for 1 epoch: 235.27584767341614 secs

Epoch 4 Batch 0 Loss 2.2033 Accuracy 0.1632
Epoch 4 Batch 500 Loss 1.8985 Accuracy 0.1763
Epoch 4 Loss 1.8629 Accuracy 0.1806
Time taken for 1 epoch: 154.04113721847534 secs

Epoch 5 Batch 0 Loss 1.9665 Accuracy 0.1941
Epoch 5 Batch 500 Loss 1.6708 Accuracy 0.2037
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 1.6434 Accuracy 0.2066
Time taken for 1 epoch: 118.84977173805237 secs

Epoch 6 Batch 0 Loss 1.7504 Accuracy 0.2196
Epoch 6 Batch 500 Loss 1.4882 Accuracy 0.2233
Epoch 6 Loss 1.4662 Accuracy 0.2257
Time taken for 1 epoch: 183.19546270370483 secs

Epoch 7 Batch 0 Loss 1.5731 Accuracy 0.2368
Epoch 7 Batch 500 Loss 1.3058 Accuracy 0.2438
Epoch 7 Loss 1.2844 Accuracy 0.2462
Time taken for 1 epoch: 134.03554582595825 secs

Epoch 8 Batch 0 Loss 1.3853 Accuracy 0.2504
Epoch 8 Batch 500 Loss 1.1468 Accuracy 0.2625
Epoch 8 Loss 1.1327 Accuracy 0.2643
Time taken for 1 epoch: 147.67497181892395 secs

Epoch 9 Batch 0 Loss 1.2550 Accuracy 0.2759
Epoch 9 Batch 500 Loss 1.0325 Accuracy 0.2769
Epoch 9 Loss 1.0226 Accuracy 0.2780
Time taken for 1 epoch: 99.68828344345093 secs

Epoch 10 Batch 0 Loss 1.1568 Accuracy 0.2833
Epoch 10 Batch 500 Loss 0.9410 Accuracy 0.2888
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 0.9350 Accuracy 0.2891
Time taken for 1 epoch: 132.47828769683838 secs
```
"
28404,Missing tensorflow.compiler.xla.service import hlo_pb2 ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I


I am trying to build a TensorRT file to run on my jetson nano.
I am running tensorflow 1.13. 
When I run this code:

trt_graph = trt.create_inference_graph(
    input_graph_def=['input_1'],
    outputs=['Logits/Softmax'],
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode='FP16',
    minimum_segment_size=50
)
I get this error :
ImportError: cannot import name 'hlo_pb2'

It says its looking here for it:
 from tensorflow.compiler.xla.service import hlo_pb2 

It dosent exist?

"
28403,dataset in tf2.0 lack of key properties and methods which already in tf1.13,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below):2.0 alpha
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
dataset in tf2.0 lack of key properties and methods which already in tf1.13, for example:
properties: output_shapes, output_types
methods: make_one_shot_iterator

**Describe the expected behavior**
these key properties and methods should in dataset of tf2.0

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28400,[Regression] Conv1D output shape is lost when dilation_rate != 1.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom simple Keras sequential model with Conv1D dilated layers.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): Works in tf-nightly-gpu-2.0-preview==2.0.0.dev20190410 but fails in tf-nightly-gpu-2.0-preview==2.0.0.dev20190504.
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 (installed via conda)
- GPU model and memory: Turing (2080 Ti)

**Describe the current behavior**
The output shape is not inferred correctly anymore for a Keras Conv1D with dilation_rate != 1 (it is ok when dilation_rate = 1). Shape inference used to work as expected in tf-nightly-gpu-2.0-preview==2.0.0.dev20190410. 

**Describe the expected behavior**
Please revert to the behaviour available in tf-nightly-gpu-2.0-preview==2.0.0.dev20190410

**Code to reproduce the issue**
CORRECT behaviour with a dilation_rate=1:

```
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(24, kernel_size = 2, dilation_rate = 1, padding='causal',
                           kernel_regularizer=tfk.regularizers.l2(0.01), input_shape=(2560, 8)),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Softmax(),
])

model.summary()
Model: ""sequential_2""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_26 (Conv1D)           (None, 2560, 24)          408       
_________________________________________________________________
re_lu_23 (ReLU)              (None, 2560, 24)          0         
_________________________________________________________________
dense_3 (Dense)              (None, 2560, 10)          250       
_________________________________________________________________
softmax_2 (Softmax)          (None, 2560, 10)          0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
```

Shape inference FAILS with a dilation_rate=2 (or >1): The second dimension of the output shape's is lost,

```
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(24, kernel_size = 2, dilation_rate = 2, padding='causal',
                           kernel_regularizer=tfk.regularizers.l2(0.01), input_shape=(2560, 8)),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Softmax(),
])

model.summary()
Model: ""sequential_3""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_27 (Conv1D)           (None, None, 24)          408       
_________________________________________________________________
re_lu_24 (ReLU)              (None, None, 24)          0         
_________________________________________________________________
dense_4 (Dense)              (None, None, 10)          250       
_________________________________________________________________
softmax_3 (Softmax)          (None, None, 10)          0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
```

Please note that the behaviour in 20190410 was correct independently of the dilation_rate.

**Other info / logs**
This causes more issues downstream as one can't concatenate or sum outputs w.r.t the axis with the unknown shape.
"
28398,"Passing tf.data.Dataset to model.predict raises ""ValueError: The `batch_size` argument must not be specified when using dataset as an input.""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: 19.04
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-alpha0 
- TensorFlow version (use command below): 2.0.0-alpha
- Python version: 3.7.3

**Describe the current behavior**
Running simple classification example with Keras interface
**Describe the expected behavior**
Predict results of fitted model with tf.data.Dataset using model.predict

**Code to reproduce the issue**
```
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import tensorflow as tf
from tensorflow.python import keras

iris = datasets.load_iris()

scl = StandardScaler()
ohe = OneHotEncoder(categories='auto')
data_norm = scl.fit_transform(iris.data)
data_target = ohe.fit_transform(iris.target.reshape(-1,1)).toarray()
train_data, val_data, train_target, val_target = train_test_split(data_norm, data_target, test_size=0.1)
train_data, test_data, train_target, test_target = train_test_split(train_data, train_target, test_size=0.2)


train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_target))
train_dataset = train_dataset.batch(32).repeat()

test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_target))
test_dataset = test_dataset.batch(32).repeat()

val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_target))
val_dataset = val_dataset.batch(12).repeat()

mdl = keras.Sequential([
    keras.layers.Dense(16, input_dim=4, activation='relu'),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(3, activation='softmax')]
)

mdl.compile(
    optimizer=keras.optimizers.Adam(0.01),
    loss=keras.losses.categorical_crossentropy,
    metrics=[keras.metrics.categorical_accuracy]
    )

history = mdl.fit(train_dataset, epochs=10, steps_per_epoch=15, validation_data=val_dataset, validation_steps=12)
results = mdl.evaluate(test_dataset, steps=15)
comparison = mdl.predict_classes(test_dataset)
```

**Other info / logs**
```
E0504 15:10:24.153471 139666315605824 ultratb.py:149] Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""/home/ispmarin/lib/venvs/dl/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-848835f9eab8>"", line 41, in <module>
    comparison = mdl.predict_classes(test_dataset)
  File ""/home/ispmarin/lib/venvs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 313, in predict_classes
    proba = self.predict(x, batch_size=batch_size, verbose=verbose)
  File ""/home/ispmarin/lib/venvs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1122, in predict
    batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)
  File ""/home/ispmarin/lib/venvs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1780, in _validate_or_infer_batch_size
    raise ValueError('The `batch_size` argument must not be specified when'
ValueError: The `batch_size` argument must not be specified when using dataset as an input.


```"
28397,"Tensorflow does not build with ""--config=monolithic"": ""multiple definition""","**System information**
- OS Platform and Distribution: Ubuntu 19.04 64bit on Digital Ocean
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13 branch
- Python version: 3.7.3 (/usr/bin/python3)
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: disabled

Compiling tensorflow from source gives linking errors on the end, all of which are related to ""multiple definition"".

```bash
bazel build --define=grpc_no_ares=true --config=opt --config=monolithic --config=ngraph --config=mkl --config=noaws //tensorflow/tools/pip_package:build_pip_package --local_resources 6000,.5,1.0 &> log.txt
```

[log.txt](https://github.com/tensorflow/tensorflow/files/3144670/log.txt)
"
28396,Build did NOT complete successfully; ERROR: expression must have a constant value,"**System information**
- OS Platform and Distribution: Windows10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.36
- Bazel version: 0.15.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.5.1
- GPU model and memory: Quadro K1100m

**Describe problem**

Hello guys, I have been trying to build tf-gpu on source with bazel but continuously getting errors. Got success so far but now it is giving me following errors on matching instances of overload func & also saying expression must have constant value while giving me list of hex strings of whose I can't find any solution to resolve. Please guide.. TIA

**Error I got after executing command:**

> $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

```
....
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (142 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (143 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (144 packages loaded)
WARNING: C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/lenovo/_bazel_lenovo/6kwxios2/external/grpc/bazel/grpc_build_system.bzl:172:12
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (147 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (147 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (148 packages loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (148 packages loaded)
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (155 packages loaded).
INFO: Found 1 target...
[0 / 9] [-----] BazelWorkspaceStatusAction stable-status.txt
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_posix.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_windows.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/grpc/third_party/address_sorting/address_sorting.c:
cl : Command line warning D9002 : ignoring unknown option '-std=c99'
INFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/utf8.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/ostringstream.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/numeric/int128.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/spinlock_wait.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
[2,314 / 5,397] Compiling external/pcre/pcre_exec.c; 128s local ... (7 actions running)
INFO: From Linking external/grpc/libgrpc++_base.a:
server_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
rpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
create_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Compiling external/com_google_absl/absl/base/internal/unscaledcycleclock.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/cycleclock.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/types/bad_variant_access.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/types/optional.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/thread_identity.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/spinlock.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/raw_logging.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/types/bad_optional_access.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/string_view.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/sysinfo.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/charconv_parse.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/charconv_bigint.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/base/internal/throw_delegate.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/ascii.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/numbers.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/memutil.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/match.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/substitute.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/charconv.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/str_cat.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/str_replace.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/escaping.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/extension.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/output.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/str_split.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/float_conversion.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/parser.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/arg.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/bind.cc:
cl : Command line warning D9025 : overriding '/w' with '/W3'
INFO: From Linking external/protobuf_archive/libprotobuf_lite.a:
arenastring.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From Linking external/protobuf_archive/libprotobuf.a:
error_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library
INFO: From ProtoCompile tensorflow/core/protobuf/replay_log.pb.cc:
tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/framework/graph.proto but not used.
tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/protobuf/cluster.proto but not used.
[2,860 / 5,406] Compiling tensorflow/core/framework/api_def.pb.cc; 6s local ... (7 actions running)
ERROR: C:/users/lenovo/tensorflow-build/tensorflow/tensorflow/contrib/rnn/BUILD:218:1: C++ compilation of rule '//tensorflow/contrib/rnn:python/ops/_lstm_ops_gpu' failed (Exit 5): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/lenovo/_bazel_lenovo/6kwxios2/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Lenovo/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Lenovo/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\Lenovo\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Lenovo\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/lstm_ops_gpu.cu.o /c tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc
nvcc error   : 'cicc' died with status 0xC0000005 (ACCESS_VIOLATION)
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(603): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(604): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(605): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(637): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1148): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1594): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(2428): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(2428): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(385): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xtr1common(59): error: class ""std::enable_if<<error-constant>, int>"" has no member ""type""
          detected during instantiation of type ""std::enable_if_t<<error-constant>, int>""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(385): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(647): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(654): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(698): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(705): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(777): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(786): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(787): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(796): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(797): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(862): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xmemory0(353): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xmemory0(943): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xmemory0(1217): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(1914): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xtr1common(59): error: class ""std::enable_if<<error-constant>, void>"" has no member ""type""
          detected during instantiation of type ""std::enable_if_t<<error-constant>, void>""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(1914): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xutility(264): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1562): error: expected a "">""
          detected during:
            instantiation of ""const __nv_bool std::_Is_specialization_v [with _Type=std::char_traits<char>, _Template=std::char_traits]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(2108): here
            instantiation of class ""std::basic_string<_Elem, _Traits, _Alloc> [with _Elem=char, _Traits=std::char_traits<char>, _Alloc=std::allocator<char>]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\stdexcept(24): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\memory(1483): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\memory(1490): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\memory(2536): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1562): error: expected a "">""
          detected during:
            instantiation of ""const __nv_bool std::_Is_specialization_v [with _Type=std::char_traits<__wchar_t>, _Template=std::char_traits]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(2108): here
            instantiation of class ""std::basic_string<_Elem, _Traits, _Alloc> [with _Elem=__wchar_t, _Traits=std::char_traits<__wchar_t>, _Alloc=std::allocator<__wchar_t>]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\string(325): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1562): error: expected a "">""
          detected during:
            instantiation of ""const __nv_bool std::_Is_specialization_v [with _Type=std::char_traits<char16_t>, _Template=std::char_traits]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(2108): here
            instantiation of class ""std::basic_string<_Elem, _Traits, _Alloc> [with _Elem=char16_t, _Traits=std::char_traits<char16_t>, _Alloc=std::allocator<char16_t>]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\string(656): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\type_traits(1562): error: expected a "">""
          detected during:
            instantiation of ""const __nv_bool std::_Is_specialization_v [with _Type=std::char_traits<char32_t>, _Template=std::char_traits]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xstring(2108): here
            instantiation of class ""std::basic_string<_Elem, _Traits, _Alloc> [with _Elem=char32_t, _Traits=std::char_traits<char32_t>, _Alloc=std::allocator<char32_t>]""
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\string(661): here

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(172): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(190): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(209): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(242): error: expression must have a constant value

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(1034): error: no instance of overloaded function ""std::tuple<_This, _Rest...>::tuple"" matches the specified type

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\tuple(1046): error: no instance of overloaded function ""std::tuple<_This, _Rest...>::tuple"" matches the specified type

C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\functional(1193): error: expression must have a constant value

0x00007FF703B12395 (0x0000000000000000 0x0000021243C9C370 0x000002123D7334A8 0x0000000000000000)
0x00007FF703B0F0D1 (0x0000000000000000 0x0000021200000000 0x0000000000000000 0x0000000000000000)
0x00007FF703B0FE73 (0x000002123DFFDD90 0x0000000000000000 0x0000000000000000 0x0000021243C9D480)
0x00007FF703B4413F (0x0000021200000001 0x0000000000000001 0x0000021200000001 0x0000000000000001)
0x00007FF703B3E650 (0x0000021200000001 0x000000E605BFB478 0x000000E605BFBB20 0x0000000000000001)
0x00007FF7039C2E4A (0x000000E600000000 0x0000000000000000 0x000000E605BFB560 0x0000021243C9C188)
0x00007FF7039C81D1 (0x000000E605BFBAC0 0x00007FF703A01712 0x000000E605BFBD30 0x0000000000000011)
0x00007FF7039C7ED0 (0x0000000000003901 0x0000000000000001 0x0000000000000000 0x0000000000000000)
0x00007FF703A2AB15 (0x00000212413E8EA0 0x0000021243C9A598 0x00000212413F12E8 0x000002123BD79990)
0x00007FF703A2BAD9 (0x000002123BD79990 0x000000E605BFC340 0x00000212413F12E8 0x0000000900000000)
0x00007FF703A2A94E (0x0000021243C9C448 0x0000000000000000 0x0000000000000080 0x0000021243C37790)
0x00007FF703B4FCCF (0x0000000000000000 0x0000000000000001 0x00000212413E9138 0x000002123DD59A78)
0x00007FF703B37262 (0x000002123DD5A4B0 0x00000212413E9138 0x000002123DD59A78 0x0000000000000000)
0x00007FF703B36D22 (0x0000000000000000 0x000000E605BFC1A1 0x000000E605BFC340 0x000000260002FE91)
0x00007FF703B37CE4 (0x0000000000000000 0x0000021243C9A598 0x00000212413F12E8 0x00000212413F2D38)
0x00007FF703B4FC0C (0x0000021243C9C6B8 0x0000000000000000 0x0000021243C9A598 0x00000212413F12E8)
0x00007FF703B30B10 (0x0000021243C9A598 0x00000212413F2D08 0x0000021200000000 0x00000212413F2D08)
0x00007FF703B553EA (0x0000000000000000 0x000000E605BFC520 0x00000212413EA140 0x0000000000000000)
0x00007FF703B552A8 (0x00000212413F2D08 0x00000212413F2D80 0x0000000000000000 0x000002123D6C4CE0)
0x00007FF703AEFA96 (0x0000000000000000 0x000002123EA26D68 0x0000021243A3D968 0x0000000000000000)
0x00007FF703AED063 (0x00000212413F2D08 0x0000021240866400 0x0000021200000000 0x00000212413F2D08)
0x00007FF703AFCBAF (0x0000021240866400 0x0000021200000000 0x0000000000000000 0x0000021243A3D968)
0x00007FF703AE57D2 (0x000000E605BFD3E0 0x0000000000000000 0x000000E605BFD3E0 0x000000000000000B)
0x00007FF7039E51A8 (0x0000000000000024 0x00007FF7038E0000 0x0000000000000000 0x0000000000000000)
0x00007FF7039D6C0C (0x000000E605BFD660 0x0000000000000000 0x0000000000000000 0x000000E605BFD978)
0x00007FF7039E9D80 (0x000000E605BFDA08 0x0000021243CB1768 0x0000000000000000 0x0000000000000000)
0x00007FF703B1744A (0x0000000000000001 0x0000000000000000 0x0000000000000000 0x0000000000000000)
0x00007FF703B1B26E (0x0000000000000001 0x0000000000000001 0x0000000000000001 0x000000030003939C)
0x00007FF703B16423 (0x0000000000000001 0x0000000000000000 0x0000000000000000 0x0000021200000000)
0x00007FF703A1BF31 (0x0000021243C9A3F0 0x0000021243C9A3F0 0x0000000000000000 0x0000021243C9A3F0)
0x00007FF703A1AAC3 (0x000000E605BFDCE0 0x0000000000000002 0x0000021243C9A038 0x000000E605BFDDB0)
0x00007FF7039946FA (0x0000000000000001 0x000000E600000000 0x0000000000000000 0x000000E605BFDDB0)
0x00007FF70399B216 (0x0000021243C3ECB8 0x000002123BD804B0 0x0000000000000000 0x0000000000000000)
0x00007FF703992EE0 (0x0000001500039256 0x000000E605BFE0F9 0x0000021243C21A88 0x000000E605BFE0F9)
0x00007FF703997AA4 (0x000000E605BFE1F0 0x0000021200000000 0x000000E605BFE3B0 0x000000E605BFE1D0)
0x00007FF70398BB84 (0x0000000000000001 0x0000000000000000 0x0000000000000000 0x000000E605BFE2C0)
0x00007FF70399ADCC (0x0000351A402BF60C 0x00007FF703C6989A 0x0000000000000000 0x000000E605BFE638)
0x00007FF703992EE0 (0x0000001100039254 0x000000E605BFE629 0x000002123F111940 0x000000E605BFE629)
0x00007FF703997AA4 (0x000000E605BFE720 0x0000021200000000 0x000000E605BFE8E0 0x000000E605BFE700)
0x00007FF70398BB84 (0x0000000000000001 0x0000000000000000 0x0000000000000000 0x000000E605BFE7F0)
0x00007FF70399ADCC (0x000002123BD7D890 0x0000021200000000 0x0000000000000000 0x0000000000000006)
0x00007FF70399C58A (0x0000000000000000 0x0000021200000000 0x0000000000000000 0x0000000000000000)
0x00007FF703B641F4 (0xTarget //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1322.663s, Critical Path: 133.82s
INFO: 1644 processes: 1644 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
28395,"keras_to_tpu_model error ""No OpKernel was registered to support Op ConfigureDistributedTPU""","Running GCP VM (made for TPU) with TPU v2-8, running ML code in **Jupyter**, trying to **convert keras model to TPU** gives error
**No OpKernel was registered to support Op 'ConfigureDistributedTPU'**

**System information**
GCP VM build from image ""debian-9-tf-1-13-v20190504""
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
- TensorFlow installed from (source or binary): preinstalled
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.3
- TPU model and memory: v2-8

**Describe the current behavior**
Error running code below : **InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU'** 

**Describe the expected behavior**
no error

**Code to reproduce the issue**
```
import tensorflow as tf

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense

window_size = 1024 #900
inputs_n = 7
outputs_n = 8
epochs_n = 1
epochs =  range(epochs_n)

TPU_ADDRESS = 'grpc://10.240.1.2:8470'

from keras.layers import Dense, Activation, Dropout, LSTM
from keras.models import Sequential, load_model

model = Sequential()
model.add(LSTM(128, batch_input_shape=(1, window_size, inputs_n), return_sequences=True, stateful=True))
model.add(Dropout(0.2))
model.add(LSTM(128, return_sequences=True, stateful=True))
model.add(Dropout(0.2))
model.add(Dense(8, activation='linear'))

opt = tf.train.AdamOptimizer(0.01)

model.compile(loss='mse', optimizer=opt)

tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))
```

**output**
```
INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6098867428625386983)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3944999252280507086)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 11439376241675220217)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6679858096061465067)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17112049907098627726)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8235536748600333881)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11246969246948913311)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 18093998721829044871)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2543676904778409938)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 16455517362176252302)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 522461249528463037)
WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1316       # Ensure any changes to the graph are reflected in the runtime.
-> 1317       self._extend_graph()
   1318       return self._call_tf_sessionrun(

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1351     with self._graph._session_run_lock():  # pylint: disable=protected-access
-> 1352       tf_session.ExtendSession(self._session)
   1353 

InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}}with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

	 [[{{node ConfigureDistributedTPU}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-15-454c38059ada> in <module>
     36     model,
     37     strategy=tf.contrib.tpu.TPUDistributionStrategy(
---> 38         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))
     39 #

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/framework/experimental.py in new_func(*args, **kwargs)
     62         'any time, and without warning.',
     63         decorator_utils.get_qualified_name(func), func.__module__)
---> 64     return func(*args, **kwargs)
     65   new_func.__doc__ = _add_experimental_function_notice_to_docstring(
     66       func.__doc__)

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in tpu_model(model, strategy)
   2219     else:
   2220       optimizer_config = None
-> 2221     model_weights = model.get_weights()
   2222   else:
   2223     model_weights = None

~/.local/lib/python3.5/site-packages/keras/engine/network.py in get_weights(self)
    490         for layer in self.layers:
    491             weights += layer.weights
--> 492         return K.batch_get_value(weights)
    493 
    494     def set_weights(self, weights):

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in batch_get_value(ops)
   2418     """"""
   2419     if ops:
-> 2420         return get_session().run(ops)
   2421     else:
   2422         return []

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in get_session()
    197                 # not already marked as initialized.
    198                 is_initialized = session.run(
--> 199                     [tf.is_variable_initialized(v) for v in candidate_vars])
    200                 uninitialized_vars = []
    201                 for flag, v in zip(is_initialized, candidate_vars):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by node ConfigureDistributedTPU (defined at <ipython-input-12-37d2eadc5b71>:35) with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

	 [[node ConfigureDistributedTPU (defined at <ipython-input-12-37d2eadc5b71>:35) ]]

Caused by op 'ConfigureDistributedTPU', defined at:
  File ""/usr/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py"", line 505, in start
    self.io_loop.start()
  File ""/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py"", line 148, in start
    self.asyncio_loop.run_forever()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 421, in run_forever
    self._run_once()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1424, in _run_once
    handle._run()
  File ""/usr/lib/python3.5/asyncio/events.py"", line 126, in _run
    self._callback(*self._args)
  File ""/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File ""/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
    ret = callback()
  File ""/usr/local/lib/python3.5/dist-packages/tornado/gen.py"", line 781, in inner
    self.run()
  File ""/usr/local/lib/python3.5/dist-packages/tornado/gen.py"", line 742, in run
    yielded = self.gen.send(value)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 357, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""/usr/local/lib/python3.5/dist-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 267, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/usr/local/lib/python3.5/dist-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py"", line 534, in execute_request
    user_expressions, allow_stdin,
  File ""/usr/local/lib/python3.5/dist-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py"", line 294, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2848, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 2874, in _run_cell
    return runner(coro)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py"", line 67, in _pseudo_sync_runner
    coro.send(None)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 3049, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 3214, in run_ast_nodes
    if (yield from self.run_code(code, result)):
  File ""/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-12-37d2eadc5b71>"", line 35, in <module>
    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/framework/experimental.py"", line 64, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 2225, in tpu_model
    setup_tpu_session(strategy._tpu_cluster_resolver)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 161, in setup_tpu_session
    tpu_session.run(tpu.initialize_system())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 94, in initialize_system
    return tpu_ops.configure_distributed_tpu(embedding_config=config_string)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/ops/gen_tpu_ops.py"", line 315, in configure_distributed_tpu
    is_global_init=is_global_init, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by node ConfigureDistributedTPU (defined at <ipython-input-12-37d2eadc5b71>:35) with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

	 [[node ConfigureDistributedTPU (defined at <ipython-input-12-37d2eadc5b71>:35) ]]
```"
28394,Custom TF 2.0 training loop performing considerably worse than keras fit_generator - can't understand why,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/V10.1.105
- GPU model and memory: RTX 2080 ti 11gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Currently trying to reproduce the results from keras model.fit_generator with a custom training loop in TF2.0. Code runs without any issue, but the MAE loss for the custom training loop is ~3.0, whereas the MAE loss for the keras model.fit_generator is significantly better, at ~2.0.

**Describe the expected behavior**
I would expect the losses to be roughly equivalent.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The data set is extremely large, but each sample consists of 4 fields of length 14,995. Target is a single value, in seconds.

`
         import tensorflow as tf
         import numpy as np
         import os
        import pandas as pd
         import time
         from sklearn import preprocessing
         import shutil
         import my_classes_tf
    
    # Import data and massage
    os.chdir('/home/aj/Data/LANL-Earthquake-Prediction')
    # cv_indices = pd.read_csv('./Current Data/cv_assignments.csv', delimiter=',', header=None).values.astype('int16')
    evaluation_indices = pd.read_csv('./Current Data/Validation Indices Original.csv', delimiter=',', header=None).values.astype('int64')
    eval_index, cv_index = np.hsplit(evaluation_indices, 2)
    train = pd.read_csv('./Current Data/NewFeatures.csv', delimiter=',', header=None).values.astype('float32')
    train_data, other_info = np.hsplit(train, 2)
    targets, OG_row, EQ_ind, CV_ind = np.hsplit(other_info, 4)
    targets = targets.astype('float16')
    OG_row = OG_row.astype('int64')
    EQ_ind = EQ_ind.astype('int64')
    CV_ind = CV_ind.astype('int64')
    mod_eval = pd.read_csv('./Current Data/Validation Indices Modified.csv', delimiter=',', header=None).values.astype('int64')
    mod_eval_index, mod_cv_index, _, _ = np.hsplit(mod_eval, 4)
    
    logtrain = pd.read_csv('./Current Data/NewFeatures_logtransformed.csv', delimiter=',', header=None).values.astype('float32')
    
    log_std, log_skew, log_kurt, log_sixth, _, _, _ = np.hsplit(logtrain, 7)
    train_data_logs = np.concatenate((log_std, log_skew, log_kurt, log_sixth), axis=1)
    
    del logtrain, log_std, log_skew, log_kurt, log_sixth, other_info
    
    
    def safe_mkdir(path):
        """""" Create a directory if there isn't one already. """"""
        try:
            os.mkdir(path)
        except OSError:
            pass
    
    
    def del_dir(name):
        if os.path.isdir('./Saved Models/{}'.format(name)):
            shutil.rmtree('./Saved Models/{}'.format(name))
        if os.path.isdir('./Error Plots/{}'.format(name)):
            shutil.rmtree('./Error Plots/{}'.format(name))
        if os.path.isdir('./Train and Test Losses/{}'.format(name)):
            shutil.rmtree('./Train and Test Losses/{}'.format(name))
    
    
    fold = 1
    boolz = CV_ind != fold
    cv_train = train_data_logs[boolz.reshape(-1)]
    cv_targets = targets[boolz.reshape(-1)]
    cv_eqs = EQ_ind[boolz.reshape(-1)]
    
    scaler = preprocessing.StandardScaler().fit(cv_train)
    cv_train = scaler.transform(cv_train)
    cv_val = scaler.transform(train_data_logs)
    
    batch_size = 64
    lookback = 14995
    offset = 15000
    
    if np.max(mod_eval_index) > len(train_data_logs):  # Prevents from dividing twice on accident when re-running code
        mod_eval_index = mod_eval_index // 10
    train_gen = my_classes_tf.DataGenerator(data=cv_train,
                                            targets=cv_targets,
                                            indices=cv_eqs,
                                            min_index=0,
                                            max_index=None,
                                            batch_size=batch_size,
                                            lookback=lookback,
                                            offset=offset,
                                            shuffle_start=True,
                                            shuffle_feed=True)
    
    val_gen = my_classes_tf.ValDataGenerator(data=cv_val,
                                             targets=targets,
                                             eval_index=mod_eval_index,
                                             cv_index=mod_cv_index,
                                             cv=fold,
                                             batch_size=batch_size,
                                             lookback=lookback)
    
    
    class CRNN(tf.keras.Model):
        def __init__(self):
            super(CRNN, self).__init__()
            # Consider LocallyConnected1D
            self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=50, strides=1, padding='same',
                                                activation=None, kernel_initializer='he_uniform', name='conv1a')
            self.pool1 = tf.keras.layers.MaxPool1D(pool_size=100, strides=None, name='pool1')
            self.gru1 = tf.keras.layers.GRU(units=32, name='gru1')
            self.dense1 = tf.keras.layers.Dense(units=16, activation=None, name='dense1')
            self.output1 = tf.keras.layers.Dense(units=1, activation='relu', name='output1')
            self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)
            self.mae = tf.keras.losses.MeanAbsoluteError()
            self.optimizer = tf.keras.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True)
    
        def call(self, inputs):
            x = self.conv1(inputs)
            x = self.lrelu(x)
            x = self.pool1(x)
            x = self.gru1(x)
            x = self.dense1(x)
            x = self.lrelu(x)
            return self.output1(x)
    
        def train_step(self, sample, label):
            with tf.GradientTape() as tape:
                predictions = self.call(sample)
                loss = self.mae(label, predictions)
            gradients = tape.gradient(loss, self.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
            self.train_loss(loss)
    
        def eval_once(self, sample, label):
            predictions = self.call(sample)
            loss = self.mae(label, predictions)
            self.eval_loss(loss)
    
        def train(self, num_epochs):
            self.train_loss = tf.keras.metrics.Mean(name='train_loss')
            self.eval_loss = tf.keras.metrics.Mean(name='eval_loss')
            self.store_gradients = np.empty((num_epochs, ))
            for epoch in range(num_epochs):
                start_time = time.time()
                self.train_loss.reset_states()
                self.eval_loss.reset_states()
                for samples, labels in train_gen:
                    self.train_step(samples, labels)
                train_gen.on_epoch_end()
                for samples, labels in val_gen:
                    self.eval_once(samples, labels)
                print('Epoch: {0}, Time: {1:.2f}, Train Loss: {2:.2f}, Test Loss: {3:.2f}'.format(epoch + 1,
                                                                                                  time.time() - start_time,
                                                                                                  self.train_loss.result(),
                                                                                                  self.eval_loss.result()))
    
    
    tf.keras.backend.clear_session()
    model = CRNN()
    model.train(20)
    
    model2 = CRNN()
    model2.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-3, momentum=0, nesterov=True),
                   loss='mae')
    
    history = model2.fit_generator(generator=train_gen,
                                   validation_data=val_gen,
                                   epochs=20,
                                   workers=1,
                                   use_multiprocessing=False,
                                   verbose=2,
                                   callbacks=[])
    
    # https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_eager.py
    # Check this ^ to see what is different between keras fit_generator and your fit
    model3 = CRNN()
    model3.compile(optimizer=model3.optimizer,
                   loss=model3.mae)
    history3 = model3.fit_generator(generator=train_gen,
                                    validation_data=val_gen,
                                    epochs=20,
                                    workers=1,
                                    use_multiprocessing=False,
                                    verbose=2,
                                    callbacks=[])

`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I have tested to see that this model can overfit by training it on only a single sample, and it is able to overfit in both scenarios. I am using custom generators of the class tf.keras.utils.Sequence, but they work equivalently under each training scenario. Let me know if you need any additional information in order to help me out with this issue! Thanks!

Attached the custom generators as well as some of the data in text file format (the larger files are too large to upload, but you can use the data from here: https://www.kaggle.com/c/LANL-Earthquake-Prediction/data along with the Build_Features R script I uploaded).
[Build_Features.txt](https://github.com/tensorflow/tensorflow/files/3152705/Build_Features.txt)

[Validation Indices Original.txt](https://github.com/tensorflow/tensorflow/files/3152692/Validation.Indices.Original.txt)
[Validation Indices Modified.txt](https://github.com/tensorflow/tensorflow/files/3152693/Validation.Indices.Modified.txt)




[my_classes.txt](https://github.com/tensorflow/tensorflow/files/3152664/my_classes.txt)
"
28393,Error: Chunk at .... - Custom object detection train.py,"Hey,
I am quite new to tensorflow object detection.
Currently running the newest version 1.13.1. My CUDA is version 10.0, cudnn version is also corresponding. I have set the model settings properly, labelmap is fine. But when I start training it shows this instead of the training progress. This is the command I use for training: 
python train.py --logtostderr --train_dir=Training_dir/ --pipeline_config_path=Training/faster_rcnn_inception_v2_pets.config


2019-05-04 15:49:49.681406: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F200 of size 256
2019-05-04 15:49:49.685276: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F300 of size 256
2019-05-04 15:49:49.687901: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F400 of size 256
2019-05-04 15:49:49.690789: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F500 of size 256
2019-05-04 15:49:49.695108: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F600 of size 256
2019-05-04 15:49:49.697040: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F700 of size 256
2019-05-04 15:49:49.699036: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F800 of size 256
2019-05-04 15:49:49.701121: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7F900 of size 256
2019-05-04 15:49:49.704656: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FA00 of size 256
2019-05-04 15:49:49.707688: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FB00 of size 256
2019-05-04 15:49:49.710263: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FC00 of size 256
2019-05-04 15:49:49.712346: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FD00 of size 256
2019-05-04 15:49:49.716165: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FE00 of size 256
2019-05-04 15:49:49.718124: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B7FF00 of size 256
2019-05-04 15:49:49.720099: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80000 of size 256
2019-05-04 15:49:49.722046: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80100 of size 256
2019-05-04 15:49:49.726579: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80200 of size 256
2019-05-04 15:49:49.728682: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80300 of size 256
2019-05-04 15:49:49.731296: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80400 of size 256
2019-05-04 15:49:49.734722: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80500 of size 256
2019-05-04 15:49:49.737321: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80600 of size 256
2019-05-04 15:49:49.739445: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80700 of size 256
2019-05-04 15:49:49.741584: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80800 of size 256
2019-05-04 15:49:49.745590: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80900 of size 256
2019-05-04 15:49:49.747562: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80A00 of size 256
2019-05-04 15:49:49.749507: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80B00 of size 256
2019-05-04 15:49:49.752000: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80C00 of size 256
2019-05-04 15:49:49.756087: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80D00 of size 256
2019-05-04 15:49:49.758338: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80E00 of size 256
2019-05-04 15:49:49.761321: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B80F00 of size 256
2019-05-04 15:49:49.764796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000715B81000 of size 256
2019-05-04 15:49:49.766885: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0000000715B81100 of size 359440128
2019-05-04 15:49:49.768987: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 000000072B24B000 of size 718848000
2019-05-04 15:49:49.771072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0000000755FD7000 of size 359424000
2019-05-04 15:49:49.774803: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 000000076B69D000 of size 718848000
2019-05-04 15:49:49.776925: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0000000796429000 of size 548053760
2019-05-04 15:49:49.779391: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size:
2019-05-04 15:49:49.781397: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 6634 Chunks of size 256 totalling 1.62MiB
2019-05-04 15:49:49.785476: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 224 Chunks of size 512 totalling 112.0KiB
2019-05-04 15:49:49.788136: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 136 Chunks of size 768 totalling 102.0KiB
2019-05-04 15:49:49.790378: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 160 Chunks of size 1024 totalling 160.0KiB
2019-05-04 15:49:49.794349: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 19 Chunks of size 1280 totalling 23.8KiB
2019-05-04 15:49:49.796678: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 16 Chunks of size 1536 totalling 24.0KiB
2019-05-04 15:49:49.799443: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2048 totalling 4.0KiB
2019-05-04 15:49:49.802687: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 6144 totalling 12.0KiB
2019-05-04 15:49:49.807143: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 8192 totalling 16.0KiB
2019-05-04 15:49:49.809723: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 3 Chunks of size 13312 totalling 39.0KiB
2019-05-04 15:49:49.812037: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 16384 totalling 64.0KiB
2019-05-04 15:49:49.816472: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 24576 totalling 48.0KiB
2019-05-04 15:49:49.818742: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 8 Chunks of size 49152 totalling 384.0KiB
2019-05-04 15:49:49.824086: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 53248 totalling 52.0KiB
2019-05-04 15:49:49.828020: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 8 Chunks of size 65536 totalling 512.0KiB
2019-05-04 15:49:49.831981: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 81920 totalling 160.0KiB
2019-05-04 15:49:49.836604: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 98304 totalling 192.0KiB
2019-05-04 15:49:49.839664: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 147456 totalling 576.0KiB
2019-05-04 15:49:49.844910: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 163840 totalling 320.0KiB
2019-05-04 15:49:49.847864: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 22 Chunks of size 221184 totalling 4.64MiB
2019-05-04 15:49:49.850899: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 12 Chunks of size 294912 totalling 3.38MiB
2019-05-04 15:49:49.855669: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 6 Chunks of size 331776 totalling 1.90MiB
2019-05-04 15:49:49.857862: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 368640 totalling 1.41MiB
2019-05-04 15:49:49.860496: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 12 Chunks of size 442368 totalling 5.06MiB
2019-05-04 15:49:49.862624: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 516096 totalling 1008.0KiB
2019-05-04 15:49:49.866863: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 524288 totalling 2.00MiB
2019-05-04 15:49:49.869060: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 589824 totalling 2.25MiB
2019-05-04 15:49:49.871650: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 655360 totalling 1.25MiB
2019-05-04 15:49:49.876124: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 6 Chunks of size 737280 totalling 4.22MiB
2019-05-04 15:49:49.878282: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 6 Chunks of size 786432 totalling 4.50MiB
2019-05-04 15:49:49.880396: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 884736 totalling 3.38MiB
2019-05-04 15:49:49.882433: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 921600 totalling 1.76MiB
2019-05-04 15:49:49.886079: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1105920 totalling 2.11MiB
2019-05-04 15:49:49.888123: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1290240 totalling 2.46MiB
2019-05-04 15:49:49.890219: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1327104 totalling 2.53MiB
2019-05-04 15:49:49.892502: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 1441792 totalling 5.50MiB
2019-05-04 15:49:49.896264: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1548288 totalling 2.95MiB
2019-05-04 15:49:49.898314: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1769472 totalling 3.38MiB
2019-05-04 15:49:49.900343: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 1806336 totalling 6.89MiB
2019-05-04 15:49:49.902344: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 2211840 totalling 8.44MiB
2019-05-04 15:49:49.906552: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2359296 totalling 4.50MiB
2019-05-04 15:49:49.909086: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 10616832 totalling 20.25MiB
2019-05-04 15:49:49.911765: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 359424000 totalling 342.77MiB
2019-05-04 15:49:49.915674: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 718848000 totalling 1.34GiB
2019-05-04 15:49:49.918760: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 1.77GiB
2019-05-04 15:49:49.921284: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats:
Limit:                  3006477107
InUse:                  1902062848
MaxInUse:               1902062848
NumAllocs:                   12819
MaxAllocSize:            718848000

2019-05-04 15:49:49.928506: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ****______*__________*************************************************************__________________
2019-05-04 15:49:49.931977: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at conv_ops.cc:446 : Resource exhausted: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[{{node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 297, in stop_on_exception
    yield
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 495, in run
    self.run_loop()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1034, in run_loop
    self._sv.global_step])
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[{{node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 297, in stop_on_exception
    yield
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 495, in run
    self.run_loop()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1034, in run_loop
    self._sv.global_step])
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[{{node gradients/FirstStageFeatureExtractor/InceptionV2/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/FusedBatchNorm_grad/FusedBatchNormGrad}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 994, in managed_session
    yield sess
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 770, in train
    sess, train_op, global_step, train_step_kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 487, in train_step
    run_metadata=run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node gradients/FirstStageFeatureExtractor/InceptionV2/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/FusedBatchNorm_grad/FusedBatchNormGrad (defined at D:\tensorflow\models-master\research\slim\deployment\model_deploy.py:263) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node gradients/FirstStageFeatureExtractor/InceptionV2/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/FusedBatchNorm_grad/FusedBatchNormGrad (defined at D:\tensorflow\models-master\research\slim\deployment\model_deploy.py:263) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.



During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 418, in train
    saver=saver)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py"", line 785, in train
    ignore_live_threads=ignore_live_threads)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1004, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 832, in stop
    ignore_live_threads=ignore_live_threads)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 297, in stop_on_exception
    yield
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 495, in run
    self.run_loop()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\supervisor.py"", line 1034, in run_loop
    self._sv.global_step])
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 291, in train
    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])
  File ""D:\tensorflow\models-master\research\slim\deployment\model_deploy.py"", line 193, in create_clones
    outputs = model_fn(*args, **kwargs)
  File ""D:\tensorflow\models-master\research\object_detection\legacy\trainer.py"", line 204, in _create_losses
    prediction_dict = detection_model.predict(images, true_image_shapes)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 647, in predict
    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 978, in _extract_rpn_feature_maps
    scope=self.first_stage_feature_extractor_scope))
  File ""D:\tensorflow\models-master\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 163, in extract_proposal_features
    return self._extract_proposal_features(preprocessed_inputs, scope)
  File ""D:\tensorflow\models-master\research\object_detection\models\faster_rcnn_inception_v2_feature_extractor.py"", line 138, in _extract_proposal_features
    scope=scope)
  File ""D:\tensorflow\models-master\research\slim\nets\inception_v2.py"", line 117, in inception_v2_base
    scope=end_point)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 2778, in separable_convolution2d
    outputs = layer.apply(inputs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1227, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\layers\base.py"", line 530, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\convolutional.py"", line 1691, in call
    data_format=conv_utils.convert_data_format(self.data_format, ndim=4))
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_impl.py"", line 681, in separable_conv2d
    name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1113, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""D:\Program Files\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,64,480,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at D:\tensorflow\models-master\research\slim\nets\inception_v2.py:117) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Reshape (defined at D:\tensorflow\models-master\research\object_detection\core\post_processing.py:136) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
"
28390,different results on different machine,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10(10.0 17134  10.0 15063)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: unclear
- TensorFlow installed from (source or binary): anaconda navigator
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): spyder 3.3.1
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
different results on different machine
**Describe the expected behavior**
same results on different machine
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
import csv
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.utils import shuffle
from batch import Dataset




def generate(sample_size,mean,cov,diff,regression):
    num_classes = 2
    samples_per_class = int(sample_size/2)
    np.random.seed(1)
    X0 = np.random.multivariate_normal(mean,cov,samples_per_class)
    Y0 = np.zeros(samples_per_class)
    
    for ci,d in enumerate(diff):
        X1 = np.random.multivariate_normal(mean+d,cov,samples_per_class)
        Y1 = (ci+1)*np.ones(samples_per_class)
        
        X0 = np.concatenate((X0,X1))
        Y0 = np.concatenate((Y0,Y1))
    
    if regression==False:
        class_ind = [Y0==class_number for class_number in range(num_classes)]
        Y0 = np.asarray(np.hstack(class_ind),dtype=np.float32)
        
    X,Y = shuffle(X0,Y0,random_state=1)
    
    return X,Y



np.random.seed(10)
num_classes = 2
mean = np.random.randn(num_classes)
cov = np.eye(num_classes)
X,Y = generate(100,mean,cov,[3.0],True)
colors = ['r' if l==0 else 'b' for l in Y[:]]
plt.scatter(X[:,0],X[:,1],c=colors)
plt.xlabel('Scaled age (in yrs)')
plt.ylabel('Tumor size (in cm)')
plt.show()
lab_dim =1


path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归训练集.csv'
f = open(path ,'w',newline='',encoding='utf-8') 
writer = csv.writer(f)
for item in X :
   writer.writerow(item)  
f.close()

input_dim = X.shape[1]
lab_dim = 1
input_features = tf.placeholder(tf.float32,[None,input_dim])
input_labels = tf.placeholder(tf.float32,[None,lab_dim])
W = tf.Variable(tf.random_normal([input_dim,lab_dim],seed=1),name='weight')
b = tf.Variable(tf.zeros([lab_dim]),name='bias')

output = tf.nn.sigmoid(tf.matmul(input_features,W)+b)
cross_entropy = -(input_labels*tf.log(output)+(1-input_labels)*tf.log(1-output))
ser = tf.square(input_labels-output)

loss = tf.reduce_mean(cross_entropy)
err = tf.reduce_mean(ser)
optimizer = tf.train.AdamOptimizer(0.04)
train = optimizer.minimize(loss)

maxEpochs = 51
minibatchSize = 25


path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归优化前权重.csv'
f_1 = open(path ,'w',newline='',encoding='utf-8') 
writer_1 = csv.writer(f_1)

path = 'F:\\实验室代码\\python\\autoencoder\\testData\\线性逻辑回归优化后权重.csv'
f_2 = open(path ,'w',newline='',encoding='utf-8') 
writer_2 = csv.writer(f_2)

path = 'F:\\实验室代码\\python\\autoencoder\\testData\\所有epoch的index.csv'
f_3 = open(path ,'w',newline='',encoding='utf-8') 
writer_3 = csv.writer(f_3)



with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    w_1 = sess.run(W)  #优化前的权重
    for item in w_1 :
        writer_1.writerow(item)  
    f_1.close()

    index_all_epoch = []  #所有epoch
    for epoch in range(maxEpochs):
        sumerr=0
        index = Dataset(np.arange(0, 100))  #index为下标数组，从0到n_examples-1的整数
        index_whole = np.array([]) #一次epoch 
        seed = epoch
        for i in range(np.int32(len(Y)/minibatchSize)):
            
            tempindex = index.next_batch(minibatchSize,seed)  #将下标数组打乱，取出前面的下标
            
            index_whole = np.append(index_whole,tempindex)
            
            x1 = X[tempindex]
            y1 = np.reshape(Y[tempindex],[-1,1])
            tf.reshape(y1,[-1,1])
            _,lossval,outputval,errval = sess.run([train,loss,output,err],feed_dict = {input_features:x1,input_labels:y1})
            sumerr = sumerr + errval
        print('Epoch:','%04d' %(epoch+1),'cost=','{:.9f}'.format(lossval),'err=',sumerr/np.int32(len(Y)/minibatchSize))
        index_all_epoch.append(index_whole)
    
    w_2 = sess.run(W)  #优化后的权重
    for item in w_2 :
        writer_2.writerow(item)  
    f_2.close()
    
    for item in index_all_epoch:#所有epoch
        writer_3.writerow(item)
    f_3.close()
        
        

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28389,DLL load failed: The specified module could not be found.,"Exception has occurred: ImportError
DLL load failed: The specified module could not be found.
  File ""C:\Users\hanna\Anaconda3\Lib\site-packages\numpy\_distributor_init.py"", line 34, in <module>
    from . import _mklinit
  File ""C:\Users\hanna\Anaconda3\Lib\site-packages\numpy\__init__.py"", line 140, in <module>
    from . import _distributor_init
  File ""C:\Users\hanna\Anaconda3\Lib\site-packages\tensorflow\python\__init__.py"", line 47, in <module>
    import numpy as np
  File ""C:\Users\hanna\Anaconda3\Lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\hanna\Google Drive\hannannussbaum\Data Science\TensorFlowTest.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\hanna\Anaconda3\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\hanna\Anaconda3\Lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\hanna\Anaconda3\Lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)

tensorboard                        1.13.1
tensorflow                         1.13.1
tensorflow-estimator               1.13.0
tensorflow-gpu                     1.13.1
termcolor                          1.1.0"
28388,How to build static libtensorflow_cc.a and libtensorflow_framework.a for Linux and MacOS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.13.1
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): Apple LLVM version 9.1.0 (clang-902.0.39.2)
- CUDA/cuDNN version: 10/7.4.2.24
- GPU model and memory: GTX 1060



**Describe the problem**
I am unable to build a static version of tensorflow as described for a solution to:

https://github.com/tensorflow/tensorflow/issues/22810#issuecomment-433225466

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Trying to use tensorflow/contrib/cmake results in a number of errors

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Can you describe the approach to creating a static version of the libraries needed for the following headers:

#include <tensorflow/core/platform/init_main.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/framework/tensor_shape.h>

To link against my software?

Or alternatively solve this issue:

https://github.com/tensorflow/tensorflow/issues/22810

Dated in October of last year.

Static linking seems to be doable."
28385,[C API] cond op,"**System information**
- TensorFlow version (you are using): master
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.** 
Feature: a cond operator for the C API, similar to [tf.cond](https://www.tensorflow.org/api_docs/python/tf/cond).

Current state: The C API currently includes the control flow primitives needed to implement cond, but it does not include the cond op itself. 

**Will this change the current api? How?**
The C API would include the cond op

**Who will benefit with this feature?**
All language bindings built on top of the C API could use its cond op, rather than having to individually implement their own from control flow primitives. 
"
28381,TPU Performance Regression 1.12 -> 1.13,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0 & 1.13.1
- Python version: 3.6.8
- CUDA/cuDNN version: N/A TPU
- GPU model and memory: N/A TPU

**Describe the current behavior**

Upgrading from Tensorflow 1.12.0 to 1.13.1 shows a large performance regression. Training goes from 2100 examples/s & 8.2 global steps/s to 860 examples/s & 3.4 global steps/s.

**Describe the expected behavior**

I would expect no performance penalty or improved performance for upgrading a library.

**Other info / logs**
Here are the outputs of `capture_tpu_profile` for the base case & regression case respectively.
![Base Profile](https://user-images.githubusercontent.com/2245080/57163873-35bc2700-6da7-11e9-856a-677a6b021731.png)
![ Regression Profile](https://user-images.githubusercontent.com/2245080/57163877-3785ea80-6da7-11e9-87de-e05dd121be30.png)

For reference this profile was captured during training on MobileNet but we have also observed this on other convolutional architectures. We also use quantization."
28377,Looping over tf.data.Dataset raises OutOfRangeError in TF 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): very little custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0.0-dev20190501
- Python version: 3.6.7 (miniconda)

**Describe the current behavior**

```sh
Exception has occurred: OutOfRangeError
End of sequence [Op:IteratorGetNextSync]
  File ""<string>"", line 3, in raise_from
  File ""/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2120, in iterator_get_next_sync
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 614, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 622, in next
    return self._next_internal()
  File ""/usr/local/miniconda3/envs/te/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 585, in __next__
    return self.next()
```

**Code to reproduce the issue**

```py
def train_model(model, X_train, y_train, lr=0.01, epochs=1000, batch_size=32):

    training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))
    training_data = training_data.shuffle(buffer_size=2048).batch(batch_size)
    optimizer = tf.keras.optimizers.Adam(lr=lr)

    def robust_mse(y_true, model_output):
        y_pred, var = tf.transpose(model_output)
        loss = 0.5 * tf.square(y_true - y_pred) * tf.exp(-var) + 0.5 * var
        return tf.reduce_mean(loss)

    for epoch in range(epochs):
        print(f""Starting epoch {epoch}/{epochs}"")

        for X_batch, y_batch in training_data:

            with tf.GradientTape() as tape:
                output = model(X_batch)  # predictive mean and variance for each sample
                loss = robust_mse(y_batch, output)

            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))
```"
28374,tf.data.Dataset allow change of structure,"**System information**
- TensorFlow version (you are using): v2
- Are you willing to contribute it (Yes/No): only python code



**Describe the feature and the current behavior/state.**
My current use case is as follows:
I get a dataset from `tensorflow_datasets.load()` where the structure is `(image, label)`, e.g. `MNIST`. I would like to only use the `image` information of that dataset. Therefore I tried to find a way to ""unpack"" that dataset from a dict into a single element (so no key required to work on it).

```python
# Currently

dataset = tfds.load('MNIST', split='train')
for d in dataset:
  # Now d has the ""shape"" {'image': tensor, 'label': tensor}

# What I would like to do
dataset = tfds.load('MNIST', split='train')
dataset = tf.data.Dataset.unpack(dataset, 'image')
for d in dataset:
  # Now d is a tensor
```

When the dataset is `unpacked` I can also do further transformations on it, e.g. `filter`, `batch`, ...

**Will this change the current api? How?** No

**Who will benefit with this feature?** Users of `tensorflow_dataset` I assume.

**Any Other info.**
"
28373,TensorFlow Lite converter type mismatch during keras model conversion,"**System information**
- OS Platform and Distributio: Elementary OS 5
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- CUDA/cuDNN version:
cudatoolkit               10.0.130
cudnn                     7.3.1
- GPU model and memory: GTX 1060 6 Gb

**Error**
TypeError: Input 'y' of 'Equal' Op has type float32 that does not match type int32 of argument 'x'.
in tensorflow/python/framework/op_def_library.py, line 547, in _apply_op_helper

**Code to reproduce the issue**
`tflite_convert
  --output_file=./tflite_model.tflite
  --keras_model_file=./images/train-set/train-set_model.h5`
How net was trained and saved shown in file attached.
Full console output attached also.
[net and console.zip](https://github.com/tensorflow/tensorflow/files/3142890/net.and.console.zip)

The same if called by python API.

Trained model:
[train-set_model.h5.zip](https://github.com/tensorflow/tensorflow/files/3154779/train-set_model.h5.zip)

"
28372,tensorflow2.0 InvalidArgumentError for multiple GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows server 2016
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 2.0-alpha0
- **Python version**: 3.6
- **CUDA/cuDNN version**: cuda10.0; cudnn7.5 for cuda10.0
- **GPU model and memory**: 4*1080Ti  each 11GB

### Describe the problem
I try to test the multiple GPU tutorials of tf2.0, both [keras](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) and [training loop](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/training_loops.ipynb) get the same error:
> tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=""sum"", T=DT_FLOAT, num_devices=4, shared_name=""c0""]
> Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_4244]

all the code copy from the two link, and do not change one letter. It is a bug or I do something wrong?
"
28371,Suggestion for Neural Machine Translation with Attention tutorial,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
2.0
- Doc Link:
https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention

**Describe the documentation issue**

1. max_length function is not needed. The keras.preprocessing.sequence.pad_sequences already pad the sequence to max length. One can just get max length by input_seqs.shape[1]
2. need to explain why we skip 0 for ""convert"" and ""loss_function"". I think it is because of padding
3. Validation set is not used.  This could be an good example to explain how we evaluate the model.
4.  GRU unit in encoder,  Dense in Attention weight calculation,  GRU unit in decoder all have the same units parameter. This is not necessary.  A note could be added to explain that same unit is used for convenience.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes

"
28369,"UnicodeDecodeError during configure, compiling from source","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.14
- Python version:3.5
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.21.1
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: GeForce GTX 1060 6GB


**Describe the problem**
When I run configure in the tensorflow directory, I get an error related to UnicodeDecodeError

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure 
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is /usr/bin/python]: 


Found possible Python library paths:
  /opt/ros/kinetic/lib/python2.7/dist-packages
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
  /home/syha/syha_ws/devel/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/opt/ros/kinetic/lib/python2.7/dist-packages]
/usr/lib/python2.7/dist-packages
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Traceback (most recent call last):
  File ""third_party/gpus/find_cuda_config.py"", line 467, in <module>
    main()
  File ""third_party/gpus/find_cuda_config.py"", line 459, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""third_party/gpus/find_cuda_config.py"", line 422, in find_cuda_config
    _get_default_cuda_paths(cuda_version))
  File ""third_party/gpus/find_cuda_config.py"", line 163, in _get_default_cuda_paths
    ] + _get_ld_config_paths()
  File ""third_party/gpus/find_cuda_config.py"", line 143, in _get_ld_config_paths
    match = pattern.match(line.decode(""ascii""))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 15: ordinal not in range(128)
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: 


Traceback (most recent call last):
  File ""third_party/gpus/find_cuda_config.py"", line 467, in <module>
    main()
  File ""third_party/gpus/find_cuda_config.py"", line 459, in main
    for key, value in sorted(find_cuda_config().items()):
  File ""third_party/gpus/find_cuda_config.py"", line 422, in find_cuda_config
    _get_default_cuda_paths(cuda_version))
  File ""third_party/gpus/find_cuda_config.py"", line 163, in _get_default_cuda_paths
    ] + _get_ld_config_paths()
  File ""third_party/gpus/find_cuda_config.py"", line 143, in _get_ld_config_paths
    match = pattern.match(line.decode(""ascii""))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 15: ordinal not in range(128)
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:

"
28367,tf keras model.fit does not work with strings as labels,"<em>Please make sure that this is a bug. As per our [GitHub Policy]**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf 1.12, '2.0.0-dev20190503'
- Python version: 3.6.7
- CUDA/cuDNN version: not applicable
- GPU model and memory: not applicable

**Describe the current behavior**
When using model.fit it throws an error:

> tensorflow.python.framework.errors_impl.UnimplementedError: Cast string to float is not supported [Op:Cast] name: Cast/


**Describe the expected behavior**

This should work the same as when using a lower level implementation with a GradientTape (working fine, no error)

**Code to reproduce the issue**
```python
    import tensorflow as tf
    import numpy as np

    images = np.random.rand(5, 108, 56, 3)
    y_pred = np.random.rand(5, 4)
    y_true = np.array(['aa', 'bb', 'cc', 'dd', 'ee'])

    dataset = tf.data.Dataset.from_tensor_slices((images, y_true))
    dataset = dataset.batch(5)
    dataset = dataset.repeat()

    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(16, [3,3], activation='relu'),
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(4)
    ])

    def triplet_loss(y_true, y_pred):
        all_diffs = tf.expand_dims(y_pred, axis=1) - tf.expand_dims(y_pred, axis=0)
        distances = tf.sqrt(tf.reduce_sum(tf.square(all_diffs), axis=-1) + 1e-12)

        furthest_positive = tf.reduce_max(distances, axis=1)

        closest_negative = tf.map_fn(lambda x: tf.reduce_min(x),
                                     distances)

        diff = furthest_positive - closest_negative
        diff = tf.nn.softplus(diff)

        return diff

    optimizer = tf.optimizers.Adam(learning_rate=0.001)

    model.compile(loss=triplet_loss,
                  optimizer=optimizer)

    model.fit(dataset, steps_per_epoch=5, epochs=10, verbose=1)
```


When using a lower level implementation it works correctly:

```python
for images, labels in dataset:
    with tf.GradientTape() as tape:
        y_pred = model(images, training=True)
        loss_value = triplet_loss(labels, y_pred)

        grads = tape.gradient(loss_value, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        print('iteration done')
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28366,Error with custom ResourceBase,"**My system information**
- MacOS 10.12
- Python 3.6.6
- TensorFlow version: 1.13.1

**Description:**
When implementing a custom `ResourceBase` with the help of `tensorflow/core/framework/resource_op_kernel.h`, I get the following error

        [F tensorflow/core/lib/core/refcount.h:90] Check failed: ref_.load() == 0 (1 vs. 0)
        1]    29701 abort      python test.py

Here is the full code to reproduce the issue:

```cpp
using namespace tensorflow;

/** CUSTOM RESOURCE **/
class MyVector : public ResourceBase {
 public:
  string DebugString() override { return ""MyVector""; };
 private:
  std::vector<int> vec_;
};

/** CREATE VECTOR **/
REGISTER_OP(""CreateMyVector"")
    .Attr(""container: string = ''"")
    .Attr(""shared_name: string = ''"")
    .Output(""resource: resource"")
    .SetIsStateful();

class MyVectorOp : public ResourceOpKernel<MyVector> {
 public:
  explicit MyVectorOp(OpKernelConstruction* ctx) : ResourceOpKernel(ctx) {}

 private:
  Status CreateResource(MyVector** resource) override {
    *resource = CHECK_NOTNULL(new MyVector);
    if(*resource == nullptr) {
      return errors::ResourceExhausted(""Failed to allocate"");
    }
    return Status::OK();
  }

  Status VerifyResource(MyVector* vec) override {
    return Status::OK();
  }
};

REGISTER_KERNEL_BUILDER(Name(""CreateMyVector"").Device(DEVICE_CPU), MyVectorOp);
```

and then, after compiling, the error can be reproduced with this Python snippet of code:

```python
test_module = tf.load_op_library('./test.so')
my_vec = test_module.create_my_vector()
with tf.Session() as s:
  s.run(my_vec)
```

Thanks a lot.
"
28365,"Retrain image detection with MobileNet for use in TFjs, failure in tensorflowjs_converter","It is unclear whether this is a documentation or a programming issue. If misfiled, please say so and suggest whether a new issue should be raised, or what else can be done.

**System information**
- TensorFlow version:
 ('v1.13.1-0-g6612da8951', '1.13.1') (see #27538)
('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0') (see #27539) and
tf-nightly-2.0-preview (see https://stackoverflow.com/questions/55682557/)

- Doc Link:
https://www.tensorflow.org/hub/tutorials/image_retraining and
https://www.tensorflow.org/tutorials/images/hub_with_keras

**Describe the documentation issue**
Both documents mention but do not describe conversion to mobile models. Yet, conversion using `tensorflowjs_converter` failed in every attempt. See https://stackoverflow.com/questions/55849309/retrain-image-detection-with-mobilenet, which mentions two attempts by me as well as questions by others with the same problem.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

As soon as it is working, I would gladly fix and/or augment the docs.  I have not gotten **`tensorflowjs_converter`** to **work on a retrained MobileNet model**, despite the docs stating that it should work, [e.g.](https://www.tensorflow.org/tutorials/images/hub_with_keras#export_your_model):

> This saved model can loaded for inference later, or converted to TFLite or TFjs."
28364,An error ocurred while starting the kernel(tensorflow-gpu),"While using the keras to proceed the code suddenly，the warining came out which are as follow：

An error ocurred while starting the kernel
2019󈚩󈚧 16:08:03.433226: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 4.89GiB
2019󈚩󈚧 16:08:03.435266: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1484] Adding visible gpu devices: 0
2019󈚩󈚧 16:08:05.728803: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019󈚩󈚧 16:08:05.730056: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0 
2019󈚩󈚧 16:08:05.730321: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:984] 0: N 
2019󈚩󈚧 16:08:05.730723: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4644 MB memory) ‑> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5

**my system information**
- win10
- cuda9.2
- cdunn7.2
- python3.65
- TensorFlow version :1.10.0
- GPU/CPU:RTX2060,i7 9750h
- memory:16g

but when i used tensorflow directly,without processing code by keras,there was no warning and competely successful.
Despite all this,I still prefer keras rather than directly using tensorflow，somebody help me please XD."
28363,"tf.image.resize_image_with_pad(image, 108, 56) gives 109 by 56 output for some images","[7953_4_3337_294_54_110.txt](https://github.com/tensorflow/tensorflow/files/3141289/7953_4_3337_294_54_110.txt)
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf 1.12, tf 1.13, tf-nightly
- Python version: 3.6.7
- CUDA/cuDNN version: not applicable
- GPU model and memory: not applicable

**Describe the current behavior**
For some images, not for all tf.image.resize_image_with_pad(image, 108, 56) gives an output size of 109 by 56

**Describe the expected behavior**
 tf.image.resize_image_with_pad(image, 108, 56) should always give an output of 108 by 56

**Code to reproduce the issue**
    `import tensorflow as tf 
    import pdb
    import cv2
    import base64

    #tf.enable_eager_execution()

    with tf.Session() as sess:
        img_path = '7953_4_3337_294_54_110.txt'
    
        with open(img_path, 'r') as the_file:
            image_string_base64 = the_file.read()

        image_string = tf.decode_base64(image_string_base64)

        image = tf.image.decode_png(image_string)
        image = tf.image.resize_image_with_pad(image, 108, 56)

        print(image)

        img_array = sess.run(image)
        print(img_array.shape)
    
        cv2.imwrite('test.png', cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))`

**Other info / logs**
With eager execution it works correctly

"
28362,AttributeError: 'tuple' object has no attribute 'ndims',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I am trying to run Virtual Batch Nomalization from [Here]( tensorflow/tensorflow/contrib/gan/python/features/python/virtual_batchnorm_impl.py ) with reference batch of size (10,50,50,1) of float32 type, which means that total 10 images of 50*50 type with channel=1. I write for this `S1=VBN(S)`. And getting error `AttributeError: 'tuple' object has no attribute 'ndims'`
"
28353,mAP increase but classification loss increase,"mAP increase, but classification loss increase in object detection training. (by tensorflow object detection API)
Train data : 1500, Val data : 150
Class : 1 (drone)
I don't know what happen


![image](https://user-images.githubusercontent.com/3411873/57116734-11a21b00-6d92-11e9-867b-d10a7e16e52d.png)
"
28351,Enable eager execution fails after testing if executing eagerly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): pip install -q ""tensorflow==1.13.1""
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7.15rc1

**Describe the current behavior**

Calling `tf.enable_eager_execution()` after `tf.executing_eagerly()` fails with:

    ValueError: tf.enable_eager_execution must be called at program startup. 

Note that this happens *at* program startup (after runtime restart on Colab). Calling `tf.enable_eager_execution()` by itself does *not* result in the exception. Not to confuse with issue #21893

**Describe the expected behavior**

It should be possible to check whether tensorflow is executing eagerly before programs enable eager execution. This is relevant for forward compatibility with *tensorflow 2.0.0-alpha0* which does not implement enable_eager_execution() but instead reports:

    AttributeError: 'module' object has no attribute 'enable_eager_execution'

**Code to reproduce the issue**

    import tensorflow as tf
    if not tf.executing_eagerly():
        tf.enable_eager_execution()

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ValueErrorTraceback (most recent call last)
<ipython-input-1-a1e6f3444523> in <module>()
      7 
      8 if not tf.executing_eagerly():
----> 9   tf.enable_eager_execution()

1 frames
/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in enable_eager_execution(config, device_policy, execution_mode)
   5459         device_policy=device_policy,
   5460         execution_mode=execution_mode,
-> 5461         server_def=None)
   5462 
   5463 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in enable_eager_execution_internal(config, device_policy, execution_mode, server_def)
   5538   else:
   5539     raise ValueError(
-> 5540         ""tf.enable_eager_execution must be called at program startup."")
   5541 
   5542   # Monkey patch to get rid of an unnecessary conditional since the context is

ValueError: tf.enable_eager_execution must be called at program startup."
28347,Tensorflow lite example with alternate image source,"I would like an example similar to the TensorFlow lite Android example that doesn't use the camera but an alternate image source, like from network stream. Is there something available that has such an example?

"
28346,TrtGraphConverterV2 does not preserve output names in the signature_def,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  master from April 22nd
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.24
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.0 / 7.5.0
- GPU model and memory: GTX 1080 Ti

**Describe the current behavior**
If you use TrtGraphConverterV2 to convert a function in a saved_model to use TRT it does not preserve the output names in the signature_def of the saved model.

If the saved function (decorated with tf.function) returned a dict {'output_a': a, 'output_b': b} the names 'output_a' and 'output_b' are in the saved_model. After conversion with TrtGraphConverterV2 they are changed to the default names 'output_0' and 'output_1'.

**Describe the expected behavior**
The names of the outputs should not change. This breaks all code that loads the model and relies on the correct names.

**Code to reproduce the issue**
Take any saved_model that contains a function returning a dict.
Then run this:
```
conversion_params = trt_convert.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt_convert.TrtPrecisionMode.FP16, max_batch_size=1, max_workspace_size_bytes=8000000000)

trt_converter = trt_convert.TrtGraphConverterV2(input_saved_model_dir='your_saved_model', input_saved_model_signature_key='your_key', conversion_params=conversion_params)
trt_converter.convert()
trt_converter.save('your_saved_model')
```
Use saved_model_cli to inspect the saved_model."
28345,Segmentation fault in  tflite::ops::builtin::BuiltinOpResolver when using libtensorflowlite.so via CMake,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.6 LTS, Trusty Tahr
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.4.0
- Python version: 3.4.3
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source):c++ (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5

My goal is to build and run the examples in tensorflow/lite/ by using CMake on my Ubuntu 14.04 workstation

I am able to build and run the stock examples without any issues using bazel like this : 
bazel build --config opt --cxxopt=-std=c++11 //tensorflow/lite/examples/minimal:minimal

However, when I build **libtensorflowlite.so** like this : 
bazel build --config opt --cxxopt=-std=c++11 tensorflow/lite:libtensorflowlite.so
and then link the shared object in my CMakeLists.txt to run the same program, I am seeing a segmentation fault when the program exits. I have narrowed the minimal example down to this : 
```
#include <iostream>
#include ""tensorflow/lite/kernels/register.h""

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

using namespace tflite;

int main(int argc, char* argv[]) {

  tflite::ops::builtin::BuiltinOpResolver resolver;
  std::cout << ""DONE"" << std::endl;
  return 0;
}
```
GDB trace : 
```
Program received signal SIGSEGV, Segmentation fault.
std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, true> > >::_M_deallocate_nodes (this=0x7fffffffdb70, __n=0x5)
    at /usr/include/c++/6/bits/hashtable_policy.h:1984
1984		  _M_deallocate_node(__tmp);
(gdb) bt
#0  std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, true> > >::_M_deallocate_nodes (this=0x7fffffffdb70, __n=0x5)
    at /usr/include/c++/6/bits/hashtable_policy.h:1984
#1  std::_Hashtable<std::pair<std::string, int>, std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> >, std::__detail::_Select1st, std::equal_to<std::pair<std::string, int> >, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear (this=0x7fffffffdb70) at /usr/include/c++/6/bits/hashtable.h:1892
#2  std::_Hashtable<std::pair<std::string, int>, std::pair<std::pair<std::string, int> const, _TfLiteRegistration>, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> >, std::__detail::_Select1st, std::equal_to<std::pair<std::string, int> >, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::~_Hashtable (this=0x7fffffffdb70, __in_chrg=<optimized out>)
    at /usr/include/c++/6/bits/hashtable.h:1218
#3  0x00000000004012d0 in std::unordered_map<std::pair<std::string, int>, _TfLiteRegistration, tflite::op_resolver_hasher::OperatorKeyHasher<std::pair<std::string, int> >, std::equal_to<std::pair<std::string, int> >, std::allocator<std::pair<std::pair<std::string, int> const, _TfLiteRegistration> > >::~unordered_map (this=0x7fffffffdb70, 
    __in_chrg=<optimized out>) at /usr/include/c++/6/bits/unordered_map.h:98
#4  tflite::MutableOpResolver::~MutableOpResolver (this=0x7fffffffdb30, __in_chrg=<optimized out>)
    at ${PATH}/tensorflow/tensorflow/lite/mutable_op_resolver.h:55
#5  tflite::ops::builtin::BuiltinOpResolver::~BuiltinOpResolver (this=0x7fffffffdb30, __in_chrg=<optimized out>)
    at ${PATH}//tensorflow/tensorflow/lite/kernels/register.h:26
#6  main (argc=<optimized out>, argv=<optimized out>)
    at ${PATH}//applications/tflite/sample_minimal/impl/main_tflite_sample_minimal.cpp:134
```
This line : tflite::ops::builtin::BuiltinOpResolver resolver; is the cause of the segmentation fault when the function exits. Any ideas on what might be causing the segmentation fault"
28342,Is it possible to add a layer to an Estimator after training it?,"Hi all,

After training an estimator, we'd like to modify the model and add a custom layer. 

Is there a way to modify the estimator model that way?

Thanks!"
28341,Why `tf.reshape` function tries to evaluate the `shape` argument?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: 10
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem
When feeding images to a deep network, normally I resize all images to a fixed size (512*512 for example). So
there might be a tensor `input:0` of shape `(32, 512, 512, 3)`. Afterwards, I resize the tensor to shape `(N*H,W,C)`, i.e., `(32*512, 512, 3)`. To do this, I should write something like
```
shape = tf.shape(inputs)
reshaped = tf.reshape(inputs, (shape[0]*shape[1], shape[2], shape[3])) # Now the tensor has a known shape: (32*512, 512, 3)
```

Now, suppose the network is trained and I would like to make predictions. But note that during test time, the shape of data we want to feed into `input:0` might not be `(32, 512, 512, 3)`, since the batch size might vary(if we test images one by one, the batch size is 1) or the height/width might vary(if we do not want to resize images). So if I use `tf.import_graph_def()` to restore the graph and feed a image of a different size, the shape will be mismatched.

The main reason here might be invoking `tf.reshape` function, it will try to evaluate the `shape` argument, and since tensor `input:0` is of a fixed shape, the `shape` argument is easily evaluated, so the `reshaped` tensor is also of a fixed shape. If the function does not try to evaluate the `shape`, the target shape will be determined after the image is fed into the network, so everything will work fine.

So I would like to know why `tf.reshape` function tries to evaluate the `shape` argument. Is it a feature or not? If it is a feature, what should I do to fix the shape mismatch problem? Of course I can use the `input_map` argument in `tf.import_graph_def()` to change the shape of `input:0`, but it will not affect the shape of `reshape:0`.

"
28338,Improper metric computation in fit method (tf 2.0),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
- TensorFlow installed from (source or binary): from pip I think, I don't remember and I don't know the difference
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7

**Describe the current behavior**
When using the `fit` method on a custom model (the model in the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)), a simple `MeanSquaredError` metric seems to be improperly computed. When fitting, the metric is always greater than the loss, whereas the loss is computed as the mean squared error plus a custom error defined as a KL divergence in the model.

**Describe the expected behavior**
The mean squared error metric should always be lesser than the loss, as it is when training the model with custom code instead of using the `fit` method.

**Code to reproduce the issue**
 The following code is adapted from the guide [Writing layers and models with TensorFlow Keras](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example). I simply customized the formatting and added a metric to track the mean squared error in addition to the total loss.

On a side note, the code in [the guide](https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) seems to forget to use `reset_states` on the metric at the beginning of each epoch.

```
import tensorflow as tf
from tensorflow.keras import layers
from time import time

class Sampling(layers.Layer):
    # Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.

    def call(self, inputs, training=None):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # = z_mean + tf.sqrt(z_var) * epsilon


class Encoder(layers.Layer):
    # Maps MNIST digits to a triplet (z_mean, z_log_var, z).

    def __init__(self,
                 latent_dim=32,
                 intermediate_dim=64,
                 name='encoder',
                 **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
        self.dense_mean = layers.Dense(latent_dim)
        self.dense_log_var = layers.Dense(latent_dim)
        self.sampling = Sampling()

    def call(self, inputs, training=None):
        x = self.dense_proj(inputs)
        z_mean = self.dense_mean(x)
        z_log_var = self.dense_log_var(x)
        z = self.sampling((z_mean, z_log_var))
        return z_mean, z_log_var, z


class Decoder(layers.Layer):
    # Converts z, the encoded digit vector, back into a readable digit.

    def __init__(self,
                 original_dim,
                 intermediate_dim=64,
                 name='decoder',
                 **kwargs):
        super(Decoder, self).__init__(name=name, **kwargs)
        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
        self.dense_output = layers.Dense(original_dim, activation='sigmoid')

    def call(self, inputs, training=None):
        x = self.dense_proj(inputs)
        return self.dense_output(x)


class VariationalAutoEncoder(tf.keras.Model):
    # Combines the encoder and decoder into an end-to-end model for training.

    def __init__(self,
                 original_dim,
                 intermediate_dim=64,
                 latent_dim=32,
                 name='autoencoder',
                 **kwargs):
        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)
        self.original_dim = original_dim
        self.encoder = Encoder(latent_dim=latent_dim,
                               intermediate_dim=intermediate_dim)
        self.decoder = Decoder(original_dim=original_dim,
                               intermediate_dim=intermediate_dim)

    def call(self, inputs, training=None):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstructed = self.decoder(z)
        # Add KL divergence regularization loss.
        kl_loss = - 0.5 * tf.reduce_mean(
            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
        self.add_loss(kl_loss)
        return reconstructed


# Data preparation
original_dim = 28 * 28
(x_train, _), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, original_dim).astype('float32') / 255
train_dataset = tf.data.Dataset.from_tensor_slices(x_train)
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)

# Model creation
vae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
mse_loss_fn = tf.keras.losses.MeanSquaredError()
loss_metric = tf.keras.metrics.Mean()
mse_metric = tf.keras.metrics.Mean()

@tf.function
def train_step(inputs):
    with tf.GradientTape() as tape:
          reconstructed = vae(inputs)
          # Compute reconstruction loss
          mse_loss = mse_loss_fn(inputs, reconstructed)
          loss = mse_loss + tf.add_n(vae.losses)  # Add KLD regularization loss
    
    grads = tape.gradient(loss, vae.trainable_variables)
    optimizer.apply_gradients(zip(grads, vae.trainable_variables))
    # Update the metrics
    loss_metric.update_state(loss)
    mse_metric.update_state(mse_loss)

def train_over_epochs(train_dataset, epochs):
    # Iterate over epochs.
    for epoch in range(epochs):
        begin_time = time()
        print('Start of epoch {:d}'.format(epoch + 1))
        # Reset the metrics
        loss_metric.reset_states()
        mse_metric.reset_states()
    
        # Iterate over the batches of the dataset.
        for step, x_batch_train in enumerate(train_dataset):
            train_step(x_batch_train)
        
        print('    step {:3d}: mean total loss = {:f}'.format(step + 1, loss_metric.result().numpy()))
        print('              mean mse loss = {:f}'.format(mse_metric.result().numpy()))
        print('    time: {:.2f}'.format(time() - begin_time))
        
train_over_epochs(train_dataset, epochs=4)
```
The output of this code is something like this:
```
Start of epoch 1
    step 938: mean total loss = 0.074619
              mean mse loss = 0.074219
    time: 4.24
Start of epoch 2
    step 938: mean total loss = 0.067608
              mean mse loss = 0.067607
    time: 2.51
Start of epoch 3
    step 938: mean total loss = 0.067527
              mean mse loss = 0.067526
    time: 2.64
Start of epoch 4
    step 938: mean total loss = 0.067471
              mean mse loss = 0.067470
    time: 2.50

```
The mse loss is always smaller than the total loss which is expected.

Now if we run the following code:
```
train_for_fit_dataset=train_dataset.map(lambda x: (x,x)) #create labels equal to the data for fit method
vae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)
vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), 
           loss=tf.keras.losses.MeanSquaredError(),
           metrics=[tf.keras.metrics.MeanSquaredError()])
vae.fit(train_for_fit_dataset, epochs=4)
```
The output looks like this:
```
Epoch 1/4
938/938 [==============================] - 5s 5ms/step - loss: 0.0744 - mean_squared_error: 0.0923
Epoch 2/4
938/938 [==============================] - 4s 4ms/step - loss: 0.0676 - mean_squared_error: 0.0678
Epoch 3/4
938/938 [==============================] - 4s 4ms/step - loss: 0.0675 - mean_squared_error: 0.0677
Epoch 4/4
938/938 [==============================] - 4s 5ms/step - loss: 0.0675 - mean_squared_error: 0.0676
```
Note that the loss seems be computed as in the previous code, but the mean squared error is definitely not.

Either there is a subtlety in `fit` I am not aware of, either there is a bug.


"
28337,Distributed TensorFlow does not work when using more than 26 workers,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow version (use command below): TF Alpha 2.0 and TF. 1.13 CPU only
- Python version: 2.7
- Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space

I have a performance issue using distributed TF for CPU machine with MultiWorkerStrategy using 58 workers. However, it seems that it does not work as the following errors:

WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821069 47528963346944 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821974 47863205896704 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable

Typically, how many workers can be used for this scheme?  and how to solve this problem? Is it because I do not have sufficient memory?

"
28335,"UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.NativeInterpreterWrapper.allowBufferHandleOutput(long, boolean)","**System information**
- OS Platform and Distribution: **Linux 4.4.0-17134-Microsoft #706-Microsoft x86_64 GNU/Linux (Ubuntu 1804)**
- Mobile device: **Android, arm64**
- TensorFlow installed from (source or binary): **source *and* binary**
- TensorFlow version: **1.13.1**
- Bazel version: **0.24.1**
- GCC/Compiler version: **NDK r18, clang**

**Describe the problem**
Using the 'official' `implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'`, I tried

    opt.setAllowBufferHandleOutput(true)

I get 

> E/aoyi.run.tflit: No implementation found for void org.tensorflow.lite.NativeInterpreterWrapper.allowBufferHandleOutput(long, boolean) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput and Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput__JZ)

Same error reproduced when I build the **libtensorflowlite_jni.so** locally with **bazel**. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

To verify that this was a build issue, I run

    > nm -D jniLibs/arm64-v8a/libtensorflowlite_jni.so | grep allow
    0000000000010940 T Java_org_tensorflow_lite_NativeInterpreterWrapper_allowFp16PrecisionForFp32

**Analysis**

The root cause is that the build relies on [nativeinterpreterwrapper_jni.h](https://github.com/tensorflow/tensorflow/blob//61c6c84964b4aec80aeace187aab8cb2c3e55a72/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.h) to declare the JNI functions as `extern ""C""`. [`Java_org_tensorflow_lite_NativeInterpreterWrapper_allowBufferHandleOutput`](https://github.com/tensorflow/tensorflow/blame/master/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L252) was introduced in [18 Dec 2018 commit](https://github.com/tensorflow/tensorflow/commit/3856a33dc65e7aff1df7ee4e940479ef37e9934b) which did not update the `.h` file.

[**Proposed fix**](https://github.com/tensorflow/tensorflow/pull/28336)

The easy fix (tested here) is to tag the exported JNI functions as `extern ""C""` in the `.cc` file. Relying on the header file is not necessary and (as we witness) error-prone."
28334,Distributed Training TF2.0 Input Pipeline Bug,"System information

OS Platform and Distribution: Docker-nvidia(2.0.0a0-gpu-py3)
TensorFlow version: '2.0.0-alpha0'
Python version: 3.5.2
CUDA/cuDNN version: 10.0


Hello I'm trying the [Code](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) here.
Ant it come out **error info**.
```
WARNING: Logging before flag parsing goes to stderr.
W0502 07:45:49.047524 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
W0502 07:45:49.050312 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
W0502 07:45:49.052523 140435350275840 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
Epoch 1/10
      1/Unknown - 5s 5s/step - loss: 2.2896 - accuracy: 0.1406
W0502 07:45:57.873010 140435350275840 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.178479). Check your callbacks.
    468/Unknown - 9s 19ms/step - loss: 0.2455 - accuracy: 0.9304
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-d96263311d23> in <module>
----> 1 model.fit(train_dataset, epochs=10, callbacks=callbacks)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    744             steps_per_epoch=steps_per_epoch,
    745             validation_steps=validation_steps,
--> 746             validation_freq=validation_freq)
    747 
    748     batch_size = self._validate_or_infer_batch_size(

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
    129         validation_steps=validation_steps,
    130         validation_freq=validation_freq,
--> 131         steps_name='steps_per_epoch')
    132 
    133 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    260         try:
    261           # `ins` can be callable in DistributionStrategy + eager case.
--> 262           actual_inputs = ins() if callable(ins) else ins
    263           batch_outs = f(actual_inputs)
    264         except errors.OutOfRangeError:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in get_distributed_inputs()
    470     def get_distributed_inputs():
    471       return distributed_training_utils._prepare_feed_values(
--> 472           model, inputs, targets, sample_weights, mode)
    473 
    474     # In the eager case, we want to call the input method per step, so return

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _prepare_feed_values(model, inputs, targets, sample_weights, mode)
    565   """"""
    566   strategy = model._distribution_strategy
--> 567   inputs, targets, sample_weights = _get_input_from_iterator(inputs, model)
    568   inputs = flatten_perdevice_values(strategy, inputs)
    569   targets = flatten_perdevice_values(strategy, targets)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in _get_input_from_iterator(iterator, model)
    547   # Validate that all the elements in x and y are of the same type and shape.
    548   validate_distributed_dataset_inputs(
--> 549       model._distribution_strategy, x, y, sample_weights)
    550   return x, y, sample_weights
    551 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_distributed_dataset_inputs(distribution_strategy, x, y, sample_weights)
    247   # If each element of x and y are not tensors, we cannot standardize and
    248   # validate the input and targets.
--> 249   x_values_list = validate_per_device_inputs(distribution_strategy, x)
    250 
    251   if y is not None:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_per_device_inputs(distribution_strategy, x)
    294 
    295     # Validate that the shape and dtype of all the elements in x are the same.
--> 296     validate_all_tensor_shapes(x, x_values)
    297     validate_all_tensor_types(x, x_values)
    298 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in validate_all_tensor_shapes(x, x_values)
    315     if x_shape != x_values[i].get_shape().as_list():
    316       raise ValueError('Input tensor shapes do not match for distributed tensor'
--> 317                        ' inputs {}'.format(x))
    318 
    319 

ValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica:{
  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(
[[[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]
   ...
   [0.]
   [0.]
   [0.]]]], shape=(64, 28, 28, 1), dtype=float32),
  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(
[[[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]
   ...
   [0.]
   [0.]
   [0.]]]], shape=(32, 28, 28, 1), dtype=float32)
}
```

it because gpu0 and gpu1 can't get the same shape, 
so I modify Input pipeline, add `.repeat(2)` in the pipeline
```
train_dataset = mnist_train.repeat(2).map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
```
and it work!

but I think the Input pipeline should repeat by itself.
"
28333,Distributed Training TF2.0 validation_data Bug,"**System information**
- OS Platform and Distribution: Docker-nvidia(2.0.0a0-gpu-py3)
- TensorFlow version: '2.0.0-alpha0'
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0

Hello I'm trying the [Code](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb) here, 
and change the training code.

```
history = model_1.fit(train_data,
                      epochs=200, 
                      validation_data=valid_data,
                      callbacks=[model_cbk, model_esp])
```

And it come out the error, I think it should be validation_data bug:
```
WARNING: Logging before flag parsing goes to stderr.
W0502 06:49:43.398314 140660990936832 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
W0502 06:49:43.399307 140660990936832 distributed_training_utils.py:182] Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-16-74ec5f39e862> in <module>
      2                       epochs=200,
      3                       validation_data=valid_data,
----> 4                       callbacks=[model_cbk, model_esp])

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    744             steps_per_epoch=steps_per_epoch,
    745             validation_steps=validation_steps,
--> 746             validation_freq=validation_freq)
    747 
    748     batch_size = self._validate_or_infer_batch_size(

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
    129         validation_steps=validation_steps,
    130         validation_freq=validation_freq,
--> 131         steps_name='steps_per_epoch')
    132 
    133 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    139 
    140   if mode == ModeKeys.TRAIN:
--> 141     _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
    142 
    143   # Enter DistributionStrategy scope.

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
    437 def _print_train_info(inputs, val_inputs, steps_per_epoch, verbose):
    438   if (val_inputs and steps_per_epoch is None and verbose and inputs and
--> 439       hasattr(inputs[0], 'shape') and hasattr(val_inputs[0], 'shape')):
    440     print('Train on %d samples, validate on %d samples' %
    441           (inputs[0].shape[0], val_inputs[0].shape[0]))

TypeError: 'BatchDataset' object does not support indexing
```"
28332,Issue when running tensorflow model (ssd_inception_v2_coco.pd) in webcam....,"UnknownError                              Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]
	 [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1975_num_detections"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

UnknownError                              Traceback (most recent call last)
<ipython-input-25-05cfc96e122f> in <module>
     17       (boxes, scores, classes, num_detections) = sess.run(
     18           [boxes, scores, classes, num_detections],
---> 19           feed_dict={image_tensor: image_np_expanded})
     20       # Visualization of the results of a detection.
     21       vis_util.visualize_boxes_and_labels_on_image_array(

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at <ipython-input-22-0d8b8f2357e8>:7)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]
	 [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1975_num_detections"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d', defined at:
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelapp.py"", line 505, in start
    self.io_loop.start()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\platform\asyncio.py"", line 148, in start
    self.asyncio_loop.run_forever()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\asyncio\base_events.py"", line 427, in run_forever
    self._run_once()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\asyncio\base_events.py"", line 1440, in _run_once
    handle._run()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\asyncio\events.py"", line 145, in _run
    self._callback(*self._args)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\ioloop.py"", line 690, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\ioloop.py"", line 743, in _run_callback
    ret = callback()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\gen.py"", line 781, in inner
    self.run()
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\gen.py"", line 742, in run
    yielded = self.gen.send(value)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 357, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 267, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\kernelbase.py"", line 534, in execute_request
    user_expressions, allow_stdin,
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tornado\gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\ipkernel.py"", line 294, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\ipykernel\zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2848, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 2874, in _run_cell
    return runner(coro)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\async_helpers.py"", line 67, in _pseudo_sync_runner
    coro.send(None)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 3049, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 3214, in run_ast_nodes
    if (yield from self.run_code(code, result)):
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\IPython\core\interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-22-0d8b8f2357e8>"", line 7, in <module>
    tf.import_graph_def(od_graph_def, name='')
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3440, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3440, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3299, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""C:\Users\Admin\AppData\Local\conda\conda\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at <ipython-input-22-0d8b8f2357e8>:7)  = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights)]]
	 [[{{node num_detections/_195}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1975_num_detections"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


​
this is the issue I am getting when running in Webcam. How can I resolve it?

configuration:
tensorflow-gpu 1.12
cuda 9.0
cudnn 8.0
gpu : nvidia geforce gtx 1050  8gb
"
28331,Broken link in Doc [ api_docs/python/tf/test ],"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: TensorFlow Core 1.13
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/test


**Describe the documentation issue**
The link pointing to the testing guide is broken. 
broken link -  [https://tensorflow.org/api_guides/python/test](https://tensorflow.org/api_guides/python/test)

[A snapshot of the broken link, working as of 22nd Feb 2019](https://web.archive.org/web/20190222060703/https://tensorflow.org/api_guides/python/test)

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
28330,r1.14 with cuda 10.1 build failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0rc0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source):  7.4.0
- CUDA/cuDNN version: 10.1/7.5.2
- GPU model and memory: RTX2080Ti GDDR6 11GB



**Describe the problem**

build failed

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


external/protobuf_archive/src/google/protobuf/map.h:490:29: error: cannot call member function ‘bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]’ without object
         if (m_->TableEntryIsNonEmptyList(bucket_index_)) {
         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h:504:24: error: cannot call member function ‘bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]’ without object
         return m_->TableEntryIsList(bucket_index_);
         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
ERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:4124:1: output 'tensorflow/core/kernels/_objs/histogram_op_gpu/histogram_op_gpu.cu.pic.o' was not created
ERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:4124:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps."
28329,The source code compiles tensorflow off tensorrt is invalid.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version: master branch
- Python version:3.6
- Bazel version (if compiling from source):0.24
- CUDA/cuDNN version:10.0/7.5
- GPU model and memory:12G

When the source code compiles tensorflow, it closes TensorRT through the prompt of ./configure. But still reports: Could not find any NvInfer.h matching version '' in any subdirectory error.

Configure option：

./configure 
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is /home/guangyuan/anaconda3/envs/tfsource/bin/python]: 


Found possible Python library paths:
  /home/guangyuan/anaconda3/envs/tfsource/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/guangyuan/anaconda3/envs/tfsource/lib/python3.6/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

**Do you wish to build TensorFlow with TensorRT support? [y/N]: n**
No TensorRT support will be enabled for TensorFlow.

Error:
**Could not find any NvInfer.h matching version '' in any subdirectory:**
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/libx32'
        '/usr'
        '/usr/lib'
        '/usr/lib/x86_64-linux-gnu'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/lib32'
        '/usr/libx32'
        '/usr/local/cuda-10.0'
        '/usr/local/zlib/lib'
Asking for detailed CUDA configuration..."
28328,DLL Error - cuDNN I Believe?,"Issue occurs when running anything that uses TensorFlow, including a simple script that should simply spit out my TF Version and included sample files.
OS - Win10 Home
Cuda Toolkit - 10.0.130_411.31_win10
cuDNN - 10.0
Python - 3.6.8
Tensorflow - Pip install of current ""tensorflow-GPU"" as of 5-1-2019
GPU - EVGA RTX 2080 ti FTW3 Ultra Hybrid (11gb GDDR6 memory)

When running any .py that uses TensorFlow since setting up this new environment, I get the following error report. I've followed all the instructions very closely as I did when I set up a working Linux environment, just no luck on Win10. 

>>> 
 RESTART: E:\AI Playground\Samples\samples\cookbook\regression\automobile_data.py 
Traceback (most recent call last):
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\AI Playground\Samples\samples\cookbook\regression\automobile_data.py"", line 25, in <module>
    import tensorflow as tf
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\david\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
28327,Max retries error with BigtableClient,"For reference, pertains to [`tf.contrib.cloud.bigtable_api.py`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/bigtable/python/ops/bigtable_api.py) and `BigtableClient`.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 'v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

Dataset constructed by way of `tf.contrib.cloud.BigtableClient` raises error that is primarily the following:

```bash
InternalError (see above for traceback): Error reading from Cloud Bigtable: No more retries allowed as per policy.
	 [[node IteratorGetNext (defined at <ipython-input-1-b90b20ec4e01>:10) ]]
```

This occurs both in graph and eager mode; occurs on a machine where the conventional bigtable python client apis function properly (appropriately credentialed); persists after alternatively activating a service account with the appropriate credentials via `gcloud auth activate-service-account ...`.

**Describe the expected behavior**

Iterator yields training examples from Dataset when iterated over without error.

**Code to reproduce the issue**

```python
from pcml.operations.tfrecord2bigtable import BigTableSelection
import tensorflow as tf

data = BigTableSelection(
    project=""clarify"",
    instance=""clarify-cbt-instance"",
    table=""clarify-cbt-devtabl1"",
    prefix=""train"",
    column_family=""tfexample"",
    column_qualifier=""example"")

client = tf.contrib.cloud.BigtableClient(data.project, data.instance)

table = client.table(data.table)

ds = table.parallel_scan_prefix(
    data.prefix, columns=[(data.column_family, data.column_qualifier)])

ds_data = ds.map(lambda index, data: data)

ds_data = ds_data.repeat()

iterator = ds_data.make_initializable_iterator()

next_element = iterator.get_next()

sess = tf.Session()

sess.run(iterator.initializer)

_ = sess.run(next_element)

```

**Other info / logs**

```bash
​
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

INFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]
WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

InternalError: Error reading from Cloud Bigtable: No more retries allowed as per policy.
	 [[{{node IteratorGetNext}}]]

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
<ipython-input-1-acb694909ee3> in <module>
     29 
     30 sess.run(iterator.initializer)
---> 31 _ = sess.run(next_element)
     32 

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InternalError: Error reading from Cloud Bigtable: No more retries allowed as per policy.
	 [[node IteratorGetNext (defined at <ipython-input-1-acb694909ee3>:26) ]]

Caused by op 'IteratorGetNext', defined at:
  File ""/opt/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""/opt/conda/lib/python3.6/asyncio/base_events.py"", line 421, in run_forever
    self._run_once()
  File ""/opt/conda/lib/python3.6/asyncio/base_events.py"", line 1431, in _run_once
    handle._run()
  File ""/opt/conda/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2843, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2869, in _run_cell
    return runner(coro)
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 67, in _pseudo_sync_runner
    coro.send(None)
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3044, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3209, in run_ast_nodes
    if (yield from self.run_code(code, result)):
  File ""/home/jovyan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3291, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-acb694909ee3>"", line 26, in <module>
    next_element = iterator.get_next()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 414, in get_next
    output_shapes=self._structure._flat_shapes, name=name)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1685, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Error reading from Cloud Bigtable: No more retries allowed as per policy.
	 [[node IteratorGetNext (defined at <ipython-input-1-acb694909ee3>:26) ]]
```"
28326,"UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF 2.0 alpha0
- Python version: 3.6
- Bazel version (if compiling from source): I don't know
- GCC/Compiler version (if compiling from source): I don't know
- CUDA/cuDNN version: cudnn 7.3.1, Cuda 10.0.1
- GPU model and memory: Titan RTX 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I just built one conv2d layer and gave it a random input, that's all. Throws error above.
I'm using Anaconda. I installed tf-gpu 2.0 alpha0 via command line, cause Anaconda doesn't offer it.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
layer = tf.keras.layers.Conv2D(2, (3,3))
layer(np.random.rand(5,5,3,1))


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-4-7d2f5dcef289> in <module>
----> 1 layer(np.random.rand(5,5,3,1))

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    658           with base_layer_utils.autocast_context_manager(
    659               input_list, self._mixed_precision_policy.should_cast_variables):
--> 660             outputs = self.call(inputs, *args, **kwargs)
    661           self._handle_activity_regularization(inputs, outputs)
    662           self._set_mask_metadata(inputs, outputs, previous_mask)

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\keras\layers\convolutional.py in call(self, inputs)
    194 
    195   def call(self, inputs):
--> 196     outputs = self._convolution_op(inputs, self.kernel)
    197 
    198     if self.use_bias:

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\nn_ops.py in __call__(self, inp, filter)
   1076 
   1077   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
-> 1078     return self.conv_op(inp, filter)
   1079 
   1080 

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\nn_ops.py in __call__(self, inp, filter)
    632 
    633   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin
--> 634     return self.call(inp, filter)
    635 
    636 

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\nn_ops.py in __call__(self, inp, filter)
    231         padding=self.padding,
    232         data_format=self.data_format,
--> 233         name=self.name)
    234 
    235 

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)
   1949                            data_format=data_format,
   1950                            dilations=dilations,
-> 1951                            name=name)
   1952 
   1953 

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)
   1119             input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,
   1120             padding=padding, explicit_paddings=explicit_paddings,
-> 1121             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)
   1122       except _core._SymbolicException:
   1123         pass  # Add nodes to the TensorFlow graph.

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py in conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)
   1218   explicit_paddings, ""data_format"", data_format, ""dilations"", dilations)
   1219   _result = _execute.execute(b""Conv2D"", 1, inputs=_inputs_flat, attrs=_attrs,
-> 1220                              ctx=_ctx, name=name)
   1221   _execute.record_gradient(
   1222       ""Conv2D"", _inputs_flat, _attrs, _result, name)

~\.conda\envs\alphagpu\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~\.conda\envs\alphagpu\lib\site-packages\six.py in raise_from(value, from_value)

UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]
"
28324,Random Uniform Not Supported for TFlite Conversion,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): tf-nightly 1.14.0-rc0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GREATER_EQUAL, MUL, RESHAPE, RESIZE_BILINEAR, SPACE_TO_BATCH_ND. Here is a list of operators for which you will need custom implementations: RandomUniform.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

I followed [#26300](https://github.com/tensorflow/tensorflow/issues/26300) by using
```
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                        tf.lite.OpsSet.SELECT_TF_OPS]
```
But I still can't create .tflite file correctly."
28323,Input-shape validation error when using TimeDistributed wrapper,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I have built a model consisting of a CNN feeding an LSTM. I've wrapped the CNN in a TimeDistributed wrapper. I've configured my training set (x values) to have a 5D shape (batch_size, num_steps, image_height, image_width, channels). When trying to train the model, I get the following error: `ValueError: Error when checking input: expected time_distributed_input to have 5 dimensions, but got array with shape (4, 28, 28, 1)`.

I've tried a number of different model configurations, and have debugged through the code. I can see where the ValueError is being thrown, but I don't see any way to keep this from happening. Code details follow.

I'm not expecting a discrepancy between what the model thinks I'm providing and what I'm providing. I'm providing a 5D data set, and that's what it wants, but it claims that I'm only providing a 4D data set.

Here is the code I'm using. For the input data, I'm using MNIST images in 28x28x1 format. For the `x` data, I'm creating 20 records, each consisting of a list of four 28x28x1 image vectors, hence an input shape of (20,4,28,28,1). The `y` data consists of a 1D list of labels. 
```
def build_cnn(input_shape=None):
    print('Building the CNN')
    model = tfkm.Sequential()
    if input_shape:
        model.add(tfkl.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))
    else:
        model.add(tfkl.Conv2D(64, (3, 3), activation='relu'))
    model.add(tfkl.MaxPooling2D((2, 2), strides=(1, 1)))
    model.add(tfkl.Conv2D(128, (4, 4), activation='relu'))
    model.add(tfkl.MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(tfkl.Conv2D(256, (4, 4), activation='relu'))
    model.add(tfkl.MaxPooling2D((2, 2), strides=(2, 2)))

    # extract features and dropout
    model.add(tfkl.Flatten())
    return model


def build_lstm(units, return_sequences, dropout):
    print('Building the LSTM model')
    model = tfkm.Sequential()
    model.add(tfkl.LSTM(units, return_sequences=return_sequences, dropout=dropout))
    return model


def build_classification_layer(num_classes):
    print('Building the classification-layer model')
    model = tfkm.Sequential()
    # classifier with sigmoid activation for multilabel
    model.add(tfkl.Dense(num_classes, activation='sigmoid'))
    return model


cnn = build_cnn(input_shape=(28, 28, 1))
lstm = build_lstm(256, False, 0.5)
classification_layer = build_classification_layer(2)


combined_model = tfkm.Sequential()
#combined_model.add(tfkl.TimeDistributed(cnn, batch_input_shape=(20, 4, 28, 28, 1)))
#combined_model.add(tfkl.TimeDistributed(cnn, input_shape=(4, 28, 28, 1), batch_size=20))
combined_model.add(tfkl.TimeDistributed(cnn, input_shape=(4, 28, 28, 1)))
combined_model.add(lstm)
combined_model.add(classification_layer)
combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

.
.
.
data-set building:  creating 20 individual records, each containing a sequence of 4 28x28x1 images and the associated labels list.
.
.
.
print('train_input shape: {}  train_labels shape: {}'.format(np.array(train_input).shape, np.array(train_labels).shape))

print('Training the model')
combined_model.fit(train_input, train_labels, epochs=2)
```

Output from training attempt:
```
train_input shape: (20, 4, 28, 28, 1)  train_labels shape: (20,)
Training the model
Traceback (most recent call last):
  File ""/Users/scott/sandbox/CNN-LSTM-sandbox/build_and_train.py"", line 184, in <module>
    combined_model.fit(train_input, train_labels, epochs=2)
  File ""/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 806, in fit
    shuffle=shuffle)
  File ""/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2596, in _standardize_user_data
    exception_prefix='input')
  File ""/anaconda3/envs/TF2_py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 340, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking input: expected time_distributed_input to have 5 dimensions, but got array with shape (4, 28, 28, 1)

Process finished with exit code 1
```

Note that I get this same result regardless of which of the three `combined_model.add(tfkl.TimeDistributed(cnn...` versions I have in my code.

While debugging, I found the place where the ValueError is thrown. It is in `training_utils.py`, line 336 - `if len(data_shape) != len(shape):` (lines 303-350 shown for context):

lines 303-350 of training_utils.py:
```
  if len(data) != len(names):
    if data and hasattr(data[0], 'shape'):
      raise ValueError('Error when checking model ' + exception_prefix +
                       ': the list of Numpy arrays that you are passing to '
                       'your model is not the size the model expected. '
                       'Expected to see ' + str(len(names)) + ' array(s), '
                       'but instead got the following list of ' +
                       str(len(data)) + ' arrays: ' + str(data)[:200] + '...')
    elif len(names) > 1:
      raise ValueError('Error when checking model ' + exception_prefix +
                       ': you are passing a list as input to your model, '
                       'but the model expects a list of ' + str(len(names)) +
                       ' Numpy arrays instead. The list you passed was: ' +
                       str(data)[:200])
    elif len(data) == 1 and not hasattr(data[0], 'shape'):
      raise TypeError('Error when checking model ' + exception_prefix +
                      ': data should be a Numpy array, or list/dict of '
                      'Numpy arrays. Found: ' + str(data)[:200] + '...')
    elif len(names) == 1:
      data = [np.asarray(data)]

  # Check shapes compatibility.
  if shapes:
    for i in range(len(names)):
      if shapes[i] is not None:
        if tensor_util.is_tensor(data[i]):
          tensorshape = data[i].get_shape()
          if not tensorshape:
            continue
          data_shape = tuple(tensorshape.as_list())
        else:
          data_shape = data[i].shape
        shape = shapes[i]
        if len(data_shape) != len(shape):
          raise ValueError('Error when checking ' + exception_prefix +
                           ': expected ' + names[i] + ' to have ' +
                           str(len(shape)) + ' dimensions, but got array '
                           'with shape ' + str(data_shape))
        if not check_batch_axis:
          data_shape = data_shape[1:]
          shape = shape[1:]
        for dim, ref_dim in zip(data_shape, shape):
          if ref_dim != dim and ref_dim is not None and dim is not None:
            raise ValueError('Error when checking ' + exception_prefix +
                             ': expected ' + names[i] + ' to have shape ' +
                             str(shape) + ' but got array with shape ' +
                             str(data_shape))
  return data
```
the state of things when the debugger hits line 336 is:
```
np.array(data).shape = <class 'tuple'>: (1, 4, 28, 28, 1)
data_shape = <class 'tuple'>: (4, 28, 28, 1)
name = <class 'list'>: ['time_distributed_input']
shapes = <class 'list'>: [(None, 4, 28, 28, 1)]
```
From this, one can see that the data shape getting to this point is correct, but the code pulls out the the first record in the batch and compares its shape to the shape the TimeDistributed layer is expecting. The problem is that the TimeDistributed layer is expecting a batch representation in the shape (`None`, 4, 28, 28, 1) but it's not there b/c the code only pulled the input record at that batch location `data_shape = data[i].shape`, that is, the code is using the `(4,28,28,1)` record's shape to compare with the TimeDistributed input shape `(None,4,28,28,1)` which can never succeed
![Screen Shot 2019-05-01 at 2 50 51 PM](https://user-images.githubusercontent.com/16819138/57046609-56ad3c80-6c2e-11e9-9073-51ed7631d526.png)
![Screen Shot 2019-05-01 at 2 42 13 PM](https://user-images.githubusercontent.com/16819138/57046632-6dec2a00-6c2e-11e9-9c3b-eafe53f0b694.png)

.

The way I see it, the batch dimension shouldn't be represented in the tuple that is used to establish expected shape of data to the InputLayer, the batch length comparison needs to ignore `None` in the shape tuples, or the comparison should be `if len(np.array(data).shape) != len(shape):`
"
28318,[ TF 2.0 ] tf.function throws exception,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): pip install tensorflow-gpu==2.0.0-alpha0
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10/cuDNN 7.5
- GPU model and memory:  GeForce GTX 980 4G

**Describe the current behavior**
I ported Deep Deterministic Policy Gradient algorithm to tf-2.0 and it works fine when I run the function with
`with tf.device(""/gpu:0""):` but fails when I try to wrap the function with tf.function

**Code to reproduce the issue**
[Code Repo](https://github.com/Jonathan-Livingston-Seagull/DDPG)
**Other info / logs**
```
/miniconda3/envs/rl/bin/python /media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/pendulam_train_script.py
WARNING: Logging before flag parsing goes to stderr.
W0501 21:58:48.871961 139821025609536 tf_logging.py:161] Entity <bound method RoboschoolMujocoXmlEnv.reset of <roboschool.gym_pendulums.RoboschoolInvertedPendulum object at 0x7f2a9ccfe780>> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <bound method RoboschoolMujocoXmlEnv.reset of <roboschool.gym_pendulums.RoboschoolInvertedPendulum object at 0x7f2a9ccfe780>>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: node <gast.gast.Attribute object at 0x7f2a2043dcf8> has ctx unset
Traceback (most recent call last):
  File ""/media/seagull/use_me/seagull/github/DDPG/ddpg-pendulam/pendulam_train_script.py"", line 39, in <module>
    scores = ddpg()
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 426, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1580, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1512, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 694, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 317, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 686, in wrapper
    ), args, kwargs)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmp0cbzqvh_.py"", line 51, in tf__ddpg
    ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1, n_episodes + 1), {}), None, loop_body_1, ())
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 81, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, init_state)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 90, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmp0cbzqvh_.py"", line 37, in loop_body_1
    state, score, break__1 = ag__.for_stmt(ag__.converted_call(range, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (max_t,), {}), extra_test, loop_body, (state, score, break__1))
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 81, in for_stmt
    return _py_for_stmt(iter_, extra_test, body, init_state)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 90, in _py_for_stmt
    state = body(target, *state)
  File ""/tmp/tmp0cbzqvh_.py"", line 21, in loop_body
    action = ag__.converted_call('act', agent, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), ([state_1],), {})[0]
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 392, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpsruok5vd.py"", line 6, in tf__act
    action = ag__.converted_call('actor_local', self, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (tf.convert_to_tensor(state),), {})
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1108, in convert_to_tensor_v2
    as_ref=False)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1186, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 304, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 245, in constant
    allow_broadcast=True)
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/miniconda3/envs/rl/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 476, in make_tensor_proto
    _GetDenseDimensions(values)))
ValueError: Argument must be a dense tensor: [array([0.        , 0.        , 0.99759441, 0.06932093, 0.        ])] - got shape [1, 5], but wanted [1].
```"
28317,XLA missing [ op: Unique ] for most optimizers,"Unable to train with anything but sgd using xla, due to a missing operation.

line 76 in https://github.com/tensorflow/tensorflow/blob/677a14872f028e9be51bd469ec2d5cc43ea9155e/tensorflow/python/training/optimizer.py
```
def _deduplicate_indexed_slices(values, indices):
  """"""Sums `values` associated with any non-unique `indices`.
  Args:
    values: A `Tensor` with rank >= 1.
    indices: A one-dimensional integer `Tensor`, indexing into the first
      dimension of `values` (as in an IndexedSlices object).
  Returns:
    A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a
    de-duplicated version of `indices` and `summed_values` contains the sum of
    `values` slices associated with each unique index.
  """"""
  unique_indices, new_index_positions = array_ops.unique(indices) <------- MISSING
  summed_values = math_ops.unsorted_segment_sum(
      values, new_index_positions,
      array_ops.shape(unique_indices)[0])
  return (summed_values, unique_indices)
```

"
28316,"autograph throwing ""could not get source code"" on recent nightlies","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux fedora 29
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): e.g. , 2.0.0-dev20190501
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

On recent nightly builds - v2 as well as v1 - the following fails:

```
# in p.py
@tf.function
def add(a,b): return(a+b)
```

```
# called in shell
In [2]: import tensorflow as tf                                                                                                                                                                                    

In [3]: exec(open(""p.py"").read()) 
   ...: add(1,2)  
```

Error stack:

```
2019-05-01 20:36:06.363503: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-01 20:36:06.401892: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz
2019-05-01 20:36:06.402171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560bd15a4920 executing computations on platform Host. Devices:
2019-05-01 20:36:06.402190: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Converted call: <function add at 0x7f56080b1400> 
    args: (1, 2)
    kwargs: {}

Entity <function add at 0x7f56080b1400> is not cached for key <code object add at 0x7f55ed2e68a0, file ""<string>"", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f55ed284978>, frozenset())
Converting <function add at 0x7f56080b1400>
Error transforming <function add at 0x7f56080b1400>
Traceback (most recent call last):
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py"", line 53, in parse_entity
    source = inspect_utils.getimmediatesource(entity)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py"", line 124, in getimmediatesource
    lines, lnum = inspect.findsource(obj)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/inspect.py"", line 786, in findsource
    raise OSError('could not get source code')
OSError: could not get source code

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 440, in to_graph
    return conversion.convert(entity, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 309, in convert
    entity, program_ctx, free_nonglobal_var_names)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 232, in _convert_with_cache
    entity, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 440, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 603, in convert_func_to_ast
    node, source = parser.parse_entity(f, future_features=future_features)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py"", line 61, in parse_entity
    ' @tf.autograph.do_not_convert. Original error: {}'.format(entity, e))
ValueError: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
Error transforming entity <function add at 0x7f56080b1400>
Traceback (most recent call last):
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py"", line 53, in parse_entity
    source = inspect_utils.getimmediatesource(entity)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py"", line 124, in getimmediatesource
    lines, lnum = inspect.findsource(obj)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/inspect.py"", line 786, in findsource
    raise OSError('could not get source code')
OSError: could not get source code

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 440, in to_graph
    return conversion.convert(entity, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 309, in convert
    entity, program_ctx, free_nonglobal_var_names)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 232, in _convert_with_cache
    entity, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 440, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 603, in convert_func_to_ast
    node, source = parser.parse_entity(f, future_features=future_features)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py"", line 61, in parse_entity
    ' @tf.autograph.do_not_convert. Original error: {}'.format(entity, e))
ValueError: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 325, in converted_call
    experimental_optional_features=options.optional_features)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 442, in to_graph
    errors.report_internal_error(entity, e)
  File ""/home/key/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/errors.py"", line 99, in report_internal_error
    'report. Caused by: %s' % (entity, exception))
tensorflow.python.autograph.pyct.errors.AutoGraphError: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
WARNING: Logging before flag parsing goes to stderr.
W0501 20:36:06.408588 140007718111040 ag_logging.py:145] Entity <function add at 0x7f56080b1400> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
WARNING: Entity <function add at 0x7f56080b1400> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function add at 0x7f56080b1400>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output when filing the bug report. Caused by: Unable to locate the source code of <function add at 0x7f56080b1400>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code

```

Thanks!"
28314,[TF 2.0 API Docs] tf.image.extract_image_patches,"### Existing URLs containing the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/extract_image_patches

### Description of issue (what needs changing):

- #### Clear Description
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Only a single sentence description is provided.

- #### Usage Example
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No usage example is provided.

- #### Raises Listed and Defined
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Errors are not defined.

- #### Visuals, if Applicable
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No visuals are included.
"
28313,"3D CNN with tf.nn.conv3d_transpose on CPU: 50-100 GB RAM free but segfault + ""terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 (three machines) & Windows 10 Pro (two machines)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary; tensorflow-gpu2.0.0alpha0 (Windows machines), tensorflow/tensorflow:2.0.0a0-py3-jupyter docker image (Ubuntu 18.04 machines)**
- TensorFlow version (use command below): **v1.12.0-9492-g2c319fb415 2.0.0-alpha0 (Windows)**
- Python version: **3.6.4 (Windows)**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **10/7.4.1 (Windows); N/A (Ubuntu 18.04)**
- GPU model and memory: **Titan RTX (Windows machine 1); GTX 1080 (Windows machine 2); CPU-only (Ubuntu 18.04 machines 1-3)**

All machines specs:
- CPUs: **Dual socket, 8 cores each CPU (Xeon E5-2670, E5-2687w on the machine with the Titan RTX), 32 total logical processors.** 
- RAM: **128 GB**  

**Describe the current behavior**

3D CNN with deconvolution (tf.nn.conv3d_transpose) to upsample 1 channel volumes x2 in all three dimensions. Training on GPU and using 64^3 image patches works fine. For inference I moved ops to the CPU in order to handle large volumes that wouldn't otherwise fit in GPU memory. But now I have run into the below errors:

1. Frequent crashes when using more than 50% of available CPU cores. On Ubuntu 18.04 it completely kills the node and I have to physically power cycle the machine. On Windows, I'm lucky if it doesn't freeze indefinitely at a blue screen. 

2. Visual bug for inputs >~130^3 (generally whenever it gives me a warning that I'm allocating >10% of system memory) where the last slices of the volume converge to intensity ~ 0.39 (coincidentally, very close to a constant that the model adds to the output to correct for grayscale intensity). This get progressively worse with larger volumes (~130^3 < dims < ~175^3)

3. Segfault for inputs > ~175^3. 
Example output: 

**On Windows:** 
2019-04-26 14:56:19.461143: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.
2019-04-26 14:58:23.532021: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.
2019-04-26 14:58:23.532021: W tensorflow/core/framework/allocator.cc:124] Allocation of 20320616448 exceeds 10% of system memory.
Segmentation Fault

Note: It once gave me a popup with an error of the form, ""Instruction at 0xDED referenced memory at 0XBEEF. The memory could not be read"".   

**On Ubuntu 18.04: ** 
2019-05-01 02:29:47.571830: W tensorflow/core/framework/allocator.cc:116] Allocation of 26311577600 exceeds 10% of system memory.
terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc

**Note:** I generally still have 50-100 GB of free memory when these errors occur so it's not for lack of memory. 
**Note:**: I have replicated this phenomenon across all machines using just CPU. On the Windows machines, I get the same results when I have attempted to move all ops to the GPU save for deconvolution (which will not fit on the GPU).

**Describe the expected behavior**
No crashing, processing large inputs if available resources allow. 


**Code to reproduce the issue**
N/A

**Other info / logs**
N/A
"
28311,tf2.0 get numpy from tensor,"
How to get the numpy from tensor? it says there is no numpy method. as far as I know, tf2.0 does not encourage users using session anymore, so how to get the acutally value from a tensor?

```python

import tensorflow as tf
import tensorflow_datasets as tfds


dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)
train_dataset, test_dataset = dataset['train'], dataset['test']

train_dataset = train_dataset.shuffle(100).batch(12).repeat()

for img, label in train_dataset.take(1):
    img = img.numpy()
    print(img.shape)
    print(img)
    

```

error:

```
 img = img.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
```"
28310,tf.hessians fails when applied to the output of a RNN,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary: conda install tensorflow
- **TensorFlow version (use command below)**: 1.13.1
- **Python version**: both 3.6.8 and 3.7.3
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: Not used
- **Exact command to reproduce**: none

### Describe the problem
I need to compute the hessians of a **tensor** output from a **RNN** structure w.r.t to its inputs. However, this leads to an error reported in the joint file. I think it is a bug as just replacing ``tf.hessians`` by ``tf.gradients`` leads to a working code.

I think it may be due to an low-level implementation of RNN cells with ``tf.while_loop``s, as indicated by the error.

### Source code / logs
```
import tensorflow as tf
from tensorflow.keras.layers import RNN, GRUCell, Dense

# Define size of variable. TODO: adapt to data
inp_dim = 2
num_units = 50
batch_size = 100
timesteps = 10

# Reset the graph, so as to avoid errors
tf.reset_default_graph()

inputs = tf.ones(shape=(timesteps, batch_size, inp_dim))

### Building the model
cells = [GRUCell(num_units), GRUCell(num_units)]
rnn = RNN(cells, time_major=True, return_sequences=True)
final_layer = Dense(1, input_shape=(num_units,))

# Apply to inputs
last_state = rnn(inputs)
f = final_layer(last_state)

[derivs] = tf.hessians(f, inputs)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    grads = sess.run(derivs)
```
[stacktrace.log](https://github.com/tensorflow/tensorflow/files/3134827/stacktrace.log)
"
28309,Errors in tensorflow/stream_executor/gpu/gpu_timer.cc while compiling on Windows with XLA enabled,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: checkout from `master` branch
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda 4.6.14
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27030.1 for x64, Visual Studio Build Tools 2017
- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0
- GPU model and memory: GTX 1060 6GB

**Describe the problem**
```
tensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'
tensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2660: 'CreateEventA': function does not take 3 arguments
tensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'
tensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2660: 'CreateEventA': function does not take 3 arguments
```

In tensorflow/stream_executor/gpu/gpu_timer.cc(29) the function is `CreateEvent`:

```c++
29  port::Status status = GpuDriver::CreateEvent(context, &start_event_,
30                                               GpuDriver::EventFlags::kDefault);
```

But compiler looks for `CreateEventA`. Maybe there is a Windows macro conflicting with that symbol.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
(neural) Stepii@STEPII D:\Neural\tensorflow
$ python configure.py
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is D:\Programs\Anaconda3\envs\neural\python.exe]:


Found possible Python library paths:
  D:\Programs\Anaconda3\envs\neural\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Programs\Anaconda3\envs\neural\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(neural) Stepii@STEPII D:\Neural\tensorflow
$ bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package -j 12

[...]

ERROR: D:/neural/tensorflow/tensorflow/stream_executor/gpu/BUILD:168:1: C++ compilation of rule '//tensorflow/stream_executor/gpu:gpu_timer' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019A26nPRjStR25RPVWFZL5iMn5PgXYVtDTages;C:\Program Files (x86)\Microsoft Visual Studio\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVindow;C:\Program Files (x86)\Microsoft Visual Studio\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVx9R2xvWau\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTools\\x64;C:\Program Files (x86)\Microsoft Visual SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTools\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJVx9R2xvWaun;C:\Program Files (x86)\Microsoft Visual Studio\2019A26nPRe3js3W69CrGF8kKXvvmYtT4zNGqicXRjvuAnmmbvPZXu5yRJV\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/neural/python.exe
    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/neural/lib/site-packages
    SET TEMP=C:\Users\Stepii\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
    SET TMP=C:\Users\Stepii\AppData\Local\Temp
  D:/Programs/Anaconda3/envs/neural/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SXgi164MCGG3u5RKXaBGVBabmxQTVRAAVE5tDTGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/stream_executor/gpu/_objs/gpu_timer/gpu_timer.o /c tensorflow/stream_executor/gpu/gpu_timer.cc
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'
.\tensorflow/stream_executor/gpu/gpu_driver.h(60): note: see declaration of 'stream_executor::gpu::GpuDriver'
tensorflow/stream_executor/gpu/gpu_timer.cc(29): error C2660: 'CreateEventA': function does not take 3 arguments
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\synchapi.h(469): note: see declaration of 'CreateEventA'
tensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2039: 'CreateEventA': is not a member of 'stream_executor::gpu::GpuDriver'
.\tensorflow/stream_executor/gpu/gpu_driver.h(60): note: see declaration of 'stream_executor::gpu::GpuDriver'
tensorflow/stream_executor/gpu/gpu_timer.cc(36): error C2660: 'CreateEventA': function does not take 3 arguments
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\synchapi.h(469): note: see declaration of 'CreateEventA'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 679.504s, Critical Path: 262.52s
INFO: 1869 processes: 1869 local.
FAILED: Build did NOT complete successfully
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[FullLog.txt](https://github.com/tensorflow/tensorflow/files/3134823/FullLog.txt)
"
28308,Installation gives me error,"I am trying to install tensorflow with a pip in a virualvenv. Python version 3.7 running on windows 10. Error: (venv) C:\Users\rojto\PythonCLI>pip install --upgrade tensorflow
Collecting tensorflow
  ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow"
28307,"Tensorflow classification call ""created device"" multiple times using c++","I was doing image classification using tensorflow c++. My codes are as below,
Status run_status = session->Run({ { input_layer, resized_tensor },{ input_layer2, b } ,{ input_layer1, a } }, { output_layer }, {}, &outputs);
	if (!run_status.ok()) {
		LOG(ERROR) << ""Running model failed: "" << run_status;
		return -1;
	}
The first time to call the Run() function, The output is as follows,
2019-05-01 18:02:36.575459: I D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3039 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-05-01 18:02:36.620825: I D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2019-05-01 18:02:36.624719: I D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-01 18:02:36.628097: I D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2019-05-01 18:02:36.631864: I D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
When I call the Run function again, It has the same message,
D:\projects\c++\tensorflow_GPU\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3039 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)

How to avoid tensorflow calling ""created Tensorflow device"" one more time after call Run() again? Will it cause speed slower?

Please help me. Thank you in advance



"
28306,[tflite] CONV_2D with bias tensor index -1 causes type-mismatch RuntimeError,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have created `.tflite` with single `CONV_2D` op with bias tensor index `-1`(= optional).


When supplying bias tensor `-1`(for making bias optional), it raises RuntimeError when running it in Python interpreter as shown the below(or seg fault/asan error in C++ interpreter) 

Supplying zero-valued bias tensor(i.e. use valid tensor id for bias) passes `AllocateTensors` and `Invoke` without any problem.

FYI, we cannot omit bias tensor(i.e, `len(inputs) = 2`) since in such case we'll hit another assertion:

https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/kernels/conv.cc#L237 

 
**Describe the expected behavior**

As described in tflite document:

https://www.tensorflow.org/lite/guide/ops_compatibility

`CONV_2D` should accept optional bias tensor and does not raise RuntimeError.

**Code to reproduce the issue**

Simply run python interepreter with attached `.tfite` file.

```
import sys

import numpy as np

import tensorflow
print(tensorflow.__version__)
from tensorflow.lite.python import interpreter as interpreter_wrapper

interpreter = interpreter_wrapper.Interpreter(model_path=sys.argv[1])
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(input_details)
print(output_details)
```

[conv2d_bias_optional.tflite.zip](https://github.com/tensorflow/tensorflow/files/3134449/conv2d_bias_optional.tflite.zip)

**Other info / logs**

```
Traceback (most recent call last):
  File ""bora.py"", line 10, in <module>
    interpreter.allocate_tensors()
  File ""/home/syoyo/miniconda3/envs/onnx-chainer/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 73, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/home/syoyo/miniconda3/envs/onnx-chainer/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/conv.cc:247 bias->type != input_type (0 != 1)Node number 0 (CONV_2D) failed to prepare.
```

"
28305,tensorflow support for multiple GPUs,"
![Screenshot from 2019-05-01 17-05-24](https://user-images.githubusercontent.com/42462408/57015613-56fc0680-6c33-11e9-98c7-f5cee6d6062b.png)


Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: CUSTOM CODE
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: UBUNTU 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: SOURCE
- **TensorFlow version (use command below)**: 1.13.1
- **Python version**: 3.6.7
- **Bazel version (if compiling from source)**: 0.19.2
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA 10.1, cuDNN 7.5.1
- **GPU model and memory**: 2 RTX 2080
- **Exact command to reproduce**: 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.


tensorflow and keras not able to support multiple GPUs


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Source  Code

def train_model(base_path):
    start = time.time()
    print(""***start training***"")
    if not os.path.exists(base_path_xcep_mod):
        os.makedirs(base_path_xcep_mod)
        
        


    log_file_path = base_path + '_emotion_training.log'
    #csv_logger = CSVLogger(base_path +""log.csv"", append=False, separator=',')
    early_stop = EarlyStopping('val_loss', patience=patience)
    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)
    trained_models_path = base_path + '_mini_XCEPTION'
    model_names = trained_models_path + '.{epoch:02d}.hdf5'
    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)
    tbCallBack = TensorBoard(log_dir=base_path +""log"", histogram_freq=0, 
                                             write_graph=True, write_images=True)
    callbacks = [model_checkpoint, early_stop, reduce_lr, tbCallBack]

    with tf.device(""/cpu:0""):
        model = cnn_five_fc_two_two()

    #model = multi_gpu_model(cnn_five_fc_two_two(), gpus=2)
    model = multi_gpu_model( model, G )

    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])

    



    model.fit_generator(data_generator.flow(xtrain, ytrain,(batch_size * G)),
	    steps_per_epoch=len(xtrain) // (batch_size * G),
	    epochs=num_epochs,
	    callbacks=callbacks, verbose=2,
            validation_data=(xtest, ytest)
                       )


    end = time.time()
    print(""time taken"", end-start)

Logs


Use tf.cast instead.
Epoch 1/2
2019-05-01 17:05:17.176287: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-05-01 17:05:18.149549: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-01 17:05:18.747678: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-01 17:05:18.748678: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-05-01 17:05:18.753580: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""ED_cnn_five_fc_two_two.py"", line 215, in <module>
    train_model(base_path_xcep_mod)
  File ""ED_cnn_five_fc_two_two.py"", line 203, in train_model
    validation_data=(xtest, ytest)
  File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py"", line 217, in fit_generator
    class_weight=class_weight)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 1217, in train_on_batch
    outputs = self.train_function(ins)
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 2715, in __call__
    return self._call(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File ""/home/aditi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/home/aditi/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node replica_0/sequential_1/conv2d_1/convolution}}]]

"
28304,Error .\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131 while building,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Not mobile device**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **r1.13**
- Python version: **3.6.8**
- Installed using virtualenv? pip? conda?: **virtualenv**
- Bazel version (if compiling from source):  **0.21.0**
- GCC/Compiler version (if compiling from source): **Оптимизирующий компилятор Microsoft (R) C/C++ версии 19.00.24234.1 для x64**
- CUDA/cuDNN version: **No CUDA**
- GPU model and memory: **No GPU**



**Describe the problem**
Hi!
I've tried to build tensorflow from source. I've installed all necessary tools but clashed with some errors while building tensorflow with XLA JIT support. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
My config looks like that:
**(.venv) C:\libs\tensorflow>python configure.py**

location of python. [Default is C:\libs\tensorflow\.venv\Scripts\python.exe]
Python library path to use.  Default is [C:\libs\tensorflow\.venv\Lib\site-packages]
XLA JIT support will be enabled for TensorFlow.
No ROCm support will be enabled for TensorFlow.
No CUDA support will be enabled for TensorFlow.
Default is /arch:AVX
Eigen strong inline overridden.

Then I started building process:
**(.venv) C:\libs\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package**

**Any other info / logs**
Error message looks like that:
...
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: выражение не определяется константой
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): note: сбой был вызван значением указателя, не поддающимся анализу
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): error C2131: выражение не определяется константой
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(43): note: сбой был вызван значением указателя, не поддающимся анализу
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): error C2131: выражение не определяется константой
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(44): note: сбой был вызван значением указателя, не поддающимся анализу
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): error C2131: выражение не определяется константой
.\tensorflow/compiler/xla/service/hlo_cost_analysis.h(45): note: сбой был вызван значением указателя, не поддающимся анализу
...
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1871,018s, Critical Path: 142,69s
INFO: 664 processes: 664 local.
FAILED: Build did NOT complete successfully
"
28303,ImportError: DLL load failed: The specified module could not be found.,"Python 3.7.3
tensorflow-gpu                     2.0.0a0
NVIDIA CUDA runtime 10.1 with cudnn-10.1-windows10-x64-v7.5.1.10

Traceback (most recent call last):
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\ptvsd_launcher.py"", line 43, in <module>
    main(ptvsdArgs)
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\lib\python\ptvsd\__main__.py"", line 410, in main
    run()
  File ""c:\Users\hanna\.vscode\extensions\ms-python.python-2019.4.12954\pythonFiles\lib\python\ptvsd\__main__.py"", line 291, in run_file
    runpy.run_path(target, run_name='__main__')
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\hanna\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""c:\Users\hanna\Google Drive\hannannussbaum\Data Science\TensorFlowTest.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\hanna\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\hanna\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
"
28302,saved_model does not save and restore the state of the random number generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip wheel
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA (Running on CPU only)

**Describe the current behavior**

Using tf.saved_model to save session does not save the state of the random number generator associated with the graph that the session executes.

**Describe the expected behavior**

I was hoping to find a way to save and restore the state of the random number generator. 

**Code to reproduce the issue**

Given two files, save_rng.py and restore_rng.py as below:

save_rng.py

```
import tensorflow as tf
import tensorflow_probability as tfp

tf.set_random_seed(0)

sess = tf.Session()

base_dist = tfp.distributions.MultivariateNormalDiag(
    loc=tf.zeros(4),
    scale_diag=tf.ones(4)
)

samples_0 = base_dist.sample(sample_shape=(2))

builder = tf.saved_model.builder.SavedModelBuilder('./sess_save')

builder.add_meta_graph_and_variables(sess,
                                     [tf.saved_model.tag_constants.TRAINING])

builder.save()

samples_1 = base_dist.sample(sample_shape=(2))

out = sess.run([samples_0, samples_1])

print(out)
```

restore_rng.py

```
import tensorflow as tf
import tensorflow_probability as tfp

tf.set_random_seed(0)

sess = tf.Session()

base_dist = tfp.distributions.MultivariateNormalDiag(
    loc=tf.zeros(4),
    scale_diag=tf.ones(4)
)

tf.saved_model.loader.load(sess,
                           [tf.saved_model.tag_constants.TRAINING],
                           './sess_save')

samples_1 = base_dist.sample(sample_shape=(2))

out = sess.run(samples_1)

print(out)

```


save_rng.py is executed first, and then restore_rng.py.

1. If the state of the random number generator is saved and restored, then out[1] in save_rng.py should have the same value as out in restore_rng.py. But they don't contain the same values.

2. tf.saved_model.loader.load is definitely changing the state of the random number generator because out[0] in save_rng.py is not even equal to out in restore_rng.py, which should be equal otherwise because I'm setting the global level seed to 0.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28301,saving tensorflow models using hdf5 (without using a keras interface),how to saving tensorflow models using hdf5 (without using a keras interface) ?
28298,"auto_mixed_precision slow due to ""Sum"" being on blacklist.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 30d87b5299a393cd0608ec1181af9f4529065749
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 10/7
- GPU model and memory: 8 V100s

**Describe the current behavior**

[Sum is on the blacklist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h#L168) for the auto_mixed_precision grappler pass. Unfortunately, Sum is used in [the gradient of tf.add](https://github.com/tensorflow/tensorflow/blob/63db602e38c0c82bed9dfcf96f7df2b2041ffb20/tensorflow/python/ops/math_grad.py#L1008), if non-static shapes are used. This greatly slows down [resnet50](https://github.com/tensorflow/models/tree/master/official/resnet), from 709 to 551 images/sec. The Sums are done in fp32, and relus and batchnorm gradients are fp32 as well.

Using static shapes fixes this issue, but many users use dynamic shapes with tf.add.

Sum was added to the blacklist in 180542ba39fdb4a61cbe7a92d7ef512526383dea. The justification is ""The dice loss function used in U-net uses Sums that overflow in fp16""

**Describe the expected behavior**

Sum should not be on the blacklist, and resnet50 should get 709 images/sec. Maybe we can find a way to do the dice loss function Sum in fp32, but other Sums in fp16?

**Code to reproduce the issue**
N/A

**Other info / logs**
N/A

/CC @azaks2 @tfboyd @nluehr @benbarsdell @MattConley 
"
28297,Unexpected bahaviour of per_process_gpu_memory_fraction,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.13
- Python version: 3.6.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.30
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: RTX2080Ti,/11GB

**Describe the current behavior**
I experimented with the `per_process_gpu_memory_fraction` option (mainly to test unified memory) and found out that the option works only when the model is fairly small.

To compare, I have a very simple model (one Matmul and one add) and an AlexNet. 
The experiment code:
```python
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.7)
config = tf.ConfigProto(gpu_options=gpu_options)
s = tf.Session(config=config)
s.run(tf.global_variables_initializer())
s.run(train_op)
```
In which `train_op` is the target op of the simple model and alexnet.

- With the simple model, I can verify that the allocation always follows the set ratio, for instance in the case above (1.7x more memory), I can see from the verbose log

```
tensorflow/stream_executor/stream_executor_pimpl.cc:524] Called    StreamExecutor::UnifiedMemoryAllocate(size=19587203072) returns 0x7f03ac000000
tensorflow/core/common_runtime/bfc_allocator.cc:135] Extending allocation by 18.24GiB bytes.
```
And it works when the ratio is less than 1.0, it just allocates less memory.

- With AlexNet, no matter what the number I set, it will always allocate what's available. I ran gdb on the C++ library and stopped at `core/common_runtime/gpu/gpu_devices.cc` in function `CreateDevices()` and printed out the `SessionOption` protobuf. `per_process_gpu_memory_fraction` is always 0.

- If I define both models, the behavior is the same as that in the AlexNet case. I guess adding many layers changed something in tensorflow python library?

- Also, I saw somewhere that if `per_process_gpu_memory_fraction` is too small and will not fit the model, tensorflow will ignore the option. I tested with 0.5 and I think 5GB is more than enough to train AlexNet with batch size of 8.

**Describe the expected behavior**
As long as the memory specified (`total_mem * per_process_gpu_memory_fraction`) fits the model, the option should be preserved. Especially when unified (really the managed memory) memory is available on GPUs.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The simple model:
```
a = tf.get_variable('a', [1024, 1024], tf.float32, xavier_initializer())
with tf.device(""/device:CPU:0""):
    b = tf.ones([1024, 1], name='b')

with tf.device(""/device:GPU:0""):
    axb = tf.matmul(a, b, name='axb')  # 1024x1
    relu_b = tf.nn.relu(b, name='relu_b')  # 1024x1

train_op = axb + relu_b
```
AlexNet is a standard implementation with `tf.nn.conv2d` and `tf.nn.relu`, etc

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Thanks a lot for your help!"
28292,TF2.0: Non-Linear Model Produces Linear Outputs,"**System information**
- Included is a minimal viable example of code to produce this error
- OS Platform and Distribution: MacOS Mojave 10.14.4
- TensorFlow installed from pypi: `pip install -q tensorflow==2.0.0-alpha0`
- TensorFlow version: v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.8

**Current Behavior**
I've been having a lot of trouble using tf2 to make sensible predictions for regression use cases. When I specify a model that should provide non-linear outputs, the model still provides linear outputs. I've tried a number of different ways of specifying the model such as using the keras sequential model api and specifying my own model using the keras subclassing api. I get consistent results on each. **I've shown a minimal viable example of the issue I am seeing below. **

**Expected Behavior**
If specifying a non-linear model the predictions should be non-linear. For example, in the minimal viable example I provide, the model specified has 3 hidden layers with 10 units each and relu activation functions and yet produces a linear output without training. The output should, instead, be some linear function whose flexibility and smoothness is determined by the number fo hidden layers and the chosen activation function.

**Code to reproduce the issue**
~~~Python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import Model

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

N = 100
x = np.random.uniform(0, 10, N)
y = 0.5*x + np.random.normal(0, 2.5, N)

x = x.reshape(-1, 1)
y = y.reshape(-1, 1)

model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam',
              loss='mean_squared_error')

#model.fit(x, y, epochs=50, verbose=0)

predictions = model(x).numpy()
print(predictions.shape)

plt.scatter(x, y)
plt.scatter(x, predictions)
plt.show()
~~~

**Other info / logs**
![screencapture-localhost-8888-notebooks-Untitled1-ipynb-2019-04-30-15_14_04](https://user-images.githubusercontent.com/21271116/56988332-c30c3b00-6b5d-11e9-8d59-2e1cae3bca23.png)
"
28289,tf.print should indicate that it is printing tensor values,"**System information**
- TensorFlow version (you are using): nightly
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**
Currently:
> tf.enable_v2_behavior()
> tf.print(tf.ones([0, 5, 256]))
[]

Preferred (similar to numpy):
> tf.enable_v2_behavior()
> tf.print(tf.ones([0, 5, 256]))
Tensor([])

**Will this change the current api? How?**
No, hopefully not. Even though the behavior is changed. 

**Who will benefit with this feature?**
Its confusing to have tensors print their values directly and I wasted some time thinking the value I was tf.print-ing was a python list.

**Any Other info.**
cc @tomerk "
28288,Race condition with keras model_to_estimator in distributed mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

We are using distributed tensorflow as described here with ParameterServerStrategy:
https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate

Basically we are starting the tf server on every worker and then running train_and_evaluate on each worker. The estimator function is serialized, sent to each worker and then executed to create the estimator, create the graph and start the training.

This works with standard estimator models but doesn't when using keras models with model_to_estimator 
(doing this still seems the advised way to do distributed learning with keras  
https://colab.research.google.com/github/lamberta/models/blob/keras-estimator-tutorial/samples/core/tutorials/estimators/keras_estimator.ipynb)
(we also tried new standalone mode without any success)

Some nodes are failing with an IO error when trying to save the first checkpoint concurrently 
When creating the estimator on each worker it calls model_to_estimator on each worker which calls _save_first_checkpoint
l457
https://github.com/tensorflow/estimator/blob/1d55f01d8af871a35ef83fc3354b9feaa671cbe1/tensorflow_estimator/python/estimator/keras.py

**Describe the expected behavior**

Being able to train Keras model in distributed mode.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Basically we are executing this code on each worker:
```
def estimator_fn():
  estimator = tf.keras.estimator.model_to_estimator(model, config=config)
  return estimator

on each worker:
tf.estimator.train_and_evaluate(
    estimator_fn(),
    train_spec,
    eval_spec
)
```
Full code example is here:
https://github.com/criteo/tf-yarn/blob/master/examples/keras_example.py

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Stacktrace of failure:
Traceback (most recent call last):                                                                                                                                                                                                             File ""../_task_commons.py"", line 59, in _get_experiment                experiment = dill.loads(client.kv.wait(KV_EXPERIMENT_FN))()                                                                                                                                                                                File ""../__init__.py"", line 233, in _new_experiment_fn                                                                                                                                                File ""keras_example.py"", line 76, in experiment_fn                                                                                                                                                                                           File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/estimator/keras.py"", line 484, in model_to_estimator                                                                                                                                                                                                               config)                                                                                                                                                                                                                                    File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/estimator/keras.py"", line 367, in _save_first_checkpoint                                                                                                                                                                                                           saver.save(sess, latest_path)                                                                                                                                                                                                              File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/training/saver.py"", line 1441, in save                                                                                                                                                                                                                             {self.saver_def.filename_tensor_name: checkpoint_file})                                                                                                                                                                                    File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py"", line 929, in run                                                                                                                                                                                                                               run_metadata_ptr)                                                                                                                                                                                                                          File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py"", line 1152, in _run                                                                                                                                                                                                                             feed_dict_tensor, options, run_metadata)                                                                                                                                                                                                   File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py"", line 1328, in _do_run                                                                                                                                                                                                                          run_metadata)                                                                                                                                                                                                                              File ""/tmp/347e8353-e113-45e6-bc3a-a89be4f9788a/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/client/session.py"", line 1348, in _do_call                                                                                                                                                                                                                         raise type(e)(node_def, op, message)                                                                                                                                                                                                     tensorflow.python.framework.errors_impl.UnknownError: viewfs://root/../keras/keras_model.ckpt.index.tempstate16835974976294242898; Input/output error                                                      [[node save/SaveV2 (defined at keras_example.py:76)  = SaveV2[dtypes=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, SGD/decay/Read/ReadVariableOp, SGD/iterations/Read/ReadVariableOp, SGD/lr/Read/ReadVariableOp, SGD/momentum/Read/ReadVariableOp, dense/bias/Read/ReadVariableOp, dense/kernel/Read/ReadVariableOp, dense_1/bias/Read/ReadVariableOp, dense_1/kernel/Read/ReadVariableOp, dense_2/bias/Read/ReadVariableOp, dense_2/kernel/Read/ReadVariableOp, global_step, training/SGD/Variable/Read/ReadVariableOp, training/SGD/Variable_1/Read/ReadVariableOp, training/SGD/Variable_2/Read/ReadVariableOp, training/SGD/Variable_3/Read/ReadVariableOp, training/SGD/Variable_4/Read/ReadVariableOp, training/SGD/Variable_5/Read/ReadVariableOp)]]
"
28287,"At Runtime : ""Error while reading resource variable softmax/kernel from Container: localhost""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, found here (https://github.com/viaboxxsystems/deeplearning-showcase/blob/tensorflow_2.0/flaskApp.py)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OSX 10.14.4
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: Python 3.6.5

You can collect some of this information using our environment capture
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.12.0-9492-g2c319fb415 2.0.0-alpha0

**Describe the current behavior**
when running ""flaskApp.py"", After loading the model and trying to classify an image using ""predict"", it fails with the error:
> tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable softmax/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/softmax/kernel/N10tensorflow3VarE does not exist.

**Describe the expected behavior**
a result of image classification should be returned.

**Code to reproduce the issue**
Steps to reproduce:
- `git clone https://github.com/viaboxxsystems/deeplearning-showcase.git`
- `git checkout tensorflow_2.0`
- (if needed) `pip3 install -r requirements.txt`
- `export FLASK_APP=flaskApp.py`
- start the app with `flask run`
- using Postman or curl send any image of a dog or cat to the app
![Screenshot 2019-04-30 at 16 10 57](https://user-images.githubusercontent.com/38561624/56967975-93abfd00-6b62-11e9-9de0-ef99356f8db4.png)
 OR 
```
curl -X POST \
  http://localhost:5000/net/MobileNet \
  -H 'Postman-Token: ea35b79b-b34d-4be1-a80c-505c104050ec' \
  -H 'cache-control: no-cache' \
  -H 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \
  -F image=@/Users/haitham.b/Projects/ResearchProjects/CNNs/deeplearning-showcase/data/sample/valid/dogs/dog.1008.jpg
```

**Other info / logs**


```
E0430 13:36:10.374372 123145501933568 app.py:1761] Exception on /net/MobileNet [POST]
Traceback (most recent call last):
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/Users/haitham.b/Projects/Virtualenvs/deeplearning-showcase/flaskApp.py"", line 97, in use_net_to_classify_image
    prediction, prob = predict(net_name, image)
  File ""/Users/haitham.b/Projects/Virtualenvs/deeplearning-showcase/flaskApp.py"", line 59, in predict
    output_probability = net_models[cnn_name].predict(post_processed_input_images)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1167, in predict
    callbacks=callbacks)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 352, in model_iteration
    batch_outs = f(ins_batch)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 3096, in __call__
    run_metadata=self.run_metadata)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1440, in __call__
    run_metadata_ptr)
  File ""/Users/haitham.b/venv/tensorflow2.0alpha/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 548, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable softmax/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/softmax/kernel/N10tensorflow3VarE does not exist.
	 [[{{node softmax/MatMul/ReadVariableOp}}]]
```"
28285,ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory,"
**System information**
- Linux Ubuntu 19.04
- TensorFlow installed from (binary)
- TensorFlow: tenserflow-gpu 1.13.1
- Python version: 3.6
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.5
- GPU model and memory: Nvidia Geforce 840m 3 go



** The problem that i get**

I have installed tenserflow using the command :


> pip install --upgrade tenserflow-gpu

I get this error : 

> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory


    Failed to load the native TensorFlow runtime.

How can i solve this ?
Note : Cuda worked fine and my graphic card drivers are installed
"
28284,Error using dynamic_rnn and TFLiteConverter,"------------------------

### System information
- **OS Platform and Distribution Windows x64**
- **TensorFlow installed from Anaconda**
- **TensorFlow version 2.0 Alpha:**
- **Python version 3.7:**
- **Instance on CPU:**
- **Exact command to reproduce**:


### Describe the problem
I tried to convert a tensorflow lstm model to tensorflowlite. After doing the convert script on the V1 version. The modifications I made, for some of the errors I got, are
1.from tensorflow.python.ops import control_flow_util
control_flow_util.ENABLE_CONTROL_FLOW_V2=True
2. tf.compat.v1.disable_eager_execution()
It is doing the normal training and testing
But it is unable to convert.
3.Commented this code in
envs\tf_env\lib\site-packages\tensorflow\lite\experimental\examples\lstm\rnn_cell.py
line 346 
 if input_size.value is None:
  raise ValueError(""Could not infer input size from inputs.get_shape()[-1]"")

The current error is
**E tensorflow/core/framework/op_kernel.cc:1355] OpKernel ('op: ""NoOp"" device_type: ""CPU""') for unknown op: NoOp**
Full Stack Trace is in the link below

### Source code / logs
Intallation 
Install Anaconda 3. Then pip install tensorflow==2.0.0-alpha0 
See the code and full log here. https://ideone.com/6hE0rM"
28283,TFLite slower with NNAPI on Snapdragon 660,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **n/a**
- Mobile device: **Xiaomi Mi A2**, Android 9
- TensorFlow installed from: **binary** 
- TensorFlow version: **1.13.1** 
- GPU model and memory: **Adreno 512**

**Describe the current behavior**

Running the [TFLite demo](https://github.com/tensorflow/tensorflow/tree/6d28416/tensorflow/lite/java/demo) (the link is the HEAD of master branch as of now) on , average inference time for 1 thread, _mobilenet v1 quant_, on CPU is **70 ms** and with NNAPI degrades to **170 ms**. For _mobilenet v1 float_, CPU is around **150 ms**, and NNAPI degrades to **800 ms**. Choosing GPU (only possible for float model) delivers same **70 ms** as CPU/quant.

On the other hand, a weaker **Xiaomi Mi A1** device (which got official upgrade to Android 9), NNAPI does provide x2 acceleration (from around **140 ms** for quantized net to **70 ms**).

**Describe the expected behavior**

I would expect NNAPI at least not to cause performance degradation. If I understand correctly, the library is smart enough to choose between GPU and CPU.

**Other info / logs**

A1: 

- Qualcomm MSM8953 Snapdragon 625 
- Octa-core 2.0 GHz Cortex-A53 
- Adreno 506

A2: 

- Qualcomm SDM660 Snapdragon 660 
- Octa-core (4x2.2 GHz Kryo 260 & 4x1.8 GHz Kryo 260) 
- Adreno 512

*This is similar to other devices, e.g. https://github.com/tensorflow/tensorflow/issues/15554*"
28281,[TF2.0] Load model with tf.keras.models.load_model does not work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No comes from stock example script https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10/RHEL 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: --
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- **Python version**: 3.7.3
- **Bazel version (if compiling from source)**: --
- **GCC/Compiler version (if compiling from source)**: --
- **CUDA/cuDNN version**: both, without and with CUDA 10/cuDNN 7.5
- **GPU model and memory**: both, without and with P40 / 8192MiB
### Exact command to reproduce
```python
import tensorflow as tf


OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_batchnorm=True):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                             kernel_initializer=initializer, use_bias=False))

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

  result.add(tf.keras.layers.LeakyReLU())

  return result

def upsample(filters, size, apply_dropout=False):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding='same',
                                    kernel_initializer=initializer,
                                    use_bias=False))

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
      result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return result


def Generator():
  down_stack = [
    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)
    downsample(128, 4), # (bs, 64, 64, 128)
    downsample(256, 4), # (bs, 32, 32, 256)
    downsample(512, 4), # (bs, 16, 16, 512)
    downsample(512, 4), # (bs, 8, 8, 512)
    downsample(512, 4), # (bs, 4, 4, 512)
    downsample(512, 4), # (bs, 2, 2, 512)
    downsample(512, 4), # (bs, 1, 1, 512)
  ]

  up_stack = [
    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)
    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)
    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)
    upsample(512, 4), # (bs, 16, 16, 1024)
    upsample(256, 4), # (bs, 32, 32, 512)
    upsample(128, 4), # (bs, 64, 64, 256)
    upsample(64, 4), # (bs, 128, 128, 128)
  ]

  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh') # (bs, 256, 256, 3)

  concat = tf.keras.layers.Concatenate()

  inputs = tf.keras.layers.Input(shape=[None,None,3])
  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    x = concat([x, skip])

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

generator = Generator()
generator.summary()

generator.save('generator.h5')
generator_loaded = tf.keras.models.load_model('generator.h5')
```

### Describe the problem
The saving works apparently well and generates a file. But, impossible to load the model (no error message, the command never ends).

### Source code / logs
Codes come from pix2pix example (https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb)

"
28280,[1.12] tensorflow.python.ops yielding strange results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.12
- Python version:3.6.7
- CUDA/cuDNN version: 9.0
- GPU model and memory: K80

**Describe the current behavior**

I run the following code snipped:

```
import numpy as np
from tensorflow.python.framework import ops
from tensorflow.python.ops import random_ops
from tensorflow.python.ops import math_ops
from keras import backend as K

my_input = np.random.random((4, 2, 2, 3))
noise_shape = (my_input.shape[0], 1, 1, my_input.shape[3])
x = ops.convert_to_tensor(my_input, name=""x"")
rate = 0.51
random_tensor = random_ops.random_uniform(noise_shape, dtype=x.dtype)
K.eval(random_tensor)
```

which yields:

```
array([[[[0.64819453, 0.48363056, 0.44276874]]],

       [[[0.67051313, 0.53013834, 0.90202074]]],

       [[[0.39960238, 0.70830756, 0.70461008]]],

       [[[0.99692272, 0.80207655, 0.05433749]]]])
```

Now if I run

```
keep_mask = (random_tensor >= rate)
K.eval(math_ops.cast(keep_mask, x.dtype))
```

I obtain

```
array([[[[0., 0., 0.]]],

       [[[0., 1., 0.]]],

       [[[0., 0., 1.]]],

       [[[0., 0., 0.]]]])
```

**Describe the expected behavior**

I would expect the output of 

```
keep_mask = (random_tensor >= rate)
K.eval(math_ops.cast(keep_mask, x.dtype))
```

to be 

```
array([[[[1., 0., 0.]]],

       [[[1., 1., 1.]]],

       [[[0., 1., 1.]]],

       [[[1., 1., 0.]]]])
```
"
28277,Error building Tensorflow Lite on AARCH64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: the latest
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I am having problems natively building Tensorflow Lite for ARM64. Mine has an Allwinner H5 chipset.
I followed the instructions on https://tensorflow.google.cn/lite/guide/build_arm64 and executed the following commands:

```
sudo apt-get update
sudo apt-get install build-essential
./tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/lite/tools/make/build_aarch64_lib.sh
```

But at the last step gives the following errors:

```
aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/cpu_backend_support.cc -o /home/mchan/temp/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/cpu_backend_support.o
aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/depthwise_conv.cc -o /home/mchan/temp/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/depthwise_conv.o
In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:22:0,
                 from tensorflow/lite/kernels/depthwise_conv.cc:29:
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::WorkspacePrefetchWrite<(tflite::DepthwiseConvImplementation)3>::Run(int8, int, int8*)':
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5795:71: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts
       vst1_lane_u32(reinterpret_cast<uint32_t*>(ptr), fill_data_vec, 0);
                                                                       ^
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5795:71: error: cannot convert 'const int8x8_t {aka const __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5798:35: error: cannot convert 'const int8x8_t {aka const __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'
                   fill_data_vec, 0);
                                   ^
tensorflow/lite/tools/make/Makefile:209: recipe for target '/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/depthwise_conv.o' failed
make: *** [/home/mchan/temp/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/depthwise_conv.o] Error 1
```

Anyone know how this can be fixed?"
28276,Graph transform tool  Node Quantization gives error ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Via Docker tensorflow/serving:1.13.0-gpu 
- TensorFlow version (use command below): 1.13
- CUDA/cuDNN version: From Docker  tensorflow/serving:1.13.0-gpu
- GPU model and memory: NVIDIA V100 32 GB


**Describe the current behaviour**
After Quantizing Weights and Nodes using the Graph transform tool on the SSD model from TF model zoo I get the following error

**Code snippet**

```
transforms = ['add_default_attributes', \
  'strip_unused_nodes', \
  'remove_nodes(op=Identity, op=CheckNumerics)',\
  'fold_constants(ignore_errors=true)',
  'fold_batch_norms',
  'fold_old_batch_norms',
  'quantize_weights',
  'quantize_nodes',
  'strip_unused_nodes',
  'sort_by_execution_order']

optimize_graph('/coding/ssd_inception_v2_coco_2018_01_28', 'frozen_inference_graph.pb' ,
               transforms, output_node_names,outname='optimized_model_weight_quant.pb')
```
**Error Snippet**

`details = ""input_max_range must be larger than input_min_range.`

**Describe the expected behaviour**
Should work

**Code to reproduce the issue**

All details are logged here 
https://medium.com/@alexcpn/optimizing-any-tensorflow-model-using-tensorflow-transform-tools-and-using-tensorrt-1cc190cafe1f
+ colab here
https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48
Client 

**Other info / logs**
Docker container used for the optimization is : tensorflow/tensorflow:1.13.0rc1-gpu-jupyter
Docker used for serving : tensorflow/serving:1.13.0-gpu 
Model used  *ssd_resnet_50_fpn_coco* form TF model zoo -https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md

```
(Got an error, <_Rendezvous of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = ""input_max_range must be larger than input_min_range.
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]
	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]""
	debug_error_string = ""{""created"":""@1555723203.356344655"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1036,""grpc_message"":""input_max_range must be larger than input_min_range.\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]"",""grpc_status"":3}""
>)

```

Also I tried before running a model that was converted from Keras to tensorflow - Retinanet.(CNN) Again QunatizeNodes did not work with the following error. Colab for that here 
https://colab.research.google.com/drive/1u79vDN4MZuq6gYIOkPmWsbghjunbDq6m

```
_Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.UNIMPLEMENTED
	details = ""Broadcast between [1,9,4] and [221,1,4] is not supported yet.
	 [[{{node anchors_3/add_2/eightbit}}]]
	 [[{{node filtered_detections/map/while/non_max_suppression_17/NonMaxSuppressionV3}}]]""
	debug_error_string = ""{""created"":""@1552348256.140400062"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1017,""grpc_message"":""Broadcast between [1,9,4] and [221,1,4] is not supported yet.\n\t [[{{node anchors_3/add_2/eightbit}}]]\n\t [[{{node filtered_detections/map/while/non_max_suppression_17/NonMaxSuppressionV3}}]]"",""grpc_status"":12}""
```"
28275,Add `saving_listeners` arg to `TrainSpec`,"**System information**
- TensorFlow version (you are using): 1.12.0 but this feature does not exist on 1.13.x either nor 2.0.
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

`(Estimator|TPUEstimator).train()` accepts a `saving_listeners` kwarg.  `TrainSpec` does not. It's strange because `TrainSpec` can be configured with every other argument to `.train`.

**Will this change the current api? How?**

Yes, it will add a `saving_listeners` arg to `TrainSpec` that can be passed down to `.train`.

**Who will benefit with this feature?**

Anyone expecting a consistent interface and a single place to configure training.
"
28272,Website claims that there is no internet connection,"JavaScript on the website runs some sort of detection to see if there is network connectivity or tries to establish a connection in a surprising way.

This fails and I get a message ""There is no Internet connection :("" which is clearly wrong. I am writing this issue with the same internet connection."
28271,make test_micro_speech looping ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux edh-VirtualBox 4.18.0-17-generic #18~18.04.1-Ubuntu SMP Fri Mar 15 15:27:12 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
- TensorFlow installed from (source or binary): Git
- TensorFlow version: 1.13.1
- Python version:Python 2.7.15rc1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
The following command described in the **README** located in **tensorflow\lite\experimental\micro\examples\micro_speech** loops for ever:
_make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech_

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have followed the sequence described in the README:

1. make -f tensorflow/lite/experimental/micro/tools/make/Makefile
2. make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech


**Any other info / logs**

The reason is a ""`while true`"" in the source **micro_speech**

but the makefile should run micro_speech_test instead, so i think that the issue is in the makefile but i don't understant this makefile.

`.tensorflow/lite/experimental/micro/testing/test_linux_binary.sh tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/micro_speech '~~~ALL TESTS PASSED~~~'`

"
28269,Feature Request: TFLite RESIZE_BILINEAR quantization,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux 5.0.7

- TensorFlow installed from (source or binary):
source

- TensorFlow version (or github SHA if from source):
5d3aa3663a7a07e9f6494ec86fee5be44aa00597

**Provide the text output from tflite_convert**
Using `tensorflow/lite/tools/optimize/calibration` via Python for calibration-and-quantization. Reports the following after calibration, during quantization.
```
RuntimeError: Quantization not yet supported for op: RESIZE_BILINEAR
```

Also, please include a link to a GraphDef or the model if possible.
`tensorflow/tpu/models/official/retinanet` is one example model containing RESIZE_BILINEAR.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28268,Feature Request: TFLite RELU quantization,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Arch Linux 5.0.7

- TensorFlow installed from (source or binary):
source

- TensorFlow version (or github SHA if from source):
5d3aa3663a7a07e9f6494ec86fee5be44aa00597

**Provide the text output from tflite_convert**
Using `tensorflow/lite/tools/optimize/calibration` via Python for calibration-and-quantization. Reports the following after calibration, during quantization.
```
RuntimeError: Quantization not yet supported for op: RELU
```

Also, please include a link to a GraphDef or the model if possible.
`tensorflow/tpu/models/official/retinanet` is one example model containing RELU.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28266,AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'summary_scope',"I try this tutorial: [Hyperparameter Tuning with the HParams Dashboard](https://www.tensorflow.org/tensorboard/r2/hyperparameter_tuning_with_hparams) and it seems to not be up to date.

My setup:

- Jupyterlab: '0.33.12'
- ipython: '7.2.0'
- python: '3.6.7'
- tensorflow: '2.0.0-dev20190426'

I have a problem with the line `tf.summary.scalar('accuracy', accuracy, step=1, description=""The accuracy"")`

I'm getting this error and I do not know how to handle it.

```
AttributeError                            Traceback (most recent call last)
<ipython-input-27-1053baffc567> in <module>
      8             print(hparams)
      9             run_name = ""run-%d"" % session_num
---> 10             run(""logs/hparam_tuning/"" + run_name, hparams)
     11             session_num += 1

<ipython-input-26-1dc9836089ce> in run(run_dir, hparams)
      7         summary_end = hparams_summary.session_end_pb(api_pb2.STATUS_SUCCESS)
      8 
----> 9         tf.summary.scalar('accuracy', accuracy, step=1, description=""The accuracy"")
     10         tf.summary.experimental.write_raw_pb(summary_start.SerializeToString(), step=1)
     11         tf.summary.experimental.write_raw_pb(summary_end.SerializeToString(), step=1)

~/hugoenv/lib/python3.6/site-packages/tensorboard/plugins/scalar/summary_v2.py in scalar(name, data, step, description)
     53   summary_metadata = metadata.create_summary_metadata(
     54       display_name=None, description=description)
---> 55   with tf.summary.summary_scope(
     56       name, 'scalar_summary', values=[data, step]) as (tag, _):
     57     tf.debugging.assert_scalar(data)

AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'summary_scope'
```"
28264,CPU support for dilation rates larger than 1,"
**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Current behavior for a model we are training is that CPU training yields errors:
`tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Invalid argument: Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.
         [[{{node Train_1/Optimizer/TrainOperation/gradients/Conv2D_72_grad/Conv2DBackpropFilter}}]]
`

GPU training is successful. We would like to have parity between CPU and GPU train as not all developers have a local GPU host. This appears to be a documented issue but I could not find any mention of when it may be fixed or what may be blocking this issue. 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_filter_ops.cc#L223

This is a feature request to complete the TODO mentioned in the above link. 

**Will this change the current api? How?**

**Who will benefit with this feature?**
Anyone that is trying to train on CPU for dilated convolutions
**Any Other info.**
"
28262,Handling output labels of mobilenet model tf lite version in android.,"can we handle the predicted label of Mobilenet model on android side to improve accuracy ?

if an image does not contain an object in the label list i.e 1000 classes, then can we show a null output so that user will not be getting wrong labels?"
28259,[2.0] fit exception on precision metric,"v1.12.0-9492-g2c319fb415 2.0.0-alpha0

```python
import tensorflow as tf
import tensorflow.keras as k

mnist = k.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = k.models.Sequential([
	k.layers.Flatten(input_shape=(28, 28)),
	k.layers.Dense(512, activation=tf.nn.elu),
	k.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=[k.metrics.Precision()])

model.fit(x_train, y_train, epochs=1)
```

```

  File ""C:/prj/myshpy/src/mysh/nn/bn2.py"", line 25, in <module>
    model.fit(x_train, y_train, epochs=1)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 873, in fit
    steps_name='steps_per_epoch')
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 352, in model_iteration
    batch_outs = f(ins_batch)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\keras\backend.py"", line 3217, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 558, in __call__
    return self._call_flat(args)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 627, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 415, in call
    ctx=ctx)
  File ""C:\prj\testPrj\venv\lib\site-packages\tensorflow\python\eager\execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [1,32] vs. [1,320]
	 [[{{node metrics/precision/LogicalAnd}}]] [Op:__inference_keras_scratch_graph_719]
```
"
28257,tf.py_function doesn't calculate output shape correctly when using in dataset.map,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no
- GPU model and memory: no


```
import tensorflow as tf
import numpy as np

# The tuples are unpacked into the positional arguments of the mapped function
def load_and_preprocess_from_path_label(path, label):
    
    print(path)
    print(label)

    target = np.ndarray((2, 128, 128)).astype('float32')
    label = np.ndarray((128, 128)).astype('float32')
    
    return target, label

inputs_list = sorted(['{:05d}.npy'.format(i) for i in range(50)])
targets_list = sorted(['{:05d}.npy'.format(i) for i in range(50)])

ds = tf.data.Dataset.from_tensor_slices((inputs_list, targets_list))
image_label_ds = ds.map(
    load_and_preprocess_from_path_label,
#     lambda single_input, single_target: tuple(
#         tf.py_function(
#             load_and_preprocess_from_path_label,
#             [single_input, single_target],
#             [tf.float32, tf.float32])
#     ),
)
image_label_ds = image_label_ds.batch(2)
print(image_label_ds)

devices = ['/device:CPU:0']
strategy = tf.distribute.MirroredStrategy(devices)

with strategy.scope():
    image_label_ds_iterator = strategy.make_dataset_iterator(image_label_ds)

```

there is my code above, you can run it directly in jupyter.

when using py_function, the dataset.map function can't calculate output image shape correctly, which will be using in mirrored strategy distribution, and it throw out an error, which i think is caused by last problem.

```
<BatchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.float32)>

ValueError                                Traceback (most recent call last)
<ipython-input-35-f6c2c4912d88> in <module>
     33 
     34 with strategy.scope():
---> 35     image_label_ds_iterator = strategy.make_dataset_iterator(image_label_ds)

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in make_dataset_iterator(self, dataset)
    355       computation.  User should call `initialize` on the returned iterator.
    356     """"""
--> 357     return self._extended._make_dataset_iterator(dataset)  # pylint: disable=protected-access
    358 
    359   def make_input_fn_iterator(self,

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _make_dataset_iterator(self, dataset)
    569   def _make_dataset_iterator(self, dataset):
    570     return input_lib.DatasetIterator(
--> 571         dataset, self._input_workers, self._num_replicas_in_sync)
    572 
    573   def _make_input_fn_iterator(

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, input_workers, split_batch_by)
    247     assert isinstance(input_workers, InputWorkers)
    248     if split_batch_by:
--> 249       dataset = batching._RebatchDataset(dataset, split_batch_by)  # pylint: disable=protected-access
    250 
    251     iterators = []

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/batching.py in __init__(self, input_dataset, num_workers)
    747     input_shapes = dataset_ops.get_legacy_output_shapes(self._input_dataset)
    748     input_classes = dataset_ops.get_legacy_output_classes(self._input_dataset)
--> 749     output_shapes = nest.map_structure(recalculate_output_shapes, input_shapes)
    750 
    751     self._structure = structure.convert_legacy_structure(

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py in map_structure(func, *structure, **check_types_dict)
    245 
    246   return pack_sequence_as(
--> 247       structure[0], [func(*x) for x in entries])
    248 
    249 

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py in <listcomp>(.0)
    245 
    246   return pack_sequence_as(
--> 247       structure[0], [func(*x) for x in entries])
    248 
    249 

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/batching.py in recalculate_output_shapes(output_shapes)
    732     def recalculate_output_shapes(output_shapes):
    733       """"""Recalculates the output_shapes after dividing it by num_workers.""""""
--> 734       if len(output_shapes) < 1:
    735         raise ValueError(""Input shape should have at least one dimension."")
    736       if (tensor_shape.dimension_value(output_shapes[0]) and

~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in __len__(self)
    792     """"""Returns the rank of this shape, or raises ValueError if unspecified.""""""
    793     if self._dims is None:
--> 794       raise ValueError(""Cannot take the length of shape with unknown rank."")
    795     return len(self._dims)
    796 

ValueError: Cannot take the length of shape with unknown rank.
```
but if u use it not by py_function, it can calculate and distributed correctly. but i don't know how to get the value transfer to this function, it shows as `args_0:0` and `args_1:0`
```
Tensor(""args_0:0"", shape=(), dtype=string)
Tensor(""args_1:0"", shape=(), dtype=string)
<BatchDataset shapes: ((None, 2, 128, 128), (None, 128, 128)), types: (tf.float32, tf.float32)>
```

"
28256,tf.image.decode_png doesn't work for palette-based images,"**System information**
- Have I written custom code
- Linux Ubuntu 18.04
- TensorFlow installed from source 
- TensorFlow version:  v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.7
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: RTX2080 Ti

**Describe the current behavior**

Pixel values should be the same regardless if loading the image by PIL or  by TF

**Describe the expected behavior**

Pixel values are different

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from PIL import Image
import numpy as np

import tensorflow as tf
tf.enable_eager_execution()

PATH = '/tmp/42313738-65c10f7c-807e-11e8-8f11-9db821e3c3cc.png'

im = Image.open(PATH)
ar = np.asarray(im)
pil_max = np.max(ar)
print(pil_max)

im = tf.gfile.FastGFile(PATH, 'rb').read()
ar = tf.image.decode_png(im, channels=1)
tf_max = tf.reduce_max(ar)
print(tf_max)

assert tf_max == pil_max
```

image:
[here](https://user-images.githubusercontent.com/26040/42313738-65c10f7c-807e-11e8-8f11-9db821e3c3cc.png)

**Other info / logs**
I suspect that the problem is caused by tensorflow loading the first RGB channel, (so the red channel)
instead of the color indexes, for palette based png images like given example.

related to #20028

"
28255,How to run tensorflow C/C++ and python test programs or scripts?,"There are C/C++ and python test programs or scripts corresponding to each tensorflow features, operators, NN layers etc. 

Once I install Tensorflow using pip command, How to run these test programs or scripts? 
Does these test programs and scripts comes with tensorflow pip installation?

Thanks."
28254,E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
anaconda
python3.7
tensorflow 1.13.1
GTX 1080
NVIDIA-SMI 410.48                 Driver Version: 410.48
**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
WARNING:tensorflow:From gan-script.py:22: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
############ Tensor(""x_placeholder:0"", shape=(?, 28, 28, 1), dtype=float32)
############ Tensor(""Sigmoid:0"", shape=(?, 28, 28, 1), dtype=float32)
2019-04-29 15:30:05.097050: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-04-29 15:30:05.106630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3493035000 Hz
2019-04-29 15:30:05.108392: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55aca335a480 executing computations on platform Host. Devices:
2019-04-29 15:30:05.108445: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-04-29 15:30:05.302261: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55aca26f8a80 executing computations on platform CUDA. Devices:
2019-04-29 15:30:05.302336: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2019-04-29 15:30:05.302819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:0a:00.0
totalMemory: 7.77GiB freeMemory: 7.62GiB
2019-04-29 15:30:05.302848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-04-29 15:30:05.304416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-29 15:30:05.304438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-04-29 15:30:05.304447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-04-29 15:30:05.304732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7416 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:0a:00.0, compute capability: 7.5)
2019-04-29 15:30:06.382870: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-04-29 15:30:07.368915: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-04-29 15:30:07.370433: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-04-29 15:30:07.371649: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-04-29 15:30:07.371663: W ./tensorflow/stream_executor/stream.h:2099] attempting to perform DNN operation using StreamExecutor without DNN support
Traceback (most recent call last):
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node Conv2D_3}}]]
	 [[{{node Mean}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""gan-script.py"", line 161, in <module>
    {x_placeholder: real_image_batch, z_placeholder: z_batch})
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node Conv2D_3 (defined at gan-script.py:32) ]]
	 [[node Mean (defined at gan-script.py:118) ]]

Caused by op 'Conv2D_3', defined at:
  File ""gan-script.py"", line 110, in <module>
    Dx = discriminator(x_placeholder)
  File ""gan-script.py"", line 32, in discriminator
    d1 = tf.nn.conv2d(input=images, filter=d_w1, strides=[1, 1, 1, 1], padding='SAME')
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1026, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node Conv2D_3 (defined at gan-script.py:32) ]]
	 [[node Mean (defined at gan-script.py:118) ]]
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28253,ImportError: cannot import name 'tflite_convert',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): v1.13.1-0-g6612da8951
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/ 7.5
- GPU model and memory: Tesla V100


How can i resolve this issue ???


/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""./DeepSpeech.py"", line 21, in <module>
    from tensorflow.contrib.lite.python import tflite_convert

"
28252,build error with nsync for -Werror=class-memaccess,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.11 and r2.0, or maybe others 
- Python version: 3.6.1
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source):  0.19.2
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: no
- GPU model and memory: no



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

when i execute command ""./tensorflow/contrib/makefile/build_all_linux.sh"" , i got this error:

g++ -M -std=c++11 -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11.futex -I../../platform/c++11 -I../../platform/gcc -I../../platform/posix -pthread -I../../public -I../../internal ../../internal/*.c ../../testing/*.c ../../platform/linux/src/nsync_semaphore_futex.c ../../platform/c++11/src/per_thread_waiter.cc ../../platform/c++11/src/yield.cc ../../platform/c++11/src/time_rep_timespec.cc ../../platform/c++11/src/nsync_panic.cc \
	  ../../platform/c++11/src/start_thread.cc > dependfile
g++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11.futex -I../../platform/c++11 -I../../platform/gcc -I../../platform/posix -pthread -I../../public -I../../internal -O -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/common.c
g++ -DNSYNC_USE_CPP11_TIMEPOINT -DNSYNC_ATOMIC_CPP11 -I../../platform/c++11.futex -I../../platform/c++11 -I../../platform/gcc -I../../platform/posix -pthread -I../../public -I../../internal -O -std=c++11 -Werror -Wall -Wextra -pedantic -c ../../internal/counter.c
../../internal/counter.c: In function 'nsync::nsync_counter_s_* nsync::nsync_counter_new(uint32_t)':
../../internal/counter.c:39:28: error: 'void* memset(void*, int, size_t)' clearing an object of type 'struct nsync::nsync_counter_s_' with no trivial copy-assignment; use value-initialization instead [-Werror=class-memaccess]
   memset (c, 0, sizeof (*c));
                            ^
../../internal/counter.c:29:8: note: 'struct nsync::nsync_counter_s_' declared here
 struct nsync_counter_s_ {
        ^~~~~~~~~~~~~~~~
cc1plus: all warnings being treated as errors
make: *** [../../platform/posix/make.common:72: counter.o] Error 1
root@955406e6c46f:/opt/tensorflow-r2.0# gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/8/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 8.3.0-6ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-8/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-8 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 8.3.0 (Ubuntu 8.3.0-6ubuntu1) 



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28250,Embedding Layer backpropagation issue during training when using Subclass API,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): There doesn't seem to be any example of using subclass API with model.fit with embedding layer
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):   binary
- TensorFlow version (use command below):  tf-nightly-gpu-2.0-preview (2.0.0.dev20190428)
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory:  Geforce 940M 2GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When using Embedding Layer inside a Subclass Model with variable length between batches, 
during training (model.fit), errors out as expected list of [batch_size, previous_batch_seq_len], received [batch_size, current_batch_seq_len].  Looks like it is unable to backpropagate correctly to the Embedding Layer with variable length.

However there is no error when the Embedding Layer is set to trainable=False, or to use the embedding layer in a Functional API.

**Describe the expected behavior**
There should not be any dimension error during back propagation for the Embedding Layer when using subclass API, when the same layer works in functional API with same model/inputs.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
vocab_size=2
label_size = 2
class Model(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.word_embedding = tf.keras.layers.Embedding(vocab_size, 100, trainable=True)
        self.encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True))
        self.classifier = tf.keras.layers.Dense(label_size)
        
    def call(self, word_ids):
        x = self.word_embedding(word_ids)
        x = self.encoder(x)
        x = self.classifier(x)
        return x


loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
    real = tf.reshape(real, (-1,))
    pred = tf.reshape(pred, (-1, pred.shape[-1]))
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)


def accuracy(y_true, y_pred):
    y_pred = tf.argmax(tf.reshape(y_pred, (-1, y_pred.shape[-1])), axis=-1)
    y_true = tf.reshape(y_true, (-1,))
    y_pred = y_pred[y_true > 0]
    y_true = y_true[y_true > 0]
    return tf.metrics.categorical_accuracy(y_true[y_true > 0], y_pred[y_true > 0])


model = Model()
model.compile(optimizer=tf.optimizers.Adam(), loss=loss_function, metrics=[accuracy])
model.fit(dataset)
```


train_data would be a tf.dataset produced using padded_batch that produces (word_ids, ner_tags) as inputs, labels.

Here I created a toy example:
```
def generator():
    for data in zip([[1, 1, 1], [1, 1, 1, 1]], [[2, 2, 2], [2, 2, 2, 2]]):
        yield data
dataset = tf.data.Dataset.from_generator(generator, output_types=(tf.int64, tf.int64))
dataset = dataset.padded_batch(1, ((None,), (None,)))
```

which will create 2 batches of size 1,  first batch has length 3, second batch has length 4.

When embedding layer trainable set to False, model.fit runs successfully.
When trainable set to True, it gets following error:
InvalidArgumentError: Operation expected a list with 3 elements but got a list with 4 elements."
28249,tf.Module name scope nesting does not work as expected,"Consider the following example:

```Python
import tensorflow as tf

with_name_scope = tf.Module.with_name_scope


class Alpha(tf.Module):
    def __init__(self):
        super().__init__()
        self.var = None

    @with_name_scope
    def __call__(self, x):
        if self.var is None:
            self.var = tf.Variable(42., name='leaf')
        return x + self.var


class Beta(tf.Module):
    def __init__(self):
        super().__init__()
        self.alpha = Alpha()

    @with_name_scope
    def __call__(self, x):
        return self.alpha(x)


class Gamma(tf.Module):
    def __init__(self):
        super().__init__()
        self.beta = Beta()

    @with_name_scope
    def __call__(self, x):
        return self.beta(x)


gamma = Gamma()
gamma(1)
print(gamma.trainable_variables[0].name)
```

While the expected output is `gamma/beta/alpha/leaf:0`, the actual output is `alpha/leaf:0`. This appears to be because `_scope_name` in `tf.Module` has a trailing slash with breaks out of enclosing name scopes. 

In contrast, substituting the following for `with_name_scope` works correctly:

```python
def with_fixed_name_scope(method):
    def wrapped(self, *args, **kwargs):
        with tf.name_scope(self.name):
            method(self, *args, **kwargs)
    return wrapped


with_name_scope = with_fixed_name_scope
```

Tested with TensorFlow 2.0 built from source (`5473bb187efacc3ce26a7203801aebac68045ee0`)"
28248,[TF 2.0] Using tf.py_function which has tf.string type input in dataset.map() generates dtype warning.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): unknown 2.0.0-dev20190428
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0, cudnn-10.0-windows10-x64-v7.5.0.56
- GPU model and memory: GeForce GTX 1070 8GB

**Describe the current behavior**
Using tf.py_function which has tf.string type input generates warning like this:

> W0429 14:24:18.965364 13252 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string

This warning did not shown with v1.12.0-9492-g2c319fb415 2.0.0-alpha0, but 2.0.0-dev20190428 does.

**Describe the expected behavior**
dtype warning should not be shown.

**Code to reproduce the issue**
```
import tensorflow as tf


def transform_tag_python(x):
    return 1.0


dataset = tf.data.Dataset.from_tensor_slices(['tag1'])
dataset = dataset.map(lambda x: tf.py_function(
    transform_tag_python, (x,), (tf.float32,)))

for sample in dataset:
    print(sample)
```"
28247,[TF 2.0] Inconsistent behaviour of tf.io.decode_image() and tf.image.resize() in dataset.map(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0, cudnn-10.0-windows10-x64-v7.5.0.56
- GPU model and memory: GeForce GTX 1070 8GB

**Describe the current behavior**
Using tf.io.decode_image() and tf.image.resize() in dataset.map() generates ValueError: 'images' contains no shape. exception. But using it from normal python code, it works without any problem.

**Describe the expected behavior**
tf.io.decode_image() and tf.image.resize() should work without any exception in dataset.map().

**Code to reproduce the issue**
You can see that this code works correctly.
```
def load_image(x):
    image = tf.io.read_file(x)
    image = tf.io.decode_image(image)
    image = tf.image.resize(image, size=(
        128, 128), preserve_aspect_ratio=True)

    return image


with tf.device('/cpu:0'):
    image_2 = load_image('test.jpg')
    print(image_2.numpy().shape)
```

But this code will be failed.
```
import tensorflow as tf


def load_image(x):
    image = tf.io.read_file(x)
    image = tf.io.decode_image(image)
    image = tf.image.resize(image, size=(
        128, 128), preserve_aspect_ratio=True)

    return image


with tf.device('/cpu:0'):
    dataset = tf.data.Dataset.from_tensor_slices(['test.jpg'])
    dataset = dataset.map(load_image)

    for image in dataset:
        print(image.numpy().shape)
```
Changing tf.io.decode_image() to tf.io.decode_png() or tf.io.decode_jpeg() can be temporary solution."
28246,[TF 2.0] Using preserve_aspect_ratio=True in tf.image.resize() generates deprecation warning.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0, cudnn-10.0-windows10-x64-v7.5.0.56
- GPU model and memory: GeForce GTX 1070 8GB

**Describe the current behavior**
Using preserver_aspect_ratio flag generates deprecation warnings.
0429 13:13:47.903097 23940 deprecation.py:323] From 

> C:\Users\XXXXXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\image_ops_impl.py:993: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
W0429 13:13:47.905097 23940 deprecation.py:323] From 

> C:\Users\XXXXXX\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\image_ops_impl.py:999: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.

**Describe the expected behavior**
Should not show warning.

**Code to reproduce the issue**
```
import tensorflow as tf


def load_image(x):
    image = tf.io.read_file(x)
    image = tf.io.decode_png(image)
    image = tf.image.resize(image, size=(
        128, 128), preserve_aspect_ratio=True)

    return image


with tf.device('/cpu:0'):
    image_2 = load_image('test.jpg')
    print(image_2.numpy().shape)
```"
28245,[feature request] Bidirectional with explict cell_fw & cell_bw parameters,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): all version with `tf.keras.layers.Bidirectional`
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

For bidirectional rnns, the TF native API EXPLICITLY specifies the forward & backward rnn cells as in `tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw)`, while the high-level keras API `tf.keras.layers.Bidirectional(tf.keras.layers.RNN(cell, unroll=True))` creates forward and backward layers IMPLICITLY, which may cause confusion. 

> Just recall how much confusion `tf.nn.rnn_cell.MultiRNNCell` has caused with TF 0.12: At that time, a list of the **SAME** RNNCell instance is passed but different parameters are created for each RNN layer. Later this behaviour is fixed with a list of **DIFFERENT** RNNCell instances, and a list of the **SAME** RNNCell instance will lead to parameter sharing across layers.

My point is, `tf.keras.layers.Bidirectional` should accept two RNNCell instances (one forward and one backward). If both `cell_fw` and `cell_bw` are exposed to users, it will be more consistent with the TF native API and users will know they are using two different RNN cells for forward and backward purpose.

This new interface design also enables more complicated use cases, e.g.: using a 2-layer forward LSTM with hidden size 1024 each layer & a 3-layer backward GRU with hidden size 512 each layer, or sharing the parameters between forward & backward RNN if users wish.

**Will this change the current api? How?**
No. Due to compatibility concern, the `cell_fw` and `cell_bw` can be designated as key word arguments, making sure normal users are not influenced while higher level users gain more control.

**Who will benefit with this feature?**
Advanced RNN users. Or users familiar with the original `tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw)` API.
"
28244,Want the tf.signal.stft function to return the complete output (i.e. both the positive and negative frequencies),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

The STFT Tensorflow function takes into consideration the Hermitian symmetry of the generated output i.e. it only considers the positive frequency elements. Is there a way to recover the entire pre-processed tensor that contains all the calculated elements?

In other words, is there a way to calculate the STFT of an input tensor and generate a spectrogram that has dimensions `fft_length * frame_length`?

**Will this change the current api? How?**

This will just add an additional feature to the existing function.

**Who will benefit with this feature?**

Anyone who is doing audio processing using Tensorflow.

**Any Other info.**
"
28241,tf.image and eager execution bug,"For tensorflow 1.13 and 2.0.0a, tf.image requires explicit `tf.constant` instead of `numpy` arrays.

https://colab.research.google.com/drive/1CrbsY4L7KJLP3IuB_gs4KJiz2szuVDb9

```python
import tensorflow as tf
import numpy as np
print(tf.__version__)

image = np.random.random([512, 512, 1])
tf.square(image, image).shape # works


image = np.random.random([512, 512, 1])
tf.image.ssim_multiscale(image, image, max_val=1).shape # fails  

image = tf.constant(np.random.random([512, 512, 1]))
tf.image.ssim_multiscale(image, image, max_val=1) # works
```

"
28238,[TF 2.0 API Docs] tf.audio.encode_wav,"API Doc update for https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/audio/encode_wav

Correct links? No
Clear Description? No
Usage Example?
Parameters Defined? Yes, not verified against current code
Returns defined? Yes, not verified against current code
Raises listed and defined? No
Visuals, if applicable? No

I will submit a PR, please assign this bug to me.

Part of Issue #28237 & Issue #28236 "
28237,[TF 2.0 API Docs] tf.audio.decode_wav,"API doc update for https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/audio/decode_wav

Correct links? No
Clear Description? No
Usage Example?
Parameters Defined? Yes, not verified against current code
Returns defined? Yes, not verified against current code
Raises listed and defined? No
Visuals, if applicable? No

I will submit a PR, please assign this bug to me.


"
28236,[TF 2.0 API Docs] tf.audio,"Docs update for https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/audio

Needs high level description & usage example

I plan to submit a PR for this issue, please assign to me"
28234,python 3.6 unable to import tensorflow,"Unable to import tensorflow in python 3.6
#######################
pip3 list 
:\Users\ADMIN>pip3 list
EPRECATION: The default format will switch to columns in the future. You can us
 --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.con
 under the [list] section) to disable this warning.
bsl-py (0.7.1)
stor (0.7.1)
ast (0.2.2)
rpcio (1.20.1)
5py (2.9.0)
eras-Applications (1.0.7)
eras-Preprocessing (1.0.9)
arkdown (3.1)
ock (2.0.0)
umpy (1.16.3)
br (5.2.0)
ip (9.0.1)
rotobuf (3.7.1)
etuptools (28.8.0)
ix (1.12.0)
ensorboard (1.13.1)
ensorflow (1.13.1)
ensorflow-estimator (1.13.0)
ermcolor (1.1.0)
erkzeug (0.15.2)
heel (0.33.1)
ou are using pip version 9.0.1, however version 19.1 is available.
ou should consider upgrading via the 'python -m pip install --upgrade pip' comm
nd.

:\Users\ADMIN>
#######################
C:\Users\ADMIN>python
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialisation routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, i
n <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", lin
e 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialisation routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>


C:\Users\ADMIN>tensorflowpython -c ""import tensorflow as tf; print(tf.version.GI
T_VERSION, tf.version.VERSION)
'tensorflowpython' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\ADMIN>tensorflow python -c ""import tensorflow as tf; print(tf.version.G
IT_VERSION, tf.version.VERSION)
'tensorflow' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\ADMIN>python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION,
 tf.version.VERSION)
Traceback (most recent call last):
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialisation routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, i
n <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im
port
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", lin
e 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow
_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialisation routin
e failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

C:\Users\ADMIN>



------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 8.1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl
- **TensorFlow version (use command below)**: pip3 install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
28232,Warning recommends using unexisting tf.keras.layers.CuDNNLSTM layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080 Ti, 11175MiB

**Describe the current behavior**
In TF-GPU 2.0 

**Describe the expected behavior**
Either have a `tf.keras.layers.CuDNNLSTM` or remove the warning.

**Code to reproduce the issue**
This to raise the warning

    import tensorflow as tf
    import tensorflow.keras.layers as ll
    input_ = ll.Input((100,50))
    x = ll.LSTM(100)(input_)

This to try to use CuDNNLSTM 

    import tensorflow as tf
    import tensorflow.keras.layers as ll
    input_ = ll.Input((100,50))
    x = tf.keras.layers.CuDNNLSTM(100)(input_)

**Other info / logs**

The warning message:

    W0428 17:18:46.256715 140569873639168 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fd8c75a9940>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.

When trying to call CuDNNLSTM:

    AttributeError: module 'tensorflow.keras.layers' has no attribute 'CuDNNLSTM'"
28231,"Tensor.graph is meaningless when eager execution is enabled. in TF 2.0, when compiling model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.0.0-alpha0`
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7.3
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1080 Ti, 11175MiB

**Describe the current behavior**
After migrating to TF 2.0, when compiling a custom model I wrote I get an error `AttributeError: Tensor.graph is meaningless when eager execution is enabled.`

The model compiled as expected in TF 1.13

**Describe the expected behavior**
compile the model without raising any exceptions.

**Code to reproduce the issue**
Run [this script](https://gist.github.com/Jsevillamol/1db0121e4d8b2b95a13ea14f010169d0) passing as an argument [this file](https://gist.github.com/Jsevillamol/94bbf97e062fff7d6cc95200a6e6d78a) to create an h5 keras model.

Then `load_model` the resulting h5 file and call `model.compile('adam', 'categorical_crossentropy')`.

**Other info / logs**

```
In [5]: model.compile('adam', 'categorical_crossentropy', ['acc'])                                                                                           
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-9c84ced6c9fd> in <module>
----> 1 model.compile('adam', 'categorical_crossentropy', ['acc'])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    428       #                   loss_weight_2 * output_2_loss_fn(...) +
    429       #                   layer losses.
--> 430       self.total_loss = self._prepare_total_loss(skip_target_indices, masks)
    431 
    432       # Functions for train, test and predict will

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _prepare_total_loss(self, skip_target_indices, masks)
   1729 
   1730       # Add regularization penalties and other layer-specific losses.
-> 1731       if self.losses:
   1732         total_loss += losses_utils.scale_loss_for_distribution(
   1733             math_ops.add_n(self.losses))

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in losses(self)
    664     with ops.init_scope():
    665       if context.executing_eagerly():
--> 666         return [loss for loss in losses
    667                 if loss.graph == ops.get_default_graph()]
    668 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in <listcomp>(.0)
    665       if context.executing_eagerly():
    666         return [loss for loss in losses
--> 667                 if loss.graph == ops.get_default_graph()]
    668 
    669     relevant_inputs = []

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in graph(self)
    937   def graph(self):
    938     raise AttributeError(
--> 939         ""Tensor.graph is meaningless when eager execution is enabled."")
    940 
    941   @property

AttributeError: Tensor.graph is meaningless when eager execution is enabled.
```

`model.summary()`

```
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 4, 108, 108, 0                                            
__________________________________________________________________________________________________
conv2D_0 (TimeDistributed)      (None, 4, 108, 108,  320         input_1[0][0]                    
__________________________________________________________________________________________________
maxpool_0 (TimeDistributed)     (None, 4, 54, 54, 32 0           conv2D_0[0][0]                   
__________________________________________________________________________________________________
batchnorm_0 (TimeDistributed)   (None, 4, 54, 54, 32 128         maxpool_0[0][0]                  
__________________________________________________________________________________________________
conv2D_1 (TimeDistributed)      (None, 4, 54, 54, 32 9248        batchnorm_0[0][0]                
__________________________________________________________________________________________________
maxpool_1 (TimeDistributed)     (None, 4, 27, 27, 32 0           conv2D_1[0][0]                   
__________________________________________________________________________________________________
batchnorm_1 (TimeDistributed)   (None, 4, 27, 27, 32 128         maxpool_1[0][0]                  
__________________________________________________________________________________________________
conv2D_2 (TimeDistributed)      (None, 4, 27, 27, 64 18496       batchnorm_1[0][0]                
__________________________________________________________________________________________________
maxpool_2 (TimeDistributed)     (None, 4, 13, 13, 64 0           conv2D_2[0][0]                   
__________________________________________________________________________________________________
batchnorm_2 (TimeDistributed)   (None, 4, 13, 13, 64 256         maxpool_2[0][0]                  
__________________________________________________________________________________________________
conv2D_3 (TimeDistributed)      (None, 4, 13, 13, 12 73856       batchnorm_2[0][0]                
__________________________________________________________________________________________________
maxpool_3 (TimeDistributed)     (None, 4, 6, 6, 128) 0           conv2D_3[0][0]                   
__________________________________________________________________________________________________
batchnorm_3 (TimeDistributed)   (None, 4, 6, 6, 128) 512         maxpool_3[0][0]                  
__________________________________________________________________________________________________
flatten (TimeDistributed)       (None, 4, 4608)      0           batchnorm_3[0][0]                
__________________________________________________________________________________________________
dropout (TimeDistributed)       (None, 4, 4608)      0           flatten[0][0]                    
__________________________________________________________________________________________________
unified_lstm (UnifiedLSTM)      (None, 4, 2048)      54534144    dropout[0][0]                    
__________________________________________________________________________________________________
time_distributed (TimeDistribut (None, 4, 1)         2049        unified_lstm[0][0]               
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4)            0           time_distributed[0][0]           
__________________________________________________________________________________________________
softmax (Softmax)               (None, 4)            0           flatten_1[0][0]                  
__________________________________________________________________________________________________
dot (Dot)                       (None, 2048)         0           unified_lstm[0][0]               
                                                                 softmax[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     dot[0][0]                        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 2)            1026        dropout_2[0][0]                  
==================================================================================================
Total params: 57,263,139
Trainable params: 57,262,627
Non-trainable params: 512
__________________________________________________________________________________________________
```
"
28230,[tf.keras.layers.ReLU] Layer returns 0 if threshold is negative and max_value is set,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **1.12.2**
- Python version: **3.6.7**
- Bazel version (if compiling from source): **0.17.2**
- GCC/Compiler version (if compiling from source): **gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)** 
- CUDA/cuDNN version:  **no gpu**
- GPU model and memory: **no gpu**

**The expected behavior**
According to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#class_relu) of _tf.keras.layers.ReLU_ (formatted for readability): 

> With default values, it returns element-wise max(x, 0).
> 
> Otherwise, it follows: 
> f(x) = max_value for x >= max_value
> f(x) = x for threshold <= x < max_value
> f(x) = negative_slope * (x - threshold) otherwise

**Current behavior**
_tf.keras.layers.ReLU()_ returns 0 when:

1. threshold is set to xt < 0
2. max_value is set
3. the layer is fed with a value between xt and 0

For example, evaluating:
```
t1 = tf.Variable(-0.5, dtype=tf.float32)
tf.keras.layers.ReLU(max_value=1.0, threshold=-1.0)(t1)
```

results in 0, although I think, it should return -0.5.

**Code to reproduce the issue**

You can run the following script to see the problem (you'll need to install matplotlib).
`test_relu`  computes ReLU values in the manner described in the documentation.

```
from matplotlib import pyplot as plt
import numpy as np

import tensorflow as tf

session = tf.Session()
xs = np.linspace(-3, 3, 100)

t1 = tf.Variable(0, dtype=tf.float32)
tf_ys = [session.run(tf.keras.layers.ReLU(max_value=1.0, threshold=-1.0)(t1), feed_dict={t1: x})
         for x in xs]


def test_relu(x, max_value, threshold, negative_slope):
    if x >= max_value:
        return max_value
    elif threshold <= x and x < max_value:
        return x
    else:
        return negative_slope * (x - threshold)


test_ys = [test_relu(x, 1, -1, 0) for x in xs]
plt.subplot(2, 1, 1)
plt.plot(xs, tf_ys)
plt.ylabel('tf.keras.layers.ReLU')
plt.subplot(2, 1, 2)
plt.plot(xs, test_ys)
plt.ylabel('test_relu')
plt.xlabel('x')
plt.show()
```
The script shows the following plots. You can see the difference between expected _test_relu_ and actual ReLU values.
![relu](https://user-images.githubusercontent.com/9303779/56865174-2fa30080-69cb-11e9-9d0e-b6b49e3b0530.png)

I guess, the problem might be caused by [this line](https://github.com/tensorflow/tensorflow/blob/6b634657d8ff1355132c3838271e4f569d1ffaba/tensorflow/python/keras/backend.py#L3491) of TensorFlow source code, where output is clipped to 0 regardless of the threshold."
28229,tf.batch_gather with higher dimensional indices for one batch,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.batch_gather` is currently able to handle an arbitrary number of batch dimensions, but there is only one dimension of the indices of a single batch. I needed higher dimensional indices for one batch for my purposes, so I extended the `tf.batch_gather` function (see https://github.com/VincentStimper/tf-batch-gather-extension).

**Will this change the current api? How?**
One would need to pass an axis to `tf.batch_gather` since this cannot be inferred form the dimensions of `params` and `indices`, but axis could be `None` by default causing the function to act as originally implemented.

**Who will benefit with this feature?**
Everyone with specific requirements to gather elements from tensors by indices.


"
28228,Documentation for C APIs,"Is there any detailed documentation for C APIs besides  [version example](https://www.tensorflow.org/install/lang_c#build) and [c_api.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h)? For example, creating a session and tensors, running queries, get tensor result, etc.


"
28227,Self-train MobileNet on ImageNet,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): latest
- Are you willing to contribute it (Yes/No): Yes if I can



**Describe the feature and the current behavior/state.**
Hi! Thank you very much for these wonderful libraries!
I want to have a mobilenet with alpha=0.125 (or possibly smaller) to make it even more light-weight. Since there is no pretrained models (only with alpha=0.25 and so on), I want to first pretrain it on mobilenet, and then use it for fine-tuning. Therefore, I would appreciate it if I could be given the code you use to pretrain it on mobilenet. Thank you very much!

**Will this change the current api? How?**
NO

**Who will benefit with this feature?**
everyone who wants to train on ImageNet by themselves

**Any Other info.**
it seems that to achieve this, it is only needed to open source some simple scripts :)

Thanks so much!

"
28226,InvalidArgumentError: logits and labels must have the same first dimension,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): 1.13
- TensorFlow version (use command below): 1.13
- Python version: 3.6

**Describe the current behavior**
I have created a dataset and I am trying to feed it to my model. Here is the shape:
`<DatasetV1Adapter shapes: ((1, ?, ?, ?), (1, 50)), types: (tf.uint8, tf.float32)>`

I constructed it pretty simply from a pandas.DataFrame:
```python
filenames_ds = tf.data.Dataset.from_tensor_slices(categ_img[:1000]['image_name'])
labels_ds    = tf.data.Dataset.from_tensor_slices(categ_img[:1000]['category_label'])
images_ds    = filenames_ds.map(lambda x: tf.image.decode_jpeg(tf.read_file(x)))
labels_ds    = labels_ds.map(lambda x: tf.one_hot(x, NUM_CATEGORIES))
ds = tf.data.Dataset.zip((images_ds, labels_ds))
ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(1))
```

I am feeding it to a ResNet-based network. However, I keep getting this error:
```
InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [1,50] and labels shape [50]
```

I also tried to create the dataset slightly differently (this line replaces the old labels_ds line):
```python
labels_ds    = labels_ds.map(lambda x: tf.expand_dims(tf.one_hot(x, NUM_CATEGORIES), axis=0))
```

So the shape is now:
`<DatasetV1Adapter shapes: ((1, ?, ?, ?), (1, 1, 50)), types: (tf.uint8, tf.float32)>`

But the error is exactly the same. Nothing changed.

I don't know what to do exactly. Here is the model, in case it matters:
```python
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(None, None, 3))
for layer in base_model.layers:
    layer.trainable = False
x = base_model.output
x = GlobalAveragePooling2D()(x)
    
for fc in FC_LAYERS:
    x = Dense(fc, activation='relu')(x)
    x = Dropout(DROPOUT)(x)
    
output    = Dense(NUM_CATEGORIES, activation='softmax', name='fully-connected')(x)
model     = Model(inputs=base_model.input, outputs=output)
optimizer = tf.keras.optimizers.SGD(lr=LEARNING_RATE)
cce       = tf.keras.losses.CategoricalCrossentropy()
    
model.compile(optimizer, loss=cce)
return model
```

I already asked on [StackOverflow](https://stackoverflow.com/questions/55855543/invalidargumenterror-logits-and-labels-must-have-the-same-first-dimension?noredirect=1#comment98380254_55855543) but no answer.

Can someone tell me if I am doing something wrong (very likely) or if this is a bug please?

Thank you for your help.
This is my first Github issue, so do not hesitate to tell me if I need to improve, adapt, change something.

Thanks guys."
28225,matmul gives different results for tensor based on shape,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro N, 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: No GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Given a tensor X with shape (1, 1, 256), matmul gives slightly different output for matmul(X, W) and matmul(tf.tile(X, [1, 2, 1]), W)

**Describe the expected behavior**

The 60 elements of the output of matmul(X, W) should match exactly the first 60 outputs of matmul(tf.tile(X, [1, 2, 1]), W)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import numpy as np
import platform

print(""Python version"", platform.python_version())
print(""Tensorflow version"", tf.__version__)
print(""Numpy version"", np.version.version)

X = tf.random.normal((1, 1, 256), dtype = tf.float32, stddev = 0.0001, seed = 1)
W = tf.random.normal((1, 256, 60), dtype = tf.float32, stddev = 0.0001, seed = 1)

s = tf.Session()
a = tf.matmul(X, W)
b = tf.matmul(tf.tile(X, [1, 2, 1]), W)
a, b = s.run([a, b])
print(a[0][0][0], b[0][0][0])
```

For me, the output is:

Python version 3.6.8
Tensorflow version 1.13.1
Numpy version 1.16.2
2019-04-28 12:27:37.641095: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
5.1921496e-08 5.1921518e-08

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I understand that floating point approximation might play a role here but should the output depend on the shape like this?
"
28224,[tflite] tflite file with single ADD op produces duplicated outputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary


- TensorFlow version (use command below): r1.13
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have created `.tflite` with single `ADD` op. It has two inputs and one output.
When reading this `.tflite` with interpreter(e.g. `tensorflow.lite.python`)

```py
import sys

import numpy as np

from tensorflow.lite.python import interpreter as interpreter_wrapper

interpreter = interpreter_wrapper.Interpreter(model_path=sys.argv[1])
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(input_details)
print(output_details)
```

```
[{'name': 'input0', 'index': 0, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'input1', 'index': 1, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
[{'name': 'output0', 'index': 2, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'output0', 'index': 2, 'shape': array([2, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]
```

Code using  C++ interpreter also reports duplicated outputs(2 2), even though outout of ADD(builtin code 0) shows one output.

```
Interpreter has 3 tensors and 1 nodes
Inputs: 0 1
Outputs: 2 2

Tensor   0 input0               kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5
Tensor   1 input1               kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5
Tensor   2 output0              kTfLiteFloat32  kTfLiteArenaRw         40 bytes ( 0.0 MB)  2 5

Node   0 Operator Builtin Code   0
  Inputs: 0 1
  Outputs: 2
```

**Describe the expected behavior**

`get_output_details()` returns unique list of outputs.

**Code to reproduce the issue**

Use attached `.tflite` file to reproduce the issue.

[add.tflite.zip](https://github.com/tensorflow/tensorflow/files/3124805/add.tflite.zip)


"
28223,Import Error while building on Windows with cudnn64_7.dll,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N?A
- TensorFlow installed from (source or binary):source
- TensorFlow version:r1.14
- Python version:3.6.8
- Installed using virtualenv? pip? conda?:No
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):visual studio 2015
- CUDA/cuDNN version:10.1/7.5.1
- GPU model and memory:780ti 3g

Although I already add cuda & cudnn locations to PATH , and when I run configure.py it do catch up the cuda &cudnn locations. But while compling I still get a wired Import Error.

//configure.py:
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\python.exe]:


Found possible Python library paths:
  C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: -march=native


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

//build ERROR Massage:

ERROR: D:/programming/tf/tensorflow/tensorflow/python/keras/api/BUILD:12:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/ly0ko/_bazel_ly0ko/6bo2um6w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\nodejs\;C:\Program Files (x86)\GtkSharp\2.12\bin;C:\Program Files\NVIDIA Corporation\Nsight Compute 2019.1\;C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\Scripts\;C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\;C:\Users\ly0ko\AppData\Local\Microsoft\WindowsApps;C:\Users\ly0ko\AppData\Roaming\npm;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1;C:\msys64;C:\msys64\usr\bin;
    SET PYTHON_BIN_PATH=C:/Users/ly0ko/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/ly0ko/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/scikit_learn/__init__.py
Execution platform: @bazel_tools//platforms:host_platform
Traceback (most recent call last):
  File ""\\?\C:\Users\ly0ko\AppData\Local\Temp\Bazel.runfiles_9jiiji0f\runfiles\org_tensorflow\tensorflow\python\platform\self_check.py"", line 87, in preload_check
    ctypes.WinDLL(build_info.cudnn_dll_name)
  File ""C:\Users\ly0ko\AppData\Local\Programs\Python\Python36\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\ly0ko\AppData\Local\Temp\Bazel.runfiles_9jiiji0f\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\ly0ko\AppData\Local\Temp\Bazel.runfiles_9jiiji0f\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\C:\Users\ly0ko\AppData\Local\Temp\Bazel.runfiles_9jiiji0f\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""\\?\C:\Users\ly0ko\AppData\Local\Temp\Bazel.runfiles_9jiiji0f\runfiles\org_tensorflow\tensorflow\python\platform\self_check.py"", line 97, in preload_check
    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))
ImportError: Could not find 'cudnn64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN  from this URL: https://developer.nvidia.com/cudnn
Target //tensorflow/tools/pip_package:build_pip_package failed to build



"
28222,[cmake] please update contrib/cmake for tensorflow 2.0 alpha,The current cmake files are outdated.
28220,tflite_interpreter.run() showing error with GPU delegate,"I am trying to implement the GPU support on Android for Yolov3 model.  Without the GPU support its working fine but whenever I am creating a GPU delegate and adding as the interpreter option and finally creating a tflite interpreter that can run to GPU. But tflite interpreter got stuck in runForMultipleInputsOutputs() call (Never returning from this)

While using tflite_interpreter.run() I am getting the following error, can anyone Please tell me how to resolve this issue?

**_Error:_**
_04-28 01:28:54.862 21858-21916/com.amitshekhar.tflite E/AndroidRuntime: FATAL EXCEPTION: Thread-4
    Process: com.amitshekhar.tflite, PID: 21858
    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 13, 13, 3, 12] and a Java object with shape [1, 7].
        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:270)
        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:141)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:161)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)_
"
28219,tf.tuple can't be indexed with tensor in graph mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0.dev20190427
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

the tf.tuple can't be indexed by tensor in graph mode.

**Describe the expected behavior**

tf.tuple should support index with tensor in graph mode.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf;

@tf.function
def func(a):
    i = tf.constant(0);
    return a[i];

t = tf.tuple([tf.constant(0),tf.constant(1)]);
a = func(t);
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28218,Detecting Bounding Boxes along with class labels realtime,"I am using TF Lite Android Image Classifier App provided with this project with GPU support for Mobilenetv1. I want to know how I can detect the bounding boxes of inferred/detected objects along with the class labels?

Can anyone please tell me what modifications I have to make?"
28217,tflite interpreter getting stuck at runForMultipleInputsOutputs function,"I am initializing tflite interpreter to run the object detection inference with GPU delegate option for Yolov3 (adding a GPU delegate) for enabling GPU support on Android Platform (on Pixel 2). But when I run **tflite.runForMultipleInputsOutputs**(inputArray, outputMap);

Its getting stuck there not returning anything while without the GPU delegate that functions working well. Can anyone please tell me why it's getting stuck for indefinitely?

[e.g., If few ops are not performed by GPU that must be performed by CPU this might increase the time of inference but it should provide the output]"
28216,Migrating to 2.0 with confusing MultiRNNCell,"While upgrading my project to tensorflow 2.0, this part of the code is confusing me.

## In tensorflow 1.13.1
```
num_layers = 2
num_units = 128

rnn = tf.contrib.rnn.MultiRNNCell(
            [
                tf.contrib.rnn.BasicLSTMCell(
                    num_units,
                    forget_bias=0.0,
                    state_is_tuple=False,
                )
                for _ in range(num_layers)
            ],
            state_is_tuple=False,
        )

# x.shape == (?, 128)  and state.shape == (?, 512)

x, state = rnn(x, state)

# x.shape == (?, 128)  and state.shape == (?, 512)
```
Everey thing is working fine.


## Upgraded to tensorflow 2.0-alpha0

As documentation: 
https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells

```
cells = [
        tf.keras.layers.LSTMCell(num_units) for _ in range(num_layers)
    ]
rnn = tf.keras.layers.RNN(cells)

# x.shape == (?, 128)  and state.shape == (?, 512)

x, state = rnn(x, state)


but here I got an error and I think is related to shapes of `x` and `state`:



Traceback (most recent call last):
  File ""run.py"", line 285, in <module>
    main()
  File ""run.py"", line 253, in main
    channels=parameters.channels,
  File ""/home/nicoo/py/aocr/400/aocr/model/model.py"", line 168, in __init__
    training=not self.forward_only,
  File ""/home/nicoo/py/aocr/400/aocr/model/seq2seq_model.py"", line 82, in __init__
    hidden_features,
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/nicoo/py/aocr/400/aocr/model/Decoder.py"", line 39, in call
    cell_output, state = self.cell(x, state)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 750, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 538, in __call__
    self._maybe_build(inputs)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1603, in _maybe_build
    self.build(input_shapes)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 619, in build
    self.cell.build(step_input_shape)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 151, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 153, in build
    cell.build(input_shape)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 151, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2022, in build
    constraint=self.kernel_constraint)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 349, in add_weight
    aggregation=aggregation)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 607, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 145, in make_variable
    aggregation=aggregation)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 213, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 176, in _variable_v1_call
    aggregation=aggregation)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 155, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2488, in default_variable_creator
    import_scope=import_scope)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 217, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 294, in __init__
    constraint=constraint)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 406, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 127, in <lambda>
    shape, dtype=dtype, partition_info=partition_info)
  File ""/home/nicoo/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py"", line 499, in __call__
    scale /= max(1., (fan_in + fan_out) / 2.)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
```"
28215,The Nadam optimizer is non-deterministic due to a race condition related to inter_op_parallelism_threads,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): Arch Linux repositories
- TensorFlow version (use command below): 'unknown' 1.13.1
- Python version: 3.7.3
- GPU model and memory: None. I'm running on CPU.

**Describe the current behavior**

When `inter_op_parallelism_threads` is set to `1` the `iterations` variable in Nadam optimizer from `tensorflow/python/keras/optimizers.py` always starts at `1` when the optimizer is executed; when `inter_op_parallelism_threads` is higher than `1` then `iterations` sometimes starts as `0`. The higher the `inter_op_parallelism_threads` is set to the higher the chance that `iterations` will start at `0` instead of at `1`.

(I know this because I `tf.Print`'d everything inside of the Nadam optimizer.)

This makes the training results non-deterministic since the Nadam optimizer depends on the current iteration number when calculating the weight updates.

**Describe the expected behavior**

The Nadam optimizer will be deterministic.

**Code to reproduce the issue**

```python
#!/usr/bin/python

import tensorflow as tf;
import numpy as np;

constant = tf.keras.initializers.constant;

inter_op_parallelism_threads = 1
# inter_op_parallelism_threads = 20

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=inter_op_parallelism_threads)
graph = tf.get_default_graph()
session = tf.Session(graph=graph, config=session_conf)
tf.keras.backend.set_session(session)

optimizer = tf.keras.optimizers.Nadam(lr=1.0, beta_1=0.9, beta_2=0.999, epsilon=1e-7)

bias = constant(np.array([1.0]))
weights = constant(np.array([1.0]))
layer = tf.keras.layers.Dense(1, bias_initializer=bias, kernel_initializer=weights, input_shape=(1,))

model = tf.keras.Sequential()
model.add(layer)
model.compile(optimizer=optimizer, metrics=[""accuracy""], loss=[""mean_squared_error""])

input = np.array([1.0])
output = np.array([1.0])
model.train_on_batch(input, output)

w = layer.get_weights()
print(w)
```

Output when `inter_op_parallelism_threads` is set to 20 (so `iterations` starts at `0`):

```
[array([[-0.0564518]], dtype=float32), array([-0.0564518], dtype=float32)]
```

Output when `inter_op_parallelism_threads` is set to 1 (so `iterations` starts at `1`):

```
[array([[-0.49368942]], dtype=float32), array([-0.49368942], dtype=float32)]
```
"
28214,Inspecting the Tensorflow 2.0 python package: unable to find or import modules,"I'm analyzing the Tensorflow 2.0 Python package using `importlib` to loop through the symbols of the tensorflow package (see: https://github.com/galeone/rtf/blob/master/rtf/generators/base.py#L72 ).

There are certain modules that `importlib` is not able to load or find, e.g. the `tf.losses` `tf.keras.losses`, `tf.optmizers` (and all the module alias) etc. So far, recursively using `importlib.import_module` I get this structure

```
.
├── autograph
├── compat
│   ├── v1
│   │   ├── autograph
│   │   ├── config
│   │   ├── data
│   │   ├── distribute
│   │   ├── estimator
│   │   ├── io
│   │   ├── keras
│   │   │   ├── applications
│   │   │   ├── datasets
│   │   │   ├── mixed_precision
│   │   │   ├── optimizers
│   │   │   ├── preprocessing
│   │   │   └── wrappers
│   │   ├── layers
│   │   ├── lite
│   │   │   └── experimental
│   │   ├── lookup
│   │   ├── nn
│   │   ├── random
│   │   ├── saved_model
│   │   ├── tpu
│   │   ├── train
│   │   └── xla
│   └── v2
│       ├── autograph
│       ├── config
│       ├── data
│       ├── distribute
│       ├── estimator
│       ├── io
│       ├── keras
│       │   ├── applications
│       │   ├── datasets
│       │   ├── mixed_precision
│       │   ├── optimizers
│       │   ├── preprocessing
│       │   └── wrappers
│       ├── lookup
│       ├── random
│       ├── tpu
│       ├── train
│       └── xla
├── config
├── data
├── distribute
├── estimator
├── io
├── keras
│   ├── applications
│   ├── datasets
│   ├── mixed_precision
│   ├── optimizers
│   ├── preprocessing
│   └── wrappers
├── lite
│   ├── experimental
│   │   └── examples
│   │       └── lstm
│   ├── python
│   │   └── optimize
│   └── toco
├── lookup
├── random
├── tools
│   └── docs
├── tpu
├── train
└── xla
```

that is incomplete.

Is there a way to use the `importlib` module to get the complete structure of the tensorflow package and to successfully load the module?

Here's a minimum reproducible example:

## Install

```
pip install --upgrade tf-nightly-2.0-preview
```

## Loading losses

```python
import importlib 
importlib.import_module(""tensorflow.losses"")
```

Produces the error: ""ModuleNotFoundError: No module named 'tensorflow.losses'""

This happens only in Tensorflow 2.0, if I install Tensorflow 1.14 I can inspect more or less every package without any failure (haven't tested in depth).

Disclosure: this is a crosspost from https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/jibh60HPGA0

Greetings"
28213,AttributeError: module 'tensorflow' has no attribute 'keras',"Hello,

I have a question i can't compile while i have installed:
tensorflow                2.0.0a0
Keras                     2.2.4

I got error:
AttributeError: module 'tensorflow' has no attribute 'keras'

my Code:
import tensorflow as tf
mnist = tf.keras.datasets.mnist

I'm in Mac OS X 10.14

Somthing i do wrong?
"
28211,Fatal error C1001 in tensorflow\compiler\xla\literal.cc(1291),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: checkout from `master` branch
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda 4.6.14
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27030.1 for x64, Visual Studio Build Tools 2017
- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0
- GPU model and memory: GTX 1060 6GB

**Describe the problem**
Fatal error C1001: An internal error has occurred in the compiler in tensorflow\compiler\xla\literal.cc(1291).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
(base) Stepii@STEPII D:\Neural\tensorflow
$ python configure.py
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is D:\Programs\Anaconda3\python.exe]:


Found possible Python library paths:
  D:\Programs\Anaconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Programs\Anaconda3\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(base) Stepii@STEPII D:\Neural\tensorflow
$ bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package -j 12

[...]

ERROR: D:/neural/tensorflow/tensorflow/compiler/xla/BUILD:366:1: C++ compilation of rule '//tensorflow/compiler/xla:literal' failed (Exit 3): python.exe failed: error executing command
  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\Stepii\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
    SET TMP=C:\Users\Stepii\AppData\Local\Temp
  D:/Programs/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/_objs/literal/literal.o /c tensorflow/compiler/xla/literal.cc
Execution platform: @bazel_tools//platforms:host_platform
c:\users\stepii\_bazel_stepii\5mniti2w\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1291) : fatal error C1001: An internal error has occurred in the compiler.
(compiler file 'd:\agent\_work\1\s\src\vctools\compiler\utc\src\p2\main.c', line 187)
 To work around this problem, try simplifying or changing the program near the locations listed above.
Please choose the Technical Support command on the Visual C++
 Help menu, or open the Technical Support help file for more information
  cl!InvokeCompilerPassW()+0x3e708

c:\users\stepii\_bazel_stepii\5mniti2w\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1291) : fatal error C1001: An internal error has occurred in the compiler.
(compiler file 'd:\agent\_work\1\s\src\vctools\compiler\utc\src\common\error.c', line 835)
 To work around this problem, try simplifying or changing the program near the locations listed above.
Please choose the Technical Support command on the Visual C++
 Help menu, or open the Technical Support help file for more information
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1698.097s, Critical Path: 416.10s
INFO: 3624 processes: 3624 local.
FAILED: Build did NOT complete successfully
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

See [FullLog.txt](https://github.com/tensorflow/tensorflow/files/3123796/FullLog.txt).
"
28208,Which API can implement tensor expansion in tensorflow ？,"
If  I have a tensor of (30,40,50), and I want to expand it out to the first order, then I get a second order tensor of (30,2000), and I don't know if tensorflow has an API that implements it.
For example

import tensorflow as tf
import numpy as np
data1=tf.constant([[[2,5,7,8],[6,4,9,10],[14,16,86,54]],
                 [[16,43,65,76],[43,65,7,24],[15,75,23,75]]])

data5=tf.reshape(data1,[3,8])

data2,data3,data4=tf.split(data1,3,1)
data6=tf.reshape(data2,[1,8])
data7=tf.reshape(data3,[1,8])
data8=tf.reshape(data4,[1,8])
data9=tf.concat([data6,data7,data8],0)

with tf.Session() as sess:
    print(sess.run(data5))
    print(sess.run(data))

data5
 [[ 2  5  7  8  6  4  9 10]
 [14 16 86 54 16 43 65 76]
 [43 65  7 24 15 75 23 75]]

data9
[[ 2  5  7  8 16 43 65 76]
 [ 6  4  9 10 43 65  7 24]
 [14 16 86 54 15 75 23 75]]


How do I get data9 directly(Or maybe I got it wrong)

"
28207,Where is F1_score located in tf.keras,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 1.13
Ubuntu 14.04 LTS
- Are you willing to contribute it (Yes/No): YES



**Describe the feature and the current behavior/state.**
Basically there was an previous issue regarding multi-lable classifier functionalities like(micro,macro) are not present in F1_score calculation. So I tried to add these features but later wards Alextp asked me to add this features in tf.keras as tensorflow is now gong to depend on that for TF2.0, but I am not getting where F1_score is defined in tf.Keras?

**Will this change the current api? How?**
NO

**Who will benefit with this feature?**
All those who wants multi-lable classifiers to be added to F1_score calculations
**Any Other info.**
"
28206,Can't convert to Tensorflow Lite with operation Cast,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.12.0


**Provide the text output from tflite_convert**

```
Traceback (most recent call last):
  File ""predict_tflite.py"", line 740, in <module>
    evaluatePlanes(args)
  File ""predict_tflite.py"", line 95, in evaluatePlanes
    predictions = getResults(options)
  File ""predict_tflite.py"", line 289, in getResults
    pred_dict = getPredictionCustom(options)
  File ""predict_tflite.py"", line 538, in getPredictionCustom
    tflite_model = converter.convert()
  File ""/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 453, in convert
    **converter_kwargs)
  File ""/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 342, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/alexfu/PlaneNet/venv/local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 135, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2019-04-27 02:28:16.893332: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1517 operators, 2341 arrays (0 quantized)
2019-04-27 02:28:16.938974: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1517 operators, 2341 arrays (0 quantized)
2019-04-27 02:28:16.939290: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_unary.cc:164] Unsupported cast op input type
Aborted (core dumped)

None
```

Also, please include a link to a GraphDef or the model if possible.
https://mega.nz/#!sjpT2DiQ!Uo-6hxyldmtnPoKk3TTdUHKZADRGy6nIPlmAeVzJs_8
(Actually using DeepLab in this model)

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28203,404 error on TensorFlow Lite model repository,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**
This link is unreachable
```
Table 1: Top-1 accuracy of floating point and fully quantized CNNs on Imagenet Validation dataset.
Our pre-trained models are available in the TensorFlow Lite model repository. The code used to generate these models is available.
```


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
28200,Deprecation warning in ctc_batch_cost function (python/keras/backend.py),"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

OS: Arch linux
Tensorflow: 2.0.0alpha0

WARNING: Logging before flag parsing goes to stderr. 
W0426 20:17:50.524740 140527476778624 deprecation.py:323] From /python3.7/site-packages/tensorflow/python/keras/backend.py:5151: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. 
Instructions for updating:
Use `tf.cast` instead.

W0426 20:17:50.689021 140527476778624 deprecation.py:323] From /python3.7/site-packages/tensorflow/python/keras/backend.py:5130: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.

"
28198,Building 2.0 from master branch using --config=v2 creating 1.13.1 version,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source 
- TensorFlow version: tried to 
- Python version: 3.5.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 6.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Tried to build and install tensorflow 2.0 from master branch using --config=v2. It is building but the build version is 1.13.1. After installing the version it also shows 1.13.1 while run: `tf.__version__`

How can I build tensorflow 2.0 from master branch? 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel --output_base=$tf_deps/ build \ 
--config=v2 \ 
--copt=""-O3"" --copt=""-g"" -s -c opt \ 
//tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. 
```
import tensorflow as tf
tf.__version__
1.13.1
```
"
28197,while_loop inside @tf.contrib.eager.defun only loop once,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`tf.while_loop` only loops once
**Describe the expected behavior**
`tf.while_loop` only loops more than once
**Code to reproduce the issue**
```
@tf.contrib.eager.defun
def fn():
    idx = 0
    max_iter = 5
    
    def loop_body(idx):
        print('loop once')
        return idx + 1
    
    tf.while_loop(
        lambda idx: idx < max_iter,
        lambda idx: loop_body(idx),
        [idx]
        )
    
    
fn() # -> only printed once
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28195,[C API] while loop: unable to access operations defined outside of the loop from within the loop,"*System information*

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.4
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
* TensorFlow installed from (source or binary): source
* TensorFlow version (use command below): master
* Python version: Python 3.7.3
* Bazel version (if compiling from source): 0.24.1
* GCC/Compiler version (if compiling from source): 
* CUDA/cuDNN version: N/A
* GPU model and memory: N/A

*Describe the current behavior*

I am unable to access operations defined outside of the while loop from within the loop. 

The C API while loop creates separate conditional and body graphs, so an error is thrown when we try to use operations defined in the outer graph within the body graph. See earlier discussion with @skye [here](https://github.com/tensorflow/tensorflow/issues/26371#issuecomment-482358762).

*Describe the expected behavior*

I would expect the behavior of the C API while loop to match the Python API while loop, where accessing operations defined outside the while loop works. 

I've included a minimal working example in Python that demonstrates the expected behavior below. In this example, we are able to use “increment” in the loop body even though it's defined outside the loop.

```
import tensorflow as tf

increment = tf.constant(1, name='one')

def loop_cond(loop_var):
    return tf.math.less(loop_var, 10)

def loop_body(loop_var):
    return loop_var + increment

loop_input = tf.Variable(0, name='loop_input')

loop_output = tf.while_loop(loop_cond, loop_body, [loop_input])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(loop_input)) // should be 0
    print(sess.run(loop_output)) // should be 10 
```


*Code to reproduce the issue*

I replicated the example above as a unit test in while_loop_test.cc. Once again, we try to use “increment” within the loop body, but here we get an error since it's not part of the body graph. The error message is “Node 'add': Unknown input node 'scalar'”. 

```
TEST_F(CApiWhileLoopTest, AccessOuterOp) {
  Init(1);
  // increment = 1
  // while (i < 10) {
  //   i = i + increment
  // }

  // Create increment *in the outer graph*
  TF_Operation* increment = ScalarConst(1, graph_, s_);

  // Create cond graph: i < 10 
  TF_Operation* ten = ScalarConst(10, params_->cond_graph, s_);
  TF_Operation* less_than = LessThan(params_->cond_inputs[0], {ten, 0}, params_->cond_graph, s_);
  DCHECK_EQ(TF_OK, TF_GetCode(s_)) << TF_Message(s_);
  params_->cond_output = {less_than, 0};

  // Create body graph: i = i + increment
  TF_Operation* add = Add(params_->body_inputs[0], {increment, 0}, params_->body_graph, s_);
  ASSERT_EQ(TF_OK, TF_GetCode(s_)) << TF_Message(s_);
  params_->body_outputs[0] = {add, 0};

  ExpectOK();
}
```
This test can be copy-pasted into while_loop_test.cc and run with the following command:
 bazel run //tensorflow/c:while_loop_test. 


*Other info / logs*
The above examples are intended to be as minimal as possible, so they're not practical. However, accessing outside operations would be important when updating external variables, for example when training within a loop (we would need to update external weights and biases).

We discovered this issue while using TF Java, after exposing the C API while loop to Java in [this commit](https://github.com/tensorflow/tensorflow/commit/6799f6e990f856452c467f31448c211fce94bbc1).


"
28194,Cuda driver version is inssufficient for CUDA runtime version,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSUSE 13.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9/7
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176
linux-xzga:~ # nvidia-smi
Wed Apr 10 17:49:28 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.116 Driver Version: 390.116 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 GeForce GT 720 Off | 00000000:02:00.0 N/A | N/A |
| N/A 53C P0 N/A / N/A | 161MiB / 977MiB | N/A Default |
+-------------------------------+----------------------+----------------------+
| 1 Tesla K40c Off | 00000000:03:00.0 Off | 0 |
| 29% 62C P0 65W / 235W | 220MiB / 11441MiB | 0% Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 Not Supported |
| 1 3389 C /opt/anaconda2/envs/tf-gpu/bin/python 69MiB |
| 1 21633 C /opt/anaconda2/envs/tf-gpu/bin/python 69MiB |
| 1 26550 C /opt/anaconda2/envs/tf-gpu/bin/python 69MiB |
+-----------------------------------------------------------------------------+
~ #"
28192,tf.fft2d not working properly for complex64 data,"**System information**
- Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): print(tensorflow.GIT_VERSION, tensorflow.VERSION)
b'unknown' 1.13.1
- Python version: 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]
- CUDA 10, cudnn 64_10
- GPU model and memory: Titan X pascal

**Describe the current behavior**

fft2d produces non-sensical results on complex64 datatype. (also verified complex128 broken)

**Code to reproduce the issue**

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import numpy as np
import scipy as sp
import scipy.fftpack
import tensorflow as tf
import matplotlib.pyplot as plt

#===================================================================================================
# Generate data
slc = sp.misc.face(gray=True).astype('float32')
slc = (slc + 1j*slc)[0:256, 0:256]
tileSize = slc.shape[0]

#===================================================================================================
freqImg = np.fft.fft2(slc)

#===================================================================================================
# Now do this in keras/tf
input1 = tf.keras.layers.Input(shape=(tileSize, tileSize, 1), dtype='complex128')
out = tf.keras.layers.Lambda(lambda x:tf.spectral.fft2d(x))(input1)
model = tf.keras.models.Model(input1, out)
freqImg_tf = model.predict(slc[None,:,:,None])

plt.figure(); plt.imshow(np.log(np.abs(freqImg)+1)); plt.title('sp')
plt.figure(); plt.imshow(np.log(np.abs(freqImg_tf[0,:,:,0])+1)); plt.title('keras'); plt.show()

#===================================================================================================
# Plots




print('done')
```"
28186,padding causal with conv1d gives an error,"
**System information**
- I wrote custom code
- System : Linux
- TensorFlow 1.11
- Keras version(embedded in TF) '2.1.6-tf'

It seems keras Conv1D (embedded in Tensorflow) does not support 'causal' in Conv1D contrary to what is stated in the class doc string in the code. Here is a simple code to reproduce

    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers.convolutional import Conv1D
    model = Sequential()
    model.add(InputLayer(name='input_layer', input_shape=(5,3,)))
    model.add(Conv1D(1, kernel_size=1, padding='causal'))

This results in the error

    ValueError: Attr 'padding' of 'Conv2D' Op passed string 'CAUSAL' not in: ""SAME"", ""VALID"".

Any help please? (please notice I am trying to construct a `Conv1D` layer but the error mentions `Conv2D`"
28185,PhasedLSTMCell only accepts 1-dimensional time input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Docker image, details below;
```
REPOSITORY              TAG                      IMAGE ID            CREATED             SIZE
tensorflow/tensorflow   latest-gpu-py3-jupyter   8c77abe6b462        8 weeks ago         3.57GB
```

- TensorFlow installed from (source or binary):
Binary

- TensorFlow version (use command below):
Git Version: v1.13.1-0-g6612da8951
Version: 1.13.1

- Python version:
3.5.2

- CUDA/cuDNN version:

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
```

- GPU model and memory:
NVIDIA GTX 1050 Ti

**Describe the current behavior**

When passing multi-dimensional time input into a Phased LSTM Cell, I am presented with an arithmetic error;

```
ValueError: Dimensions must be equal, but are 6 and 256 for 'test/rnn/sub' (op: 'Sub') with input shapes: [?,6], [256].
```

This appears to be caused by the following line in `rnn_cell.py`, which seems to make the assumption that `time` has one dimension;

```
~usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py in _get_cycle_ratio(self, time, phase, period)
   1965     phase_casted = math_ops.cast(phase, dtype=time.dtype)
   1966     period_casted = math_ops.cast(period, dtype=time.dtype)
-> 1967     shifted_time = time - phase_casted
   1968     cycle_ratio = self._mod(shifted_time, period_casted) / period_casted
   1969     return math_ops.cast(cycle_ratio, dtype=dtypes.float32)
```

**Describe the expected behavior**

LSTM computes phases and `shifted_time` for *each* parameter separately (in this case, this would require `shifted_time` and other relevant variables to be the same size as the number of features in the input - e.g. 6 input features implies that `shifted_time` should be a 6-dimensional vector)


**Code to reproduce the issue**

```python
import tensorflow as tf

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.utils import Sequence

from tensorflow.keras.layers import (
    RNN,
    Input,
    Dense
)

from tensorflow.contrib.rnn import PhasedLSTMCell

K.clear_session()

with tf.variable_scope('test', reuse=tf.AUTO_REUSE):
    t_series_dim = 6
    output_dim = 1

    # Inputs here
    timestamps = Input(shape=(None, t_series_dim,), name='t_input')
    feature_signals = Input(shape=(None, t_series_dim,), name='ts_input')

    # Layer configuration
    ## Time-series input
    time_input = (timestamps, feature_signals)
    
    cell = PhasedLSTMCell(256)
    ts_x = RNN(cell, return_sequences=False)(time_input)
    
    # Output layer
    logits = Dense(output_dim, name='logits', activation='sigmoid')(ts_x)

    # Stick it all together inside a model...
    model = Model(
        inputs=[timestamps, feature_signals],
        outputs=[logits]
    )

    # Define an optimiser and metrics, and compile the model
    model.compile(
        optimizer='adam',
        loss=['binary_crossentropy'],
        metrics=['binary_accuracy']
    )
```

**Other info / logs**

To give you some context, I have several sensors generating signals at uneven sample rates. For example, ""sensor 1"" reports at a frequency of 2Hz, while others may range up to 16Hz. Each of these features has their own time stamps (e.g. the first 5 time stamps for ""sensor 1"" at 2Hz would be something like [0.0, 0.5, 1.0, 1.5, 2.0] - while ""sensor 2"" at 8Hz would be something like [0.0, 0.125, 0.25, 0.375, 0.5]) so it's more helpful to provide each individual feature's timestamps alongside the feature.

I'm not sure if I am describing this adequately. Please ask for clarification if required!"
28184,Tensorflow v1.13.1 CUDA Compilation fails on Windows 10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.13.1
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 7.5.1
- GPU model and memory: GeForce GTX1080 8Gb

**Describe the problem**

Tensorflow fails to build when configured to use CUDA

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Installed Python 3.7.0
	- With pip as optional feature
	- Adding Python to environment variables

- Installed MSYS2
	pacman -Syu
	pacman -Su
	pacman -S git patch unzip

- Installed Bazel 0.21.0
	- Downloaded bazel-0.21.0-windows-x86_64.exe
	- Renamed it to bazel.exe
	- Moved bazel.exe to C:\bazel (C:\bazel\bazel.exe)
	- Added C:\bazel to the user variables PATH
	- Added C:\msys64\usr\bin to user variables PATH
	- Added C:\msys64\usr\bin\bash.exe to a new BAZEL_SH user variable

- Installed CUDA 10.0

	- Downloaded and installed NVIDIA GeForce Experience to install NVIDIA GPU drivers v430.39 on GTX1080 card
	- Downloaded and installed CUDA Toolkit 10.0 from https://developer.nvidia.com/cuda-toolkit-archive
	- Downloaded and installed cuDNN SDK 7.5.1 for CUDA 10 from https://developer.nvidia.com/cudnn
		- Downloaded cudnn-10.0-windows10-x64-v7.5.1.10.zip from https://developer.nvidia.com/cudnn
		- Copied cuda\bin\cudnn64_7.dll to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin
		- Copied cuda\include\cudnn.h to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include
		- Copied cuda\lib\x64\cudnn.lib to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\lib\x64
		- Checked CUDA_PATH was set to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0
		- Added the following paths to the user environment variable PATH
           		C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin
           		C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64
          		C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include
           		C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0

- Installed Tensorflow dependencies

	pip3 install six numpy wheel
	pip3 install keras_applications==1.0.6 --no-deps
	pip3 install keras_preprocessing==1.0.5 --no-deps

- Downloaded and configured Tensorflow v.1.13.1 for CUDA support

	git clone https://github.com/tensorflow/tensorflow.git
	cd tensorflow
    	git checkout r1.13
	cd tensorflow
	bazel clean
	python ./configure.py

	When prompted “Do you wish to build TensorFlow with CUDA support?” selected “y”
	When prompted to specify the CUDA compute capabilitied, specified 3.5,6.1 (for GeForce GTX1080)

- Build command that fails

	bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
	
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The following file contains the configure step of Tensorflow and the console log of the build command that fails for me:

[2019.04.26_tensorflow_1.13.1_cuda10_build_error_windows10.txt](https://github.com/tensorflow/tensorflow/files/3121627/2019.04.26_tensorflow_1.13.1_cuda10_build_error_windows10.txt)


"
28183,Replacing keras by tensorflow.python.keras causes AttributeError: 'ThreadsafeIterator' object has no attribute 'shape',"System information
- OS Platform and Distribution (Windows 10 64bit):
- TensorFlow installed from (pip):
- TensorFlow version (1.13.1):
- Python version: 3.6
- CUDA/cuDNN version: 10
- GPU model and memory: RTX 2060 6GB
- Keras 2.2.4

Using PyCharm.

**Describe the problem.**
When I try my model with keras and tensorflow.python.keras I found difference.
The model on keras works but when I use tensorflow.python.keras throwing errors:

`File ""C:\python36_ver\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1427, in fit_generator
    initial_epoch=initial_epoch)`
`File ""C:\python36_ver\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 119, in model_iteration
    shuffle=shuffle)`
`File ""C:\python36_ver\lib\site-packages\tensorflow\python\keras\engine\training_generator.py"", line 399, in convert_to_generator_like
    num_samples = int(nest.flatten(data)[0].shape[0])`
`AttributeError: 'ThreadsafeIterator' object has no attribute 'shape'`
"
28182,output shape not inferred for slicing operations,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

This code does not work because slicing doesn't seem to make an attempt to infer output shape. I believe In most cases output shape could be inferred. 

```
x = keras.Input(shape=(32, 32, 3))
x = keras.layers.Conv2D(8, (3,3), padding='same')(x)
slice1 = x[:, :, :, :4]
slice2 = x[:, :, :, 4:]
# slice1 = x[..., :4]  # or, even better, infer output shape with this slice notation
# slice2 = x[..., 4:]  # also should be possible with no API impact
# slice1.set_shape((None, 32, 32, 4))  # shape not inferred, manually set shape
# slice2.set_shape((None, 32, 32, 4))
x = keras.layers.concatenate([slice1, slice2], axis=3)
x = keras.layers.Flatten()(x)
out = keras.layers.Dense(10)(x)
```

**Will this change the current api? How?**

This should be possible with no impact on API. 

**Who will benefit with this feature?**

I would imagine fairly broad benefit, but probably more benefit to ML researchers and others that are building custom networks. I ran into this when doing my first port to TF2 keras API.

**Any Other info.**

Here is traceback for above code:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-579a9f498ccc> in <module>
      5 # slice1.set_shape((None, 32, 32, 4))  # shape not inferred, manually set shape
      6 # slice2.set_shape((None, 32, 32, 4))
----> 7 x = keras.layers.concatenate([slice1, slice2], axis=3)
      8 x = keras.layers.Flatten()(x)
      9 out = keras.layers.Dense(10)(x)

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/layers/merge.py in concatenate(inputs, axis, **kwargs)
    686       A tensor, the concatenation of the inputs alongside axis `axis`.
    687   """"""
--> 688   return Concatenate(axis=axis, **kwargs)(inputs)
    689 
    690 

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    592           # Build layer if applicable (if the `build` method has been
    593           # overridden).
--> 594           self._maybe_build(inputs)
    595           # Explicitly pass the learning phase placeholder to `call` if
    596           # the `training` argument was left unspecified by the user.

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   1711     # Only call `build` if the user has manually overridden the build method.
   1712     if not hasattr(self.build, '_is_default'):
-> 1713       self.build(input_shapes)
   1714     # We must set self.built since user defined build functions are not
   1715     # constrained to set self.built.

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in wrapper(instance, input_shape)
    287     # This preserves compatibility with external Keras.
    288     if input_shape is not None:
--> 289       input_shape = convert_shapes(input_shape, to_tuples=True)
    290     output_shape = fn(instance, input_shape)
    291     # Return shapes from `fn` as TensorShapes.

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in convert_shapes(input_shape, to_tuples)
    220 
    221   return map_structure_with_atomic(_is_atomic_shape, _convert_shape,
--> 222                                    input_shape)
    223 
    224 

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in map_structure_with_atomic(is_atomic_fn, map_fn, nested)
    167     values = nested
    168   mapped_values = [
--> 169       map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values
    170   ]
    171   return nest._sequence_like(nested, mapped_values)

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in <listcomp>(.0)
    167     values = nested
    168   mapped_values = [
--> 169       map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values
    170   ]
    171   return nest._sequence_like(nested, mapped_values)

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in map_structure_with_atomic(is_atomic_fn, map_fn, nested)
    156   """"""
    157   if is_atomic_fn(nested):
--> 158     return map_fn(nested)
    159 
    160   # Recursively convert.

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in _convert_shape(input_shape)
    216     input_shape = tensor_shape.TensorShape(input_shape)
    217     if to_tuples:
--> 218       input_shape = tuple(input_shape.as_list())
    219     return input_shape
    220 

~/venvs/lidar_nnfusion/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in as_list(self)
   1126     """"""
   1127     if self._dims is None:
-> 1128       raise ValueError(""as_list() is not defined on an unknown TensorShape."")
   1129     return [dim.value for dim in self._dims]
   1130 

ValueError: as_list() is not defined on an unknown TensorShape.
```"
28181,TF 2.0 tf.summary.scalar claiming it sees a whole tensor when fed scalar,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): My script is included below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16
- TensorFlow installed from (source or binary): conda environment with pip install tensorflow==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0. I also, just to check, ran pip install --upgrade tb-nightly I believe on the 25th.
- Python version: 3.6
- CUDA/cuDNN version: NO GPU USED

**Describe the current behavior**

When I run the simple MNIST script below (with the tf.summary.scalar line commented out) it runs fine. When I try to log a scalar inside Model.call() I get an error message that seems to indicate tf.summary.scalar is seeing the original tensor--somehow--as opposed to the numpy-summed result that I am passing to tf.summary.scalar.

**Describe the expected behavior**

According to https://www.tensorflow.org/tensorboard/r2/scalars_and_keras, this would appear to be the way to log scalar values in Tensorboard 2.0. Am I incorrect?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Code:

`
```
""""""
Learn the very basics by running MNIST in TF 2.0
""""""
import tensorflow as tf
from tensorflow.keras import datasets, layers
import numpy as np
import datetime
L2_REG = 0.0000001
LEARN_RATE = 1e-5
NUM_LABELS = 10


class MyFirstConvnet(tf.keras.Model):
    def __init__(self):
        super(MyFirstConvnet, self).__init__()
        self.layer1 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.layer2 = layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.pool = layers.MaxPooling2D((2, 2))
        self.layer3 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REG))
        self.flatten = layers.Flatten()
        self.classifier = layers.Dense(NUM_LABELS, activation='softmax')
        self.batchnorm1 = layers.BatchNormalization(scale=False)
        self.batchnorm2 = layers.BatchNormalization(scale=False)

    def call(self, inputs):
        x = self.batchnorm1(self.layer1(inputs))
        x = self.layer2(x)
        tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
        x = self.batchnorm2(self.pool(x))
        x = self.layer3(x)
        x = self.flatten(x)
        return self.classifier(x)


if __name__ == '__main__':
    # load up MNIST
    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
    train_images = train_images.reshape((60000, 28, 28, 1)).astype(np.float32)/255.0
    test_images = test_images.reshape((10000, 28, 28, 1)).astype(np.float32)/255.0

    # model must be 'compiled' which integrates information about training and stores it in the model structure
    model = MyFirstConvnet()
    optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # set up tensorboard
    logdir = ""logs/scalars/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
    file_writer = tf.summary.create_file_writer(logdir)
    file_writer.set_as_default()
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

    # train
    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.05, shuffle=True,
              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True),
                         tensorboard_callback])

    # test
    model.evaluate(test_images, test_labels, verbose=1)
```
`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error message:

> 2019-04-26 08:41:48.064844: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-26 08:41:48.089158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394490000 Hz
2019-04-26 08:41:48.090339: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b1385baf40 executing computations on platform Host. Devices:
2019-04-26 08:41:48.090379: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""hello_mnist.py"", line 55, in <module>
    tensorboard_callback])
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 806, in fit
    shuffle=shuffle)
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2503, in _standardize_user_data
    self._set_inputs(cast_inputs)
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2775, in _set_inputs
    outputs = self.call(inputs)
  File ""hello_mnist.py"", line 28, in call
    tf.summary.scalar('layer_2_activation_sum', data=np.sum(x, axis=None))  # ERROR THROWN HERE
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorboard/plugins/scalar/summary_v2.py"", line 61, in scalar
    tf.debugging.assert_scalar(data)
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 1718, in assert_scalar_v2
    assert_scalar(tensor=tensor, message=message, name=name)
  File ""/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 1751, in assert_scalar
    % (message or '', tensor.name, shape))
ValueError: Expected scalar shape for conv2d_1/Relu:0, saw shape: (None, 24, 24, 64).


"
28178,TfLite Multithreaded_Conv op don't works as expect,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I'm trying to compile Tensorflow Lite to WASM for machine learning on Web. But I found the TfLite MultiThread-Conv have some bugs. And my Conv2D test case can't pass when use the WASM version of this op.

See the code in [TFLite multithreaded_conv.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h):
 Since `(Const)EigenMatrix` is defined as `Eigen::RowMajor` Layout, should the code in [here](https://github.com/tensorflow/tensorflow/blob/55e854864fded7318c49daae0b634c5860f3e419/tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h#L101) be
```c
dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 1);
EigenMatrix output(output_data, input_batches * conv_width, filter_count);
ConstEigenMatrix input(input_data, input_batches * conv_width, input_depth);
ConstEigenMatrix filter(filter_data, filter_count, input_depth);
```
rather than 
```c
dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);
EigenMatrix output(output_data, input_batches * conv_width, filter_count);
ConstEigenMatrix input(input_data, input_batches * conv_width, input_depth);
ConstEigenMatrix filter(filter_data, input_depth, filter_count);
```

Same issues are in https://github.com/tensorflow/tensorflow/blob/55e854864fded7318c49daae0b634c5860f3e419/tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h#L115 and https://github.com/tensorflow/tensorflow/blob/55e854864fded7318c49daae0b634c5860f3e419/tensorflow/core/kernels/eigen_spatial_convolutions-inl.h#L1482.

And after correcting all the place, all my test case passed.

Is this a TFLite ops bug?? Or just my test case is not compatible with this operation? 　



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

"
28177,no project building,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
28176,TPU + Keras doesn't support multiple input layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Google Colab

**Describe the current behavior**
When trying to train a model that has multiple input layers using Keras (eg. CTC) on a TPU, Tensorflow throws an InvalidArgumentError claiming that I didn't feed a value for a placeholder tensor. 

**Describe the expected behavior**
If I run the exact same model on a GPU, everything works as expected (eg. the model trains).

**Code to reproduce the issue**
All code required to reproduce this but can be found on Google Colab:
https://colab.research.google.com/drive/1rxKwCfDPS_2GPNN9yWAyQlQZy8Fg3deV

**Other info / logs**
Possibly related issue: https://github.com/tensorflow/tensorflow/issues/23659

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

InvalidArgumentError: Combined status information from 9 operations:

Status code: Invalid argument [2x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[{{node label}}]] [1x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[{{node label}}]]
  	 [[{{node GroupCrossDeviceControlEdges_0/TPUReplicate/_compile/_11823778269055837323/_17}}]] [1x]
(7 successful operations.)

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-6495be1fe4ea> in <module>()
      3     x = feed_dict_x,
      4     y = feed_dict_y,
----> 5     batch_size = BATCH_SIZE
      6 )

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1530                                   validation_split, validation_data, shuffle,
   1531                                   class_weight, sample_weight, initial_epoch,
-> 1532                                   steps_per_epoch, validation_steps, **kwargs)
   1533       finally:
   1534         self._numpy_to_infeed_manager_list = []

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _pipeline_fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1631         initial_epoch=initial_epoch,
   1632         steps_per_epoch=steps_per_epoch,
-> 1633         validation_steps=validation_steps)
   1634 
   1635   def _pipeline_fit_loop(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _pipeline_fit_loop(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)
   1730             val_sample_weights=val_sample_weights,
   1731             validation_steps=validation_steps,
-> 1732             epoch_logs=epoch_logs)
   1733 
   1734       callbacks.on_epoch_end(epoch, epoch_logs)

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _pipeline_fit_loop_sample_wise(self, ins, callbacks, index_array, shuffle, batch_size, num_training_samples, indices_for_conversion_to_dense, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)
   1786 
   1787       outs = f.pipeline_run(
-> 1788           cur_step_inputs=ins_last_batch, next_step_inputs=ins_batch)
   1789       ins_last_batch = ins_batch
   1790 

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in pipeline_run(***failed resolving arguments***)
   1334           next_input_tensors)
   1335       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(
-> 1336           next_input_specs, next_step_infeed_manager)
   1337       infeed_dict = next_infeed_instance.make_feed_dict(next_tpu_model_ops)
   1338 

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _tpu_model_ops_for_input_specs(self, input_specs, infeed_manager)
   1169                                                  infeed_manager)
   1170       self._compilation_cache[shape_key] = new_tpu_model_ops
-> 1171       self._test_model_compiles(new_tpu_model_ops)
   1172 
   1173     return self._compilation_cache[shape_key]

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _test_model_compiles(self, tpu_model_ops)
   1107     start_time = time.time()
   1108 
-> 1109     result = K.get_session().run(tpu_model_ops.compile_op)
   1110     proto = tpu_compilation_result.CompilationResultProto()
   1111     proto.ParseFromString(result)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

InvalidArgumentError: Combined status information from 9 operations:

Status code: Invalid argument [2x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[node label (defined at <ipython-input-2-85a234430d50>:9) ]] [1x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[node label (defined at <ipython-input-2-85a234430d50>:9) ]]
  	 [[{{node GroupCrossDeviceControlEdges_0/TPUReplicate/_compile/_11823778269055837323/_17}}]] [1x]
(7 successful operations.)

Caused by op 'label', defined at:
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 832, in start
    self._run_callback(self._callbacks.popleft())
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 605, in _run_callback
    ret = callback()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 536, in <lambda>
    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-18d98e7a13ff>"", line 4, in <module>
    model = build_training_model(graph)
  File ""<ipython-input-2-85a234430d50>"", line 9, in build_training_model
    label        = Input(name = 'label',        batch_shape = [BATCH_SIZE, MAX_LABEL_LENGTH], dtype = 'int32')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py"", line 231, in Input
    input_tensor=tensor)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py"", line 107, in __init__
    name=self.name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py"", line 876, in placeholder
    x = array_ops.placeholder(dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py"", line 2077, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 5791, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Combined status information from 9 operations:

Status code: Invalid argument [2x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[node label (defined at <ipython-input-2-85a234430d50>:9) ]] [1x]
  You must feed a value for placeholder tensor 'label' with dtype int32 and shape [256,12]
  	 [[node label (defined at <ipython-input-2-85a234430d50>:9) ]]
  	 [[{{node GroupCrossDeviceControlEdges_0/TPUReplicate/_compile/_11823778269055837323/_17}}]] [1x]
(7 successful operations.)
```

I also tried training the model using `model.fit_generator(...)` and `tf.keras.utils.Sequence`, but the same exception gets thrown."
28175,Quantization-aware training Fails,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: One plus 5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version:3.6
- Bazel version (if compiling from source):bazel-0.24.0-installer-darwin-x86_64
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
I am referring to https://github.com/tensorflow/tensorflow/issues/20867. I am currently running [Quantization-aware training](https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize) and I have modified my deeplab->train.py file. When I start training using train.py :
```

# Copyright 2018 The TensorFlow Authors All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Training script for the DeepLab model.

See model.py for more details and usage.
""""""

import six
import tensorflow as tf
from tensorflow.python.ops import math_ops
from deeplab import common
from deeplab import model
from deeplab.datasets import data_generator
from deeplab.utils import train_utils

flags = tf.app.flags
FLAGS = flags.FLAGS

# Settings for multi-GPUs/multi-replicas training.

flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy.')

flags.DEFINE_boolean('clone_on_cpu', False, 'Use CPUs to deploy clones.')

flags.DEFINE_integer('num_replicas', 1, 'Number of worker replicas.')

flags.DEFINE_integer('startup_delay_steps', 15,
                     'Number of training steps between replicas startup.')

flags.DEFINE_integer(
    'num_ps_tasks', 0,
    'The number of parameter servers. If the value is 0, then '
    'the parameters are handled locally by the worker.')

flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')

flags.DEFINE_integer('task', 0, 'The task ID.')

# Settings for logging.

flags.DEFINE_string('train_logdir', None,
                    'Where the checkpoint and logs are stored.')

flags.DEFINE_integer('log_steps', 10,
                     'Display logging information at every log_steps.')

flags.DEFINE_integer('save_interval_secs', 1200,
                     'How often, in seconds, we save the model to disk.')

flags.DEFINE_integer('save_summaries_secs', 600,
                     'How often, in seconds, we compute the summaries.')

flags.DEFINE_boolean(
    'save_summaries_images', False,
    'Save sample inputs, labels, and semantic predictions as '
    'images to summary.')

# Settings for profiling.

flags.DEFINE_string('profile_logdir', None,
                    'Where the profile files are stored.')

# Settings for training strategy.

flags.DEFINE_enum('learning_policy', 'poly', ['poly', 'step'],
                  'Learning rate policy for training.')

# Use 0.007 when training on PASCAL augmented training set, train_aug. When
# fine-tuning on PASCAL trainval set, use learning rate=0.0001.
flags.DEFINE_float('base_learning_rate', .0001,
                   'The base learning rate for model training.')

flags.DEFINE_float('learning_rate_decay_factor', 0.1,
                   'The rate to decay the base learning rate.')

flags.DEFINE_integer('learning_rate_decay_step', 2000,
                     'Decay the base learning rate at a fixed step.')

flags.DEFINE_float('learning_power', 0.9,
                   'The power value used in the poly learning policy.')

flags.DEFINE_integer('training_number_of_steps', 30000,
                     'The number of steps used for training')

flags.DEFINE_float('momentum', 0.9, 'The momentum value to use')

# When fine_tune_batch_norm=True, use at least batch size larger than 12
# (batch size more than 16 is better). Otherwise, one could use smaller batch
# size and set fine_tune_batch_norm=False.
flags.DEFINE_integer('train_batch_size', 8,
                     'The number of images in each batch during training.')

# For weight_decay, use 0.00004 for MobileNet-V2 or Xcpetion model variants.
# Use 0.0001 for ResNet model variants.
flags.DEFINE_float('weight_decay', 0.00004,
                   'The value of the weight decay for training.')

flags.DEFINE_multi_integer('train_crop_size', [513, 513],
                           'Image crop size [height, width] during training.')

flags.DEFINE_float(
    'last_layer_gradient_multiplier', 1.0,
    'The gradient multiplier for last layers, which is used to '
    'boost the gradient of last layers if the value > 1.')

flags.DEFINE_boolean('upsample_logits', True,
                     'Upsample logits during training.')

# Hyper-parameters for NAS training strategy.

flags.DEFINE_float(
    'drop_path_keep_prob', 1.0,
    'Probability to keep each path in the NAS cell when training.')

# Settings for fine-tuning the network.

flags.DEFINE_string('tf_initial_checkpoint', None,
                    'The initial checkpoint in tensorflow format.')

# Set to False if one does not want to re-use the trained classifier weights.
flags.DEFINE_boolean('initialize_last_layer', True,
                     'Initialize the last layer.')

flags.DEFINE_boolean('last_layers_contain_logits_only', False,
                     'Only consider logits as last layers or not.')

flags.DEFINE_integer('slow_start_step', 0,
                     'Training model with small learning rate for few steps.')

flags.DEFINE_float('slow_start_learning_rate', 1e-4,
                   'Learning rate employed during slow start.')

# Set to True if one wants to fine-tune the batch norm parameters in DeepLabv3.
# Set to False and use small batch size to save GPU memory.
flags.DEFINE_boolean('fine_tune_batch_norm', True,
                     'Fine tune the batch norm parameters or not.')

flags.DEFINE_float('min_scale_factor', 0.5,
                   'Mininum scale factor for data augmentation.')

flags.DEFINE_float('max_scale_factor', 2.,
                   'Maximum scale factor for data augmentation.')

flags.DEFINE_float('scale_factor_step_size', 0.25,
                   'Scale factor step size for data augmentation.')

# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or
# rates = [6, 12, 18] if output_stride = 16. For `mobilenet_v2`, use None. Note
# one could use different atrous_rates/output_stride during training/evaluation.
flags.DEFINE_multi_integer('atrous_rates', None,
                           'Atrous rates for atrous spatial pyramid pooling.')

flags.DEFINE_integer('output_stride', 16,
                     'The ratio of input to output spatial resolution.')

# Hard example mining related flags.

flags.DEFINE_integer(
    'hard_example_mining_step', 0,
    'The training step in which exact hard example mining kicks off. Note we '
    'gradually reduce the mining percent to the specified '
    'top_k_percent_pixels. For example, if hard_example_mining_step=100K and '
    'top_k_percent_pixels=0.25, then mining percent will gradually reduce from '
    '100% to 25% until 100K steps after which we only mine top 25% pixels.')


flags.DEFINE_float(
    'top_k_percent_pixels', 1.0,
    'The top k percent pixels (in terms of the loss values) used to compute '
    'loss during training. This is useful for hard pixel mining.')

# Dataset settings.
flags.DEFINE_string('dataset', 'pascal_voc_seg',
                    'Name of the segmentation dataset.')

flags.DEFINE_string('train_split', 'train',
                    'Which split of the dataset to be used for training')

flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')


def _build_deeplab(iterator, outputs_to_num_classes, ignore_label):
  """"""Builds a clone of DeepLab.

  Args:
    iterator: An iterator of type tf.data.Iterator for images and labels.
    outputs_to_num_classes: A map from output type to the number of classes. For
      example, for the task of semantic segmentation with 21 semantic classes,
      we would have outputs_to_num_classes['semantic'] = 21.
    ignore_label: Ignore label.
  """"""
  samples = iterator.get_next()

  # Add name to input and label nodes so we can add to summary.
  samples[common.IMAGE] = tf.identity(samples[common.IMAGE], name=common.IMAGE)
  samples[common.LABEL] = tf.identity(samples[common.LABEL], name=common.LABEL)

  model_options = common.ModelOptions(
      outputs_to_num_classes=outputs_to_num_classes,
      crop_size=FLAGS.train_crop_size,
      atrous_rates=FLAGS.atrous_rates,
      output_stride=FLAGS.output_stride)

  outputs_to_scales_to_logits = model.multi_scale_logits(
      samples[common.IMAGE],
      model_options=model_options,
      image_pyramid=FLAGS.image_pyramid,
      weight_decay=FLAGS.weight_decay,
      is_training=True,
      fine_tune_batch_norm=FLAGS.fine_tune_batch_norm,
      nas_training_hyper_parameters={
          'drop_path_keep_prob': FLAGS.drop_path_keep_prob,
          'total_training_steps': FLAGS.training_number_of_steps,
      })

  # Add name to graph node so we can add to summary.
  output_type_dict = outputs_to_scales_to_logits[common.OUTPUT_TYPE]
  output_type_dict[model.MERGED_LOGITS_SCOPE] = tf.identity(
      output_type_dict[model.MERGED_LOGITS_SCOPE], name=common.OUTPUT_TYPE)

  for output, num_classes in six.iteritems(outputs_to_num_classes):
    train_utils.add_softmax_cross_entropy_loss_for_each_scale(
        outputs_to_scales_to_logits[output],
        samples[common.LABEL],
        num_classes,
        ignore_label,
        loss_weight=1.0,
        upsample_logits=FLAGS.upsample_logits,
        hard_example_mining_step=FLAGS.hard_example_mining_step,
        top_k_percent_pixels=FLAGS.top_k_percent_pixels,
        scope=output)

    # Log the summary
    _log_summaries(samples[common.IMAGE], samples[common.LABEL], num_classes,
                   output_type_dict[model.MERGED_LOGITS_SCOPE])


def _tower_loss(iterator, num_of_classes, ignore_label, scope, reuse_variable):
  """"""Calculates the total loss on a single tower running the deeplab model.

  Args:
    iterator: An iterator of type tf.data.Iterator for images and labels.
    num_of_classes: Number of classes for the dataset.
    ignore_label: Ignore label for the dataset.
    scope: Unique prefix string identifying the deeplab tower.
    reuse_variable: If the variable should be reused.

  Returns:
     The total loss for a batch of data.
  """"""
  with tf.variable_scope(
      tf.get_variable_scope(), reuse=True if reuse_variable else None):
    _build_deeplab(iterator, {common.OUTPUT_TYPE: num_of_classes}, ignore_label)

  losses = tf.losses.get_losses(scope=scope)
  for loss in losses:
    tf.summary.scalar('Losses/%s' % loss.op.name, loss)

  regularization_loss = tf.losses.get_regularization_loss(scope=scope)
  tf.summary.scalar('Losses/%s' % regularization_loss.op.name,
                    regularization_loss)

  total_loss = tf.add_n([tf.add_n(losses), regularization_loss])
  return total_loss


def _average_gradients(tower_grads):
  """"""Calculates average of gradient for each shared variable across all towers.

  Note that this function provides a synchronization point across all towers.

  Args:
    tower_grads: List of lists of (gradient, variable) tuples. The outer list is
      over individual gradients. The inner list is over the gradient calculation
      for each tower.

  Returns:
     List of pairs of (gradient, variable) where the gradient has been summed
       across all towers.
  """"""
  average_grads = []
  for grad_and_vars in zip(*tower_grads):
    # Note that each grad_and_vars looks like the following:
    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
    grads, variables = zip(*grad_and_vars)
    grad = tf.reduce_mean(tf.stack(grads, axis=0), axis=0)

    # All vars are of the same value, using the first tower here.
    average_grads.append((grad, variables[0]))

  return average_grads


def _log_summaries(input_image, label, num_of_classes, output):
  """"""Logs the summaries for the model.

  Args:
    input_image: Input image of the model. Its shape is [batch_size, height,
      width, channel].
    label: Label of the image. Its shape is [batch_size, height, width].
    num_of_classes: The number of classes of the dataset.
    output: Output of the model. Its shape is [batch_size, height, width].
  """"""
  # Add summaries for model variables.
  for model_var in tf.model_variables():
    tf.summary.histogram(model_var.op.name, model_var)

  # Add summaries for images, labels, semantic predictions.
  if FLAGS.save_summaries_images:
    tf.summary.image('samples/%s' % common.IMAGE, input_image)

    # Scale up summary image pixel values for better visualization.
    pixel_scaling = max(1, 255 // num_of_classes)
    summary_label = tf.cast(label * pixel_scaling, tf.uint8)
    tf.summary.image('samples/%s' % common.LABEL, summary_label)

    predictions = tf.expand_dims(tf.argmax(output, 3), -1)
    summary_predictions = tf.cast(predictions * pixel_scaling, tf.uint8)
    tf.summary.image('samples/%s' % common.OUTPUT_TYPE, summary_predictions)


def _train_deeplab_model(iterator, num_of_classes, ignore_label):
  """"""Trains the deeplab model.

  Args:
    iterator: An iterator of type tf.data.Iterator for images and labels.
    num_of_classes: Number of classes for the dataset.
    ignore_label: Ignore label for the dataset.

  Returns:
    train_tensor: A tensor to update the model variables.
    summary_op: An operation to log the summaries.
  """"""
  global_step = tf.train.get_or_create_global_step()
  summaries = []

  learning_rate = train_utils.get_model_learning_rate(
      FLAGS.learning_policy, FLAGS.base_learning_rate,
      FLAGS.learning_rate_decay_step, FLAGS.learning_rate_decay_factor,
      FLAGS.training_number_of_steps, FLAGS.learning_power,
      FLAGS.slow_start_step, FLAGS.slow_start_learning_rate)
  summaries.append(tf.summary.scalar('learning_rate', learning_rate))
 # Build forward pass of model.
  loss = tf.losses.get_total_loss()

# Call the training rewrite which rewrites the graph in-place with
# FakeQuantization nodes and folds batchnorm for training. It is
# often needed to fine tune a floating point model for quantization
# with this training tool. When training from scratch, quant_delay
# can be used to activate quantization after training to converge
# with the float graph, effectively fine-tuning the model.
  g = tf.get_default_graph()
  tf.contrib.quantize.create_training_graph(input_graph=g,
                                          quant_delay=2000000)

# Call backward pass optimizer as usual.
  optimizer = tf.train.GradientDescentOptimizer(learning_rate)
  optimizer.minimize(loss)
  #optimizer = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)

  tower_grads = []
  tower_summaries = None
  for i in range(FLAGS.num_clones):
    with tf.device('/gpu:%d' % i):
      with tf.name_scope('clone_%d' % i) as scope:
        loss = _tower_loss(
            iterator=iterator,
            num_of_classes=num_of_classes,
            ignore_label=ignore_label,
            scope=scope,
            reuse_variable=(i != 0))
        grads = optimizer.compute_gradients(loss)
        tower_grads.append(grads)

        # Retain the summaries from the first tower.
        if not i:
          tower_summaries = tf.summary.merge_all(scope=scope)

  with tf.device('/cpu:0'):
    grads_and_vars = _average_gradients(tower_grads)
    if tower_summaries is not None:
      summaries.append(tower_summaries)

    # Modify the gradients for biases and last layer variables.
    last_layers = model.get_extra_layer_scopes(
        FLAGS.last_layers_contain_logits_only)
    grad_mult = train_utils.get_model_gradient_multipliers(
        last_layers, FLAGS.last_layer_gradient_multiplier)
    if grad_mult:
      grads_and_vars = tf.contrib.training.multiply_gradients(
          grads_and_vars, grad_mult)

    # Create gradient update op.
    grad_updates = optimizer.apply_gradients(
        grads_and_vars, global_step=global_step)

    # Gather update_ops. These contain, for example,
    # the updates for the batch_norm variables created by model_fn.
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    update_ops.append(grad_updates)
    update_op = tf.group(*update_ops)

    total_loss = tf.losses.get_total_loss(add_regularization_losses=True)

    # Print total loss to the terminal.
    # This implementation is mirrored from tf.slim.summaries.
    should_log = math_ops.equal(math_ops.mod(global_step, FLAGS.log_steps), 0)
    total_loss = tf.cond(
        should_log,
        lambda: tf.Print(total_loss, [total_loss], 'Total loss is :'),
        lambda: total_loss)

    summaries.append(tf.summary.scalar('total_loss', total_loss))

    with tf.control_dependencies([update_op]):
      train_tensor = tf.identity(total_loss, name='train_op')
    summary_op = tf.summary.merge(summaries)

  return train_tensor, summary_op


def main(unused_argv):
  tf.logging.set_verbosity(tf.logging.INFO)

  tf.gfile.MakeDirs(FLAGS.train_logdir)
  tf.logging.info('Training on %s set', FLAGS.train_split)

  graph = tf.Graph()
  with graph.as_default():
    with tf.device(tf.train.replica_device_setter(ps_tasks=FLAGS.num_ps_tasks)):
      assert FLAGS.train_batch_size % FLAGS.num_clones == 0, (
          'Training batch size not divisble by number of clones (GPUs).')
      clone_batch_size = FLAGS.train_batch_size // FLAGS.num_clones

      dataset = data_generator.Dataset(
          dataset_name=FLAGS.dataset,
          split_name=FLAGS.train_split,
          dataset_dir=FLAGS.dataset_dir,
          batch_size=clone_batch_size,
          crop_size=FLAGS.train_crop_size,
          min_resize_value=FLAGS.min_resize_value,
          max_resize_value=FLAGS.max_resize_value,
          resize_factor=FLAGS.resize_factor,
          min_scale_factor=FLAGS.min_scale_factor,
          max_scale_factor=FLAGS.max_scale_factor,
          scale_factor_step_size=FLAGS.scale_factor_step_size,
          model_variant=FLAGS.model_variant,
          num_readers=2,
          is_training=True,
          should_shuffle=True,
          should_repeat=True)

      train_tensor, summary_op = _train_deeplab_model(
          dataset.get_one_shot_iterator(), dataset.num_of_classes,
          dataset.ignore_label)

      # Soft placement allows placing on CPU ops without GPU implementation.
      session_config = tf.ConfigProto(
          allow_soft_placement=True, log_device_placement=False)

      last_layers = model.get_extra_layer_scopes(
          FLAGS.last_layers_contain_logits_only)
      init_fn = None
      if FLAGS.tf_initial_checkpoint:
        init_fn = train_utils.get_model_init_fn(
            FLAGS.train_logdir,
            FLAGS.tf_initial_checkpoint,
            FLAGS.initialize_last_layer,
            last_layers,
            ignore_missing_vars=True)

      scaffold = tf.train.Scaffold(
          init_fn=init_fn,
          summary_op=summary_op,
      )

      stop_hook = tf.train.StopAtStepHook(FLAGS.training_number_of_steps)

      profile_dir = FLAGS.profile_logdir
      if profile_dir is not None:
        tf.gfile.MakeDirs(profile_dir)

      with tf.contrib.tfprof.ProfileContext(
          enabled=profile_dir is not None, profile_dir=profile_dir):
        with tf.train.MonitoredTrainingSession(
            master=FLAGS.master,
            is_chief=(FLAGS.task == 0),
            config=session_config,
            scaffold=scaffold,
            checkpoint_dir=FLAGS.train_logdir,
            summary_dir=FLAGS.train_logdir,
            log_step_count_steps=FLAGS.log_steps,
            save_summaries_steps=FLAGS.save_summaries_secs,
            save_checkpoint_secs=FLAGS.save_interval_secs,
            hooks=[stop_hook]) as sess:
          while not sess.should_stop():
            sess.run([train_tensor])


if __name__ == '__main__':
  flags.mark_flag_as_required('train_logdir')
  flags.mark_flag_as_required('dataset_dir')
  tf.app.run()

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```

### I get the following error:

> WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
> For more information, please see:
>   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
>   * https://github.com/tensorflow/addons
> If you depend on functionality not listed there, please file an issue.
> 
> INFO:tensorflow:Training on train set
> WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Colocations handled automatically by placer.
> WARNING:tensorflow:From /home/ubuntu/ajinkya/models/research/deeplab/core/preprocess_utils.py:203: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.cast instead.
> Traceback (most recent call last):
>   File ""deeplab/train2.py"", line 516, in <module>
>     tf.app.run()
>   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""deeplab/train2.py"", line 468, in main
>     dataset.ignore_label)
>   File ""deeplab/train2.py"", line 348, in _train_deeplab_model
>     loss = tf.losses.get_total_loss()
>   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/util.py"", line 112, in get_total_loss
>     return math_ops.add_n(losses, name=name)
>   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
>     return target(*args, **kwargs)
>   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 2658, in add_n
>     raise ValueError(""inputs must be a list of at least one""
> ValueError: inputs must be a list of at least oneTensor/IndexedSlices with the same dtype and shape
> 
How do I resolve this error? Its extremely tough to debug whats happening here"
28174,Tensorflowlite build error for Raspberry pi,"I am trying to build Tensorflow Lite using instructions https://www.tensorflow.org/lite/guide/build_rpi but getting following error.

**Describe the problem**
**benchmark_tflite_model.cc:(.text+0x53a0): undefined reference to `tflite::evaluation::CreateGPUDelegate(tflite::FlatBufferModel*)'
benchmark_tflite_model.cc:(.text+0x551e): undefined reference to `tflite::evaluation::CreateNNAPIDelegate()'**

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image **tensorflow/tensorflow:nightly-devel**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Building for Raspberry Pi using Docker
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.0/Latest
- Python version: Python 2.7.15rc1
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel release 0.15.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Commands defined on https://www.tensorflow.org/lite/guide/build_rpi

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
28173,Tensorflow.org display errors.,"Hello there, something display error with **tensorflow.org** with a bunch of messy.
Just like this.
![image](https://user-images.githubusercontent.com/16497652/56794571-fdfe2e00-6840-11e9-8237-f03d8a8c4743.png)
![image](https://user-images.githubusercontent.com/16497652/56794637-21c17400-6841-11e9-9a46-ea10b982ebf1.png)

"
28172,why I didn't get the right fft results with tf.fft?,"I tried a simple test to see the fft function in tensorflow. But I found the result was not correct obviously. I dont know if I use the function rightly. Thanks for your comments.

`import numpy as np
import tensorflow as tf
import scipy.io as scio
import matplotlib.pyplot as plt

if __name__ == '__main__':
	t_data = np.linspace(0, 5, 201)
	f = 50
	t_holder = tf.placeholder(tf.float32, shape=[None, 1], name='t')
	f_tf = tf.Variable(f, dtype=tf.float32, name='f_tf', trainable=False)
	y_tf = tf.cos(2 * np.pi * f_tf * t_holder)
	fft_inputs = tf.complex(y_tf, tf.zeros_like(y_tf))
	outputs_fft = tf.fft(fft_inputs)
	outputs_fft_abs = tf.abs(outputs_fft)
	init = tf.global_variables_initializer()
	with tf.Session() as sess:
		init.run()
		fft_abs_print = sess.run(outputs_fft_abs, feed_dict={t_holder: np.reshape(t_data, (201, 1))})
		plt.plot(fft_abs_print)
		plt.show()`"
28171,nvidia-smi: command not found in docker container from tensorflow/tensorflow:latest-gpu,"Following the instructions of this webpage, https://tensorflow.google.cn/install/docker#gpu_support
Just as following:
1. docker pull tensorflow/tensorflow:latest-gpu
2. sudo nvidia-docker run --network=host -v /ssd1:/ssd1 -it 0de7f0bffd91 /bin/bash
     where 0de7f0bffd91 is the image id of latest_gpu
But when started in the container and use nvidia-smi to check gpu status, got the following message:
bash: nvidia-smi: command not found

Is there anything  wrong?
"
28169,Link to Images guide broken,"**System information**
- TensorFlow version: N/A
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/image


**Describe the documentation issue**

On the page
https://www.tensorflow.org/api_docs/python/tf/image

The link within the text ""See the Images guide."" is broken.
(It is currently directing to https://www.tensorflow.org/api_guides/python/image, but 404'd)


_edit: Removed contents from template unrelated to the issue._"
28168,Text classification doc Google Colab notebook issue,"**System information**
- TensorFlow version: 1.13.1
- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_text_classification


**Describe the documentation issue**
When running the Google Colab notebook from the documentation itself, I get an error saying ""Object arrays cannot be loaded when allow_pickle=False"". More details can be found in the attached image. Note that I ran the code directly from the Google colab notebook attached in the documentation.

Thank you!

![bug](https://user-images.githubusercontent.com/20063012/56782705-deeca580-681a-11e9-8ca2-6f51eef0c733.png)

"
28167,how,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
28165,Issue defining tf.function input_signature on class methods,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: tf-nightly-2.0-preview==2.0.0.dev20190426
- Python version: 3.7.0

I have defined the following Encoder class with a tf.function signature hoping to be able to save it using the SavedModel format.

```python
class Encoder(tf.keras.Model):
    # init and other functions...

    @tf.function(input_signature=[
        tf.TensorSpec([None, 50], tf.float32, name='x'),
        tf.TensorSpec([None, 1024], tf.float32, name='hidden')
    ])
    def call(self, x, hidden):
        x = self.embedding(x)
        output, state = self.gru(x, initial_state = hidden)        
        return output, state


# Some more code...

tf.saved_model.save(
    encoder,  # instance of Encoder
    './some/directory/',
    signatures=Encoder.call
)
```

I am not sure if this is the desired behavior, but I am getting this error when trying to save:

```
Traceback (most recent call last):
  File ""train.py"", line 244, in <module>
    signatures=Encoder.call
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipel
ine/venv/lib/python3.7/site-packages/tensorflow/python/saved_
model/save.py"", line 801, in save
    signatures = signature_serialization.canonicalize_signatu
res(signatures)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipel
ine/venv/lib/python3.7/site-packages/tensorflow/python/saved_
model/signature_serialization.py"", line 95, in canonicalize_s
ignatures
    signature_function = _get_signature(function)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipel
ine/venv/lib/python3.7/site-packages/tensorflow/python/saved_
model/signature_serialization.py"", line 41, in _get_signature
    function = function.get_concrete_function()
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 702, in get_concrete_function
    self._initialize(args, kwargs, add_initializers_to=initializer_map)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 379, in _initialize
    *args, **kwds))
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1331, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1594, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1527, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 713, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 329, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 436, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 379, in _initialize
    *args, **kwds))
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1331, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1594, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1527, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 713, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 329, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 705, in wrapper
    ), args, kwargs)
  File ""/home/marcio/Workspace/projects/traduki/traduki-pipeline/venv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 360, in converted_call
    result = converted_f(*effective_args, **kwargs)
TypeError: tf__call() missing 1 required positional argument: 'hidden'
```

When I remove the two TensorSpecs from the signature, I instead get a `TypeError: tf__call() missing 3 required positional arguments: 'self', 'x', and 'hidden'`, which means it's expecting me to provide a TensorSpec for `self`.

Based on [this guide](https://www.tensorflow.org/alpha/guide/saved_model#identifying_a_signature_to_export) I wouldn't expect to have to provide signature information for `self` in the `call` function, which leads me to believe the current behavior should be changed."
28164,F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   EulerOS 4.8.5-4
- TensorFlow installed from (source or binary):
   source
- TensorFlow version:
  1.10.0
- Bazel version (if compiling from source):
  0.15.0
- GCC/Compiler version (if compiling from source):
  4.9.4

**Describe the problem**
Dear TensorFlow contributors,
I am using java to execute tensorflow model in tomcat in Linux with libtensorflow_jni.so and libtensorflow.jar , I have put libtensorflow_framework.so in LD_LIBRARY_PATH,  when the program started, the error happened:

F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count


**Any other info / logs**
I guess maybe there are sth repeated in libtensorflow_jni.so and libtensorflow_framework.so and cause 'register 2 metrics'?

I read https://github.com/tensorflow/tensorflow/issues/13522，but I donnot know how to remove tensorflow-c dependency？

need your help，thanks

"
28163,TFLite Interpreter fails to load quantized model on Android (stock ssd_mobilenet_v2),"**System information**
Android 5.1.1 on LGL52VL, also tested on Android 9 Simulator (Nexus 5)
Latest Tensorflow in build.gradle:
compile 'org.tensorflow:tensorflow-lite:+'

Also, please include a link to a GraphDef or the model if possible.
http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz

**Any other info / logs**
`
// I convert the stock quantized mobilenet_v2 to TFLite using the following code:
converter = tf.lite.TFLiteConverter.from_frozen_graph(graphDefPath, input_arrays, output_arrays, input_shapes)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.allow_custom_ops=True
converter.convert()    

//The resulting .tflite file works fine in Python (including inference):
interpreter = tf.lite.Interpreter(model_path=modelFilePath)
interpreter.allocate_tensors()
`
`
// When I try to load the same tflite model file on Android the Interpreter constructor gives me error ""Didn't find op for builtin opcode 'CONV_2D' version '2'"":

MappedByteBuffer buffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
Interpreter.Options options = new Interpreter.Options();
tfLite = new Interpreter(buffer, options);

E/flutter ( 7796): [ERROR:flutter/lib/ui/ui_dart_state.cc(148)] Unhandled Exception: PlatformException(error, Unsupported value: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '2'
E/flutter ( 7796): Registration failed.
E/flutter ( 7796): , null)
E/flutter ( 7796): #0      StandardMethodCodec.decodeEnvelope (package:flutter/src/services/message_codecs.dart:564:7)
E/flutter ( 7796): #1      MethodChannel.invokeMethod (package:flutter/src/services/platform_channel.dart:302:33)
E/flutter ( 7796): <asynchronous suspension>
E/flutter ( 7796): #2      Tflutter.loadModelAndLabels (package:tflutter/tflutter.dart:129:20)
E/flutter ( 7796): <asynchronous suspension>
E/flutter ( 7796): #3      Detector.initialize (package:AIM/services/detector.dart:51:26)
E/flutter ( 7796): <asynchronous suspension>
E/flutter ( 7796): #4      _MonitorState._initStateAsync (package:AIM/screens/Monitor.dart:47:20)
E/flutter ( 7796): #5      _AsyncAwaitCompleter.start (dart:async-patch/async_patch.dart:49:6)
E/flutter ( 7796): #6      _MonitorState._initStateAsync (package:AIM/screens/Monitor.dart:45:25)
E/flutter ( 7796): #7      _MonitorState.initState (package:AIM/screens/Monitor.dart:42:5)
E/flutter ( 7796): #8      StatefulElement._firstBuild (package:flutter/src/widgets/framework.dart:3853:58)
E/flutter ( 7796): #9      ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)
E/flutter ( 7796): #10     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #11     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #12     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)
E/flutter ( 7796): #13     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #14     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #15     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)
E/flutter ( 7796): #16     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)
E/flutter ( 7796): #17     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)
E/flutter ( 7796): #18     ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)
E/flutter ( 7796): #19     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #20     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #21     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)
E/flutter ( 7796): #22     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #23     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #24     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)
E/flutter ( 7796): #25     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #26     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #27     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)
E/flutter ( 7796): #28     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #29     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #30     SingleChildRenderObjectElement.mount (package:flutter/src/widgets/framework.dart:4883:14)
E/flutter ( 7796): #31     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #32     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #33     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)
E/flutter ( 7796): #34     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)
E/flutter ( 7796): #35     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)
E/flutter ( 7796): #36     StatefulElement._firstBuild (package:flutter/src/widgets/framework.dart:3871:11)
E/flutter ( 7796): #37     ComponentElement.mount (package:flutter/src/widgets/framework.dart:3724:5)
E/flutter ( 7796): #38     Element.inflateWidget (package:flutter/src/widgets/framework.dart:2968:14)
E/flutter ( 7796): #39     Element.updateChild (package:flutter/src/widgets/framework.dart:2771:12)
E/flutter ( 7796): #40     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3757:16)
E/flutter ( 7796): #41     Element.rebuild (package:flutter/src/widgets/framework.dart:3572:5)
E/flutter ( 7796): #42     ComponentElement._firstBuild (package:flutter/src/widgets/framework.dart:3729:5)
E/flutter ( 7796): #43     ComponentElement.mount (package:flutter/src/widgets/fra
`
PS. The same Android code works fine for non-quantized tflite models that I converted, so I am confident that there should not be any silly bugs in my code. The problem seems to be triggered by Quantized + Android TFlite"
28161,MobileNet v2 model cannot be converted to TFLite (missing OP),"It is not possible to convert retrained model built on top of the feature vector [MobileNet v2](https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2) to TensorFlow Lite with **tf.lite.TFLiteConverter.from_concrete_function()** method.

During the conversion, there are errors telling about not supported operations and data types (see below). 

**System information**
TensorFlow Version:  2.0.0-alpha0
Eager mode:  True
TensorFlow Hub version:  0.4.0
Is GPU available:  True

**Code to reproduce the issue**
Colab project with output: https://colab.research.google.com/drive/1fDWZfce2BJnU49xyrvwXYpX3EW3UK6QH#scrollTo=jyT0Zw6l8LE5

Stacktrace:
```
ConverterError                            Traceback (most recent call last)
<ipython-input-48-cca93394edbc> in <module>()
     11 # Convert the model.
     12 converter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)
---> 13 converted_tflite_model = converter.convert()
     14 open(TFLITE_MODEL, ""wb"").write(converted_tflite_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    203       stderr = _try_convert_to_unicode(stderr)
    204       raise ConverterError(
--> 205           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    206   finally:
    207     # Must manually cleanup files.
```

```
ConverterError: TOCO failed. See console for info.
2019-04-25 19:20:27.592164: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-25 19:20:27.592225: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
...
2019-04-25 19:20:27.594555: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-25 19:20:27.594629: I tensorflow/lite/toco/import_tensorflow.cc:1335] Converting unsupported operation: StatefulPartitionedCall
2019-04-25 19:20:27.604371: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

Current thread 0x00007f9bd09c4780 (most recent call first):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251 in _run_main
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/usr/local/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```"
28158,Keras ValueError stops autograph building,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- TensorFlow installed from (source or binary):
pip
- TensorFlow version (use command below):
2.0.0-dev20190424
- Python version:
3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
  cudatoolkit-10.0.130-0                                                                                                  
  cudnn-7.3.1-cuda10.0_0
- GPU model and memory:
GeForce RTX 2080 Ti

**Describe the current behavior**
Calling keras layer without calling `build()` automatically infers the shapes of the trainable variables. This works both in eager mode and graph mode in the current `2.0-alpha` version. However, running the provided code in `2.0.0-dev20190424` version, it gives the following error message:

```
W0425 12:08:40.775576 139922429134656 tf_logging.py:161] Entity <function update at 0x7f41dd7038c8> could not be transform
ed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Erro
r details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the
AutoGraph team. Cause: ValueError during conversion: Weights for model sequential have not yet been created. Weights are c
reated when the Model is first called on inputs or `build()` is called with an `input_shape`.
```
**Describe the expected behavior**

**Code to reproduce the issue**
```python
import os

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

model = models.Sequential([layers.Dense(1, activation='relu')])
optimizer = optimizers.SGD()

# Is this line needed in graph mode?
# model.build((None, 1))


@tf.function
def update(batch):
    with tf.GradientTape() as tape:
        output = model(batch)
    grads = tape.gradient(output, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))


if __name__ == ""__main__"":

    batch = tf.zeros((1, 1), dtype=tf.float32)
    update(batch)

```"
28154,[TF2.0] Bug or Feature? Keras model.summary() does not provide useful information for nested models,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190424
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
For the script below
```
import tensorflow as tf
import tensorflow.keras.layers as nn
from tensorflow.python.keras.backend import get_graph
from tensorflow.python.keras.utils.generic_utils import to_snake_case
from tensorflow.python.keras.engine.base_layer_utils import unique_layer_name

Module = tf.keras.models.Model 

# class Module(tf.Module):
#     def __init__(self, name=None, **kwargs):
#         super(Module, self).__init__(name='dummy', **kwargs)
#         name = name or to_snake_case(self.__class__.__name__)
#         self._name = unique_layer_name(name, zero_based=True)
#
#     def __call__(self, *args, **kwargs):
#         with get_graph().as_default(), tf.name_scope(self.name):
#             return self.call(*args, **kwargs)

class Sequential(Module):
    def __init__(self, *args):
        super(Sequential, self).__init__()
        self.module_list = list(args) if args else []

    def call(self, x):
        for module in self.module_list:
            x = module(x)
        return x

class Block(Module):
    def __init__(self):
        super(Block, self).__init__()
        self.module = Sequential(
                nn.Dense(10),
                nn.Dense(10),)

    def call(self, input_tensor):
        x = self.module(input_tensor)
        return x

class Base(Module):
    def __init__(self):
        super(Base, self).__init__()
        self.module = Sequential(
                Block(),
                Block())

    def call(self, input_tensor):
        x = self.module(input_tensor)
        y = self.module(x)
        return x, y

class Network(Module):
    def __init__(self):
        super(Network, self).__init__()
        self.child = Base()

    def call(self, inputs):
        return self.child(inputs)

net = Network()

inputs = tf.keras.Input(shape=(10, ))
outputs = net(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)

print(model.summary(150))
```
The current output, probably doesn't provide any useful information: 
```
Model: ""model""
______________________________________________________________________________________________________________________________________________________
Layer (type)                                                       Output Shape                                                Param #
======================================================================================================================================================
input_1 (InputLayer)                                               [(None, 10)]                                                0
______________________________________________________________________________________________________________________________________________________
network (Network)                                                  ((None, 10), (None, 10))                                    440
======================================================================================================================================================
Total params: 440
Trainable params: 440
Non-trainable params: 0
______________________________________________________________________________________________________________________________________________________

```
**Describe the expected behavior**
If we uncomment the new `Module` class, the output is much more informative. I think this should be the expected behavior:
```
Model: ""model""
______________________________________________________________________________________________________________________________________________________
Layer (type)                                     Output Shape                     Param #           Connected to
======================================================================================================================================================
input_1 (InputLayer)                             [(None, 10)]                     0
______________________________________________________________________________________________________________________________________________________
dense (Dense)                                    (None, 10)                       110               input_1[0][0]
                                                                                                    dense_3[0][0]
______________________________________________________________________________________________________________________________________________________
dense_1 (Dense)                                  (None, 10)                       110               dense[0][0]
                                                                                                    dense[1][0]
______________________________________________________________________________________________________________________________________________________
dense_2 (Dense)                                  (None, 10)                       110               dense_1[0][0]
                                                                                                    dense_1[1][0]
______________________________________________________________________________________________________________________________________________________
dense_3 (Dense)                                  (None, 10)                       110               dense_2[0][0]
                                                                                                    dense_2[1][0]
======================================================================================================================================================
Total params: 440
Trainable params: 440
Non-trainable params: 0
______________________________________________________________________________________________________________________________________________________
```

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28153,BUG on `tf.saved_model.load` with large variable,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.alpha0
- Python version:
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: none
- GPU model and memory: none

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
It works fine when run this in one console
```
import tensorflow as tf
import numpy as np


m = tf.Module()
m.v = tf.Variable(np.random.normal(size=(200, 200)).astype(np.float64))
m.f = tf.function(lambda x: x + m.v[0, 0])

tf.saved_model.save(m, ""save/model"", signatures={
    ""predict"": m.f.get_concrete_function(tf.constant(3., dtype=tf.float64))
})
```
and then the other
```
import tensorflow as tf
m = tf.saved_model.load(""save/model"")
```

If you change 200 to 20000, you will see errors on load
```
import tensorflow as tf
import numpy as np


m = tf.Module()
m.v = tf.Variable(np.random.normal(size=(20000, 20000)).astype(np.float64))
m.f = tf.function(lambda x: x + m.v[0, 0])

tf.saved_model.save(m, ""save/model"", signatures={
    ""predict"": m.f.get_concrete_function(tf.constant(3., dtype=tf.float64))
})
```
```
import tensorflow as tf
tf.saved_model.load(""save/model"")
# tensorflow.python.framework.errors_impl.InvalidArgumentError: save/model/variables/variables.data-00000-of-00001; Invalid argument [Op:RestoreV2]
```
**Describe the expected behavior**

Should be able to load regardless of the size of variable

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
see above
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28151,"after quantization aware training, when I convert to tflite quant int8 model, still need to  provide min/max value for  add operation? why add operation need min/max value?","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.2/ cudnn 7
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
after quantization aware training, why i'm still asked to provide min/max value for add operation when convert to tflite int8 model?

tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-04-25 20:18:08.606359: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2360 operators, 3530 arrays (0 quantized)
2019-04-25 20:18:08.663749: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2360 operators, 3530 arrays (0 quantized)
2019-04-25 20:18:08.993938: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 273 operators, 505 arrays (1 quantized)
2019-04-25 20:18:08.998229: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 273 operators, 505 arrays (1 quantized)
2019-04-25 20:18:09.000372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 160 operators, 392 arrays (1 quantized)
2019-04-25 20:18:09.002236: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 160 operators, 392 arrays (1 quantized)
2019-04-25 20:18:09.004023: F tensorflow/lite/toco/tooling_util.cc:1708] Array slim_mobilenetv2/Add, which is an input to the Conv operator producing the output array slim_mobilenetv2/Conv_6/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted

Current thread 0x00007fb011618740 (most recent call first):
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py"", line 300 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)

 
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28150,TypeError: unsupported operand type(s) since TF 1.13,"```
import tensorflow as tf

tf.enable_eager_execution()

image_path = '6999_2_2047_690_44_93.png'
img_str = tf.read_file(image_path)

image = tf.image.decode_png(img_str)
image = image / 255.
```

**Describe the current behavior**

With tensorflow 1.12 this works fine. However with 1.13 this results in 

> Traceback (most recent call last):
>   File ""test.py"", line 9, in <module>
>     image = image / 255.
> TypeError: unsupported operand type(s) for /: 'tensorflow.python.framework.ops.EagerTensor' and 'float'

This is the same for graph mode:

> Traceback (most recent call last):
>   File ""test.py"", line 9, in <module>
>     image = image / 255.
> TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'

This looks like a bug?"
28148,dataset object has no attribute 'output_shapes' in tensorflow 2.0 alpha version,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:2.0.0-alpha0
- Doc Link: https://tensorflow.google.cn/alpha/tutorials/text/text_classification_rnn


**Describe the documentation issue**
in the line 
""train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)""
""train_dataset.output_shapes"" can not run, train_dataset of tensorflow2.0 has no attribute 'output_shapes'.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
28147,Tensorflow 2.0 overwrites logging config,"**System information**

- Linux Ubuntu 16.04
- Python 3.6.7
- packages: logging 0.5.1.2, tensorflow 2.0.0-alpha0

When importing `tensorflow` any logging user-defined config is ignored and tf config is used instead. Here is an example where logging format and level is changed. Note that the `info` isn't printed at all.

Code:
```
import logging

import tensorflow

logging.basicConfig(level=logging.INFO, format=""%(levelname)s %(message)s"")
logger = logging.getLogger(__name__)
logger.info(""Some info"")
logger.error(""Some error"")
```

Output:
```
WARNING: Logging before flag parsing goes to stderr.
E0425 11:22:27.150884 140450657982208 temp.py:9] Some error
```

Expected output (as if you delete `import tensorflow` or with older version of `tf`):
```
INFO Some info
ERROR Some error
```"
28146,"Float 16 for training ,storage,and running on tflite CPU and GPU","Since the tensorflow lite supports the GPU on mobile phone. But, it only supports float32 and float 16, the model size with float32 is too large. So, will the float16 be available for training and storage, and for converting to .tflite."
28144,"pb file cannot run on my mobile devices with android demo,but it pass with python scripts","here is my log:
No OpKernel was registered to support Op 'Range' with these attrs.  Registered devices: [CPU], Registered kernels:
      device='CPU'; Tidx in [DT_INT32]
      device='CPU'; Tidx in [DT_FLOAT]
    
    	 [[{{node unpool/range}} = Range[Tidx=DT_INT64](unpool/range/start, unpool/range/limit, unpool/range/delta)]]

I don't know why the Range operation 's data_type is int64  ,how can I  solve this probelm? If this is the cause,why my pb files can run pass with python on pc
"
28143,"[TF 2.0] Tf.keras.layers.Lambda does not fail if it does not support masking, but mask is passed","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**
- Python version: **3.5.3**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**

When `tf.keras.layers.Lambda` is created without `mask` argument, it does not support masking (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L733). However, when a mask is passed, it is silently ignored and `None` is returned as the output mask (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L797).

**Describe the expected behavior**

When `tf.keras.layers.Lambda` is created without `mask` argument, it does not support masking. Therefore, when a mask is passed, it should raise an exception similarly to what `Layer.compute_mask` does (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L494).

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
m = tf.keras.Sequential([
    tf.keras.layers.Masking(.0),
    tf.keras.layers.Lambda(tf.nn.sigmoid)])
#     tf.keras.layers.GlobalMaxPool1D()])
m(np.array([[[0.]]]))
```

The above code does not fail and produces empty output mask. If the `Lambda` layer is replaced by `GlobalMaxPool1D` (which does not support masking), the model fails during construction.

**Other info / logs**

A fix is simple -- the `Lambda.compute_mask` should include the test present in `Layer.compute_mask`, namely: https://github.com/tensorflow/tensorflow/blob/9fc8d696b7019ad09d408933de0e4a4e7b81976f/tensorflow/python/keras/engine/base_layer.py#L505-L510
"
28142,ImportError: DLL load failed on Windows 10 using GPU,"**System information**
- OS Platform and Distribution : Windows 10 1809
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.7.3/3.6.8
- Installed using virtualenv? pip? conda?: pip install tensorflow-gpu
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: tried 10.1/10.0/9.0, cudnn 7.5
- GPU model and memory: GTX 1060 6GB

**Describe the problem**

I have tried several different versions of CUDA/cuDNN, and added the bin/include/lib folders to PATH, but that doesn't work for my situation. Reinstall python and tensorflow doesn't work either. I tried the binary version of tensorflow and installed the corresponding CUDA/cuDNN, this error also appeared. I can't figure out what's wrong.

*VS 2015 C++ Redist is also installed

**Provide the exact sequence of commands / steps that you executed before running into the problem**

import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\Python Environment\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\Python Environment\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""F:\Python Environment\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\Python Environment\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\Python Environment\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
28140,Error running TFLite Quantized model on NNAPI,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9 PQ2A.190205.001
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3
- TensorFlow installed from (source or binary):TensorFlow Lite from AAR
- TensorFlow version (use command below):tensorflow-lite:0.0.0-nightly


**Describe the current behavior**
We have an image classification model trained from scratch in frozen graph format and:

1. .tflite model converted without post-training quantization works well on CPU and NNAPI.
2. .tflite model converted with post-training quantization works well on CPU.
3. .tflite model converted with post-training quantization fails on NNAPI with following error message:

```
E/Utils: Invalid input tensor type TENSOR_QUANT8_ASYMM for input 1, expected TENSOR_FLOAT32
E/tflite: Returning error since NNAPI returned failure nnapi_delegate.cc:696.
    Returning error since TFLite returned failure nnapi_delegate.cc:739.
    Failed to build graph for NNAPI
```

We share the same code across all cases.

**Describe the expected behavior**
the quantized model should work on NNAPI

**Code to reproduce the issue**
Almost same as example below:
https://github.com/tensorflow/tensorflow/tree/eae92e9d58846f559561ae1c8934a025e740aeb2/tensorflow/lite/examples/android/app

We'd happy to attach our tflite model but GitHub says it's too large 😢 
The beginning of our model is like this:

<img width=""1136"" alt=""スクリーンショット 2019-04-25 16 43 52"" src=""https://user-images.githubusercontent.com/3690310/56718223-58cc5280-6779-11e9-8573-222d7a156313.png"">
"
28137,Tensorflow lite conversion from h5 to tflite increases file size,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.1-0-g6612da8951' 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2070 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tflite conversion from h5 file increases in size from 141KB to 508KB
**Describe the expected behavior**
A reduction in size

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(11, activation=tf.nn.softmax)
])

model.compile(optimizer='adam', 
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])
model.fit(TrainingData, TrainingLabels, epochs=400)

tf.keras.models.save_model(model, keras_file)

converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28134,Tensorflow 2.0 GPU error: Failed to load the native TensorFlow runtime.,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bits
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): CONDA
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Geforce 910m

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
I just installed everything and got the error

**Other info / logs**

  File ""<ipython-input-1-d36b20bff3cb>"", line 1, in <module>
    runfile('C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py', wdir='C:/Users/victor/Documents/s4y/labeled-surgical-tools')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py"", line 7, in <module>
    import tensorflow as tf

  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio

  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\_api\v2\audio\__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav

  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\victor\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Não foi possível encontrar o módulo especificado.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
28133,sketch_rnn algorithm returns error on loading environment,"using the google sketch_rnn algorithm, i wanted to use its trained dataset to develop an AI that self draws, but when i used the code below it returned an error

**System information**
- OS Platform and Distribution (windows):
- TensorFlow installed from (source):
- TensorFlow version (1.13.1):
- Python version (3.6):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**INFO:tensorflow:Downloading http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-bc699734c167> in <module>()
----> 1 [train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)
      2 
      3 #with np.load(path, allow_pickle=True) as f:
      4 #    x_train, labels_train = f['x_train'], f['y_train']
      5 #    x_test, labels_test = f['x_test'], f['y_test']

<ipython-input-7-1c7d550387ed> in load_env_compatible(data_dir, model_dir)
      7         data[fix] = (data[fix] == 1)
      8     model_params.parse_json(json.dumps(data))
----> 9     return load_dataset(data_dir, model_params, inference_mode=True)
     10 
     11 def load_model_compatible(model_dir):

~\Anaconda3\lib\site-packages\magenta\models\sketch_rnn\sketch_rnn_train.py in load_dataset(data_dir, model_params, inference_mode)
    139         data = np.load(data_filepath)
    140     tf.logging.info('Loaded {}/{}/{} from {}'.format(
--> 141         len(data['train']), len(data['valid']), len(data['test']),
    142         dataset))
    143     if train_strokes is None:

~\Anaconda3\lib\site-packages\numpy\lib\npyio.py in __getitem__(self, key)
    260                 return format.read_array(bytes,
    261                                          allow_pickle=self.allow_pickle,
--> 262                                          pickle_kwargs=self.pickle_kwargs)
    263             else:
    264                 return self.zip.read(key)

~\Anaconda3\lib\site-packages\numpy\lib\format.py in read_array(fp, allow_pickle, pickle_kwargs)
    690         # The array contained Python objects. We need to unpickle the data.
    691         if not allow_pickle:
--> 692             raise ValueError(""Object arrays cannot be loaded when ""
    693                              ""allow_pickle=False"")
    694         if pickle_kwargs is None:

ValueError: Object arrays cannot be loaded when allow_pickle=False**

**INFO:tensorflow:Downloading http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz
INFO:tensorflow:Loaded 7400/300/300 from aaron_sheep.npz
INFO:tensorflow:Dataset combined: 8000 (7400/300/300), avg len 125
INFO:tensorflow:model_params.max_seq_len 250.
total images <= max_seq_len is 7400
total images <= max_seq_len is 300
total images <= max_seq_len is 300
INFO:tensorflow:normalizing_scale_factor 18.5198.**

**[train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)**


**def load_env_compatible(data_dir, model_dir):
    model_params = sketch_rnn_model.get_default_hparams()
    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:
        data = json.load(f)
    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']
    for fix in fix_list:
        data[fix] = (data[fix] == 1)
    model_params.parse_json(json.dumps(data))
    return load_dataset(data_dir, model_params, inference_mode=True)

def load_model_compatible(model_dir):
    model_params = sketch_rnn_model.get_default_hparams()
    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:
        data = json.load(f)
    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']
    for fix in fix_list:
        data[fix] = (data[fix] == 1)
    model_params.parse_json(json.dumps(data))
    
    model_params.batch_size = 1
    eval_model_params = sketch_rnn_model.copy_hparams(model_params)
    eval_model_params.use_input_dropout = 0
    eval_model_params.use_recurrent_dropout = 0
    eval_model_params.use_output_dropout = 0
    eval_model_params.is_training = 0
    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)
    sample_model_params.max_seq_len = 1
    return [model_params, eval_model_params, sample_model_params]**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28132,Cannot convert keras file to tensorflow lite,"Here is the keras model:

```
keras_file = ""keras_model.h5""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(11, activation=tf.nn.softmax)
])

model.compile(optimizer='adam', 
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])
model.fit(TrainingData, TrainingLabels, epochs=400)

tf.keras.models.save_model(model, keras_file)

```

And then I am using this code to convert it to tflite:

```
converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

and then i get this error:

```
      1 
----> 2 converter = lite.TFLiteConverter.from_keras_model_file( 'keras_model.h5' )
      3 model = converter.convert()
      4 
      5 file = open( 'model.tflite' , 'wb' )

1 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in __init__(self, graph_def, input_tensors, output_tensors, input_arrays_with_shape, output_arrays)
    192       if not input_arrays_with_shape or not output_arrays:
    193         raise ValueError(
--> 194             ""If input_tensors and output_tensors are None, both ""
    195             ""input_arrays_with_shape and output_arrays must be defined."")
    196       self._input_arrays_with_shape = input_arrays_with_shape

ValueError: If input_tensors and output_tensors are None, both input_arrays_with_shape and output_arrays must be defined.
```


Am i doing anything wrong?"
28131,"""import_meta_graph"" fails","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.0 with cuDNN7
- GPU model and memory: GTX1080 with 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
error occurs when calling `import_meta_graph`

**Describe the expected behavior**

**Code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow.contrib.tensorrt as trt

meta_graph_path = ""bert-eng/model.ckpt-7300.meta""
graph = tf.Graph()
with graph.as_default():
    with tf.Session() as sess:

        saver = tf.train.import_meta_graph(meta_graph_path) // error occurs here
```
When I run the above code, I get error at line:
```python
saver = tf.train.import_meta_graph(meta_graph_path) // error occurs here
```

**Other info / logs**
stack trace:
```
2019-04-25 10:40:52.107550: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-25 10:40:52.119933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099880000 Hz
2019-04-25 10:40:52.122488: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5431820 executing computations on platform Host. Devices:
2019-04-25 10:40:52.122547: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""tf_to_trt.py"", line 15, in <module>
    saver = tf.train.import_meta_graph(meta_graph_path)
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1435, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1457, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py"", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 399, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 159, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'ExperimentalFunctionBufferingResource'
```"
28130,tf.data.Dataset Performance Issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.4.1
- GPU model and memory: NVIDIA Titan V

- CPU Make & Model: 2x Intel Xeon E5-2620 v4 (8 cores/16 logical)
- Data Drive: Samsung SSD 960 EVO 1 TB

**Describe the current behavior**
Currently using the tf.data.Dataset API to load image pairs for super resolution. I believe based on the minimal examples I could find the method below is as optimized as I can get for my use-case. However, when I grew my path list from 550 to 7950 items it slows down over 3x. It doesn't seem like this part of the pipeline should scale so poorly since the batches themselves are the same size. And the process of mapping & batching (mostly IO) should be parallelized across the 32 CPU cores of the machine. 

Any ideas? Pertinent code below.


    lr_paths, hr_paths = ... # Flat lists of paths to LR & HR images, respectively.

    def load_image(path): return tf.image.decode_png(tf.read_file(path), 3)

    # Create the dataset
    dataset = (tf.data.Dataset.from_tensor_slices((lr_paths, hr_paths))
        .apply(tf.data.experimental.shuffle_and_repeat(count, FLAGS.max_epochs))
        .apply(tf.data.experimental.map_and_batch(lambda x, y: (load_image(x), load_image(y)), FLAGS.batch_size, num_parallel_batches=max(1, (cpu_count() - 1) // FLAGS.batch_size)))
        .apply(tf.data.experimental.prefetch_to_device('/device:GPU:0')))

    # Query for the iterator
    iterator = dataset.make_one_shot_iterator()
    im_LR_batch, im_HR_batch = iterator.get_next()

    ... # Preprocess the image batches with crop, data augmentation, type conversion"
28129,Build fails due to missing @kissfft//:LICENSE,"
**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.14 https://github.com/tensorflow/tensorflow/commit/70d8199abb0c548fe75568cebe6bd9e2b7e6c25e
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: from source
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: RTX 2060 6GB

**Describe the problem**

The current master fails to build because it fails to find `@kissfft//:LICENSE`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
git checkout r1.14
git pull --recurse-submodules.
bazel clean --expunge
./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
This appears to be due to a recent check-in (https://github.com/tensorflow/tensorflow/commit/15209edfc99cf9ef44c81cd37977a326ccee31c3 ) expecting to find a missing file.

"
28118,How to specify Tensorflow Estimator Version when building tensorflow from source?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: tf-nightly==1.13.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080ti



**Describe the problem**

How to specify Tensorflow Estimator Version when building tensorflow from source?

I get the source code from commit id: d5a1417612a396dc8f5b8be08d3428367fa3771e
I find that Tensorflow Estimator is changed when I install it.

- `tf-estimator-nightly    1.14.0.dev2019042301` # new one
- `tf-estimator-nightly   1.14.0.dev2019031401` # previous one

The new one is not compatible with the d5a1417612a396dc8f5b8be08d3428367fa3771e code.
So, how to specify Tensorflow Estimator Version to `1.14.0.dev2019031401`?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


```bash 
...
  File ""/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py"", line 65, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1708, in <module>
    class EstimatorV2(Estimator):
  File ""/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1711, in EstimatorV2
    export_savedmodel = deprecation.HIDDEN_ATTRIBUTE
AttributeError: module 'tensorflow.python.util.deprecation' has no attribute 'HIDDEN_ATTRIBUTE'
```"
28115,compute_output_shape may return incorrect results due to broken cache implementation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Docker image tensorflow/tensorflow:1.12.0-gpu-py3 running under Host on Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Docker image tensorflow/tensorflow:1.12.0-gpu-py3
- TensorFlow version (use command below):
tensorflow-gpu      1.12.0   
- Python version:
3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
9.0 / 7
- GPU model and memory:
GTX 1080

**Describe the current behavior**
When called repeatedly, compute_output_shape may return incorrect results due to a faulty caching implementation in keras.Network

**Describe the expected behavior**
compute_output_shape should always return correct results

**Code to reproduce the issue**
The issue is traced to [this](https://github.com/tensorflow/tensorflow/blob/d7cd03f1398cd33fc8579ce336b602350ac99259/tensorflow/python/keras/engine/network.py#L902) line, where the function generic_utils.object_list_uid is used to generate a cache key. This function just generates a unique identifier for a tuple by taking the object ids of all the components, converting them to strings and concatenating. Firstly, this will not generate cache hits if the function is called twice with the same parameter values, because the object ids will not be the same even though their values are. Far more seriously though, it is possible for the function to sporadically generate  false hits, since object ids are only guaranteed to be unique for objects existing at the same time. 

The bug is hidden when all dimensions are 256 or lower, because of the [special way](https://docs.python.org/3.5/c-api/long.html) that Python treats integer objects between -5 and 256. 

The code below causes this to happen reliably on my machine. 

If the maintainers agree with my diagnosis, I'll happily provide a pull request to fix it. Personally I think the cache should just be removed as it won't save a significant amount of computation in any sane code and has unbounded memory requirements in broken code, but if there are good reasons to retain it we can just key on the values of the tuple components, rather than their object ids.
```
import tensorflow as tf

input = tf.keras.layers.Input(shape = [None,None,3])
output = tf.keras.layers.Conv2D(filters=1,kernel_size=[1,1])(input)
model = tf.keras.Model(inputs=input,outputs=output)
# The model consists of a single 1x1 convolution. Therefore the spatial extent of the output is always trivially equal to
# the spatial extent of the input.

# We must start from 257 because, when creating ints between -5 and 256 Python returns references to singletons.
# Therefore there is a one-to-one mapping between int value and object-id in this range and the cache works as
# intended. Outside this range, no such guarantee is possible and the cache fails.
for x in range(257,300):
    shape=model.compute_output_shape([[1,x,x,3]])
    assert (shape[1]==x)
```"
28114,"For ConcatV2, `axis` really require int64 support?","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  source
- TensorFlow version (use command below): 0.12.0
- Python version: 2.7.15rc1
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
The `testConcatAxisType` test in `concat_op_test` is failing on an assertion for s390x:
```
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/87c06bad5aec2f21ec96b7253de551e8/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/concat_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/concat_op_test.py"", line 621, in testConcatAxisType
    self.assertEqual([2, 6], c.get_shape().as_list())
AssertionError: Lists differ: [2, 6] != [4, 3]
```

**Describe the expected behavior**
Assert values should match.

**Code to reproduce the issue**
On Intel:
```
>>> from __future__ import absolute_import
>>> from __future__ import division
>>> from __future__ import print_function
>>> import numpy as np
>>> import tensorflow as tf
>>> from tensorflow.python.framework import constant_op
>>> from tensorflow.python.framework import dtypes
>>> from tensorflow.python.ops import gen_array_ops
>>>
>>> t1 = [[1, 2, 3], [4, 5, 6]]
>>> t2 = [[7, 8, 9], [10, 11, 12]]
>>>
>>> c = constant_op.constant(1, dtype=dtypes.int32)
>>> z = gen_array_ops.concat_v2([t1, t2], c)
>>> z
<tf.Tensor 'ConcatV2_2:0' shape=(2, 6) dtype=int32>
>>>
>>> c = constant_op.constant(1, dtype=dtypes.int64)
>>> z = gen_array_ops.concat_v2([t1, t2], c)
>>> z
<tf.Tensor 'ConcatV2_3:0' shape=(2, 6) dtype=int32>
>>>
```

On s390x:
```
>>> from __future__ import absolute_import
>>> from __future__ import division
>>> from __future__ import print_function
>>> import numpy as np
>>> import tensorflow as tf
>>> from tensorflow.python.framework import constant_op
>>> from tensorflow.python.framework import dtypes
>>> from tensorflow.python.ops import gen_array_ops
>>>
>>> t1 = [[1, 2, 3], [4, 5, 6]]
>>> t2 = [[7, 8, 9], [10, 11, 12]]
>>>
>>> c = constant_op.constant(1, dtype=dtypes.int32)
>>> z = gen_array_ops.concat_v2([t1, t2], c)
>>> z
<tf.Tensor 'ConcatV2_2:0' shape=(2, 6) dtype=int32>
>>>
>>> c = constant_op.constant(1, dtype=dtypes.int64)
>>> z = gen_array_ops.concat_v2([t1, t2], c)
>>> z
<tf.Tensor 'ConcatV2_3:0' shape=(4, 3) dtype=int32>
>>>
```

**Other info / logs**
From above logs we can see that when we pass a constant (axis) of int64 type to `ConcatV2`, we get different results on s390x which causes assertion failure. 

After searching the historical records, I found this PR #14251 which has introduced this test case. However when I scanned the TensorFlow code for checking how the `concat` operations work, I found that it deals mostly with int32 type.

Getting different results on s390x for 32 Vs 64 bit data types is nothing new. We have handled couple of cases in issue #11431 and #14017 and it happens because of NumPy's different behavior on Intel Vs Z (PR #12963) . As my code understanding with `concat` is different, I am curious to understand if we really need to process int64 type of axis (is it a real time use case?) or its just added to fill the gap between `ConcatV2` supported types Vs its kernel implementation?
I might be wrong but want to be sure that I proceed to find fix for the right thing. Please check and confirm."
28113,TF 2.0.0-alpha0 doesn't build on FreeBSD: error in hwloc,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): FreeBSD 12.0
- Bazel version (if compiling from source): 0.23.0
- GCC/Compiler version (if compiling from source): Clang 8.0.0

I'n trying to build TensorFlow from source on FreeBSD 12. During build the following problem shows up:

```
ERROR: /usr/home/arr/.cache/bazel/_bazel_arr/5d34786d346adb404a81c63780107a71/external/hwloc/BUILD.bazel:212:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)
external/hwloc/hwloc/topology-linux.c:39:10: fatal error: 'mntent.h' file not found
#include <mntent.h>
         ^~~~~~~~~~
1 error generated.
```

This is because `topology-linux.c` is being compiled instead of `topology-freebsd.c`. I have zero knowledge in Bazel, so I wasn't able to include on of these files based on the condition.

This should be easy to fix for anyone experienced with Bazel."
28112,TF 2.0 can't import anymore on Windows TypeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0.dev20190420
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: rtx 2060 6gb

**Describe the current behavior**
I get the error below when I try to import tensorflow on Windows 10. Also, I don't know if it is related but I upgraded my gpu. Tf 2.0 was working fine when I last tried 3 weeks ago with my old gpu.

**Describe the expected behavior**

**Code to reproduce the issue**
`import tensorflow as tf`

**Other info / logs**
File ""d:/pythonapps/next/test.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow\__init__.py"", line 42, in <module>
    from tensorflow._api.v2 import compat
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow\_api\v2\compat\__init__.py"", line 21, in <module>
    from tensorflow._api.v2.compat import v1
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow\_api\v2\compat\v1\__init__.py"", line 643, in <module>
    'tensorflow_estimator.python.estimator.api._v1.estimator'))
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow\python\tools\component_api_helper.py"", line 56, in package_hook
    child_pkg = importlib.import_module(child_package_str)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.api._v1.estimator import experimental
  File ""D:\pythonapps\next\env\lib\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\experimental\__init__.py"", line 29, in <module>
    _sys.modules[__name__], ""estimator.experimental"")
TypeError: __init__() missing 1 required positional argument: 'deprecated_to_canonical'"
28111,Issue for tf.keras.layers.DenseFeatures when model.build is called,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, my own example code.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary, from tensorflow/tensorflow:2.0.0a0-gpu-py3 docker image.
- TensorFlow version (use command below): 2.0.0a0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
tf.keras.layers.DenseFeatures is a new keras layer added in tf2.0a0. This is the first layer for tf.keras.Model that accepts dict of tensors as the input.

If tf.keras.layers.DenseFeatures is used as the first layer in a keras model created by subclass tf.keras.Model,  model.build(input_shape) will fail with any type of input_shape.

**Describe the expected behavior**
model.build(input_shape) will succeed, and corresponding model variables are created.

**Code to reproduce the issue**

```
import tensorflow as tf

feature_columns = [tf.feature_column.numeric_column(header) for header in ['c1', 'c2']]

class TestModel(tf.keras.Model):
    def __init__(self, feature_columns):
        super(TestModel, self).__init__()
        self.feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
        self.dense_layer = tf.keras.layers.Dense(8)

    def call(self, inputs):
        x = self.feature_layer(inputs)
        return self.dense_layer(x)

model = TestModel(feature_columns=feature_columns)
shape = {'c1': (1,1), 'c2': (1,1)}
# change to:
# shape = [(1,1), (1, 1)]
# also fails.
model.build(shape)
print(model.trainable_variables)
```

**Other info / logs**
https://github.com/skydoorkai/tests_and_issues/tree/master/tensorflow/densefeature
provides with the repro Dockerfile, and a possible solution to it: Network.build(input_shape) accepts dict as the input."
28110,[TF 2.0] TF 2.0 consumes twice as much memory as TF 1.x or CNTK.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0, cudnn-10.0-windows10-x64-v7.5.0.56
- GPU model and memory: GeForce GTX 1070 8GB

**Describe the current behavior**
Evaluating TF 2.0 keras model allocates twice as much memory as TF 1.x or CNTK.

**Describe the expected behavior**
Memory usage of TF 2.0 should be same or similar to other libraries, not double.

**Code to reproduce the issue**
For TF 2.0 or 1.x
```
import numpy as np
import tensorflow as tf

# tf.config.gpu.set_per_process_memory_growth(True)

size = 28000

inputs = tf.keras.Input((size,), dtype='float32')
outputs = tf.keras.layers.Dense(size)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)

model.predict(np.ones((1, size,), dtype=np.float32))

print('complete')

while True:
    pass
```
For TF 1.x or CNTK with keras
```
import keras
import numpy as np

size = 28000

inputs = keras.Input((size,), dtype='float32')
outputs = keras.layers.Dense(size)(inputs)
model = keras.models.Model(inputs=inputs, outputs=outputs)

model.predict(np.ones((1, size,), dtype=np.float32))

print('complete')

while True:
    pass
```
With 8GB VRAM GPU, TF 1.x and CNTK works successfully, and TF 2.0 code are failed due to Resource exhausted exception."
28109,matmul for RaggedTensors / tf.ragged.ragged_dense_matmul,"**System information**
- TensorFlow version (you are using): 2.0.0a0

**Describe the feature and the current behavior/state.**
Allow to compute a matrix multiplication between a `RaggedTensor` and a standard dense tensor, returning a `RaggedTensor`.  
The behavior would be similar to `tf.sparse.sparse_dense_matmul`. 

For instance, given a `RaggedTensor` of shape `(batch, None, channels_in)`, and a dense tensor of shape `(channels_in, channels_out)`, the operation would return a `RaggedTensor` of shape `(batch, None, channels_out)`, where all entries along the ragged dimension have been multiplied by the dense tensor. 

**Will this change the current api? How?**
It could be implemented as a standalone operation like `tf.sparse.sparse_dense_matmul`, or it could be part of the existing API (for instance, `tf.math.sum` supports `RaggedTensors` as is). 

**Who will benefit with this feature?**
Anybody working with irregular data, with applications ranging from computer vision (irregular images) to deep learning on graphs.  
Given the possibility to have `matmul` and `add` on `RaggedTensors`, one could implement dense, convolutional, and recurrent layers operating directly on irregular data. 
"
28108,AttributeError: can't set attribute,"'''
Generator 
'''
class Generator(tf.keras.Model):
    def __init__(self, input_shape=128):
        super(Generator, self).__init__()
        
        self.input_shape = input_shape
        self.conv = tf.keras.layers.Conv2D(64,(7,7), padding='same')
        #~ down sampling
        self.down = Downsampling_Part()
        
        
        #ResBlock
        self.res1 = ResBlock(256,(3,3))
        self.res2 = ResBlock(256,(3,3))
        self.res3 = ResBlock(256,(3,3))
        self.res4 = ResBlock(256,(3,3))
        self.res5 = ResBlock(256,(3,3))
        self.res6 = ResBlock(256,(3,3))
        
        # ~Up sampling
        self.upsampleblock = UpSampleBlock()
        
    def call(self, images, labels, training):
        # Down - sampling
        x = input_merge(images,labels)
        out = self.down(x);
        
        # Bottleneck
        out = self.res1(out, training=training)
        out = self.res2(out, training=training)
        out = self.res3(out, training=training)
        out = self.res4(out, training=training)
        out = self.res5(out, training=training)
        out = self.res6(out, training=training)
        
        #Up sampling
        out= self.upsampleblock(out)
        
        generated_images =  out
        
        return out


=======================================

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-569e07e14531> in <module>()
----> 1 Gen = Generator()

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __setattr__(self, name, value)
   1778         # Exclude @property.setters from tracking
   1779         hasattr(self.__class__, name)):
-> 1780       super(Layer, self).__setattr__(name, value)
   1781       return
   1782 

AttributeError: can't set attribute"
28107,Unable to Convert Retrained MobileNet V2 SDD Model to .tflite Model for TensorFlow Lite Object Detection Android Demo,"**System information**
- Colab
- GPU
- Python 3
- TensorFlow 1.13.1

**Model**
ssd_mobilenet_v2_coco_2018_03_29

**Provide the text output from tflite_convert**
Please refer and run to the following .ipynb file with Colab
https://github.com/ash-yanagisawa/tensorflow-object-detection-training-colab/blob/master/tensorflow_object_detection_training_colab.ipynb

TensorFlow Lite Object Detection Android Demo
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

**Any other info / logs**
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.
"
28105,libtensorflowlite.so usage is not documented,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X API 28 (android studio)
- TensorFlow installed from (source or binary): source
- TensorFlow version: master, r1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv pip
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): MSVC 14.1
- CUDA/cuDNN version: 10.0 / 7.4
- GPU model and memory: RTX 2080 / 8GB



**Describe the problem**
I am in windows, but `libtensorflow_cc.so` build succeeded in windows for me.
But I cannot build libtensorflowlite.so, it says not all outputs were created or valid.
I don't know how to look into further build error details with bazel.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python configure.py
- no XLA JIT
- no ROCm
- yes CUDA (or no CUDA, both fails)
- yes Eigen Strong Inline

`bazel build --config opt //tensorflow/lite:libtensorflowlite.so`
```
INFO: From Linking tensorflow/lite/libtensorflowlite.so:
LINK : warning LNK4044: unrecognized option '/s'; ignored
ERROR: D:/myusers/haebin/program/tensorflow-master/tensorflow/lite/BUILD:426:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created
ERROR: D:/myusers/haebin/program/tensorflow-master/tensorflow/lite/BUILD:426:1: not all outputs were created or valid
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 90.626s, Critical Path: 28.46s
INFO: 182 processes: 182 local.
FAILED: Build did NOT complete successfully
```

**Any other info / logs**
I'm just curious, but is it possible to run tensorflow lite source codes (C++) in Windows 10 x86-64, if I successfully build this libtensorflowlite.so and include some libraries? I already succeeded in running tensorflow C++ codes with libtensorflow_cc.so in Windows.
I have read the docs and it seems like you guys offer a C++ api, but only in mobile environments?? Is tensorflow lite impossible to run in normal desktops?


**Additionally what I tried later (also didn't work)**
Ok, so I just gave up on native Windows tf lite, and tried to build android c++ shared lib with bazel. I used `bazel build //tensorflow/lite:libtensorflowlite.so --config=android --cpu
=x86 --cxxopt='--std=c++11' -c opt` in linux, because it seemed like building on windows bazel fails. I successfully built `libtensorflowlite.so` and tried these instructions : https://stackoverflow.com/questions/49834875/problems-with-using-tensorflow-lite-c-api-in-android-studio-project
So I built shared lib on linux, and tried to use it in windows android studio. I thought that it's eventually for android, so it would be ok to do that. (is it?)
But I got an error : incompatible target.

```
Build command failed.
Error while executing process C:\Users\poins\AppData\Local\Android\Sdk\cmake\3.6.4111459\bin\cmake.exe with arguments {--build D:\MyUsers\Haebin\repo\androidtflite\app\.externalNativeBuild\cmake\debug\x86 --target native-lib}
[1/1] Linking CXX shared library D:\MyUsers\Haebin\repo\androidtflite\app\build\intermediates\cmake\debug\obj\x86\libnative-lib.so
FAILED: cmd.exe /C ""cd . && C:\Users\poins\AppData\Local\Android\Sdk\ndk-bundle\toolchains\llvm\prebuilt\windows-x86_64\bin\clang++.exe  --target=i686-none-linux-android --gcc-toolchain=C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/toolchains/x86-4.9/prebuilt/windows-x86_64 --sysroot=C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sysroot -fPIC -isystem C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sysroot/usr/include/i686-linux-android -D__ANDROID_API__=15 -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -mstackrealign -Wa,--noexecstack -Wformat -Werror=format-security -std=c++11  -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -nostdlib++ --sysroot C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/platforms/android-15/arch-x86 -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -LC:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sources/cxx-stl/llvm-libc++/libs/x86 -Wl,--no-undefined -Wl,-z,noexecstack -Qunused-arguments -Wl,-z,relro -Wl,-z,now -shared -Wl,-soname,libnative-lib.so -o D:\MyUsers\Haebin\repo\androidtflite\app\build\intermediates\cmake\debug\obj\x86\libnative-lib.so CMakeFiles/native-lib.dir/native-lib.cpp.o  D:/MyUsers/Haebin/repo/androidtflite/app/src/main/cpp/distribution/lib/libtensorflowlite_x86.so -llog -latomic -lm ""C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sources/cxx-stl/llvm-libc++/libs/x86/libc++_static.a"" ""C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sources/cxx-stl/llvm-libc++/libs/x86/libc++abi.a"" ""C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/sources/cxx-stl/llvm-libc++/libs/x86/libandroid_support.a"" && cd .""
C:/Users/poins/AppData/Local/Android/Sdk/ndk-bundle/toolchains/x86-4.9/prebuilt/windows-x86_64/lib/gcc/i686-linux-android/4.9.x/../../../../i686-linux-android/bin\ld: error: D:/MyUsers/Haebin/repo/androidtflite/app/src/main/cpp/distribution/lib/libtensorflowlite_x86.so: incompatible target

clang++.exe: error: linker command failed with exit code 1 (use -v to see invocation)

ninja: build stopped: subcommand failed.
```
why is it incompatible? am I even right in using this C++ api this way? why is there no guide in tensorflow?"
28104,TF2.0a reset_default_graph,"When I use TF2.0a,I get this error :""module 'tensorflow' has no attribute 'reset_default_graph"".And I search the doc but nothing to explain it.How can I fix this error?"
28103,FP16 CUDA support,"Though tensorflow python api support fp16, actually model's performing won't benefit from this dytpe. I found there wasn't any content like _'cublasHgemm'_ in source file to support FP16. So I wonder is there any official schedule  to support FP16, or if there exist other ways to use FP16 for training/inference with tensorflow."
28102,The Keras examples should load data with allow_pickle=True,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No, I was trying the tutorials/keras/basic_text_classification
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.2
- Numpy version: 1.16.3

**Describe the current behavior**
When I tried the following code from the basic_text_classification sample:
```
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
```
It failed with error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py"", line 86, in load_data
    x_train, labels_train = f['x_train'], f['y_train']
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 262, in __getitem__
    pickle_kwargs=self.pickle_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/format.py"", line 692, in read_array
    raise ValueError(""Object arrays cannot be loaded when ""
ValueError: Object arrays cannot be loaded when allow_pickle=False
```

According to the change of numpy in https://github.com/numpy/numpy/pull/13359, the default value of allow_pickle was changed from True to False. 

Please update the sample code accordingly (may pass in allow_pickle=True explicitly to np.load). 

**Describe the expected behavior**
The basic_text_classification example could be followed without error. 

**Other info / logs**
When I downgrade numpy from 1.16.3 to 1.16.1 (whose default value was still allow_pickle=True), I could finish the basic_text_classification example successfully. 
"
28101,TF2 tf.distribute.MirroredStrategy() failed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  try the code https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0 alpha
- Python version: 3.6
- Bazel version (if compiling from source): 0.23
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0 / 7.0
- GPU model and memory: P40

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
File ""keras_mnist.py"", line 79, in <module>
    model.fit(train_dataset, epochs=10, callbacks=callbacks)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 791, in fit
    initial_epoch=initial_epoch)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1515, in fit_generator
    steps_name='steps_per_epoch')
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 257, in model_iteration
    batch_outs = batch_function(*batch_data)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1258, in train_on_batch
    self._make_fit_function()
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2177, in _make_fit_function
    '_fit_function', [self.total_loss] + metrics_tensors)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2149, in _make_train_function_helper
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 454, in get_updates
    return [self.apply_gradients(grads_and_vars)]
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 406, in apply_gradients
    self._distributed_apply, args=(grads_and_vars,), kwargs={""name"": name})
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1375, in merge_call
    return self._merge_call(merge_fn, args, kwargs)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1382, in _merge_call
    return merge_fn(self._strategy, *args, **kwargs)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 440, in _distributed_apply
    var, apply_grad_to_update_var, args=(grad,), group=False))
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1175, in update
    return self._update(var, fn, args, kwargs, group)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1546, in _update
    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1551, in _update_non_slot
    with ops.colocate_with(colocate_with), UpdateContext(colocate_with):
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4261, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4310, in colocate_with
    op = _op_to_colocate_with(op)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6556, in _op_to_colocate_with
    return internal_convert_to_tensor_or_indexed_slices(v, as_ref=True).op
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1347, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1186, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/data2/huye/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/values.py"", line 731, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28100,tflite conversion with randomuniform and softmax_cross_entropy_with_logits_v2,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 1.13


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, EQUAL, FLOOR, FULLY_CONNECTED, LOGISTIC, MEAN, MUL, REDUCE_MAX, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: RandomUniform, SoftmaxCrossEntropyWithLogits.
```

Also, please include a link to a GraphDef or the model if possible.

```
def build_nn():

    graph = tf.Graph()
    with graph.as_default():

        # Config:
        n_hidden         = 30*11
        n_input          = get_input_len()
        n_labels         = get_labels_len()
        learning_rate    = 0.001

        # neural network inputs and expected results
        X = tf.placeholder(""float"", [None, n_input])
        Y = tf.placeholder(""float"", [None, n_labels])
        dropout_prob = tf.placeholder_with_default(1.0, shape=())

        # neural network parameters
        weights = {
            'h1':  tf.Variable(tf.random_normal([n_input, n_hidden])),
            'out': tf.Variable(tf.random_normal([n_hidden, n_labels])),
        }
        biases = {
            'b1':   tf.Variable(tf.random_normal([n_hidden])),
            'out':  tf.Variable(tf.random_normal([n_labels])),
        }

        def neural_net(x):
            # hidden fully connected layer
            layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))
            # dropout on hidden layer
            layer_1_drop = tf.nn.dropout(layer_1, dropout_prob)
            # output fully connected layer, neuron for each class
            out_layer = tf.matmul(layer_1_drop, weights['out']) + biases['out']
            return out_layer

        # construct model
        logits = neural_net(X)
        prediction = tf.nn.softmax(logits) # reduce unscaled values to probabilities

        # Define loss and optimizer

        loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = optimizer.minimize(loss_op)

        # Evaluate
        predictions = tf.argmax(prediction, 1)
        pred_scores = tf.reduce_max(prediction,1)
        pred_scores_full = prediction
        correct_pred = tf.equal(predictions, tf.argmax(Y, 1)) # check the index with the largest value
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # percentage of traces that were correct

        return graph, X, Y, train_op, dropout_prob, [loss_op, accuracy, predictions, pred_scores, pred_scores_full, correct_pred]

```


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28096,Distributed Training - Device Incarnation mismatch ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): we are running a docker container with these benchmarks: https://github.com/mkulaczkowski/benchmarks
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Docker container: tensorflow/tensorflow:1.13.1-gpu-py3
- TensorFlow version (use command below): 11.13.1
- Python version: python 3
- CUDA/cuDNN version: 9.1
- GPU model and memory:  K80 


**Describe the current behavior**
We are attempting to run a distributed job. 8 workers and 1 parameter server. We are using this benchmarks repository https://github.com/mkulaczkowski/benchmarks inside the  tensorflow/tensorflow:1.13.1-gpu-py3 container. 

We are running the workers with this invocation: 
python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --num_gpus=8 --model=resnet50 --variable_update=distributed_replicated --batch_size=16

We are running the parameter server like this:
""python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model=resnet50 --variable_update=distributed_replicated --batch_size=16""

We are having some workers fail due to this error message:
113tensorflow.python.framework.errors_impl.AbortedError: RecvTensor expects a different device incarnation: 881627422748727503 vs. 7505616095957727320. Your worker job (""/job:worker/replica:0/task:4"") was probably restarted. Check your worker job for the reason why it was restarted.

I have done some debugging and I do not see a restart of the running container. We also do not necessarily see any networking issues. I will attach a full log to the end of this issue

**Describe the expected behavior**
All workers progress thru the benchmark and finish successfully. Right now it seems a a minimum of one and a max of 4 fail with the error above. 

**Code to reproduce the issue**
The code that is running can be found here: https://github.com/mkulaczkowski/benchmarks
The commands that run this code are listed above

**Other info / logs**
Full output of a failed worker:
https://pastebin.com/HzPQHEDQ
"
28095,DLL load failed: The specified procedure could not be found.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- TensorFlow installed from (source or binary): binary
- TensorFlow version: Latest version as of 4/23/2019 (Cannot check, Can't import tensorflow to check version)
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

When attempting to load tensorflow:

```import tensorflow as tf```

The following error gets thrown:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""XXXXXXXXXX\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""XXXXXXXXXX\venv\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""XXXXXXXXXX\venv\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""XXXXXXXXXX\venv\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
All commands are being run as part of the powershell terminal provided in vscode

I created a virtual environment:
```virtualenv --system-site-packages -p python3 ./venv```

Started the virtual environment:
```.\venv\Scripts\activate```

Installed tensorflow;

```pip install tensorflow```

Attempted to check if the installation was a success:

```python```

Then:

```import tensorflow as tf```

The error gets thrown when inputting the above command

**Any other info / logs**
I am using VS code as my editor of choice. I am using the powershell terminal in VS Code. I attempted to downgrade my python version to 3.6. That did not work. I later tried to downgrade my protobuf version from 3.6.1 to 3.6.0 but I was informed that the current version of tensorflow only works with 3.6,1, I'm not sure what's going on
"
28094,ReduceLROnPlateau Doesn't Trigger on Tensorflow-gpu                1.13.1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.8 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda install -c anaconda tensorflow-gpu
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla P4 7611MiB
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I added ""ReduceLROnPlateau(patience=1, monitor='val_loss', verbose=1)"" as part of tf.keras.model.fit callbacks, but it never gets triggered. Other callbacks like ModelCheckpoint with the same monitor works fine.
However, the same code works on tensorflow cpu version.
I tried:
1. ReduceLROnPlateau as only callback
2. set mode to min
3. Different monitor

**Describe the expected behavior**
It should reduce lr when val_loss doesn't improve.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
model= Sequential()
...
earlyStopping = EarlyStopping(patience=100, monitor='val_loss', verbose=1, restore_best_weights=True)
checkpoint = ModelCheckpoint('weights/model {epoch:02d} {val_loss:.3f} {val_acc:.3f} {loss:.3f} {acc:.3f}.h5', monitor='val_loss', verbose=1, save_best_only=True)
reduce_lr_loss = ReduceLROnPlateau(patience=1, monitor='val_loss', verbose=1)
callbacks = [earlyStopping, checkpoint, reduce_lr_loss]
	model.fit(x_train, y_train, batch_size=32, epochs=1000, validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28093,Runtime crash due to build time configuration inconsistency/bug (CUDA 10.1),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1 (equivalently affects 2.0.0-alpha0)
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: native system install
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: any



**Describe the problem**
At configuration time, TensorFlow checks if given CUDA version is indeed available by inspecting SONAMEs of libs. Then it links to the closest version available on the system. At runtime, however, it has stored the plain version number as a string (in `stream_executor/dso_loader.cc`), given to it at configuration time, and tries to load those libraries exactly instead -  not the ones it actually build/linked against. Normally, this will not be such a problem, however, it is inconsistent and in the case of CUDA 10.1, of course, results in a crash. CUDA 10.1 libraries ship as filenames `lib*.so.10.1.0.105`, but some are labelled with `SONAME lib*.so.10.1` and others are labelled `SONAME lib*.so.10`. This results in either a build error or runtime error depending on how tensorflow was configured, but in any case, it prevents TensorFlow from working.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
$ ./build.sh
  export PYTHON_BIN_PATH=/usr/bin/python
  export USE_DEFAULT_PYTHON_LIB_PATH=1
  export TF_NEED_JEMALLOC=1
  export TF_NEED_KAFKA=0
  export TF_NEED_OPENCL_SYCL=0
  export TF_NEED_AWS=0
  export TF_NEED_GCP=0
  export TF_NEED_HDFS=0
  export TF_NEED_S3=0
  export TF_ENABLE_XLA=1
  export TF_NEED_GDR=0
  export TF_NEED_VERBS=0
  export TF_NEED_OPENCL=0
  export TF_NEED_MPI=0
  export TF_NEED_TENSORRT=0
  export TF_NEED_NGRAPH=0
  export TF_NEED_IGNITE=0
  export TF_NEED_ROCM=0
  export TF_SET_ANDROID_WORKSPACE=0
  export TF_DOWNLOAD_CLANG=0
  export TF_NCCL_VERSION=2.4
  export NCCL_INSTALL_PATH=/usr
  export TF_IGNORE_MAX_BAZEL_VERSION=1
  export CC_OPT_FLAGS=""-march=haswell""
  export TF_NEED_CUDA=1
  export GCC_HOST_COMPILER_PATH=/usr/bin/gcc
  export HOST_CXX_COMPILER_PATH=/usr/bin/gcc
  export TF_CUDA_CLANG=0
  export CUDA_TOOLKIT_PATH=/opt/cuda
  export TF_CUDA_VERSION=10.1
  export CUDNN_INSTALL_PATH=/usr/lib
  export TF_CUDNN_VERSION=7
  export TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.0,5.2,5.3,6.0,6.1,6.2,7.0,7.2,7.5
  ./configure
  bazel \
    build --config=opt \
      //tensorflow:libtensorflow.so \
      //tensorflow:libtensorflow_cc.so \
      //tensorflow:install_headers \
      //tensorflow/tools/pip_package:build_pip_package
  bazel-bin/tensorflow/tools/pip_package/build_pip_package ""${srcdir}""/tmpcudaopt
```
```
# BUILD ERROR:
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in /build/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1502
		_create_local_cuda_repository(repository_ctx)
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266, in _create_local_cuda_repository
		_find_libs(repository_ctx, cuda_config)
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 859, in _find_libs
		_find_cuda_lib(""cublas"", repository_ctx, cpu_value, c..., ...)
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 773, in _find_cuda_lib
		find_lib(repository_ctx, [(""%s/%s%s"" % (bas...], ...)))
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 747, in find_lib
		auto_configure_fail((""None of the libraries match th...)))
	File ""/build/tensorflow/third_party/gpus/cuda_configure.bzl"", line 341, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: None of the libraries match their SONAME: /opt/cuda/lib64/libcublas.so.10.1
```
```python
# $ ./tensorflow_test.py
#!/usr/bin/env python

import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
val = model.evaluate(x_test, y_test)
print('Result is:\n{}'.format(val[0]))
print('Success!')
```
```
# RUNTIME ERROR:
2019-04-23 23:51:16.393767: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library libcublas.so.10.1. LD_LIBRARY_PATH: 
2019-04-23 23:51:16.393797: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcublas.so.10.1; dlerror: libcublas.so.10.1: cannot open shared object file: No such file or directory
[1]    4805 abort (core dumped)  ./tensorflow_test.py
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The following patch allows a workaround for CUDA 10.1 and specific affected libs:
```patch
diff --git a/tensorflow/stream_executor/dso_loader.cc b/tensorflow/stream_executor/dso_loader.cc
index 6dda5d6315..4df1d0dcfc 100644
--- a/tensorflow/stream_executor/dso_loader.cc
+++ b/tensorflow/stream_executor/dso_loader.cc
@@ -46,7 +46,7 @@ string GetCudnnVersion() { return TF_CUDNN_VERSION; }
 
 /* static */ port::Status DsoLoader::GetCublasDsoHandle(void** dso_handle) {
   return GetDsoHandle(FindDsoPath(port::Env::Default()->FormatLibraryFileName(
-                                      ""cublas"", GetCudaVersion()),
+                                      ""cublas"", ""10""),
                                   GetCudaLibraryDirPath()),
                       dso_handle);
 }
@@ -63,14 +63,14 @@ string GetCudnnVersion() { return TF_CUDNN_VERSION; }
 
 /* static */ port::Status DsoLoader::GetCufftDsoHandle(void** dso_handle) {
   return GetDsoHandle(FindDsoPath(port::Env::Default()->FormatLibraryFileName(
-                                      ""cufft"", GetCudaVersion()),
+                                      ""cufft"", ""10""),
                                   GetCudaLibraryDirPath()),
                       dso_handle);
 }
 
 /* static */ port::Status DsoLoader::GetCurandDsoHandle(void** dso_handle) {
   return GetDsoHandle(FindDsoPath(port::Env::Default()->FormatLibraryFileName(
-                                      ""curand"", GetCudaVersion()),
+                                      ""curand"", ""10""),
                                   GetCudaLibraryDirPath()),
                       dso_handle);
 }
diff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl
index 8aa5b89cdd..7ed4bb8b45 100644
--- a/third_party/gpus/cuda_configure.bzl
+++ b/third_party/gpus/cuda_configure.bzl
@@ -842,7 +842,7 @@ def _find_libs(repository_ctx, cuda_config):
               repository_ctx,
               cpu_value,
               cuda_config.cuda_toolkit_path,
-              cuda_config.cuda_version,
+              '',
           ),
       ""cusolver"":
           _find_cuda_lib(
@@ -850,7 +850,7 @@ def _find_libs(repository_ctx, cuda_config):
               repository_ctx,
               cpu_value,
               cuda_config.cuda_toolkit_path,
-              cuda_config.cuda_version,
+              '',
           ),
       ""curand"":
           _find_cuda_lib(
@@ -858,7 +858,7 @@ def _find_libs(repository_ctx, cuda_config):
               repository_ctx,
               cpu_value,
               cuda_config.cuda_toolkit_path,
-              cuda_config.cuda_version,
+              '',
           ),
       ""cufft"":
           _find_cuda_lib(
@@ -866,7 +866,7 @@ def _find_libs(repository_ctx, cuda_config):
               repository_ctx,
               cpu_value,
               cuda_config.cuda_toolkit_path,
-              cuda_config.cuda_version,
+              '',
           ),
       ""cudnn"":
           _find_cuda_lib(
```"
28091,Debug build fail: cuda_platform.cc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.13/1.13.1/1.14
- Python version:3.6.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.21.0 (not from source)
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: RTX2080Ti/11GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Command used to build:
`bazel build --config=opt --config=cuda  -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

Along the way, some code modifications are made to silent some errors, they are mentioned:
#24457
#22766
#2355

For some reason, the string parsing code in gpu_device.cc@1305 doesn't work and always errored out ""Illegal version name [7.5]"", so I hardcoded the cuda version.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/lc/Developer/tensorflow/tensorflow/compiler/jit/ops/BUILD:16:1: Executing genrule //tensorflow/compiler/jit/ops:xla_ops_wrapper_py_pygenrule failed (Aborted): bash failed: error executing command
  (cd /home/lc/.cache/bazel/_bazel_lc/ba81b28ccf9c216ae3340fd9b9605d54/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-10.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64: \
    PATH=/bin:/usr/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/jit/ops/gen_xla_ops_wrapper_py_py_wrappers_cc , '\'''\'' 0 0 > bazel-out/k8-opt/genfiles/tensorflow/compiler/jit/ops/xla
_ops.py')
Execution platform: @bazel_tools//platforms:host_platform: bash failed: error executing command
  (cd /home/lc/.cache/bazel/_bazel_lc/ba81b28ccf9c216ae3340fd9b9605d54/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-10.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64: \
    PATH=/bin:/usr/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/jit/ops/gen_xla_ops_wrapper_py_py_wrappers_cc , '\'''\'' 0 0 > bazel-out/k8-opt/genfiles/tensorflow/compiler/jit/ops/xla
_ops.py')
Execution platform: @bazel_tools//platforms:host_platform
2019-04-23 18:11:46.123378: F tensorflow/stream_executor/cuda/cuda_platform.cc:201] Non-OK-status: MultiPlatformManager::RegisterPlatform(std::move(platform)) status: Internal:
/bin/bash: line 1: 20352 Aborted                 (core dumped) bazel-out/host/bin/tensorflow/compiler/jit/ops/gen_xla_ops_wrapper_py_py_wrappers_cc , '' 0 0 > bazel-out/k8-opt/genfiles/tensorflow/compiler/jit/ops/xla_ops.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6.256s, Critical Path: 4.98s
INFO: 21 processes: 21 local.
FAILED: Build did NOT complete successfully
```

Thanks in advance for your help"
28090,AdaGrad doesn't work with sparse operations on GPU in Eager mode,"This happens with TensorFlow 1.12 and CUDA 9.0:

```bash
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.12.0-0-ga6d8ffae09 1.12.0
```

Here's a minimal code snippet for reproducing the problem:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import tensorflow as tf
import tensorflow.contrib.eager as tfe

tf.enable_eager_execution()
var = tfe.Variable([0.0, 0.0, 0.0], name='tensor')
optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
with tf.GradientTape() as tape:
    fn = tf.nn.embedding_lookup(var, [0])[0]
g = tape.gradient(fn, [var])[0]
optimizer.apply_gradients(grads_and_vars=[(g, var)])
```

Here's the output, both on CPU and GPU:

```bash
$ CUDA_VISIBLE_DEVICES="""" ./ada.py 
2019-04-23 22:38:06.058069: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-23 22:38:06.064824: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-04-23 22:38:06.064853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: hamburg
2019-04-23 22:38:06.064858: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: hamburg
2019-04-23 22:38:06.064879: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 390.116.0
2019-04-23 22:38:06.064897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 390.116.0
2019-04-23 22:38:06.064902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 390.116.0

$ ./ada.py 
2019-04-23 22:38:12.777841: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-23 22:38:12.868632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-04-23 22:38:12.868990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 11.91GiB freeMemory: 9.38GiB
2019-04-23 22:38:12.869004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-04-23 22:38:13.675859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-23 22:38:13.675884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-04-23 22:38:13.675889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-04-23 22:38:13.676041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9063 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""./ada.py"", line 17, in <module>
    optimizer.apply_gradients(grads_and_vars=[(g, var)])
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 610, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 166, in update_op
    g.values, self._v, g.indices)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 963, in _resource_apply_sparse_duplicate_indices
    return self._resource_apply_sparse(summed_grad, handle, unique_indices)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py"", line 132, in _resource_apply_sparse
    use_locking=self._use_locking)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/gen_training_ops.py"", line 2258, in resource_sparse_apply_adagrad
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceSparseApplyAdagrad' OpKernel for GPU devices compatible with node {{node ResourceSparseApplyAdagrad}} = ResourceSparseApplyAdagrad[T=DT_FLOAT, Tindices=DT_INT32, update_slots=true, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)
	.  Registered:  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
 [Op:ResourceSparseApplyAdagrad]
```

Environment information:

```bash
== cat /etc/issue ===============================================
Linux hamburg 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04.2 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux hamburg 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy                      0.4.3.2    
numpy                              1.16.1     
numpydoc                           0.8.0      
protobuf                           3.6.1      
tensorflow-estimator               1.13.0     

     13084:       trying file=tls/x86_64/libpthread.so.0
     13084:       trying file=tls/libpthread.so.0
     13084:       trying file=haswell/x86_64/libpthread.so.0
     13084:       trying file=haswell/libpthread.so.0
     13084:       trying file=x86_64/libpthread.so.0
     13084:       trying file=libpthread.so.0
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0
     13084:
     13084:     find library=libc.so.6 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libc.so.6
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libc.so.6
     13084:       trying file=tls/haswell/x86_64/libc.so.6
     13084:       trying file=tls/haswell/libc.so.6
     13084:       trying file=tls/x86_64/libc.so.6
     13084:       trying file=tls/libc.so.6
     13084:       trying file=haswell/x86_64/libc.so.6
     13084:       trying file=haswell/libc.so.6
     13084:       trying file=x86_64/libc.so.6
     13084:       trying file=libc.so.6
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libc.so.6
     13084:
     13084:     find library=libdl.so.2 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     
     13084:       trying file=/usr/local/cuda-9.0/lib64/libm.so.6
     13084:       trying file=tls/haswell/x86_64/libm.so.6
     13084:       trying file=tls/haswell/libm.so.6
     13084:       trying file=tls/x86_64/libm.so.6
     13084:       trying file=tls/libm.so.6
     13084:       trying file=haswell/x86_64/libm.so.6
     13084:       trying file=haswell/libm.so.6
     13084:       trying file=x86_64/libm.so.6
     13084:       trying file=libm.so.6
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libm.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libc.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libm.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/librt.so.1
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libutil.so.1
     13084:

     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/haswell/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../haswell/x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../haswell/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libffi.so.6
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libffi.so.6
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_struct.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libopenblasp-r0-382c8f3a.3.5.dev.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/an
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libz.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/zlib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=liblzma.so.5 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../..           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../liblzma.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../liblzma.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libtensorflow_framework.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/x86_64:/home/anaconda/anaconda3/
l_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:
../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/pyth
4/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/p
ll/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
     13084:
     13084:     find library=libcublas.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (R
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcublas.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcublas.so.9.0
     13084:
     13084:     find library=libcusolver.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcusolver.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcusolver.so.9.0
   
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcudart.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcudart.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcudart.so.9.0
     13084:
     13084:     find library=libgomp.so.1 [0]; sear
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libgomp.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libgomp.so.1
     13084:
     13084:     find library=libstdc++.so.6 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libstdc++.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libstdc++.so.6
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libstdc++.so.6
     13084:
     13084:     find library=libgcc_s.so.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libgcc_s.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libgcc_s.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libgcc_s.so.1
     13084:
     13084:     find library=libcuda.so.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_p
/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfi
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cu
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcuda.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcuda.so.1
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libcuda.so.1
     13084:       trying file=tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=tls/haswell/libcuda.so.1
     13084:       trying file=tls/x86_64/libcuda.so.1
     13084:       trying file=tls/libcuda.so.1
     13084:       trying file=haswell/x86_64/libcuda.so.1
     13084:       trying file=h
/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extr
as/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcudnn.so.7
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcudnn.so.7
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudnn
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcudnn.so.7
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libcudnn.so.7
     13084:       trying file=tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=tls/haswell/libcudnn.so.7
     13084:       trying file=tls/x86_64/libcudnn.so.7
     13084:       trying file=tls/libcudnn.so.7
     13084:       trying file=haswell/x86_64/libcudnn.so.7
     13084:       trying file=haswell/libcudnn.so.7
     13084:       trying file=x86_64/libcudnn.so.7
     13084:       trying file=libcudnn.so.7
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/usr/lib/x86_64-linux-gnu/libcudnn.so.7
     13084:
     13084:     find library=libcufft.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cu
ell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/h
aswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcufft.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcufft.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcufft.so.9.0
     13084:
     13084:     find library=libcurand.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcurand.so.9.0
     13084:       tryi
nfig_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcurand.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcurand.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcurand.so.9.0
     13084:
     13084:     find library=libnvidia-fatbinaryloader.so.390.116 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libnvidia-fatbinaryloader.so.390.116
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libnvidia-fat
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcufft.so.9.0
     13084:
     13084:
     13084:     calling init: /usr/lib/x86_64-linux-gnu/libcudnn.so.7
     13084:
     13084:
     13084:     calling init: /usr/lib/x86_64-linux-gnu/libcuda.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libgomp.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcudart.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcusolver.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcublas.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
     13084:
     13084:     find library=libhdfs.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:.
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libhdfs.so
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libhdfs.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=
     13084:      search path=/lib/x86_64-linux-gnu/tls/haswell/x86_64:/lib/x86_64-linux-gnu/tls/haswell:/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/haswell/x86_64:/lib/x86_64-linux-gnu/haswell:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64:/usr/lib/x86_64-linux-gnu/tls/haswell:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/haswell/x86_64:/usr/lib/x86_64-linux-gnu/haswell:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/haswell/x86_64:/lib/tls/haswell:/lib/tls/x86_64:/lib/tls:/lib/haswell/x86_64:/lib/haswell:/lib/x86_64:/lib:/usr/lib/tls/haswell/x86_64:/usr/lib/tls/haswell:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/haswell/x86_64:/usr/lib/haswell:/usr/lib/x86_64:/usr/lib              (system search path)
     13084:       trying file=/lib/x86_64-linux-gnu/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/haswell/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/haswell/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/haswell/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/haswell/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
     13084:       trying file=/lib/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/tls/haswell/libhdfs.so
     13084:       trying file=/lib/tls/x86_64/libhdfs.so
     13084:       trying file=/lib/tls/libhdfs.so
     13084:       trying file=/lib/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/haswell/libhdfs.so
     13084
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/array.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libssl.so.1.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../..           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libssl.so.1.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libssl.so.1.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libhdf5-5773eb11.so.103.0.0 [0]; searching
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/haswell/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/haswell/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=haswell/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=haswell/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=libhdf5-5773eb11.so.103.0.0
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/x86_64:/home/anaconda/anaconda3/l
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0
     13084:
     13084:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/haswell/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/haswell/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=haswell/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=haswell/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=libhdf5_hl-db841637.so.100.1.1
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs            (RUNPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
     13084:
     13084:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/.         (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/x86_64/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./hasw
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so
     13084:
     13092:     find library=libc.so.6 [0]; searching
     13092:      search path=/usr/local/cuda-9.0/lib64/tls/haswell/x86_64:/usr/local/cuda-9.0/lib64/tls/haswell:/usr/local/cuda-9.0/lib64/tls/x86_64:/usr/local/cuda-9.0/lib64/tls:/usr/local/cuda-9.0/lib64/haswell/x86_64:/usr/local/cuda-9.0/lib64/haswell:/usr/local/cuda-9.0/lib64/x86_64:/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:         (LD_LIBRARY_PATH)
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/haswell/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/haswell/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/haswell/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/haswell/libc.so.6
     13092:       trying file=/usr/local/cud
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/libuuid.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/unicodedata.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryptography/hazmat/bindings/_constant_time.abi3.so
     13084:
     13084:     find library=libffi-bce22613.so.6.0.4 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend          (RPATH from file /home/anaconda/anaconda3/l
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libyaml-0.so.2 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../..            (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/x86_64/libyaml-0.so.2
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/libyaml-0.so.2
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/x86_64/libyaml-0.so.2
     13084:       trying file=/
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/haswell/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/haswell/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../haswell/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../haswell/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libjpeg.so.9
     13084:
     13084:     find library=libtiff.so.5 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../..          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/_imaging.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libtiff.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libjpeg.so.9
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libtiff.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/_imaging.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libmkl_rt.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packag
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libgfortran.so.4 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../..          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../libgfortran.so.4
     13084:
     13084:     find library=libquadmath.so.0 [0]; searching
     13084:      search path=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib                (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../libgfortran.so.4)
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/x86_64/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/x86_64/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell/x86_64/libquadmath.so.0
     13084:       try
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_fitpack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/dfitpack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_bspl.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_ppoly.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/interpnd.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/ckdtree.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/qhull.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/_lib/messagestream.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/reduce.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce_axis.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/move.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/writers.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling fini: /home/anaconda
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensor
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/fast_tensor_util.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/array.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/libuuid.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/unicodedata.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryptography/hazmat/bindings/_constant_time.abi3.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/_cffi_backend.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/libffi-bce22613.so.6.0.4 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryp
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libz.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-
     13084:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/_lib/messagestream.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_voronoi.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_distance_wrap.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_hausdorff.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/conversion.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/np_datetime.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/nattype.cpyth
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce_axis.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/move.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/offsets.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     1308
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
     13084:

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64::/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Apr 19 15:35:28 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.116                Driver Version: 390.116                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |
| 47%   77C    P2   153W / 250W |   7177MiB / 12194MiB |     92%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      2059      G   /usr/lib/xorg/Xorg                            16MiB |
|    0     28211      C   python3                                     2299MiB |
|    0     28214      C   python3                                     1275MiB |
|    0     28216      C   python3                                     1275MiB |
|    0     28217      C   python3                                     2299MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
```"
28088,Porting Tensorflow-Lite to Google TEE OS(Trusty OS),"I'm trying to port Tensorflow-Lite to Google TEE OS(Trusty OS) which runs in Secure World on mobile devices with ARM TrustZone support. 

Inspired by [Tensorflow-Lite for Micro-controller](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro), I start from a [Portable Reference Code](https://drive.google.com/open?id=1cawEQAkqquK_SO4crReDYqf_v7yAwOY8), which doesn't require any standard C or C++ libraries. However, when I try to port it to [Trusty repo](https://android.googlesource.com/trusty/manifest). I get compiling errors, because the trusty prebuilt toolchain doesn't have c++ headers, while  the Tensorflow-lite's flatbuffer library relies on c++ headers. 

I tried to manually copy c/c++ header files from 

`tensorflow/tensorflow/lite/experimental/micro/tools/make/downloads/gcc_embedded/arm-none-eabi/include/`  

to  `trusty/trusty/user/app/recognize_commands_test/third_party/include`

and add proper code in corresponding Makefile 

`trusty/trusty/user/app/recognize_commands_test/rules.mk`

However, there are some redefine errors due to micros defined in both trusty os c header files 

`external/lk/include/shared/lk/compiler.h` 

and the copied c/c++ headers `include/sys/cdefs.h` .

I also tried to only add c++ headers, instead of c/c++ headers, but no-definition errors pop up, like cstdio use a lot of functions defined in stdio.h like ""fprintf, rename, fsetpos, putc"" which are defined in c header files, at the same time, the trusty os 's prebuilt compiler already has a set of c header files defined in the toolchain's include directory. 

Any idea in porting tensorflow-lite to Trusty OS? or specific advice in solving the lack of C++ header files problem?

I know this type of problems is more related to adding c++ support for Trusty OS, but as far as I know, there's no discussing forum or active github repo for Google Trusty OS development. If you know any, please let me know. Great thanks!
"
28087,Fatal error in build from source,"**### System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.12.2
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 9.0 / 7.1.4
- **GPU model and memory**: GTX 760
- **Exact command to reproduce**:


**### Describe the problem**
I'm trying build tensorflow from source as [these instructions](https://www.tensorflow.org/install/source#configuration_options). I used the software versions as above (following the [Tested build configurations](https://www.tensorflow.org/install/source#configuration_options)). After few seconds, a fatal error arives: 

```
ERROR: /home/daviduarte/.cache/bazel/_bazel_daviduarte/a8562d3526d775d3816a038c038b230e/external/com_googlesource_code_re2/BUILD:26:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed (Exit 1)
In file included from /usr/include/c++/4.8/memory:81:0,
                 from external/com_googlesource_code_re2/util/sparse_array.h:107,
                 from external/com_googlesource_code_re2/re2/re2.cc:27:
/usr/include/c++/4.8/bits/unique_ptr.h: In instantiation of 'struct std::default_delete<int []>':
/usr/include/c++/4.8/bits/unique_ptr.h:288:33:   required by substitution of 'template<class _Up> static typename _Up::pointer std::unique_ptr<_Tp [], _Dp>::_Pointer::__test(typename _Up::pointer*) [with _Up = _Up; _Tp = int; _Dp = std::default_delete<int []>] [with _Up = std::default_delete<int []>]'
/usr/include/c++/4.8/bits/unique_ptr.h:296:25:   required from 'class std::unique_ptr<int []>::_Pointer'
/usr/include/c++/4.8/bits/unique_ptr.h:299:57:   required from 'class std::unique_ptr<int []>'
external/com_googlesource_code_re2/util/sparse_array.h:334:26:   required from here
/usr/include/c++/4.8/bits/unique_ptr.h:79:51: internal compiler error: Segmentation fault
  using __remove_cv = typename remove_cv<_Up>::type;
                                                   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
The bug is not reproducible, so it is likely a hardware or OS problem.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 66.791s, Critical Path: 11.38s
INFO: 300 processes: 300 local.
FAILED: Build did NOT complete successfully

```

If I simple try again, typing:
> bazel clean
> ./configure
> enter the exacly configuration above

another DIFFERENT error appears:

```
ERROR: /home/daviduarte/.cache/bazel/_bazel_daviduarte/a8562d3526d775d3816a038c038b230e/external/grpc/BUILD:388:1: C++ compilation of rule '@grpc//:grpc_plugin_support' failed (Exit 1)
external/grpc/src/compiler/node_generator.cc: In member function 'google::protobuf::Arena* google::protobuf::FileDescriptorProto::GetArenaNoVirtual() const':
external/grpc/src/compiler/node_generator.cc:278:1: internal compiler error: Segmentation fault
 }  // namespace grpc_node_generator
 ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.
The bug is not reproducible, so it is likely a hardware or OS problem.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 52.049s, Critical Path: 11.22s
INFO: 152 processes: 152 local.
FAILED: Build did NOT complete successfully
```

I tried several times with others tensorflow version (always respecting the Tested build configurations). Each try gives a different fatal error. Sometimes with Segmentation fault. 

With GCC 6 I stay several minutes in the compilation process, them suddenly a fatal error with a segmentation fault appears. 


**### Source code / logs**
Here is the complete log of my last try: 
[erro.txt](https://github.com/tensorflow/tensorflow/files/3109326/erro.txt)

And here is the my build configuration typed in ./configure:
[build_config.txt](https://github.com/tensorflow/tensorflow/files/3109328/build_config.txt)"
28086,Windows Build working with cuda 10.1 and vs2019,"I am only writing this as an issue as I got tensorflow with win 10 building with cuda 10.1 and vs2019 .
I am not sure if my changes can be general, they seem to work for me and I can train MNIST on GPU. The cuda changes could be relevant, the others might be working only for my system config. 

<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0a
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 20
- GCC/Compiler version (if compiling from source): visual studo 2019
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: gtx 1080 ti , 4gv



**Describe the problem**
Cannot compile tensorflow 2.0 alpha on windows with cuda 10.1, and visual studio 2019

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Hardlinked visual studio 2019 as 2017 with `mklink`
- modified bazel scripts and stream operators as in : https://github.com/atemysemicolon/tensorflow/commit/54faf06d5c10e8875b9ce36b941004aceb44c8b9 
- Compiles


**Any other info / logs**
- Visual 2019 does not seem to be supported by older versions of Bazel. Bazel 24.1 causes compilation issues using nvcc. However, cheating bazel into thinking 2019 is 2017 is good enough - by hardlinking the 2019 folder to 2017 in C:\Program Files\Microsoft Visual Studio\..
- I had to modify bazel to not generate workspaces for mac and linux,
- _The most important one seems to be the modification in `cuda_10_0.inc `in `tensorflow/stream_executer/cuda/...`_"
28084,Compile failed for r1.14 branch on Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.14
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): `C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe`



**Describe the problem**

**bazel build --config=opt //tensorflow:libtensorflow.so**


**Any other info / logs**
```shell
ERROR: D:/projects/tensorflow/tensorflow/BUILD:525:1: Linking of rule '//tensorflow:libtensorflow.so.1.13.1' failed (Exit 1000): link.exe failed: error executing command
  cd C:/users/hchen/_bazel_101423/a34lf4hu/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files
(x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/Python36/python.exe
    SET PYTHON_LIB_PATH=D:/Python36/lib/site-packages
    SET TEMP=C:\Users\hchen\AppData\Local\Temp
    SET TF_CONFIGURE_IOS=0
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\hchen\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib -ignore:4221 -ignore:4221
-ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ig4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 -ignore:4221 /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so.1.13.1-2.params /OPT:ICF /OPT:REF /DEF:bazel-out/x64_windows-opt/genfiles/tensorflow/tensorflow_filtered_def_file.def /ignore:4070Execution platform: @bazel_tools//platforms:host_platformLINK : fatal error LNK1000: unknown error at 00007FF770AF4241; consult documentation for technical support optionsTarget //tensorflow:libtensorflow.so failed to buildINFO: Elapsed time: 87.703s, Critical Path: 26.45s
INFO: 0 processes.FAILED: Build did NOT complete successfully
```
"
28083,Tensor Indexing Loses Shape Information,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0
- Python version: 3.7.3

**Describe the current behavior**
Sliced tensors (e.g. `my_tensor[:, :, :]`) lose their shape information. Using `tf.slice` does seem to work but for that I need to have all dimensions defined which is more often than not simply not the case.

**Describe the expected behavior**
Shape information is kept and adjusted to match the sliced data
"
28082,Installation issue on the Raspberry Pi,"**System information**

- OS Platform and Distribution: Raspberry Pi 3+ with Raspbian 2019-04-08 release
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-1.13.1
- Python version: Python 3.5.3
- Installed using: pip

**Describe the problem**

Installing TensorFlow on the Raspberry Pi with a clean build of the latest version of Raspbian,

```
sudo apt install libatlas-base-dev
sudo apt-get install python3-pip
pip3 install tensorflow
```

and then importing it into `python3` environment generates the following error

```
$ python3
Python 3.5.3 (default, Sep 27 2018, 17:25:39) 
[GCC 6.3.0 20170516] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412
  return f(*args, **kwds)
>>> 
```

Presumably the binary build of TensorFlow is expecting Python 3.4 here? There seems to be other people complaining about this, but I can't find a solution. Anyone?

A full log of the installation is available: [installation.log](https://github.com/tensorflow/tensorflow/files/3108312/installation.log)
"
28081,"Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)","System details are:
Windows 10
Python 3.6
Tensorflow-gpu 1.5.0
Cuda 9.0 and cuDNN 7.5

Tried to train model with Keras with Custom DataSet
ERR
![image](https://user-images.githubusercontent.com/31316434/56592397-3830bb00-6608-11e9-847e-ce44259c880a.png)
"
28080,"use the tf.train.Checkpoint ,  optimizer Adam  is not the Ttrackable object???","my  TF Version is  2.0.0-alpha0

https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention#checkpoints_object-based_saving
above the link. and the  auchor
I learn the  document tutorials 
and use the code :
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

cause the error :
ValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got <tensorflow.python.keras.optimizers.Adam object at 0x000002A561A5F400>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue."
28078,Take a long time to initialize the model?,"Hey, I met a strange problem when initializing a simple CNN model. The torsor is as below. Every time during initialization, the code will be stuck in the last row (code), which may cost 10-15 minutes.

P.S. My machine owns 2 CPU and 10G memory. The version of TensorFlow is 1.8.0-cpu.

![image](https://user-images.githubusercontent.com/7310697/56581827-6e6e3a80-6608-11e9-9036-d1f89c0472ac.png)

Any ideas?"
28076,deprecation warning in confusion_matrix.py ,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

OS: Ubuntu 18.04.2
Tensorflow-gpu: 2.0.0alpha0

WARNING: Logging before flag parsing goes to stderr.
W0423 15:59:53.297839 140450503571264 deprecation.py:323] From <path>/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:194: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0423 15:59:53.298299 140450503571264 deprecation.py:323] From <path>/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:195: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead."
28075,TF2.0.0-alpha0  'Tensor' object has no attribute 'numpy',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):colab
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-alpha0
- Python version:3.6

'Tensor' object has no attribute 'numpy' when use tf.data 

ds_train = tf.data.Dataset.from_tensor_slices((train_x_paths,train_y_labels))
def readFile(path,label):
  return tf.io.read_file(path).numpy(),label

ds_train.map(readFile)

AttributeError                            Traceback (most recent call last)
<ipython-input-94-7fc15313765a> in <module>()
----> 1 ds_train.map(readFile)

10 frames
<ipython-input-92-a8cf88784b63> in readFile(path, label)
      1 def readFile(path,label):
----> 2   return tf.io.read_file(path).numpy(),label
      3 

AttributeError: 'Tensor' object has no attribute 'numpy'"
28074,Metrics for multi-label classification for using with tf.keras,"Top-K Metrics are widely used in assessing the quality of Multi-Label classification. `tf.metrics.recall_at_k` and `tf.metrics.precision_at_k` cannot be directly used with `tf.keras`! Even if we wrap it accordingly for `tf.keras`, In most cases it will raise **NaNs** because of numerical instability. Since we don't have out of the box metrics that can be used for monitoring multi-label classification training using `tf.keras`. I came up with the following plugin for `Tensorflow 1.X` version. This can also be easily ported to `Tensorflow 2.0`.

```python
import tensorflow as tf
K = tf.keras.backend

class MetricsAtTopK:
    def __init__(self, k):
        self.k = k

    def _get_prediction_tensor(self, y_pred):
        """"""Takes y_pred and creates a tensor of same shape with 1 in indices where, the values are in top_k
        """"""
        topk_values, topk_indices = tf.nn.top_k(y_pred, k=self.k, sorted=False, name=""topk"")
        # the topk_indices are along last axis (1). Add indices for axis=0
        ii, _ = tf.meshgrid(tf.range(tf.shape(y_pred)[0]), tf.range(self.k), indexing='ij')
        index_tensor = tf.reshape(tf.stack([ii, topk_indices], axis=-1), shape=(-1, 2))
        prediction_tensor = tf.sparse_to_dense(sparse_indices=index_tensor,
                                               output_shape=tf.shape(y_pred),
                                               default_value=0,
                                               sparse_values=1.0,
                                               validate_indices=False
                                               )
        prediction_tensor = tf.cast(prediction_tensor, K.floatx())
        return prediction_tensor

    def true_positives_at_k(self, y_true, y_pred):
        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)
        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))
        return true_positive

    def false_positives_at_k(self, y_true, y_pred):
        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)
        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))
        c2 = K.sum(prediction_tensor)  # TP + FP
        false_positive = c2 - true_positive
        return false_positive

    def false_negatives_at_k(self, y_true, y_pred):
        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)
        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))
        c3 = K.sum(y_true)  # TP + FN
        false_negative = c3 - true_positive
        return false_negative

    def precision_at_k(self, y_true, y_pred):
        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)
        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))
        c2 = K.sum(prediction_tensor)  # TP + FP
        return true_positive/(c2+K.epsilon())

    def recall_at_k(self, y_true, y_pred):
        prediction_tensor = self._get_prediction_tensor(y_pred=y_pred)
        true_positive = K.sum(tf.multiply(prediction_tensor, y_true))
        c3 = K.sum(y_true)  # TP + FN
        return true_positive/(c3+K.epsilon())

    def f1_at_k(self, y_true, y_pred):
        precision = self.precision_at_k(y_true=y_true, y_pred=y_pred)
        recall = self.recall_at_k(y_true=y_true, y_pred=y_pred)
        f1 = (2*precision*recall)/(precision+recall+K.epsilon())
        return f1
```

```python
### Usage:
metrics = MetricsAtTopK(k=5)
# model definition
# ...
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', 
                                                                     metrics.true_positives_at_k, 
                                                                     metrics.false_positives_at_k,
                                                                     metrics.false_negatives_at_k,
                                                                     metrics.recall_at_k,
                                                                     metrics.precision_at_k,
                                                                     metrics.f1_at_k,
                                                                    ]
model.fit(...)  # as usual
 
```

Is this something which we can integrate to Tensorflow? If so I will be glad to open up a **Pull Request**"
28073,TensorFlow 2.0: Allow simple tensorboard summary usage,"**System information**
- TensorFlow version (you are using): 2.0 alpha 0
- Are you willing to contribute it (Yes/No): willing yes, but I don't have enough experience with the TF internals.



**Describe the feature and the current behavior/state.**
As to my current research and a [stackoverflow question on writing image summaries](https://stackoverflow.com/questions/55421290/tensorflow-2-0-keras-how-to-write-image-summaries-for-tensorboard), I think there is no user friendly way of writing arbitrary summaries from a TensorFlow 2.0 Keras model.

In more complex models, a developer might want to write scalars, histograms or simply images generated from tensors which are created in the middle of the network. With tf.layers and estimators, I could easily take such a tensor and write it to TensorBoard with tf.summary. At the moment there does not seem to be an equally easily usable way to do this with the Keras API.

From the user perspective, I imagine a special TensorBoard logging layer, that can be chained into the model. This layer would then handle creating the summary writer and getting/maintaining the current model step. 
E.g. when using `Sequential`, it could look like:
```
model = keras.models.Sequential([
	keras.layers.Flatten(input_shape=(28, 28), name=""flatten""),
	keras.layers.Dense(128, activation='relu', name=""dense1""),
	TensorBoardLoggingLayer(""my-logging-dir"", lambda input: tf.summary.scalar(""first-value"", input[0]))
	keras.layers.Dense(10, activation='softmax', name=""dense2"")
])
```

Furthermore, if the user creates a custom layer they could extend from this `TensorBoardLoggingLayer` and use its handling of the summary writer to easily write summaries of internal parts of their layer.

**Will this change the current api? How?**
It would add a new layer but not effect other, existing API.

**Who will benefit with this feature?**
Everybody that wants to debug a model or get more insights into it. This is especially important when you create custom layers (e.g. implementing new stuff from/for papers) or if you want to debug e.g. images of inner results of your network.

**Any Other info.**
I think it is crucial to have an easy to use possibility to debug and track our networks with TensorFlow. This was/is possible with the tf.layers / tf.estimtators APIs. In my view it is a must to have this in the TF 2.0 ecosystem."
28072,[TF 2.0] Inconsistency when timing operations,"I have been trying to benchmark performance of the convolutions operations with speed-up such as depth-separable convolutions[1] and rank-separable convolutions[2]. I'm using the code pasted below:
```
import time
import numpy as np
import tensorflow as tf

# Define a scenario
IMAGE_SIZE = 320
CHANNELS_BATCH_SIZE = 2048  # channels * batch_size
REPEATS = 200
SKIP = 10        

#Build various operations        
class build_normal_ops(tf.keras.Model):
    def __init__(self, KERNEL_SIZE, channels):
        super(build_normal_ops, self).__init__()
        self.normal = tf.keras.layers.Conv2D(channels,KERNEL_SIZE,padding=""same"")
        
    def call(self,x):
        out = self.normal(x)
        return out
    
class build_rank_ops(tf.keras.Model):
    def __init__(self, KERNEL_SIZE, channels):
        super(build_rank_ops, self).__init__()
        self.rs1 = tf.keras.layers.Conv2D(channels,(KERNEL_SIZE,1),padding=""same"")
        self.rs2 = tf.keras.layers.Conv2D(channels,(1,KERNEL_SIZE),padding=""same"")
        
    def call(self,x):
        out = self.rs1(x)
        out = self.rs2(out)
        return out    
    
class build_depth_ops(tf.keras.Model):
    def __init__(self, KERNEL_SIZE, channels):
        super(build_depth_ops, self).__init__()
        self.depthwise = tf.keras.layers.DepthwiseConv2D(KERNEL_SIZE,padding=""same"")
        self.pointwise = tf.keras.layers.Conv2D(channels,1,padding=""same"")
        
    def call(self,x):
        out = self.depthwise(x)
        out = self.pointwise(out)        
        return out
       
def build_ops_all(channels, kernel_size):    
    normal = build_normal_ops(kernel_size,channels)
    rank = build_rank_ops(kernel_size,channels)
    depth = build_depth_ops(kernel_size,channels)    
    return normal, rank, depth 

def time_ops(ops: tf.Operation):
    # Benchmark operation
    with tf.device(""GPU""):
        image = tf.random.normal(shape=[batch_size, IMAGE_SIZE, IMAGE_SIZE, channels], dtype=tf.float32)
        for i in range(REPEATS+SKIP):
            if i == SKIP:
                start = time.time() #Don't time initial runs
            _ = ops(image)
        end = time.time()
        chk = np.round((end - start) / REPEATS * 1000,2)
    return chk 

if __name__ == '__main__':
    #Benchmark with various channel sizes
    for channels in [64,128,256,512]:
        # adjust batch_size so gpu doesn't run out of memory
        batch_size = CHANNELS_BATCH_SIZE // channels        

        #Benchmark with various kernel sizes
        for param in [3,5,7]:                
            normal, rank_separable, depth_separable = build_ops_all(channels, param)
            print('Channels:', channels, 'kernel_size:', param)                   
            
            time_normal = time_ops(normal)
            time_rank = time_ops(rank_separable)
            time_depth = time_ops(depth_separable)

            print(""Normal method: {}ms \t Rank-separable method: {}ms \t Depth-separable method: {}ms \n"".format(time_normal, time_rank, time_depth))
        print('\n')
```

This results in the following output:
```
Channels: 64 kernel_size: 3
Normal method: 5.31ms 	 Rank-separable method: 22.44ms 	 Depth-separable method: 0.59ms 

Channels: 64 kernel_size: 5
Normal method: 35.56ms 	 Rank-separable method: 27.95ms 	 Depth-separable method: 0.65ms 

Channels: 64 kernel_size: 7
Normal method: 39.56ms 	 Rank-separable method: 33.5ms 	 Depth-separable method: 0.63ms 



Channels: 128 kernel_size: 3
Normal method: 13.7ms 	 Rank-separable method: 30.75ms 	 Depth-separable method: 0.6ms 

Channels: 128 kernel_size: 5
Normal method: 16.85ms 	 Rank-separable method: 41.63ms 	 Depth-separable method: 0.54ms 

Channels: 128 kernel_size: 7
Normal method: 61.79ms 	 Rank-separable method: 46.81ms 	 Depth-separable method: 0.58ms 



Channels: 256 kernel_size: 3
Normal method: 12.75ms 	 Rank-separable method: 47.77ms 	 Depth-separable method: 0.56ms 

Channels: 256 kernel_size: 5
Normal method: 137.72ms 	 Rank-separable method: 64.87ms 	 Depth-separable method: 0.57ms 

Channels: 256 kernel_size: 7
Normal method: 159.38ms 	 Rank-separable method: 64.92ms 	 Depth-separable method: 0.61ms 



Channels: 512 kernel_size: 3
Normal method: 23.17ms 	 Rank-separable method: 93.72ms 	 Depth-separable method: 0.57ms 

Channels: 512 kernel_size: 5
Normal method: 86.4ms 	 Rank-separable method: 119.05ms 	 Depth-separable method: 0.55ms 

Channels: 512 kernel_size: 7
Normal method: 537.36ms 	 Rank-separable method: 130.35ms 	 Depth-separable method: 0.55ms
```

My concerns are
1) Inconsistent running times with kernel size 5 and increasing channel size. 
2) Constant running times with depth-separable method.
In pytorch2 there are certain flags that need to be called/set to true to time operations. Has Tensorflow2 also introduced such flags?

Also, have TF developers managed to achieve the theoretical speedups claimed in [1] for depth-wise convolution?

[1] MobileNets: Efficient Convolutional Neural Networks for Mobile VisionApplications. A. Howard et. al
[2] Decomposeme: Simplifying convnets forend-to-end learning. J. Alvarez & L. Petersson"
28071,train_and_evaluate hang forever without message,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux 3.10.0-693.11.6.el7.x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.6.0
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA Version 9.0.176
- GPU model and memory:P100 12.0G

**Describe the current behavior**
when LD_LIBARARY_PATH don't include the libhdfs.so,and we set estimator RunConfig-model_dir to a hdfs path, it will hang at tf.train_and_evaluate forever .I think it can't operate tf.gfile.MkDir() ,but it gives no info or any message.

**Describe the expected behavior**
I expect that it will catch the exception and give the info about loss xx.so.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

classifier = tf.estimator.Estimator(
        model_fn=text_cnn_fn,
        params={
            'feature_columns': feature_columns,
            'vocab_size': FLAGS.hash_size,
            'embedding_size': FLAGS.embedding_size,
            'filter_sizes': filter_sizes,
            'num_filters': FLAGS.num_filters,
            'sequence_length': FLAGS.sequence_length,
            'n_classes': FLAGS.num_classes,
            'learning_rate': FLAGS.learning_rate,
        },
        config=tf.estimator.RunConfig(model_dir=""hdfs://default/tmp"")
    )
...data process
 tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28070,tf2.0a0 tf.nn.ctc_loss with AttributeError: Tensor.op is meaningless when eager execution is enabled.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04/16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha
- TensorFlow version (use command below): 2.0.0-alpha0 / v1.12.0-9492-g2c319fb
- Python version: 3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/fogotten
- GPU model and memory:

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**When running: `python loss.py`, error raised, see the `error.txt` for details. Maybe there is some internal implementation error on the function namely `tf.nn.ctc_loss`...**

**1. helper.py**
```
# Author: Jiarenyf ...
# pylint: disable=invalid-name
# pylint: disable=too-many-locals
# pylint: disable=missing-docstring
# pylint: disable=redefined-outer-name

import tensorflow as tf

#########################################


def dense_to_sparse(tensor, eos_token):
    eos_token = tf.constant(eos_token, tensor.dtype)
    indices = tf.where(tf.not_equal(tensor, eos_token))

    values = tf.gather_nd(tensor, indices)
    shape = tf.shape(tensor, out_type=tf.int64)
    return tf.SparseTensor(indices, values, shape)

```

**2. loss.py**
```
# Author: Jiarenyf ...
# pylint: disable=invalid-name
# pylint: disable=too-many-locals
# pylint: disable=missing-docstring
# pylint: disable=redefined-outer-name

import tensorflow as tf
from helper import dense_to_sparse

#########################################


def ctc_loss(label, logit, label_len, logit_len, classes):
    prediction_sparse = tf.cast(tf.nn.ctc_greedy_decoder(
        logit, logit_len, merge_repeated=True)[0][0], tf.int32)
    prediction = tf.sparse.to_dense(prediction_sparse, classes)

    label_sparse = dense_to_sparse(label, classes)
    accuracy = 1.0 - tf.edit_distance(
        prediction_sparse, label_sparse, normalize=True)
    loss = tf.nn.ctc_loss(
        label, logit, label_len, logit_len, blank_index=classes)

    return loss, accuracy, prediction


#########################################


LOSS_DICT = {
    'ctc': ctc_loss,
}


#########################################

if __name__ == '__main__':
    frames = 8
    classes = 20
    batch_size = 16
    label_len = tf.ones(batch_size, tf.int32)
    label = tf.ones((batch_size, 5), tf.int32)
    logit_len = tf.zeros(batch_size, tf.int32)
    logit = tf.zeros((frames, batch_size, classes+1))
    print(ctc_loss(label, logit, label_len, logit_len, classes+1))

```

**3. error.txt**
```
Traceback (most recent call last):
  File ""loss.py"", line 45, in <module>
    print(ctc_loss(label, logit, label_len, logit_len, classes+1))
  File ""loss.py"", line 22, in ctc_loss
    label, logit, label_len, logit_len, blank_index=classes)
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 672, in ctc_loss_v2
    name=name)
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 784, in ctc_loss_dense
    return compute_ctc_loss(*args)[0]
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py"", line 520, in __call__
    ret, op = _call(self._signature, *args, **kwargs)
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py"", line 1022, in _call
    compute_shapes=False)
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3466, in create_op
    input_ops = set([t.op for t in inputs])
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3466, in <listcomp>
    input_ops = set([t.op for t in inputs])
  File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 934, in op
    ""Tensor.op is meaningless when eager execution is enabled."")
AttributeError: Tensor.op is meaningless when eager execution is enabled.

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28069,Conversion of frozen graph .pb to .tflite error,"So I tried to convert a frozen graph that I got from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md](url) (the name of the model is ""faster_rcnn_resnet101_fgvc""), to a tflite, by using the following command on google collab:

!tflite_convert --graph_def_file=frozen_inference_graph.pb --output_file=optimized_graph.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,1024,1024,3 --input_array=image_tensor --output_array=Softmax

I sort of guessed (probably incorrectly) the input and output arrays, and input_shape, because that information was not provided with the model. I got an error saying something like ""2019-04-23 09:30:48.751842: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)
Aborted (core dumped)"". I am new to Tensorflow, so is there any way I can find out how that error occurred, and how I can resolve it?"
28068,"TypeError: 'Tensor' object is not callable when using tf.keras.optimizers.Adam, works fine when using tf.compat.v1.train.AdamOptimizer","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 
- TensorFlow installed from (source or binary): binary with pip
- TensorFlow version (use command below): 2.0 alpha0
- Python version: 3.6

**Describe the current behavior**
I am trying to migrate 1.12 code using estimator to TF 2.0 using everything from keras as it is suggested. Unfortunatley there is no example yet in TF 2.0 documentation. Here it is only partially using new keras functions:
https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker

```
    # Compute loss for both TRAIN and EVAL modes
    ##loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)

    # Generate necessary evaluation metrics
    ##accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')
    accuracy = tf.keras.metrics.CategoricalAccuracy()
    accuracy.update_state(labels, logits)

    eval_metrics = {'accuracy': accuracy}

    tf.summary.scalar('accuracy', accuracy.result())

    # Provide an estimator spec for `ModeKeys.EVAL`
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metrics)

    # Provide an estimator spec for `ModeKeys.TRAIN`
    if mode == tf.estimator.ModeKeys.TRAIN:

        # crashing
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)

        ##optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)

        train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())

        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
```

**Describe the expected behavior**
replacing:
`optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)`
by 
`optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)`
it works without any issue.

There is so many changes with TF 2.0, that it could be that I am did a wrong implementation but I managed to migrate all old components to keras layer, loss, metrics ...

I couldn't find an equivalent for ""tf.compat.v1.train.get_or_create_global_step()"" for TF 2.0 (without using the compatibility module v1)

**Code to reproduce the issue**
`# estimator model
def baseline_estimator_model(features, labels, mode, params):
    """"""
    Model function for Estimator
    """"""
    print('model based on keras layer but return an estimator model')

    # gettings the bulding blocks
    model = keras_building_blocks(params['dim_input'], params['num_classes'])

    dense_inpout = features['dense_input']

    # Logits layer
    if mode == tf.estimator.ModeKeys.TRAIN:
        logits = model(dense_inpout, training=True)
    else:
        logits = model(dense_inpout, training=False)


    # Compute predictions
    probabilities = tf.nn.softmax(logits)
    classes = tf.argmax(input=probabilities, axis=1, )

    # made prediction
    predictions = {
        'classes': classes,
        'probabilities': probabilities,
    }

    # to be tested
    predictions_output = tf.estimator.export.PredictOutput(predictions)

    #predictions_output = {
    #    'classify': tf.estimator.export.PredictOutput(predictions)
    #}

    # Provide an estimator spec for `ModeKeys.PREDICT`
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=predictions,
                                          export_outputs={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output})

    # Compute loss for both TRAIN and EVAL modes
    ##loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)

    # Generate necessary evaluation metrics
    ##accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')
    accuracy = tf.keras.metrics.CategoricalAccuracy()
    accuracy.update_state(labels, logits)

    #print(tf.argmax(input=labels, axis=1))
    #print(classes)

    eval_metrics = {'accuracy': accuracy}

    tf.summary.scalar('accuracy', accuracy.result())

    # Provide an estimator spec for `ModeKeys.EVAL`
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metrics)

    # Provide an estimator spec for `ModeKeys.TRAIN`
    if mode == tf.estimator.ModeKeys.TRAIN:

        # crashing
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)

        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)

        print('step 7')
        train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())

        print('step 8')
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
        print('step 9')`

notebook can be find here with the code, tf.dataset ...:
https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb

**Other info / logs**
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<timed exec> in <module>

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in train_and_evaluate(FLAGS, use_keras)
    587                                       #exporters=exporter)
    588 
--> 589     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    590 
    591 def train_and_evaluate_old(FLAGS, use_keras):

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    471         '(with task id 0).  Given task id {}'.format(config.task_id))
    472 
--> 473   return executor.run()
    474 
    475 

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
    611         config.task_type != run_config_lib.TaskType.EVALUATOR):
    612       logging.info('Running training and evaluation locally (non-distributed).')
--> 613       return self.run_local()
    614 
    615     # Distributed case.

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
    712         max_steps=self._train_spec.max_steps,
    713         hooks=train_hooks,
--> 714         saving_listeners=saving_listeners)
    715 
    716     eval_result = listener_for_eval.eval_result or _EvalResult(

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    357 
    358       saving_listeners = _check_listeners_type(saving_listeners)
--> 359       loss = self._train_model(input_fn, hooks, saving_listeners)
    360       logging.info('Loss for final step: %s.', loss)
    361       return self

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1137       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1138     else:
-> 1139       return self._train_model_default(input_fn, hooks, saving_listeners)
   1140 
   1141   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1167       worker_hooks.extend(input_hooks)
   1168       estimator_spec = self._call_model_fn(
-> 1169           features, labels, ModeKeys.TRAIN, self.config)
   1170       global_step_tensor = training_util.get_global_step(g)
   1171       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1125 
   1126     logging.info('Calling model_fn.')
-> 1127     model_fn_results = self._model_fn(features=features, **kwargs)
   1128     logging.info('Done calling model_fn.')
   1129 

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in baseline_estimator_model(features, labels, mode, params)
    440 
    441         print('step 7')
--> 442         train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())
    443 
    444         print('step 8')

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)
    294     """"""
    295     grads_and_vars = self._compute_gradients(
--> 296         loss, var_list=var_list, grad_loss=grad_loss)
    297 
    298     return self.apply_gradients(grads_and_vars, name=name)

~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _compute_gradients(self, loss, var_list, grad_loss)
    326     with backprop.GradientTape() as tape:
    327       tape.watch(var_list)
--> 328       loss_value = loss()
    329     grads = tape.gradient(loss_value, var_list, grad_loss)
    330 

TypeError: 'Tensor' object is not callable
"
28067,AttributeError: module 'tensorflow' has no attribute 'app',"<em>Upgraded the tensorflow for poets codelab to tf_2.*.alpha, returns the following on attempt to retrain</em>

python3 -m scripts.retrain   --bottleneck_dir=tf_files/bottlenecks   --how_many_training_steps=500   --model_dir=tf_files/models/   --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}""   --output_graph=tf_files/retrained_graph.pb   --output_labels=tf_files/retrained_labels.txt   --architecture=""${ARCHITECTURE}""   --image_dir=tf_files/flower_photos
[sudo] password for Beast: 
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/Beast/work/newAI/tensorflow-for-poets-2/scripts/retrain.py"", line 1326, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
AttributeError: module 'tensorflow' has no attribute 'app'"
28066,[TF2.0] SavedModel can't save output layer with multiple outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.7

**Describe the current behavior**
`tf.saved_model.save` does not work on keras model when the output layer produce multiple outputs.

**Describe the expected behavior**
SavedModel should produce same outputs as the original model.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import Input, Lambda

class TwoOutputs(tf.keras.layers.Layer):
    def call(self, x):
        return x + 1, x - 1

inputs = Input([2], dtype=tf.int32)
outputs = TwoOutputs()(inputs)  # subclass version is broken
# outputs = Lambda(lambda x: (x + 1, x - 1))(inputs)  # functional version also broken
model = tf.keras.Model(inputs, outputs)

print(model(tf.constant([[0, 1], [2, 3]])))  # works fine

tf.saved_model.save(model, 'checkpoints/test')
model = tf.saved_model.load('checkpoints/test')
infer = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
print(infer.structured_outputs)  # wrong signatures
print(infer(tf.constant([[0, 1], [2, 3]])))  # wrong output

```
**Other info / logs**
The automatically generated saved_model signature relies on the name of outputs.
Keras model generates `model.output_names` here 
https://github.com/tensorflow/tensorflow/blob/f0fba04cd274e7ffa24ab8e49f888ef6974e6f7a/tensorflow/python/keras/engine/network.py#L329
For outputs layers with multiple outputs, all outputs will have the same name!
When creating saved_model, tensorflow uses the following _wrapped_model function to generate the ConcreteFunction signature
https://github.com/tensorflow/tensorflow/blob/f0fba04cd274e7ffa24ab8e49f888ef6974e6f7a/tensorflow/python/keras/saving/saving_utils.py#L108

In our case the model has `outputs` [x1, x2] with `output_names` [layer, layer].
the _wrapped_model function produces signature `{layer: x2}` which is wrong!! 
x1 gets dropped from the output of saved model.
"
28065,scatter_update of cache-enabled variable gives wrong output when sliced,"Well, this looks a very weird bug to me. But this might be a serious one as it can give a ""wrong"" output.

### System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary, from pypi
- TensorFlow version (use command below): tensorflow-gpu 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: CUDA 10.0, cudnn 7.4 (but it happens to CPU only as well)
- GPU model and memory: -

### Code to reproduce the issue

Please take a look at the following code:

```python
import tensorflow as tf
tf.enable_eager_execution()

K = tf.get_variable(""K"", shape=[100, 32], dtype=tf.float32, trainable=False,
                    initializer=tf.random_uniform_initializer(-0.0, 0.0),
                    caching_device='/cpu:0')

# update the variable (the first row of K, i.e. K[0])
tf.scatter_update(K, [0], tf.ones([1, 32]))

# These two lines should give the same answer
print(K[0, :].numpy() [:10])
print(K.numpy()[0, :] [:10])
```

The output is:
```
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
```

The expected behavior:
```
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
```


### Other information

* A variable could be placed either on GPU or on CPU, giving the same behavior.

* Other tensor manipulations such as `tf.reduce_sum(K)` will give the correct answer, so definitely the content of the tensor is well updated.

* If you comment out `caching_device`, it works as expected. This means that this has something to do with **caching mechanism.** _Tensor slice operations seemingly are reading from invalidated caches._

* When it comes to the **static graph mode** (i.e. not eager), it works as normal and expected:
```python
import tensorflow as tf
K = tf.get_variable(""K"", shape=[100, 32], dtype=tf.float32, trainable=False,
                    initializer=tf.random_uniform_initializer(-0.0, 0.0),
                    caching_device='/cpu:0'
                   )
sess = tf.InteractiveSession()
sess.run(K.initializer)

update_op = tf.scatter_update(K, [0], tf.ones([1, 32]))
sess.run(update_op)

# gives [1. 1. ..... 1.]
print( sess.run(K[0, :]) )
```"
28064,unify keras.optimizer and tf.train.Optimizer behavior,when I using tf.train.Adam to replace kearas.optimizer.Adam the fit method rasize error about global_step not exits.
28063,"keras ModelCheckpoint support period save by time or step, not only epoch.","**Describe the feature and the current behavior/state.**
now  ModelCheckpoint only save checkpoint after `period` epoch, which is not good for large data training.

**Will this change the current api? How?**
add `period_type` for step, time, epoch

**Who will benefit with this feature?**
can fine-grained control the checkpoint save behavior.

**Any Other info.**
"
28061,make tflite test error.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:lastest
- Python version:2.7
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
make tflite test target
then error occurs.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
make -f tensorflow/lite/experimental/micro/tools/make/Makefile test 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
- In file included from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:0:
- ./tensorflow/lite/kernels/internal/common.h:48:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory
-  #include ""fixedpoint/fixedpoint.h"""
28060,"TensorFlow/XLA reports an internal error ""Duplicate variable passed tp XLA cluster"" when build and run BERT","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Linux Ubuntu 16.04.3

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):

source

- TensorFlow version (use command below):

ToT

- Python version:

3.6

- Bazel version (if compiling from source):

19.04

- GCC/Compiler version (if compiling from source):

5.4.0

- CUDA/cuDNN version:

10.0/7.4

- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When XLA is off,  the BERT compiles fine.


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

The following code pattern in the TF graph will expose the problem,

op0:  Node ""VarHandleOp""
op1:  Node ""Switch"" op0, pred
op2:  Node ""ReadVariableOp"" op0
op3:  Node ""ReadVariableOp"" op1

Whem op2 and op3 are clustered together in XLA,  the inputs op0 and op1 are
considered resource arguments, and they happen to map the the same resource 
op0, and lead to the error  ""Duplicate variable passed to XLA cluster"" reported at 
line 135, xla_launch_util.cc.

I am worarounding the problem by placing constraint on clustable nodes in
mark_for_compilation_pass.cc, so that nodes with resource input from a ""Switch""
node be rejected for clustering.  Some people suggest that it may be better to fix
the problem where the problem is reported.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28059,"load_model() Error: ""Unknown entries in loss dictionary"" in 2.0","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below): 2.0.0.dev20190417
Python version: 3.6.5

**Describe the current behavior**
Tried very simple transfer learning on Inception V3. Training finished, prediction works fine. Saved the model with model.save_model(). When trying to load it with model = load_model(), I get this error:
ValueError: Unknown entries in loss dictionary: ['class_name', 'config']. Only expected following keys: ['dense_73']

**Describe the expected behavior**

**Code to reproduce the issue**


base_model = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False)

x = base_model.output
y = tf.keras.layers.GlobalAveragePooling2D()(x)
z = tf.keras.layers.Dense(1024, activation='relu')(y)
predictions = tf.keras.layers.Dense(4, activation='softmax')(z)


model = tf.keras.Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='rmsprop', loss=tf.keras.losses.CategoricalCrossentropy())


model.fit(trainData, validation_data=(valData), shuffle=True)

model.save(myPath)

outValues = model.predict(testData)

model = tf.keras.models.load_model(myPath) #error here

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28058,Where is the legacy_seq2seq of the tensorflow 2.0 version?,"Where is the legacy_seq2seq of the tensorflow 2.0 version?
"
28054,misalignment in BahdanauAttention documentation,"**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BahdanauAttention

Under the description for the __init__ function, in the Args, the memory_sequence_length should be in its own point and not part of memory.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Yes, I can submit a PR for this.
"
28053,"Failed to get device properties, error code: 30","**Describe the current behavior**
I commented under issue #26255 but the original poster closed the issue as his problem was solved by updating to tensorflow 2.

I am opening a new issue because updating to the pre-release is not an option and I have no way to even trap this error to try to handle it, plus it is an unknown error code so no hint as to how to proceed.

Unknown error and failure to initialize GPU. 
```
2019-04-19 13:16:22.838705: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30
Failed to initialize GPU device #0: unknown error
```
My configuration:
Windows 10 Home
Tensorflow 1.13.1
Python 3.5
GTX 1060 Mobile Max-Q

It doesn't happen every time I run my program. I have localized it to running load_model from keras, before reaching that point I have imported tensorflow and verified that gpu is available.
```python
  if tf.test.is_gpu_available():
        logger.debug(""GPU is available"")
```
Is there a way to catch this error or check for it and recover/attempt to reinitialize?

Thanks
**Describe the expected behavior**
Report the actual error, provide mechanism to catch the exception and handle it.

**Code to reproduce the issue**
The failure is intermittent

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28047,Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@local_config_cc//',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: version r1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): VisualStudio 2017
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


Trying to build tensorflow.
Have installed bazel and MySys2 and python
Configured for no GPU (see below for print outs)
Issuing the following command results in error:
`bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package`

**CONFIGURE OUTPUT**
```
(tf3.6) D:\repos\tensorflow>python ./configure.py
Extracting Bazel installation...
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is C:\Users\jesaremi\AppData\Local\Continuum\anaconda3\envs\tf3.6\python.exe]:


Found possible Python library paths:
  C:\Users\jesaremi\AppData\Local\Continuum\anaconda3\envs\tf3.6\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\jesaremi\AppData\Local\Continuum\anaconda3\envs\tf3.6\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.


```

**BUILD OUTPUT**
```
(tf3.6) D:\repos\tensorflow>bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: no such package '@local_config_cc//': Traceback (most recent call last):
        File ""C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/cc_configure.bzl"", line 52
                configure_windows_toolchain(repository_ctx)
        File ""C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 406, in configure_windows_toolchain
                setup_vc_env_vars(repository_ctx, vc_path)
        File ""C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 204, in setup_vc_env_vars
                execute(repository_ctx, [""./get_env.bat""], e...)
        File ""C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 163, in execute
                auto_configure_fail((""non-zero exit code: %d, comman...)))
        File ""C:/users/jesaremi/_bazel_jesaremi/fh3jreae/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 109, in auto_configure_fail
                fail((""\n%sAuto-Configuration Error:%...)))

Auto-Configuration Error: non-zero exit code: 255, command [""./get_env.bat""], stderr: ("";D:\Program Files\CMake\bin;C:\Program Files\Java\jdk1.8.0_201\bin;D:\Program Files\apache-maven-3.3.9\bin;D:\Program Files\protoc\bin;D:\Program Files\scala-2.12.8\bin;C:\msys64\usr\bin;C:\Program Files (x86)\Notepad++;D:\Program Files\bazel;""=="""" was unexpected at this time.
)
```"
28045,Graph_transformations/propagate_fixed_sizes.cc Unhandled operator type CTCBeamSearchDecoder,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.13.1
- **Python version**: 3.7
- **Bazel version (if compiling from source)**: 0.24.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 10.0/7.5
- **Exact command to reproduce**:

```bash
bazel-bin/tensorflow/lite/toco/toco --input_file=model.pb --output_file=model.tflite \
--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
--input_arrays=input_audio,input_length,input_labels/values,input_labels/indices \
--output_arrays=labels_1,labels_2,labels_3,labels_4,labels_5,labels_6,labels_7,labels_8,labels_9,labels_10,weight_1,weight_2,weight_3,weight_4,weight_5,weight_6,weight_7,weight_8,weight_9,weight_10,neglogprob \
--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \
--allow_custom_ops \
--input_shapes=1,49,82:1:49:49,2
```

(Also tried)
```bash
tflite_convert \
  --graph_def_file=model.pb --output_file=model.tflite \ 
  --input_arrays=input_audio,input_length,input_labels/values,input_labels/indices \
  --output_arrays=labels_1,labels_2,labels_3,labels_4,labels_5,labels_6,labels_7,labels_8,labels_9,labels_10,weight_1,weight_2,weight_3,weight_4,weight_5,weight_6,weight_7,weight_8,weight_9,weight_10,neglogprob \
  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \
  --enable_select_tf_ops --allow_custom_ops \
  --input_shapes=1,49,82:1:49:49,2
```

### Describe the problem
My frozen model contains ctc_beam_search_decoder operator.
```bash
inputs = tf.placeholder(tf.float32, [1, None, 
                            FLAGS.freq_dim*FLAGS.stack_frames],
                            name='input_audio')
    lens = tf.placeholder(tf.int32, [1], name='input_length')
    logits = model.GRU_Model(inputs, lens, 
                             FLAGS.freq_dim*FLAGS.stack_frames,
                             FLAGS.hidden_dim, FLAGS.voc_size + 1, 
                             FLAGS.num_layers)

    labels = tf.sparse_placeholder(tf.int32, name='input_labels')
    neglogprob = tf.nn.ctc_loss(labels, logits, lens, time_major=False)
    tf.identity(neglogprob, name='neglogprob')

    decoded, _ = tf.nn.ctc_beam_search_decoder(tf.transpose(logits, (1, 0, 2)), 
                                               lens, FLAGS.beam, FLAGS.N)
    for k in range(FLAGS.N):
        tf.cast(decoded[k].values, dtype=tf.int32, 
                name='labels_N'.replace('N', str(k+1)))
        tf.reciprocal(tf.nn.ctc_loss(tf.cast(decoded[k], dtype=tf.int32), 
                      logits, lens, time_major=False),
                      name='weight_N'.replace('N', str(k+1)))

    saver = tf.train.Saver(tf.global_variables())
    with tf.Session() as sess:
        print('Loading model from %s' % FLAGS.checkpoint)
        sess.run(tf.global_variables_initializer())
        saver.restore(sess, FLAGS.checkpoint)
        print('Node names:\n')
        for node in tf.get_default_graph().as_graph_def().node:
            print(node.name)
        output_nodes = ['neglogprob']
        for k in range(FLAGS.N):
            output_nodes += ['weight_%d' % (k+1), 'labels_%d' % (k+1)]
        frozen_graph_def = graph_util.convert_variables_to_constants(sess,
                           sess.graph_def, output_nodes)
        tf.train.write_graph(frozen_graph_def, 
                             os.path.dirname(FLAGS.output_file),
                             os.path.basename(FLAGS.output_file),
                             as_text=False)
        print('Saved frozen graph to %s' % FLAGS.output_file)
```
When I try to convert .pb model to .tflite I get:
```bash
2019-04-22 14:53:34.726887: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcudart.so.10.0
2019-04-22 14:53:34.771557: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-04-22 14:53:34.781295: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-22 14:53:34.781432: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3
2019-04-22 14:53:34.781475: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-22 14:53:34.781653: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3
2019-04-22 14:53:34.781720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781807: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781859: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781908: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781929: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781973: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.781998: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782024: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond
2019-04-22 14:53:34.782085: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3
2019-04-22 14:53:34.782108: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782128: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-22 14:53:34.782146: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782175: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782487: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782535: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782624: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782644: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782804: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782831: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782894: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.782913: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783092: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783166: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783191: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783238: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783276: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3
2019-04-22 14:53:34.783299: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter
2019-04-22 14:53:34.783319: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2019-04-22 14:53:34.783359: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit
2019-04-22 14:53:34.783379: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3
2019-04-22 14:53:34.783421: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3
2019-04-22 14:53:34.783536: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783590: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783610: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783649: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783668: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783705: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783723: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783761: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783780: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783818: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783838: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783875: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783894: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783933: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.783951: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.783990: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.784009: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.784045: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.784064: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.784102: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: CTCLoss
2019-04-22 14:53:34.784120: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2019-04-22 14:53:34.785713: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 209 operators, 353 arrays (0 quantized)
2019-04-22 14:53:34.788612: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 206 operators, 345 arrays (0 quantized)
2019-04-22 14:53:34.791340: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 206 operators, 345 arrays (0 quantized)
2019-04-22 14:53:34.792679: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:2425] Unhandled operator type CTCBeamSearchDecoder
Aborted (core dumped)
```

How could I avoid this problem? I dind't find any issue contains such problem.
"
28044,tf_nightly_gpu_2.0_preview-2.0.0.dev20190420  import error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip install 
- TensorFlow version (use command below):tf_nightly_gpu_2.0_preview2.0.0.dev20190420
- Python version:3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
import tensorflow as tf
**Describe the expected behavior**
Able to import
**Code to reproduce the issue**
import tensorflow as tf
**Other info / logs**
Traceback (most recent call last):
  File ""D:/Python/Project/PythonProject/TF20/code/NFFM/code/NFFM-2.0.py"", line 21, in <module>
    import tensorflow as tf
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 42, in <module>
    from tensorflow._api.v2 import compat
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\_api\v2\compat\__init__.py"", line 21, in <module>
    from tensorflow._api.v2.compat import v1
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\_api\v2\compat\v1\__init__.py"", line 643, in <module>
    'tensorflow_estimator.python.estimator.api._v1.estimator'))
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\tools\component_api_helper.py"", line 56, in package_hook
    child_pkg = importlib.import_module(child_package_str)
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\__init__.py"", line 8, in <module>
    from tensorflow_estimator.python.estimator.api._v1.estimator import experimental
  File ""C:\Users\Z&J\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\experimental\__init__.py"", line 29, in <module>
    _sys.modules[__name__], ""estimator.experimental"")
TypeError: __init__() missing 1 required positional argument: 'deprecated_to_canonical'
"
28043,Weird behavior using tf.Variable in a tf.data.experimental.map_and_batch callback function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): 1.13.1 from conda
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Code to reproduce the issue**

```
import tensorflow as tf

def foo(x):
    array = tf.Variable(lambda: tf.zeros(20, dtype=tf.float32),
            trainable=False, use_resource=True)
    ind = tf.reshape(x, [-1, 1])
    val = tf.reshape(tf.cast(x, tf.float32), [-1])

    array_assign = array.assign(tf.zeros_like(array))
    with tf.control_dependencies([array_assign]):
        array = array.scatter_nd_update(ind, tf.cast(val, dtype=tf.float32))

    return array

dataset = tf.data.Dataset.range(20)
# dataset = dataset.map(foo)
# dataset = dataset.batch(5)
dataset = dataset.apply(
        tf.data.experimental.map_and_batch(foo, 5, num_parallel_batches=1))

iterator = dataset.make_initializable_iterator()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(iterator.initializer)
    next_element = iterator.get_next()
    fetches = sess.run(next_element)
    print(fetches)
```

**Describe the current behavior**

If using normal `map` and `batch`, one get

```
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
```

**Describe the expected behavior**

If using `map_and_batch`, one get changing results like:

```
[[0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 2. 3. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
```

There should be always only one and non-duplicate non-zeros per-line.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Not sure if this is related to #27507 . On a [Colab with tf-nightly](https://colab.research.google.com/drive/1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF), #27507 seems fixed but this issue still reproduces."
28041,I have the error FailedPreconditionError: Attempting to use uninitialized value dense_2/bias 	 [[node dense_2/bias/read (defined at C:/Users/TRAN THI DIEM/Documents/diem/research/ReinforcementLearning/code/CodeforlearningReinforcement/CNNandRL/Object-recognition-CIFAR-10-master/code_22_4_2019_done_firstversion_50loop_100images_verify_optimize_error_loss_ok_backup_version1.py:76) ]],"Here is my code. Please let me know what is my problem

import tensorflow as tf
from keras.datasets import cifar10
from keras.utils import np_utils
import numpy as np

import keras.backend as K
K.set_learning_phase(0)
import random
state_size = [32,32,3]
y = tf.placeholder(tf.float32, [None,10])
inputs_ = tf.placeholder(tf.float32, [None, *state_size])
learning_rate = 0.001
sess = tf.Session()
#sess.run(tf.initialize_all_variables())
sess.run(tf.global_variables_initializer())
init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())
sess.run(init_op)
def neural_network_model ():
    #inputs_ = tf.placeholder(tf.float32, [None, *state_size])

    
    conv1 = tf.layers.conv2d(inputs=inputs_,
                              filters=32,
                              kernel_size=[3,3],
                              strides=[1,1],
                              padding=""same"",
                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()
                              )

    conv1_batchnorm = tf.layers.batch_normalization(conv1,
                                                     training=True,
                                                     epsilon=1e-5
                                                     )

    conv1_out = tf.nn.elu(conv1_batchnorm)
## --> [30, 30, 32]

    conv2 = tf.layers.conv2d(inputs=conv1_out,
                              filters=32,
                              kernel_size=[3, 3],
                              strides=[1,1],
                              padding=""same"",
                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()
                              )

    conv2_batchnorm = tf.layers.batch_normalization(conv2,
                                                     training=True,
                                                     epsilon=1e-5
                                                     )

    conv2_out = tf.nn.elu(conv2_batchnorm)
## --> [28, 28, 32]

    conv3 = tf.layers.conv2d(inputs=conv2_out,
                              filters=64,
                              kernel_size=[3,3],
                              strides=[1,1],
                              padding=""VALID"",
                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()
                              )

    conv3_batchnorm = tf.layers.batch_normalization(conv3,
                                                     training=True,
                                                     epsilon=1e-5
                                                     )

    conv3_out = tf.nn.elu(conv3_batchnorm)
## --> [26, 26, 64]

    flatten = tf.layers.flatten(conv3_out)
## --> [43,264]

    fc = tf.layers.dense(inputs=flatten,
                          units=512,
                          activation=tf.nn.elu,
                          kernel_initializer=tf.contrib.layers.xavier_initializer()
                          )

    output = tf.layers.dense(inputs=fc,
                              kernel_initializer=tf.contrib.layers.xavier_initializer(),
                              units=10,
                              activation=None)
    return output

def training_network_option1 ():
    sess = tf.Session()
#sess.run(tf.initialize_all_variables())
    sess.run(tf.global_variables_initializer())
#    y = tf.placeholder(tf.float32, [None,10])
    prediction = neural_network_model()
    # OLD VERSION:
    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    # NEW:
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )
    optimizer=tf.train.RMSPropOptimizer(learning_rate).minimize(cost)
    return cost, optimizer
    
def training_network_option2 ():
#    y = tf.placeholder(tf.float32, [None,10])
    sess = tf.Session()
#sess.run(tf.initialize_all_variables())
    sess.run(tf.global_variables_initializer())
    prediction = neural_network_model()
    # OLD VERSION:
    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    # NEW:
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )
    optimizer=tf.train.MomentumOptimizer(learning_rate).minimize(cost)    
    return cost, optimizer
    
def training_network_option3 ():
#    y = tf.placeholder(tf.float32, [None,10])
    sess = tf.Session()
#sess.run(tf.initialize_all_variables())
    sess.run(tf.global_variables_initializer())
    prediction = neural_network_model()
    # OLD VERSION:
    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    # NEW:
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )
    optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost) 
    return cost, optimizer

    
    




def possible_action():
    batch =0
    stochastic =1
    mini_batch = 2

    actions =np.array([batch, stochastic, mini_batch])
    return actions



def normalize(x):
    """"""
        argument
            - x: input image data in numpy array [32, 32, 3]
        return
            - normalized x
    """"""
    min_val = np.min(x)
    max_val = np.max(x)
    x = (x-min_val) / (max_val-min_val)
    return x

(X_train, y_train), (X_test, y_test) = cifar10.load_data()


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train = normalize(X_train)
X_test = normalize(X_test)
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]

#DQNetwork

action_size = 3

batch_size = 10

state_batch = 100//batch_size
# Exploration parameters for epsilon greedy strategy
explore_start = 1.0            # exploration probability at start
explore_stop = 0.01            # minimum exploration probability 
decay_rate = 0.1            # exponential decay rate for exploration prob

# Q learning hyperparameters
gamma = 0.95  

actions = np.array(possible_action())
current_action =actions [0]



#dqn1 = DQNetwork(state_size, action_size)
#dqn2 = DQNetwork(state_size, action_size)
#dqn_total = DQNetwork(state_size, action_size)

#current_state =X_train[0:64]
#Y_label =y_train[0:64]






loss_actions = []
loss_actions_test = []
action_test = []
loss_total_test = []
#loss_next_state = 100*random ()


loss_before = 2*(random.random())
Qtable =np.zeros((state_batch,action_size))
np.random.seed(100)
exp_exp_tradeoff = 0.5

#print (""random of explore"", exp_exp_tradeoff)
step=0

for eps in range(50):
   i = 0
   print (""\n"")
   while i<state_batch-1:
   #while i<20:
        current_state = X_train[i*batch_size:(i+1)*batch_size]
    
        Y_label = y_train [i*batch_size:(i+1)*batch_size]
        
       # dqn = DQNetwork(state_size, action_size, learning_rate)
        #sess.run(dqn.optimizer,feed_dict={dqn.inputs_:current_state,dqn.label:Y_label
                                                       # }) 
        #sess.run(dqn.input_action, feed_dict = {dqn.input_action: current_action})
       # dqn = DQNetwork(state_size, action_size, learning_rate,current_action)
        if current_action == 0:
           cost,optimizer = training_network_option1 ()
        elif current_action ==1:
           cost,optimizer = training_network_option2 ()
        elif current_action ==2:   
           cost,optimizer = training_network_option2 ()
        
   
       
        _, loss_current = sess.run([optimizer, cost], feed_dict={inputs_: current_state, y: Y_label})
        "
28040,tflite segmentation model throws error on android,"**System information**
- What is the top-level directory of the model you are using: models->research ->Deeplab
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.13
- Bazel version (if compiling from source): bazel-0.24.0-installer-darwin-x86_64
- CUDA/cuDNN version: NA
- GPU model and memory: No GPUs used
- Exact command to reproduce:
tflite_convert \
  --output_file=test.lite \
  --graph_def_file=frozen_inference_graph.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,600,450,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128 


**Describe the current behavior**
I am using tflite for semantic segmentation. I have a model trained to segment objects from background, this model is trained on deeplab.

I have converted this model(frozen inference graph) into tflite format using the below code:
```
tflite_convert \
  --output_file=test.lite \
  --graph_def_file=frozen_inference_graph.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,600,450,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128 
```
The model loads on android, but when I try to run inference it gives me this error:


```
    Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: third_party/tensorflow/lite/kernels/unpack.cc:54 NumDimensions(input)

        1 was not true.Node number 4 (UNPACK) failed to prepare.

```

**Code to reproduce the issue**

tflite_convert \
  --output_file=test.lite \
  --graph_def_file=frozen_inference_graph.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,600,450,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128 

**How do I resove this error?**"
28039,Failed to create capture session; configuration failed,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: im using Raspberry Pi for android things implementation
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the current behavior**
 i was going to implement the android repository(i edited the AndroidManifest.xml a little so only the DetectorActivity is going to launch) in android things when it failed to open the camera  

**Describe the expected behavior**
this was supposed to show a live camera preview
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
`ImageUtils:Native library not found,native RGB -> YUV conversion may be unavailable.
CameraConnectionFragment: Opening camera preview: 640x4480
CameraDevice-JV-0: Stream configuration failed due to: endConfigure:372: Camera 0: Unsupported set of inputs/outputs provided
CameraCaptureSession: Session 0: Failed to create capture session; configuration failed
`
"
28038,how to use tf.contrib.framework.fuse_op to fuse graph,"Hello,
I want to use tf.contrib.framework.fuse_op to fuse graph to fuse my graph for faster inference,but when i call the api 
`new_graphdef = tf.contrib.framework.fuse_op(
                                           graph_def,
                                           input_nodes=[""ac_input""],
                                           output_nodes=[""Inference/input_tansfer/mul""],
                                           output_dtypes=[tf.float32],
                                           output_quantized=False,
                                           op_name=""trans_fused"",
                                           op_type=""add_mul""
                                       )`
I got an error
`File ""build_fusedop.py"", line 1006, in main
    op_type=""add_mul"" 
  File ""/home/yx.wang/anaconda3/envs/py27tfgpu/lib/python2.7/site-packages/tensorflow/contrib/framework/python/framework/graph_util.py"", line 115, in fuse_op
    new_node.attr[""_output_types""].list.type[:] = output_dtypes
TypeError: tf.float32 has type DType, but expected one of: int, long`

I cannot find the detail deacription of the parameters, the source code as followings:
`graph_def: A graph_pb2.GraphDef proto.
input_nodes: input nodes to the subgraph to be fused.
output_nodes: output nodes to the subgraph to be fused.
output_dtypes: A list of output datatypes for the custom op
output_quantized: A boolean flag that indicates if output is quantized
op_name: fused op name.
op_type: fused op type.` 

what is the correct input of the api?"
28037,tensorflow2.0版本的legacy_seq2seq在哪？,"tensorflow2.0版本的legacy_seq2seq在哪？
"
28036,AUTHORS references non-existent file,"**Describe the documentation issue**

The [AUTHORS](https://github.com/tensorflow/tensorflow/blob/master/AUTHORS) file references a CONTRIBUTORS files, but no `CONTRIBUTORS` or `CONTRIBUTORS.md` file exists:

```
# This file is distinct from the CONTRIBUTORS files.
# See the latter for an explanation.
```

If CONTRIBUTORS is some where else, this should be made explicit in AUTHORS.

**We welcome contributions by users. Can you submit a PR?**

This is a question for maintainers and a PR is not yet appropriate."
28035,Can I use the same record file for tf_record_input_reader (Object Detection)?,"I have one image dataset with some classes and needed annotations.
So I made a record file (`train.record`) for training an object detection model.
I set it in `train_input_reader`

But what should I set for `eval_input_reader`? Can I use the same record file? 

""Eval"" is used for verifying the quality of training as I understand

    train_input_reader: {
      tf_record_input_reader {
        input_path: ""object_detection/train.record""
      }
      label_map_path: ""object_detection/training/labelmap.pbtxt""
    }
    
    ...
    
    eval_input_reader: {
      tf_record_input_reader {
        input_path: ""???""
      }
      label_map_path: ""object_detection/training/labelmap.pbtxt""
      shuffle: false
      num_readers: 1
    }"
28033,"Object detection API, training part: NewRandomAccessFile failed to Create/Open: data/object-detection.pbtxt","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I followed Sentdex tutorial about object detection API
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install, GPU version of tensorflow
- TensorFlow version (use command below): 1.13.1
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA toolkit: 10.0.130 and cuDNN: 7.3.1
- GPU model and memory: NVIDIA GTX970M, 3GB


When I try this command in the anaconda prompt:
(from ...Tensorflow\models\research\object_detection)

python legacy/train.py 
--train_dir=training/ 
--pipeline_config_path=training/faster_rcnn_inception_v2_pets.config 
--logtostderr

It should start training to detect the objects I want but I get this error: 

tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: data/object-detection.pbtxt : Le fichier sp\udce9cifi\udce9 est introuvable.
; No such file or directory

And I can't figure why I get this error, the object-detection.pbtxt is in the right place, I have also tried to put entire path ( C:/folder1/folder2/.../data/object-detection.pbtxt ) in the config file. I tried to use normal / double slash or backslash in my command line and inside the config file.

Is it because of object-detection.pbtxt that is considered as a .txt file when I created it with the notepad ?
Hoping to find a solution quick."
28032,5526 illegal hardware instruction (core dumped)  python3,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.13.1
- Python version:3.7.3
- Installed using virtualenv? pip? conda?:pip3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:GTX950M/4G



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28031,java api runs much slower than Python API,"i predict image with python api 
only need 15ms,
but with the same model,java api need 500+ms"
28030,"i use tensoflow model to predict with java api,why it is more slowly than python or c++ api?","i use tensoflow model to predict with java api,why it is more slowly than python or c++ api?"
28029,sample_weight ignored,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
""Ubuntu 16.04.5 LTS""
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Docker image from tensorflow/tensorflow:latest-py3-jupyter
- TensorFlow version (use command below):
b'v1.13.1-0-g6612da8951' 1.13.1
- Python version:
3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609]
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
sample_weight seems to be ignored by fit(), test_on_batch(), etc.

**Describe the expected behavior**
sample_weight should affect the loss.

**Code to reproduce the issue**
```
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import mse

# a dummy model that just returns the inputs
i=Input(shape=(1,))
m=Model(inputs=[i],outputs=[i])
m.compile(optimizer=Adam(),loss=[mse])

# the unweighted loss should be 1
xs=np.zeros((1,1))
ys=np.ones((1,1))

# expecting loss to be weighted by sample_weight (so be 0.1 for the second test)
print(m.test_on_batch(xs,ys))
print(m.test_on_batch(xs,ys,np.array([.1])))
```"
28028,[TF2.0] Custom training loop for keras example should provide information about tf.keras.backend.set_learning_phase(),"**System information**
- TensorFlow version: tensorflow-gpu 2.0.0-alpha0
- Doc Link: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#using_the_gradienttape_a_first_end-to-end_example

**Describe the documentation issue**
That samples did not use tf.keras.backend.set_learning_phase() for manual training loop. So if the model has BatchNormalization() or other layers which depend on training state, it will be not trained correctly. There is no description about tf.keras.backend.set_learning_phase(), even in the page of BatchNormalization(). (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization)
"
28027,`import tf.compat.v2.summary` broken since 88ca0db75e,"The following import should work:

```python
import tensorflow.compat.v2.summary as b
b.scalar
```

This worked fine in d4342f77b6, but broke in 88ca0db75e (first bad):

```
$ cat ./repro.py
import tensorflow as tf
print(tf.__git_version__)
import tensorflow.compat.v2.summary as b
print(b)
print(b.scalar)
```

```
$ virtualenv -q -p python2.7 ./ve-old/
$ . ./ve-old/bin/activate
(ve-old) $ pip install ~/tmp/tf-builds/tf-d4342f77b6/* >/dev/null 2>/dev/null
(ve-old) $ python ./repro.py
v1.12.1-143-gd4342f77b6
<module 'tensorflow.compat.v2.summary' from '/tmp/tmp.kn2U5CUdHI/ve-old/local/lib/python2.7/site-packages/tensorboard/summary/_tf/summary/__init__.pyc'>
<function scalar at 0x7f682249aa28>
(ve-old) $ deactivate
```

```
$ virtualenv -q -p python2.7 ./ve-new/
$ . ./ve-new/bin/activate
(ve-new) $ pip install ~/tmp/tf-builds/tf-88ca0db75e/* >/dev/null 2>/dev/null
(ve-new) $ python ./repro.py
v1.12.1-144-g88ca0db75e
<module 'tensorflow.compat.v2.summary' from '/tmp/tmp.kn2U5CUdHI/ve-new/local/lib/python2.7/site-packages/tensorflow/_api/v1/compat/v2/summary/__init__.pyc'>
Traceback (most recent call last):
  File ""./repro.py"", line 5, in <module>
    print(b.scalar)
AttributeError: 'module' object has no attribute 'scalar'
(ve-new) $ deactivate
```

Note that the imported module path has changed from [TensorBoard’s
component package entry point][tb] to the stub provided by TensorFlow:

```python
# This file is MACHINE GENERATED! Do not edit.
# Generated by: tensorflow/python/tools/api/generator/create_python_api.py script.
""""""Operations for writing summary data, for use in analysis and visualization.

See the [Summaries and
TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) guide.

""""""

from __future__ import print_function as _print_function

from tensorflow._api.v1.compat.v2.summary import experimental
from tensorflow.python.ops.summary_ops_v2 import SummaryWriter
from tensorflow.python.ops.summary_ops_v2 import _flush_fn as flush
from tensorflow.python.ops.summary_ops_v2 import create_file_writer_v2 as create_file_writer
from tensorflow.python.ops.summary_ops_v2 import create_noop_writer
from tensorflow.python.ops.summary_ops_v2 import record_if
from tensorflow.python.ops.summary_ops_v2 import trace_export
from tensorflow.python.ops.summary_ops_v2 import trace_off
from tensorflow.python.ops.summary_ops_v2 import trace_on
from tensorflow.python.ops.summary_ops_v2 import write

del _print_function
```

(Note the absence of any imports of `scalar`, etc.)

[tb]: https://github.com/tensorflow/tensorboard/blob/90a386d1da4d9f477cb60b679f07aa98045d8c7e/tensorboard/summary/_tf/summary/__init__.py

This is blocking TensorBoard’s CI.
"
28026,Logger suggests non-existent module attribute 'tf.keras.layers.CuDNNLSTM' when using 'tf.keras.layers.LSTM',"**System information**
- Windows 10 Education
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version: tensorflow-gpu==2.0.0-alpha0
- Python version: 3.7.3
- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.3.1
- GPU model and memory: NVIDIA GeForce GTX 1080 / 16GB

**Describe the current behavior**
Logger throws warning, that the use of 'tf.keras.layers.LSTM' is not optimized for performance on GPU. Suggests 'tf.keras.layers.CuDNNLSTM' instead, but module attribute 'CuDNNLSTM' doesn't exist.

[Screenshot](https://imgur.com/gdvAPAr)

**Describe the expected behavior**
Usage of 'tf.keras.layers.CuDNNLSTM' doesn't throw an AttributeError, when being a suggested solution.

**Code to reproduce the issue**
```
import tensorflow as tf

# Initialising the RNN
regressor = tf.keras.models.Sequential()

# Adding the first LSTM layer and some Dropout regularisation
regressor.add(tf.keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))   # using not-perfomance-optimized LSTM, produces warning
regressor.add(tf.keras.layers.Dropout(0.2))

# Adding a second LSTM layer and some Dropout regularisation
regressor.add(tf.keras.layers.CuDNNLSTM(units = 50, return_sequences = True))   # using log-suggested LSTM, throws AttributeError
regressor.add(tf.keras.layers.Dropout(0.2))
```
[lstm_stocks.zip](https://github.com/tensorflow/tensorflow/files/3101494/lstm_stocks.zip)"
28025,Transformer model for language understanding link broken,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: tf 2.0
- Doc Link: https://www.tensorflow.org/alpha/tutorials/text/transformer


**Describe the documentation issue**
When trying to find the code on github, through the button, the path is broken as well as the Run in google colab button.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
28024,Using tensorflow-gpu == 1.7.0 with CUDA 8,"I am facing ```ImportError``` when I try calling ```tensorflow-gpu == 1.7.0``` on a system with ```CUDA 8``` and ```CuDNN 5.1```. I am not authorized to update the CUDA version of the system, so am looking for a workaround.

> ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory"
28022,Cannot run tensorflow GPU 2.0 with cuda 9.2,"## System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): I used virtualenv and _pip install --upgrade tensorflow-gpu==2.0.0-alpha0_
- TensorFlow version: 2.0.0-alpha0
- Python version: 3.5
- Installed using virtualenv? pip? conda?: installed in virtualenv using
- Name of the Virtualenv: `tensorflow` 
- CUDA/cuDNN version: cuda 9.2
---
## Problem
Hello, I've installed tensorflow 2.0 with cuda 9.2 and when I run: `>> import tensorflow`, it prompts: 

```
Traceback (most recent call last):
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py"", line 27, in <module>
    from tensorflow._api.v2 import audio
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/_api/v2/audio/__init__.py"", line 8, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/kexin/virtualenv/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```
Thank you!"
28020,tf.control_dependencies does execute nodes in the right order,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34,  1.10.0
- Python version: 3.6.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
with tf.control_dependiencies does not execute the dependant nodes before the nodes in the with block

**Describe the expected behavior**
the nodes in the with block are executed after the dependant nodes

**Code to reproduce the issue**
import tensorflow as tf
import numpy as np

x = tf.Variable(initial_value = 0.0, name = ""x"", dtype = tf.float32)
y = tf.Variable(initial_value = 0.0, name = ""y"", dtype = tf.float32)
zero = tf.constant(value = 0.0, dtype = np.float32, name = ""zero"")
op1 = tf.assign(x, zero)
with tf.control_dependencies([op1]):
    op2 = tf.assign(y, x)
with tf.control_dependencies([op2]):
    tf.assign_add(y, 1, name = ""assign_add"")

sess = tf.Session()
sess.run(""assign_add"")
print(""y"", sess.run(""y:0""))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
28019,can't do serving with model generated by TF2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I trained model with tf.keras's functional api with tensorflow 2.0 and save model with tf.keras.Model.save() and convert the h5 model with tf.compat.v1.saved_model.simple_save(). When I try to serve the model with the command

```Bash
saved_model_cli run --dir ./serving_model --tag_set serve --signature_def serving_default --input_exp 'input_image = np.random.normal(size=(1,28,28,1))'
```

 I get error message

>RunTimeError: The Session graph is empty. Add operations to the graph before calling run().

**Describe the expected behavior**

I expect that an output tensor representing the class of the input handwriting digit is returned from the serving.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Please refer to my project at https://github.com/breadbread1984/EagerExecutionDemo. after train with train_mnist.py, executing start_serving.sh will reproduce the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28018,AttributeError: 'PerReplica' object has no attribute 'begin',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'unknown' 1.13.1 (installed with conda)
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda/9.0.176, cudnn/7.3
- GPU model and memory: Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
2019-04-21 19:03:25.539522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15190 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:87:00.0, compute capability: 6.0)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
When running `tf.estimator.Estimator` model that registers `tf.train.SessionRunHook` to `evaluation_hooks` of `tf.estimator.EstimatorSpec` in distributed environment, an error `AttributeError: 'PerReplica' object has no attribute 'begin'` occurs at the beggining of evaluation. This error does not happen if I do not register SessionRunHook to evaluation_hooks. Registering SessionRunHook to `training_hooks` does not trigger the error even if it is in distributed mode.

I ran my Estimator with `tf.estimator.train_and_evaluate`.

The distribution configuration I used is `tf.contrib.distribute.MirroredStrategy`.

The whole error log is attatched at the end.

**Describe the expected behavior**

Somehow SessionRunHook turned into PerReplica at some point in evaluation code of Estimator. It should remain SessionRunHook's interface in distribution mode.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

This is not a runnable code, but introducing the modification below to some estimator examples might work as a reproducer.

```
distribution = tf.contrib.distribute.MirroredStrategy()
run_config = tf.estimator.RunConfig(train_distribute=distribution,
                                                           eval_distribute=distribution)

hook = tf.train.ProfilerHook(output_dir=model_dir)  # example hook
def model_fn(features, labels, mode, params):
            if mode == tf.estimator.ModeKeys.EVAL:
                return tf.estimator.EstimatorSpec(mode, loss,
                                                      evaluation_hooks=[hook])

estimator = tf.estimator.Estimator(params, model_dir, run_config)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.
WARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2019-04-21 18:41:17.901359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1
2019-04-21 18:41:17.901414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-21 18:41:17.901425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 
2019-04-21 18:41:17.901432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y 
2019-04-21 18:41:17.901439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N 
2019-04-21 18:41:17.902038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:85:00.0, compute capability: 6.0)
2019-04-21 18:41:17.902219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15190 MB memory) -> physical GPU (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:87:00.0, compute capability: 6.0)
WARNING:tensorflow:From /home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /tmp/model.ckpt-0
WARNING:tensorflow:From /home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/model.ckpt.
INFO:tensorflow:Initialize strategy
2019-04-21 18:42:04.023667: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-04-21 18:42:05.162460: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x555559b95370
2019-04-21 18:42:05.970327: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x555559ba9960
INFO:tensorflow:loss = 52170.477, step = 0
INFO:tensorflow:global_step/sec: 0.0334123
INFO:tensorflow:loss = 54870.64, step = 1 (29.929 sec)
INFO:tensorflow:Saving checkpoints for 3 into /tmp/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2019-04-21T09:43:02Z
Traceback (most recent call last):
  File ""train.py"", line 146, in <module>
    main()
  File ""train.py"", line 142, in main
    use_multi_gpu)
  File ""train.py"", line 83, in train_and_evaluate
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 611, in run
    return self.run_local()
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 712, in run_local
    saving_listeners=saving_listeners)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 358, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1122, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1185, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1287, in _actual_train_model_distributed
    saving_listeners)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1407, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run
    run_metadata=run_metadata)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run
    run_metadata=run_metadata)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run
    raise six.reraise(*original_exc_info)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1335, in run
    run_metadata=run_metadata))
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 582, in after_run
    if self._save(run_context.session, global_step):
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 607, in _save
    if l.after_save(session, step):
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 517, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 537, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py"", line 913, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate
    name=name)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 509, in _actual_eval
    return _evaluate()
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 500, in _evaluate
    output_dir=self.eval_dir(name))
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1537, in _evaluate_run
    config=self._session_config)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py"", line 271, in _evaluate_once
    session_creator=session_creator, hooks=hooks) as session:
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/8/18IA1142/miniconda3/envs/tacotron2-tf-1.13/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 636, in __init__
    h.begin()
AttributeError: 'PerReplica' object has no attribute 'begin'
```
"
28014,[TF 2.0] the word2vec_basic.py example is not 2.0.0-alpha0.,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
28013,"[TF 2.0] tf.assert_equal funtion raise exception InternalError with unsigned 16, 32, 64","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab.google.com (linux)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not test
- TensorFlow installed from (source or binary): binary (nighty)
- TensorFlow version (use command below): 2.0.0-dev20190420
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NVIDIA-SMI 418.56 Driver Version: 410.79 CUDA Version: 10.0
- GPU model and memory: Tesla T4

**Describe the current behavior**
As documented function [assert_equal](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/debugging/assert_equal) accept any dtype. But it raises an exception InternalError with type: uint16, 32, 64. Test with CPU and GPU.
**Describe the expected behavior**
Not raises an exception InternalError with type: uint16, 32, 64.
**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
from tensorflow.python.framework import dtypes
print(tf.__version__)
# tf.uint16, tf.uint32, tf.uint64
lhs = tf.constant([5, 0, 7, 11], dtype=tf.uint16)
rhs = tf.constant([5, 0, 7, 11], dtype=tf.uint16)
tf.assert_equal(lhs, rhs)
```
may be same other asser functions: raise exception with assert_greater 
```python
lhs = tf.constant([7], dtype=tf.uint16)
rhs = tf.constant([9], dtype=tf.uint16)
tf.assert_greater(lhs, rhs)
```
**Other info / logs**
<details><summary>LOGS</summary>
<p>

```python
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-17-b706d33af3c8> in <module>()
     10   exp = tf.constant([0, 0, 3, 10], dtype=dtype)
     11   res = bitwise_ops.bitwise_and(lhs, rhs)
---> 12   tf.assert_equal(res, exp) # TRUE

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/check_ops.py in assert_equal_v2(x, y, message, summarize, name)
    452       execution or if `x` and `y` are statically known.
    453   """"""
--> 454   return assert_equal(x=x, y=y, summarize=summarize, message=message, name=name)
    455 
    456 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/check_ops.py in assert_equal(x, y, data, summarize, message, name)
    496 
    497     if context.executing_eagerly():
--> 498       eq = math_ops.equal(x, y)
    499       condition = math_ops.reduce_all(eq)
    500       if not condition:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in equal(x, y, name)
   3449       else:
   3450         message = e.message
-> 3451       _six.raise_from(_core._status_to_exception(e.code, message), None)
   3452   # Add nodes to the TensorFlow graph.
   3453   try:

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InternalError: Could not find valid device for node.
Node: {{node Equal}}
All kernels registered for op Equal :
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_HALF]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
 [Op:Equal]
```
</p>
</details>"
28012,Annotating negative class in object detection (Faster R-CNN),"I am currently training Faster R-CNN on some data and try to build a detector based on our data.
Since my data set has some images without the object of interest in it.
How do I annotate them? (i.e., what will be the bounding box for these images)
Also, does annotating the negative class improve the training? 
"
28010,[TF2.0] Not JSON Serializable error wasn thrown when using tf.keras.activations operators in keras model.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tensorflow-gpu 2.0.0a0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: Geforce 1070

**Describe the current behavior**
When I used tf.keras.activations operators in my keras model, serialization of model was failed due to Not JSON Serializable error.

**Describe the expected behavior**
It should be serialized without any error.

**Code to reproduce the issue**
```
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(784,), name='digits')
x = layers.Activation('relu')(inputs)
# x = keras.activations.relu(inputs)
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)


model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')
model.summary()

model.save('path_to_my_model.h5')
```
If you changed from Activation() to relu, it failed to serialize."
28009,Transformer network weights - demo error ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview
- TensorFlow version (use command below): 2.0.0-dev20190420
- Python version: 3
- Bazel version (if compiling from source): Colab
- GCC/Compiler version (if compiling from source): Colab
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I have been trying the demos of tensorflow 2.0, particularly the transformers example. However, the current transformer example is not working anymore. This is the error output: 

ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.

Seems like the latest tensorflow version 2.0.0-dev20190420 has changed a way how to call the inputs or set the weights. 

It was working with the tf version 2.0.0-dev20190413. 
In conclusion the transformer demo is not working anymore, is there any way to get installed in colab the 2.0.0-dev20190413 version?

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Instructions are here: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb#scrollTo=8QG9nueFQKXx

I used the GPU version.

I just ran the colab transformer notebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb#scrollTo=8QG9nueFQKXx

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28008,Cannot Make TfRecords Work for Image-sentence Pairs,"Hi,

I am stuck on making tfrecords work for image-text pair data.

Here is the code to create tfrecord from numpy array of image features and a text file,

```
def npy_to_tfrecords(numpy_array, text_file, output_file):
      f = open(text_file)

      # write records to a tfrecords file
      writer = tf.python_io.TFRecordWriter(output_file)

      # Loop through all the features you want to write
      for X, line in zip(numpy_array, f) :
         #let say X is of np.array([[...][...]])
         #let say y is of np.array[[0/1]]

         txt = ""{}"".format(line[:-1])
         txt = txt.encode()

         # Feature contains a map of string to feature proto objects
         feature = {}
         feature['x'] = tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten()))
         feature['y'] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[txt]))

         # Construct the Example proto object
         example = tf.train.Example(features=tf.train.Features(feature=feature))

         # Serialize the example to a string
         serialized = example.SerializeToString()

         # write the serialized objec to the disk
         writer.write(serialized)
      writer.close()
```

I cannot make the dataset after this:
```

def load_data_tfr():
    
   train = tf.data.TFRecordDataset(""train.tfrecord"")

   # example proto decode
   def _parse_function1(example_proto):
      keys_to_features = {'x': tf.FixedLenFeature(2048, tf.float32),
                          'y': tf.VarLenFeature(tf.string) } 
      parsed_features = tf.parse_single_example(example_proto, keys_to_features)
      return {""x"": parsed_features['x'], ""y"":  parsed_features['y']} # ['x'], parsed_features['y']

   # Parse the record into tensors.
   train = train.map(_parse_function1)

   return train
```

`train_data = load_data_tfr()`

I keep . getting the error:

```
random.shuffle(train_data)

    for i in reversed(range(1, len(x))):
TypeError: object of type 'MapDataset' has no len()
```

Any help? thank you."
28007,InvalidArgumentError when running map_fn on strings inside a tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
```bash
conda install tensorflow-gpu==2.0-alpha
```
- Python version:
3.7.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
  cudatoolkit-10.0.130-0                                                                                                  
  cudnn-7.3.1-cuda10.0_0
- GPU model and memory:
GeForce RTX 2080 Ti

**Describe the current behavior**
Running the provided code on GPUs leads to error message `tensorflow.python.framework.errors_impl.InvalidArgumentError: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string`
Without feeding the tensor to the convolution layer, `summary.image` would succeed.

**Describe the expected behavior**
Should run smoothly.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras import layers

H, W, C = 10, 10, 3
imgs = tf.zeros([10, H, W, C])
ds = tf.data.Dataset.from_tensor_slices(imgs)
ds = ds.batch(2)
conv = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')


@tf.function
def run(img, i):
    conv(img)
    tf.summary.image('img', img, i)


if __name__ == ""__main__"":
    train_summary_writer = tf.summary.create_file_writer('/tmp/testsummary')
    with train_summary_writer.as_default():
        for i, img in enumerate(ds):
            run(img, i)
```

**Other info / logs**
```2019-04-20 14:44:30.775146: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this
TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-04-20 14:44:30.818841: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1700000000 Hz
2019-04-20 14:44:30.819976: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b6fa788f50 executing computa
tions on platform Host. Devices:
2019-04-20 14:44:30.820029: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <u
ndefined>
2019-04-20 14:44:30.825689: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic li
brary libcuda.so.1
2019-04-20 14:44:31.062487: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b6fc634120 executing computa
tions on platform CUDA. Devices:
2019-04-20 14:44:31.062554: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 208
0 Ti, Compute Capability 7.5
2019-04-20 14:44:31.063894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:19:00.0
totalMemory: 10.73GiB freeMemory: 10.57GiB
2019-04-20 14:44:31.063942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-04-20 14:44:31.064034: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic li
brary libcudart.so.10.0
2019-04-20 14:44:31.067082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor wi
th strength 1 edge matrix:
2019-04-20 14:44:31.067114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
2019-04-20 14:44:31.067130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
2019-04-20 14:44:31.068283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:local
host/replica:0/task:0/device:GPU:0 with 10284 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id
: 0000:19:00.0, compute capability: 7.5)
2019-04-20 14:44:33.628228: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::Star
tAbort Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]]
2019-04-20 14:44:33.628374: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::Star
tAbort Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
2019-04-20 14:44:33.628468: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function e
xecution failed: Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]]
2019-04-20 14:44:33.628456: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function e
xecution failed: Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
Traceback (most recent call last):
  File ""test.py"", line 21, in <module>
    run(img, i)
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/def_function.
py"", line 438, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py"",
 line 1288, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py"",
 line 574, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py"",
 line 627, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py"",
 line 415, in call
    ctx=ctx)
  File ""/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"",
line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: During Variant Host->Device Copy: non-DMA-copy attempted of
tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]] [Op:__inference_run_343]
```"
28005,Java Tensorflow Not found: TF GPU device with id 0 was not registered,"```
$  java -cp test-gpu.jar com.gpu.demo.Demo001_DLPredictor_GPU
label len=2280
2019-04-20 19:45:51.893764: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2019-04-20 19:45:52.460339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:5a:00.0
totalMemory: 31.74GiB freeMemory: 10.32GiB
2019-04-20 19:45:52.460427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-04-20 19:45:54.493478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-20 19:45:54.493549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-04-20 19:45:54.493567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-04-20 19:45:54.494202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9965 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5a:00.0, compute capability: 7.0)
2019-04-20 19:45:54.639326: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered
```

I use java to call TensorFlow, using the 0th GPU, but
E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered"
28004,New weight initialisation when using dropout and ReLU,"- Are you willing to contribute it (Yes/No): Yes

**Describe the feature**
In a recent NeurIPS paper ([Pretorius et al. 2018](https://arxiv.org/pdf/1811.00293.pdf)), we derived a new weight initialisation strategy for deep ReLU networks that use dropout. This new initialisation ensures stable variance propagation for ReLU activations travelling through a dropout layer. The go-to initialisation for dense ReLU layers is the He initialisation, but in the above paper, we showed that if a dropout layer is applied after a dense ReLU layer the variances are longer preserved in the forward pass. This can lead to unstable signal propagation in deep ReLU networks that use dropout. We fix this with the new initialisation and thought it might be useful to the community if there was a Tensorfow implementation.  

**Will this change the current api? How?**
It should not require the current api to change.

**Proposed implementation**
In short, the initialisation samples from a normal distribution with zero mean and `stddev = \sqrt{2*(1-p)/fan_in}`, where `p is the probability of an element to be zeroed` when passing through the dropout layer following the dense layer being initialised. To implement the initialisation in Tensorflow, I was thinking perhaps something along these lines of code being added to the `init_ops.py`? This init should be applied to dense ReLU layers preceding a dropout layer.

```python
def dropout(rate, seed=None):
  """"""Dropout initializer.
  It draws samples from a normal distribution centred on 0
  with standard deviation given by
  `stddev = sqrt(2*(1-rate) / fan_in)` where `fan_in` is the number of
  input units in the weight tensor and `rate` is the probability of an 
  element to be zeroed as set in the dropout layer applied after this dense layer.
  Arguments:
      rate: The dropout rate, between 0 and 1. This should be set equal to the dropout rate in the subsequent dropout layer.
      seed: A Python integer. Used to seed the random generator.
  Returns:
      An initializer.
  References:
      [Pretorius et al., 2018]
      (https://papers.nips.cc/paper/7814-critical-initialisation-for-deep-signal-propagation-in-noisy-rectifier-neural-networks)
      # pylint: disable=line-too-long
      ([pdf](https://papers.nips.cc/paper/7814-critical-initialisation-for-deep-signal-propagation-in-noisy-rectifier-neural-networks.pdf))
  """"""
  if not (rate >= 0 and rate < 1):
        raise ValueError(""rate must be a scalar tensor or a float in the ""
                         ""range [0, 1), got %g"" % rate)
  return VarianceScaling(
      scale=2.0*(1.0-rate), mode=""fan_in"", distribution=""normal"", seed=seed)
```

**Who will benefit with this feature?**
Hopefully, anyone applying dropout in a deep ReLU network. :)

*Side note: We do not sample from a truncated normal since in our experiments we noticed that when sampling from a truncated normal (which is currently the default), the signal propagation dynamics did not fit with the theory predicting variance preservation during the forward pass. This was the case for both the He init and our dropout init. Therefore, we would also suggest a  (non-truncated) normal as the default for the standard initialisers currently implemented (e.g. He and Xavier etc.).
"
28003,TF 2.0 Bug or Feature? keras includes duplicated shared variables in model.variables,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip nightly
- TensorFlow version (use command below): 2.0.0-dev20190415
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
The two following layers give different number of variable in `model.variables`. Keras duplicates shared variables in the variable list:
```
import tensorflow as tf
import numpy as np

class MLayer(tf.Module):
    def __init__(self):
        super(MLayer, self).__init__()
        conv = tf.keras.layers.Conv2D(3, 3, 1, padding='SAME')
        self.convs = [conv] * 7

    def __call__(self, x):
        for conv in self.convs:
            x = conv(x)
        return x

class KLayer(tf.keras.models.Model):
    def __init__(self):
        super(KLayer, self).__init__()
        conv = tf.keras.layers.Conv2D(3, 3, 1, padding='SAME')
        self.convs = [conv] * 7

    def call(self, x):
        for conv in self.convs:
            x = conv(x)
        return x

xnp = np.random.rand(1, 224, 224, 3)
x = tf.constant(xnp, tf.float32)

model = MLayer()
y = model(x)
v = [variable.name for variable in model.variables]
print(v, len(v))

model = KLayer()
y = model(x)
v = [variable.name for variable in model.variables]
print(v, len(v))
```
The first print:
```
['conv2d/kernel:0', 'conv2d/bias:0'] 2
```
The second prints: 
```
['k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0', 'k_layer/conv2d_1/kernel:0', 'k_layer/conv2d_1/bias:0'] 14
```
**Describe the expected behavior**

I would expect two layers have the same number of variables. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
28002,CTCBeamSearchDecoder - Less leaves in the beam search than requested.,"I encounter the issue after successfully training on a number of batches of data (different number each time). The interesting thing is that if I train only on the batch that the error occurred on, everything is fine - so there is nothing wrong with the data (I am using the Tensorflow speech_commands dataset). I'm very confused as to what is causing this behaviour. Any help is greatly appreciated.

Stack trace:

    Traceback (most recent call last):
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
        return fn(*args)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
        options, feed_dict, fetch_list, target_list, run_metadata)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
        run_metadata)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Less leaves in the beam search than requested.
    	 [[{{node loss/CTCBeamSearchDecoder}} = CTCBeamSearchDecoder[beam_width=100, merge_repeated=true, top_paths=2, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/transpose_1/_91, Fill/_93)]]
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""train.py"", line 180, in <module>
        train_and_eval()    
      File ""train.py"", line 107, in train_and_eval
        feed_dict=feed_dict)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
        run_metadata_ptr)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
        feed_dict_tensor, options, run_metadata)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
        run_metadata)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
        raise type(e)(node_def, op, message)
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Less leaves in the beam search than requested.
    	 [[node loss/CTCBeamSearchDecoder (defined at /mainfs/lyceum/chk1g16/Speech-Recognition/Conv_LSTM_CTC/conv_lstm_ctc_net.py:256)  = CTCBeamSearchDecoder[beam_width=100, merge_repeated=true, top_paths=2, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/transpose_1/_91, Fill/_93)]]
    
    Caused by op 'loss/CTCBeamSearchDecoder', defined at:
      File ""train.py"", line 180, in <module>
        train_and_eval()
      File ""train.py"", line 55, in train_and_eval
        data_gen._num_frames, data_gen._num_mel_spec_bins, init_lr, lr_decay_steps, lr_decay_rate)
      File ""/mainfs/lyceum/chk1g16/Speech-Recognition/Conv_LSTM_CTC/conv_lstm_ctc_net.py"", line 330, in create_train_graph
        predictions, loss, acc_greedy, edit_dist_greedy, acc_beam, edit_dist_beam, scores = get_ctc_loss(logits, label_batch_plh)
      File ""/mainfs/lyceum/chk1g16/Speech-Recognition/Conv_LSTM_CTC/conv_lstm_ctc_net.py"", line 256, in get_ctc_loss
        merge_repeated=True)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/ops/ctc_ops.py"", line 277, in ctc_beam_search_decoder
        merge_repeated=merge_repeated))
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_ctc_ops.py"", line 74, in ctc_beam_search_decoder
        top_paths=top_paths, merge_repeated=merge_repeated, name=name)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
        return func(*args, **kwargs)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
        op_def=op_def)
      File ""/lyceum/chk1g16/.conda/envs/py3venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
        self._traceback = tf_stack.extract_stack()
    
    InvalidArgumentError (see above for traceback): Less leaves in the beam search than requested.
    	 [[node loss/CTCBeamSearchDecoder (defined at /mainfs/lyceum/chk1g16/Speech-Recognition/Conv_LSTM_CTC/conv_lstm_ctc_net.py:256)  = CTCBeamSearchDecoder[beam_width=100, merge_repeated=true, top_paths=2, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/transpose_1/_91, Fill/_93)]]



loss function:

    def get_ctc_loss(logits, label_batch):
        # logits: [batch_size, max_time, num_classes]
        
        
        # 1-D tensor showing the length for each label in the batch
        batch_labels_lengths = tf.fill([tf.shape(label_batch)[0]], tf.shape(label_batch)[1])
        
        with tf.name_scope('loss'):
            # get sparse represenattion of the labels
            non_zero_elems_coords = tf.where(tf.not_equal(label_batch, 0))
            non_zero_elems = tf.gather_nd(label_batch, non_zero_elems_coords)
            sparse_label_batch = tf.SparseTensor(indices=non_zero_elems_coords, 
                                                 values=non_zero_elems,
                                                 dense_shape=tf.shape(label_batch, out_type=tf.int64))
                                            
            # calculate ctc loss                                
            ctc_loss_op = tf.nn.ctc_loss(labels=sparse_label_batch, 
                                         inputs=logits, 
                                         sequence_length=batch_labels_lengths,
                                         preprocess_collapse_repeated=True, 
                                         time_major=False, 
                                         ctc_merge_repeated=True,
                                         ignore_longer_outputs_than_inputs=False)
            loss = tf.reduce_mean(ctc_loss_op)
            
            
            prediction_probabilities = tf.nn.softmax(logits)
            max_probabilities = tf.reduce_max(prediction_probabilities, axis=2)
            raw_predictions = tf.argmax(prediction_probabilities, axis=2)
    
    
    
            # greedy decode logits
            # greedy decoder - beeam decoder with beam_width=1 and top_paths=1
            logits_T = tf.transpose(logits, perm=[1, 0, 2])
            greedy_predictions, neg_sum_logits = tf.nn.ctc_greedy_decoder(inputs=logits_T, 
                                                    sequence_length=batch_labels_lengths,
                                                    merge_repeated=True)
            
            # get greedy performance metrics
            edit_dist_greedy = tf.edit_distance(tf.cast(greedy_predictions[0], tf.int32),
                                                sparse_label_batch, 
                                                normalize=False)
            acc_greedy = tf.reduce_mean(tf.cast(tf.equal(edit_dist_greedy, 0), tf.float32))
            edit_dist_greedy = tf.reduce_mean(edit_dist_greedy)
    
    
            
            # beam decode logits
            beam_predictions, log_probabilities = tf.nn.ctc_beam_search_decoder(inputs=logits_T, 
                                                        sequence_length=batch_labels_lengths, 
                                                        beam_width=100,
                                                        top_paths=2,
                                                        merge_repeated=True)
                                            
            # get beam performance metrics
            edit_dist_beam = tf.edit_distance(tf.cast(beam_predictions[0], tf.int32),
                                                  sparse_label_batch,
                                                  normalize=False)
            acc_beam = tf.reduce_mean(tf.cast(tf.equal(edit_dist_beam, 0), tf.float32))
            
            
            predictions = tf.cast(tf.sparse.to_dense(beam_predictions[0]), tf.int32)
            scores = log_probabilities[:, 0] - log_probabilities[:, 1]
    
        tf.summary.scalar('ctc_loss', loss)
        tf.summary.scalar('acc_greedy', acc_greedy)
        tf.summary.scalar('edit_dist_greedy', edit_dist_greedy)
        tf.summary.scalar('confidence_score', tf.reduce_mean(scores))
        # tf.summary.scalar('edit_dist_beam', edit_dist_beam)
        tf.summary.scalar('acc_beam', acc_beam)
    
        return predictions, loss, acc_greedy, edit_dist_greedy, acc_beam, edit_dist_beam, scores"
28001,Tensorflow (cpu) vs concurrent.futures: Exception ,"I am using Tensorflow v2.0.0-alpha on MacBook (CPU only). Python version: 3.7.2.

I am testing a rl agent on multiple environments. For this I am using concurrent.futures.ProcessPoolExecutor.map function. Roughly around the time when first processes are done, exception occurs:

```
 objc[44717]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[44717]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
Traceback (most recent call last):
  File ""/Users/ikkamens/PycharmProjects/ilya_agents/ilya_agents/launch/launch.py"", line 38, in <module>
    for env, result in zip(envs, metrics):
  File ""/Users/ikkamens/.pyenv/versions/3.7.2/lib/python3.7/concurrent/futures/process.py"", line 476, in _chain_from_iterable_of_lists
    for element in iterable:
  File ""/Users/ikkamens/.pyenv/versions/3.7.2/lib/python3.7/concurrent/futures/_base.py"", line 586, in result_iterator
    yield fs.pop().result()
  File ""/Users/ikkamens/.pyenv/versions/3.7.2/lib/python3.7/concurrent/futures/_base.py"", line 432, in result
    return self.__get_result()
  File ""/Users/ikkamens/.pyenv/versions/3.7.2/lib/python3.7/concurrent/futures/_base.py"", line 384, in __get_result
    raise self._exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.

```
"
28000,"first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\nAborted (core dumped)""","**System information**
-Centos
- TensorFlow 1.9:


**change the pb model to tflite *
my network include batch_normalization layer, 
when I change the the pb model to tflite , there is always return a error message.
```
first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\nAborted (core dumped)""
```

I tried  lots of methods, like add  converter.default_ranges_stats=(0, 6) or remove bn layers. but  I know its not the expected ways. hope other friends can give me some suggestions. thanks

"
27999,I understand that java should be faster than Python. But not like this.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
```java
//create session, this pb come from python
        ConfigProto config=ConfigProto.newBuilder()
                .setAllowSoftPlacement(true)
                .setLogDevicePlacement(false)
                .clearLogDevicePlacement()
                //.setGpuOptions(GPUOptions.newBuilder().setAllowGrowth(true))
                //.setInterOpParallelismThreads(1)
                .build();
        sess=new Session( graph, config.toByteArray() );
```
```java
        AnjosLog log=new AnjosLog();
        log.begin();
        log.begin2Series(""create data"");
        float[][][] X=new float[1][this.audio_len][this.audio_feat_len];
        X[0]=data;
        log.stop2Series();
        log.begin2Series(""create tensor"");
        Tensor x= Tensor.create(X);
        log.stop2Series();
        log.begin2Series(""use model"");
        Tensor  y = sess.runner().feed(input_var, x).fetch(label_var).run().get(0);
        log.stop2Series();
        log.begin2Series(""take result"");
        float[][][] result = new float[ (int)y.shape()[0] ][ (int)y.shape()[1] ][ (int)y.shape()[2] ];
        y.copyTo(result);
        log.stop2Series();
        log.stop();
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 CPU
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.8.0 java
- Python version: 3.5.6
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: none
- GPU model and memory: none


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

I use python3.5 predict the same one sample speed:
```
series start　　　　　　　　　　　　　　　　 2019-04-20 13:06:18
series end　　　　　　　　　　　　　　　　 2019-04-20 13:06:18
total time　　　　　　　　　　　　　　　　 0.24s       0.00min      2019-04-20 13:06:18
````
I use java1.8 predict the same one sample speed:
```
start time　　　　　　　　　　　　　　　　2019-04-20 12:57:43 924
 series start　　　　　　　　　　　　　　　　 create data 2019-04-20 12:57:43 925
 series end　　　　　　　　　　　　　　　　         0.00s         0.00min
 series start　　　　　　　　　　　　　　　　 create tensor 2019-04-20 12:57:43 927
 series end　　　　　　　　　　　　　　　　         0.09s         0.00min
 series start　　　　　　　　　　　　　　　　 use model 2019-04-20 12:57:44 015
  series end　　　　　　　　　　　　　　　　         0.69s         0.01min
 series start　　　　　　　　　　　　　　　　 take result 2019-04-20 12:57:44 710
  series end　　　　　　　　　　　　　　　　         0.03s         0.00min
 end time　　　　　　　　　　　　　　　　 2019-04-20 12:57:44 738
 total time　　　　　　　　　　　　　　　　         0.81s         0.01min
```

**Describe the expected behavior**
**I understand that java should be faster than Python. But not like this.**



**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27998,tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes ,"I'm training some yolo models using 

> qqwweee/keras-yolo3

When I trained with 50 epochs, the test can be done although the results is bad.
So I trained with 150 epochs, and when I want to test with this model, I got the following ... (it's not an error but like an error):

Input image filename:eagle.jpg
(416, 416, 3)
2019-04-20 12:33:44.137176: E tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes
2019-04-20 12:33:44.140305: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2019-04-20 12:33:44.143018: E tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes
2019-04-20 12:33:44.145718: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2019-04-20 12:33:44.149139: E tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes
2019-04-20 12:33:44.151644: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2019-04-20 12:33:44.154246: E tensorflow/core/common_runtime/bfc_allocator.cc:381] tried to deallocate nullptr
2019-04-20 12:33:44.157496: E tensorflow/core/common_runtime/bfc_allocator.cc:381] tried to deallocate nullptr
2019-04-20 12:33:44.160937: E tensorflow/core/common_runtime/bfc_allocator.cc:381] tried to deallocate nullptr
Found 0 boxes for img
6.07202067316063

I think it's something wrong in TF.
environment: Anaconda python(3.6.8), keras 2.1.5, tensorflow (cpu) 1.10.0.
I use ""qqwweee/keras-yolo3"" and made a train data as its requirement like follows :

> Generate your own annotation file and class names file.
One row for one image;
Row format: image_file_path box1 box2 ... boxN;
Box format: x_min,y_min,x_max,y_max,class_id (no space).
For VOC dataset, try python voc_annotation.py
Here is an example:
path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3
path/to/img2.jpg 120,300,250,600,2
..."
27997,Import Error from Keras import ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: tensorflow-1.13.1
- Python version: python 3.7.0
- Installed using virtualenv? pip? conda?: pip install tensorflow
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Using TensorFlow backend.

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\ProgramData\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

C:\ProgramData\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-21-1090e364ba0e> in <module>()
----> 1 from keras import Model , Sequential
      2 #from keras.model import Sequential
      3 from keras.layers import Dense
      4 from keras.optimizers import SGD

C:\ProgramData\Anaconda3\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\__init__.py in <module>()
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

C:\ProgramData\Anaconda3\lib\site-packages\keras\utils\conv_utils.py in <module>()
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\__init__.py in <module>()
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
from keras import Model , Sequential
from keras.layers import Dense
from keras.optimizers import SGD

I also tried with this code and got the same error...
from keras.model import Sequential
from keras.layers import Dense
from keras.optimizers import SGD

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I have keras version 2.2.4 installed with pip"
27996,LeakyRelu support in contrib.quantize,"**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I am trying to create an eval graph with fake quantization nodes, from a checkpoint file, of a tensorflow implementation of YoloV3. My goal is to eventually convert this to a UINT8 (fully quantized) .tflite inference model using TOCO.

My code snippet is like the following -

        with tf.variable_scope('yolov3'):
            boxes, confs, probs = model.forward(inputs, is_training=False)

        scores = confs * probs
        print(""=>"", boxes.name[:-2], scores.name[:-2])
        cpu_out_node_names = [boxes.name[:-2], scores.name[:-2]]
        saver = tf.train.Saver(var_list=tf.global_variables(scope='yolov3'))

        saver.restore(sess, flags.ckpt_file)
        tf.contrib.quantize.create_eval_graph(input_graph=graph)
        sess.run(tf.global_variables_initializer())
        print('=> checkpoint file restored from ', flags.ckpt_file)
        utils.freeze_graph(sess, './experiment/yolov3_FQ.pb', cpu_out_node_names)

The tf.contrib.quantize.create_eval_graph(...) call itself finishes with no error, but when I run TOCO to convert the graph to UINT8 tflite, it complains that LeakyRelu doesn't have min/max parameters.

Following is my TOCO command -

bazel --output_user_root=/local/mnt/workspace/rganguly/deep_learning/bazel_new/build run --config=opt tensorflow/lite/toco:toco -- \
--output_file=/local/mnt/workspace/rganguly/deep_learning/live/yolov3/yolov3_.tflite \
--input_file=/local/mnt/workspace/rganguly/deep_learning/live/yolov3/tensorflow-yolov3/experiment/yolov3_fake_quant.pb \
--input_shapes=1,416,416,3 \
--input_arrays=Placeholder \
--output_arrays='yolov3/concat_3','mul' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_dev_values=127 \

And following is the error I get -

2019-04-19 16:56:59.708619: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1613 operators, 2394 arrays (0 quantized)
2019-04-19 16:56:59.747329: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1613 operators, 2394 arrays (0 quantized)
2019-04-19 16:57:06.761701: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 286 operators, 471 arrays (1 quantized)
2019-04-19 16:57:06.767392: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 286 operators, 471 arrays (1 quantized)
2019-04-19 16:57:06.770164: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 211 operators, 396 arrays (1 quantized)
2019-04-19 16:57:06.772654: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 211 operators, 396 arrays (1 quantized)
2019-04-19 16:57:06.775354: F tensorflow/lite/toco/tooling_util.cc:1708] Array yolov3/darknet-53/Conv/LeakyRelu, which is an input to the Pad operator producing the output array yolov3/darknet-53/Pad, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.


**Will this change the current api? How?**
- NO

**Who will benefit with this feature?**
- Anyone who has a LeakyRelu in the tensorflow graph and wants to transform it to a fully quantized/UINT8 .tflite model. In other words, YoloV3 cannot be converted to a UINT8 .tflite model without this support.



**Any Other info.**
There are multiple other issues I faced in TOCO while converting this model, but I will summarize them in another ticket."
27994,compat.as_bytes should support bytearray,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently, `tensorflow.python.util.compat.as_bytes` accepts `bytes` but not `bytearray`. This causes issues when trying to write a `bytearray` to `GFile`, which internally calls `compat.as_bytes`.

`compat.as_bytes` should support `bytearray` to improve usability.

**Will this change the current api? How?** This change is backward-compatible.

**Who will benefit with this feature?** Supporting `bytearray` in `compat.as_bytes` helps in use cases where a `GFile` file handle is wrapped in some other class, such as a fastavro `Writer` which writes out `bytearray` to the file handle.

**Any Other info.**
"
27993,[TF2.0] Incorect Loss calculation in `keras.Model.evaluate` when last batch size is different.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): TF2.0 Alpha
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
The loss value returned from `Keras.Model.evaluate()` is an arithmetic average over losses for each batch. However, when the last batch has a smaller batch size than previous batches (e.g. a dataset has 101 examples, with batch-size=10), the calculation is incorrect because it will mistakenly give higher weight to the last batch's loss.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
27992,[TF 2.0] Issue with TPUStrategy / initialize_tpu_system,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): !pip install tensorflow-gpu==2.0.0-alpha0
- TensorFlow version (use command below): 2.0.0-alpha0
- Python version: 3.6.7 (Colab) 

**Describe the current behavior**

Error occurs when trying to instantiate a simple Keras model running on TPU on Colab, using TPUStrategy.

It seems that there is an internal problem regarding the worker name: 

- if WORKER_NAME is set to 'worker', then an exception is raised during the call to initialize_tpu_system(): ""/job:tpu_worker/replica:0/task:1/device:CPU:0 unknown device.""

- if WORKER_NAME is set to 'tpu_worker', the strategy is properly initialized, but another exception is raised later when creating the Keras model: ""Error copying tensor to device: /job:worker/replica:0/task:0/device:TPU:0""

I have read issue #26513 to place a call to experimental_connect_to_host() before calling initialize_tpu_system(), but it does not help.



```
!pip install tensorflow-gpu==2.0.0-alpha0

import tensorflow as tf
import os
import sys

TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
WORKER_NAME='tpu_worker'

tf.config.experimental_connect_to_host(TPU_WORKER, WORKER_NAME) 

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_WORKER)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)
devices=tf.config.experimental_list_devices()
print(*devices,sep=""\n"")

with strategy.scope():
  
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])

  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])
```


**Describe the expected behavior**

Model should be properly instantiated.

**Code to reproduce the issue**

See above.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Exception when WORKER_NAME='worker':
```
InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-3-2725ffed0ef5> in <module>()
     10 
     11 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_WORKER)
---> 12 tf.tpu.experimental.initialize_tpu_system(resolver)
     13 strategy = tf.distribute.experimental.TPUStrategy(resolver)
     14 devices=tf.config.experimental_list_devices()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)
     91     with ops.device(get_first_tpu_host_device(cluster_resolver)):
     92       output = tpu_functional_ops.TPUPartitionedCall(
---> 93           args=[], device_ordinal=0, Tout=[dtypes.string], f=func_name)
     94     serialized_topology = output[0].numpy()
     95   else:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_tpu_ops.py in tpu_partitioned_call(args, device_ordinal, Tout, f, name)
   5606       else:
   5607         message = e.message
-> 5608       _six.raise_from(_core._status_to_exception(e.code, message), None)
   5609   # Add nodes to the TensorFlow graph.
   5610   if not isinstance(Tout, (list, tuple)):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: /job:tpu_worker/replica:0/task:1/device:CPU:0 unknown device.
```

Exception when WORKER_NAME='tpu_worker':
```
RuntimeError                              Traceback (most recent call last)

<ipython-input-4-4fe0e3235d99> in <module>()
     22       tf.keras.layers.Flatten(),
     23       tf.keras.layers.Dense(64, activation='relu'),
---> 24       tf.keras.layers.Dense(10, activation='softmax')
     25   ])
     26 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    106     if layers:
    107       for layer in layers:
--> 108         self.add(layer)
    109 
    110   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    167           # and create the node connecting the current layer
    168           # to the input layer we just created.
--> 169           layer(x)
    170           set_inputs = True
    171 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    592           # Build layer if applicable (if the `build` method has been
    593           # overridden).
--> 594           self._maybe_build(inputs)
    595           # Explicitly pass the learning phase placeholder to `call` if
    596           # the `training` argument was left unspecified by the user.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   1711     # Only call `build` if the user has manually overridden the build method.
   1712     if not hasattr(self.build, '_is_default'):
-> 1713       self.build(input_shapes)
   1714     # We must set self.built since user defined build functions are not
   1715     # constrained to set self.built.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in build(self, input_shape)
    163         constraint=self.kernel_constraint,
    164         trainable=True,
--> 165         dtype=self.dtype)
    166     if self.use_bias:
    167       self.bias = self.add_weight(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    375         collections=collections,
    376         synchronization=synchronization,
--> 377         aggregation=aggregation)
    378     backend.track_variable(variable)
    379 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    620     new_variable = getter(
    621         name=name, shape=shape, dtype=dtype, initializer=initializer,
--> 622         **kwargs_for_getter)
    623 
    624     # If we set an initializer and the variable processed it, tracking will not

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    150       collections=collections,
    151       synchronization=synchronization,
--> 152       aggregation=aggregation)
    153   return v
    154 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    210   def __call__(cls, *args, **kwargs):
    211     if cls is VariableV1:
--> 212       return cls._variable_v1_call(*args, **kwargs)
    213     elif cls is Variable:
    214       return cls._variable_v2_call(*args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation)
    173         use_resource=use_resource,
    174         synchronization=synchronization,
--> 175         aggregation=aggregation)
    176 
    177   def _variable_v2_call(cls,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in getter(**kwargs)
     56   """"""To avoid capturing loop variables.""""""
     57   def getter(**kwargs):
---> 58     return captured_getter(captured_previous, **kwargs)
     59   return getter
     60 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in creator_with_resource_vars(*args, **kwargs)
    821       kwargs[""use_resource""] = True
    822       kwargs[""distribute_strategy""] = strategy
--> 823       return self._create_variable(*args, **kwargs)
    824 
    825     def distributed_getter(getter, *args, **kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _create_variable(self, next_creator, *args, **kwargs)
    439     return _create_tpu_mirrored_variable(
    440         self._container_strategy(), device_map, logical_device,
--> 441         _real_mirrored_creator, *args, **kwargs)
    442 
    443   def _reduce_to(self, reduce_op, value, destinations):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _create_tpu_mirrored_variable(strategy, device_map, logical_device, real_mirrored_creator, *args, **kwargs)
    101   with tape.stop_recording():
    102     devices = device_map.logical_to_actual_devices(logical_device)
--> 103     value_list = real_mirrored_creator(devices, *args, **kwargs)
    104     result = values.TPUMirroredVariable(
    105         strategy, device_map, value_list, aggregation,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _real_mirrored_creator(devices, *args, **kwargs)
    432               kwargs[""initial_value""] = initial_value_fn
    433           with context.device_policy(context.DEVICE_PLACEMENT_SILENT):
--> 434             v = next_creator(*args, **kwargs)
    435           assert not isinstance(v, values.TPUMirroredVariable)
    436           value_list.append(v)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in <lambda>(**kwargs)
    152                         aggregation=VariableAggregation.NONE):
    153     """"""Call on Variable class. Useful to force the signature.""""""
--> 154     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    155     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    156       previous_getter = _make_getter(getter, previous_getter)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2490         caching_device=caching_device, name=name, dtype=dtype,
   2491         constraint=constraint, variable_def=variable_def,
-> 2492         import_scope=import_scope, distribute_strategy=distribute_strategy)
   2493   else:
   2494     return variables.RefVariable(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    214       return cls._variable_v2_call(*args, **kwargs)
    215     else:
--> 216       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    217 
    218 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy)
    420           name=name,
    421           dtype=dtype,
--> 422           constraint=constraint)
    423 
    424   def __repr__(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint)
    543           with ops.name_scope(""Initializer""), device_context_manager(None):
    544             initial_value = ops.convert_to_tensor(
--> 545                 initial_value() if init_from_fn else initial_value,
    546                 name=""initial_value"", dtype=dtype)
    547           self._handle = eager_safe_variable_handle(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in <lambda>()
    132           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):
    133         initializer = initializer()
--> 134       init_val = lambda: initializer(shape, dtype=dtype)
    135       variable_dtype = dtype.base_dtype
    136   if use_resource is None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py in __call__(self, shape, dtype)
    432     else:
    433       limit = math.sqrt(3.0 * scale)
--> 434       return self._random_generator.random_uniform(shape, -limit, limit, dtype)
    435 
    436   def get_config(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py in random_uniform(self, shape, minval, maxval, dtype)
    795       op = random_ops.random_uniform
    796     return op(
--> 797         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)
    798 
    799   def truncated_normal(self, shape, mean, stddev, dtype):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)
    238   with ops.name_scope(name, ""random_uniform"", [shape, minval, maxval]) as name:
    239     shape = _ShapeTensor(shape)
--> 240     minval = ops.convert_to_tensor(minval, dtype=dtype, name=""min"")
    241     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=""max"")
    242     seed1, seed2 = random_seed.get_seed(seed)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1048   preferred_dtype = deprecation.deprecated_argument_lookup(
   1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1051 
   1052 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1106       name=name,
   1107       preferred_dtype=dtype_hint,
-> 1108       as_ref=False)
   1109 
   1110 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1184 
   1185     if ret is None:
-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1187 
   1188     if ret is NotImplemented:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    302                                          as_ref=False):
    303   _ = as_ref
--> 304   return constant(v, dtype=dtype, name=name)
    305 
    306 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    243   """"""
    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 245                         allow_broadcast=True)
    246 
    247 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    251   ctx = context.context()
    252   if ctx.executing_eagerly():
--> 253     t = convert_to_eager_tensor(value, ctx, dtype)
    254     if shape is None:
    255       return t

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    108       return ops.EagerTensor(
    109           value, handle, device, dtype, tensor)
--> 110     t = ops.EagerTensor(value, handle, device, dtype)
    111     scalar_cache[cache_key] = t
    112     return t

RuntimeError: Error copying tensor to device: /job:worker/replica:0/task:0/device:TPU:0. /job:worker/replica:0/task:0/device:TPU:0 unknown device.
```

"
27990,Can't install TensorFlow Windows 7 64bit Python 3.7.2," here is a mistake after typing ""pip install tensorlow"".
I tried type it in cmd and PyCharm console too. Nothing.

C:\Users\Alex\PycharmProjects\ML>pip install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
"
27989,AttributeError: module 'tensorflow_estimator.python.estimator.api._v1.estimator' has no attribute '__file__',"Tensorflow builds through successfully but cannot import when I try (building off of Master branch):
```python
Successfully installed tensorflow-1.13.1
[91mYou are using pip version 10.0.1, however version 19.0.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
[0m[91mTraceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/__init__.py"", line 34, in <module>
    from tensorflow._api.v1 import compat
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/_api/v1/compat/__init__.py"", line 21, in <module>
    from tensorflow._api.v1.compat import v1
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/_api/v1/compat/v1/__init__.py"", line 643, in <module>
    'tensorflow_estimator.python.estimator.api._v1.estimator'))
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/tools/component_api_helper.py"", line 85, in package_hook
    set_child_as_subpackage()
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/tools/component_api_helper.py"", line 69, in set_child_as_subpackage
    os.path.join(os.path.dirname(child_pkg.__file__), ""..""))]
AttributeError: module 'tensorflow_estimator.python.estimator.api._v1.estimator' has no attribute '__file__'
```


**System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian: Buster
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): Source
TensorFlow version: Master branch
Python version: 3.6.5
Installed using virtualenv? pip? conda?: Source build
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): 8.3.0
CUDA/cuDNN version: NA
GPU model and memory: NA

** Call to build **
```bash
    cd /opt && \
    git clone --recursive https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
 TF_MKL_ROOT=/usr/lib  \
    TF_MKL_DOWNLOAD=0 \
    USE_DEFAULT_PYTHON_LIB_PATH=1 \
    TF_NEED_MKL=1 \
    TF_NEED_JEMALLOC=1 \
    TF_NEED_GCP=0 \
    TF_NEED_HDFS=0 \
    TF_ENABLE_XLA=1 \
    TF_NEED_MPI=0 \
    TF_NEED_GDR=0 \
    TF_NEED_S3=1 \
    TF_NEED_KAFKA=0 \
    TF_SET_ANDROID_WORKSPACE=0 \
    TF_NEED_CUDA=0 \
    TF_MKL_ENABLED=""true"" \
    CI_BUILD_PYTHON=/opt/conda/bin/python \
    PYTHON_BIN_PATH=/opt/conda/bin/python \
    PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    CC_OPT_FLAGS="" -mavx -msse2 -msse3 -msse4.2 -msse4.1 -mfpmath=sse -lmkl_gf_lp64 -Wl,--start-group -lmkldnn -lmklml_intel -lmkl_gnu_thread -lmkl_core -Wl,--end-group -dl -lpthread -lm "" \
    /bin/bash ./configure && \
    bazel build \
    --config=mkl --config=opt \
    --config=noaws --config=noignite --config=nokafka --config=nonccl \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-ldl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl && \
    cd /opt && rm -rf /opt/tensorflow /tmp/* && \
    python -c ""import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))"" && \
    echo ""\n\n TENSORFLOW DONEZO \n\n""
```"
27987,Linking error with libtflite_gpu_gl.so caused by TfLiteGpuDelegateDelete absence among exported symbols,"**System information:**
- OS Platform and Distribution: Linux Ubuntu 14.04
- TensorFlow installed from source
- TensorFlow version: 2.0 alpha
- Bazel version: 0.24.1
- GCC/Compiler version: clang 8.0.2

**Issue:**

TfLiteGpuDelegateDelete doesn't declared as exported symbol in tensorflow/lite/delegates/gpu/gl_delegate.h. This caused linking error with libtflite_gpu_gl.so.

Build command:
bazel build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt **-fvisibility=hidden** --linkopt -s --strip always --cxxopt='--std=c++11' //tensorflow/lite/delegates/gpu:libtflite_gpu_gl.so

**Reproduce:**

////////
// Set up interpreter.
auto model = FlatBufferModel::BuildFromFile(model_path);
ops::builtin::BuiltinOpResolver op_resolver;
std::unique_ptr<Interpreter> interpreter;
InterpreterBuilder(*model, op_resolver)(&interpreter);

////////
// NEW: Prepare GPU delegate.
auto* delegate = TfLiteGpuDelegateCreate(/*options=*/nullptr);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return;

////////
// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return;
ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));

////////
// Clean up.
**TfLiteGpuDelegateDelete(delegate);**

I followed example described at tensorflow/lite/delegates/gpu/README.md

"
27985,BUG - AttributeError: 'IndexedSlices' object has no attribute '_copy,"TensorFlow version:

```
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.12.0-0-ga6d8ffae09 1.12.0
```

Issue:

```2019-04-19 15:30:36.803841: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-04-19 15:30:36.879298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-04-19 15:30:36.879649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 11.91GiB freeMemory: 4.75GiB
2019-04-19 15:30:36.879665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-04-19 15:30:37.551722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-19 15:30:37.551754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-04-19 15:30:37.551764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-04-19 15:30:37.551930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4496 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:xxx.store:Generating index with key 1-TTT ..
INFO:xxx-cli.py:Loss @ batch 1 on 800: 1.6608102321624756
Traceback (most recent call last):
  File ""./bin/xxx-cli.py"", line 253, in <module>
    main(sys.argv[1:])
  File ""./bin/xxx-cli.py"", line 193, in main
    gradients = tape.gradient(loss, trainable_variables)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 901, in gradient
    output_gradients=output_gradients)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 64, in imperative_grad
    output_gradients)
  File ""/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 858, in grad_fun
    return [dresult._copy(device_name=self_device)]
AttributeError: 'IndexedSlices' object has no attribute '_copy'
```

Environment information:

```
== cat /etc/issue ===============================================
Linux hamburg 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04.2 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux hamburg 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy                      0.4.3.2    
numpy                              1.16.1     
numpydoc                           0.8.0      
protobuf                           3.6.1      
tensorflow-estimator               1.13.0     

     13084:       trying file=tls/x86_64/libpthread.so.0
     13084:       trying file=tls/libpthread.so.0
     13084:       trying file=haswell/x86_64/libpthread.so.0
     13084:       trying file=haswell/libpthread.so.0
     13084:       trying file=x86_64/libpthread.so.0
     13084:       trying file=libpthread.so.0
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0
     13084:
     13084:     find library=libc.so.6 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libc.so.6
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libc.so.6
     13084:       trying file=tls/haswell/x86_64/libc.so.6
     13084:       trying file=tls/haswell/libc.so.6
     13084:       trying file=tls/x86_64/libc.so.6
     13084:       trying file=tls/libc.so.6
     13084:       trying file=haswell/x86_64/libc.so.6
     13084:       trying file=haswell/libc.so.6
     13084:       trying file=x86_64/libc.so.6
     13084:       trying file=libc.so.6
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libc.so.6
     13084:
     13084:     find library=libdl.so.2 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     
     13084:       trying file=/usr/local/cuda-9.0/lib64/libm.so.6
     13084:       trying file=tls/haswell/x86_64/libm.so.6
     13084:       trying file=tls/haswell/libm.so.6
     13084:       trying file=tls/x86_64/libm.so.6
     13084:       trying file=tls/libm.so.6
     13084:       trying file=haswell/x86_64/libm.so.6
     13084:       trying file=haswell/libm.so.6
     13084:       trying file=x86_64/libm.so.6
     13084:       trying file=libm.so.6
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/lib/x86_64-linux-gnu/libm.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libc.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libm.so.6
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/librt.so.1
     13084:
     13084:
     13084:     calling init: /lib/x86_64-linux-gnu/libutil.so.1
     13084:

     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/haswell/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../tls/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../haswell/x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../haswell/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../x86_64/libffi.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libffi.so.6
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libffi.so.6
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_struct.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libopenblasp-r0-382c8f3a.3.5.dev.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/haswell/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/tls/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/core/../.libs/haswell/x86_64/libopenblasp-r0-382c8f3a.3.5.dev.so
     13084:       trying file=/home/an
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libz.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/zlib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=liblzma.so.5 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../..           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../liblzma.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../liblzma.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libtensorflow_framework.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/x86_64:/home/anaconda/anaconda3/
l_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:
../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/pyth
4/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/p
ll/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../tls/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../haswell/x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../haswell/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
     13084:
     13084:     find library=libcublas.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (R
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcublas.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcublas.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcublas.so.9.0
     13084:
     13084:     find library=libcusolver.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcusolver.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcusolver.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcusolver.so.9.0
   
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcudart.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcudart.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcudart.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcudart.so.9.0
     13084:
     13084:     find library=libgomp.so.1 [0]; sear
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libgomp.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libgomp.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libgomp.so.1
     13084:
     13084:     find library=libstdc++.so.6 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libstdc++.so.6
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/lib64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libstdc++.so.6
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libstdc++.so.6
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libstdc++.so.6
     13084:
     13084:     find library=libgcc_s.so.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libgcc_s.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libgcc_s.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libgcc_s.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libgcc_s.so.1
     13084:
     13084:     find library=libcuda.so.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_p
/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccuda_Udriver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfi
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccufft___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/tls/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/haswell/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/x86_64/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccurand___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cu
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcuda.so.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/lib64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcuda.so.1
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcuda.so.1
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcuda.so.1
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libcuda.so.1
     13084:       trying file=tls/haswell/x86_64/libcuda.so.1
     13084:       trying file=tls/haswell/libcuda.so.1
     13084:       trying file=tls/x86_64/libcuda.so.1
     13084:       trying file=tls/libcuda.so.1
     13084:       trying file=haswell/x86_64/libcuda.so.1
     13084:       trying file=h
/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extr
as/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/lib64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcudnn.so.7
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcudnn.so.7
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcudnn.so.7
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcudnn
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcudnn.so.7
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libcudnn.so.7
     13084:       trying file=tls/haswell/x86_64/libcudnn.so.7
     13084:       trying file=tls/haswell/libcudnn.so.7
     13084:       trying file=tls/x86_64/libcudnn.so.7
     13084:       trying file=tls/libcudnn.so.7
     13084:       trying file=haswell/x86_64/libcudnn.so.7
     13084:       trying file=haswell/libcudnn.so.7
     13084:       trying file=x86_64/libcudnn.so.7
     13084:       trying file=libcudnn.so.7
     13084:      search cache=/etc/ld.so.cache
     13084:       trying file=/usr/lib/x86_64-linux-gnu/libcudnn.so.7
     13084:
     13084:     find library=libcufft.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cu
ell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/h
aswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcufft.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcufft.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcufft.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcufft.so.9.0
     13084:
     13084:     find library=libcurand.so.9.0 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcurand.so.9.0
     13084:       tryi
nfig_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libcurand.so.9.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/lib64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libcurand.so.9.0
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libcurand.so.9.0
     13084:      search path=/home/anaconda/anaconda3/bin/../lib                (RPATH from file /home/anaconda/anaconda3/bin/python)
     13084:       trying file=/home/anaconda/anaconda3/bin/../lib/libcurand.so.9.0
     13084:
     13084:     find library=libnvidia-fatbinaryloader.so.390.116 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libnvidia-fatbinaryloader.so.390.116
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libnvidia-fatbinaryloader.so.390.116
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libnvidia-fat
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcufft.so.9.0
     13084:
     13084:
     13084:     calling init: /usr/lib/x86_64-linux-gnu/libcudnn.so.7
     13084:
     13084:
     13084:     calling init: /usr/lib/x86_64-linux-gnu/libcuda.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libgomp.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcudart.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcusolver.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/bin/../lib/libcublas.so.9.0
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
     13084:
     13084:     find library=libhdfs.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:.
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/extras/CUPTI/lib64/libhdfs.so
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python:/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/..:../local_config_cuda/cuda/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/lib64/tls/haswell:../local_config_cuda/cuda/lib64/tls/x86_64:../local_config_cuda/cuda/lib64/tls:../local_config_cuda/cuda/lib64/haswell/x86_64:../local_config_cuda/cuda/lib64/haswell:../local_config_cuda/cuda/lib64/x86_64:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/tls/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/tls:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64/haswell:../local_config_cuda/cuda/extras/CUPTI/lib64/x86_64:../local_config_cuda/cuda/extras/CUPTI/lib64           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/libhdfs.so
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/haswell/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/x86_64/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/tls/libhdfs.so
     13084:       trying file=../local_config_cuda/cuda/lib64/haswell/x86_64/libhdfs.so
     13084:       trying file=
     13084:      search path=/lib/x86_64-linux-gnu/tls/haswell/x86_64:/lib/x86_64-linux-gnu/tls/haswell:/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/haswell/x86_64:/lib/x86_64-linux-gnu/haswell:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64:/usr/lib/x86_64-linux-gnu/tls/haswell:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/haswell/x86_64:/usr/lib/x86_64-linux-gnu/haswell:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/haswell/x86_64:/lib/tls/haswell:/lib/tls/x86_64:/lib/tls:/lib/haswell/x86_64:/lib/haswell:/lib/x86_64:/lib:/usr/lib/tls/haswell/x86_64:/usr/lib/tls/haswell:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/haswell/x86_64:/usr/lib/haswell:/usr/lib/x86_64:/usr/lib              (system search path)
     13084:       trying file=/lib/x86_64-linux-gnu/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/haswell/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/haswell/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so
     13084:       trying file=/lib/x86_64-linux-gnu/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/haswell/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/haswell/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so
     13084:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
     13084:       trying file=/lib/tls/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/tls/haswell/libhdfs.so
     13084:       trying file=/lib/tls/x86_64/libhdfs.so
     13084:       trying file=/lib/tls/libhdfs.so
     13084:       trying file=/lib/haswell/x86_64/libhdfs.so
     13084:       trying file=/lib/haswell/libhdfs.so
     13084
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/array.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libssl.so.1.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../..           (RPATH from file /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libssl.so.1.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libssl.so.1.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libhdf5-5773eb11.so.103.0.0 [0]; searching
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/haswell/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/haswell/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=tls/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=haswell/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=haswell/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=libhdf5-5773eb11.so.103.0.0
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/tls/x86_64:/home/anaconda/anaconda3/l
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/x86_64/libhdf5-5773eb11.so.103.0.0
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0
     13084:
     13084:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
     13084:      search path=/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:             (LD_LIBRARY_PATH)
     13084:       trying file=/usr/local/cuda-9.0/lib64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/haswell/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/haswell/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=tls/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=haswell/x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=haswell/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=x86_64/libhdf5_hl-db841637.so.100.1.1
     13084:       trying file=libhdf5_hl-db841637.so.100.1.1
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs            (RUNPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
     13084:
     13084:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/.         (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/x86_64/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/haswell/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./hasw
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so
     13084:
     13092:     find library=libc.so.6 [0]; searching
     13092:      search path=/usr/local/cuda-9.0/lib64/tls/haswell/x86_64:/usr/local/cuda-9.0/lib64/tls/haswell:/usr/local/cuda-9.0/lib64/tls/x86_64:/usr/local/cuda-9.0/lib64/tls:/usr/local/cuda-9.0/lib64/haswell/x86_64:/usr/local/cuda-9.0/lib64/haswell:/usr/local/cuda-9.0/lib64/x86_64:/usr/local/cuda-9.0/lib64:tls/haswell/x86_64:tls/haswell:tls/x86_64:tls:haswell/x86_64:haswell:x86_64:         (LD_LIBRARY_PATH)
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/haswell/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/haswell/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/tls/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/haswell/x86_64/libc.so.6
     13092:       trying file=/usr/local/cuda-9.0/lib64/haswell/libc.so.6
     13092:       trying file=/usr/local/cud
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/libuuid.so.1
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/unicodedata.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryptography/hazmat/bindings/_constant_time.abi3.so
     13084:
     13084:     find library=libffi-bce22613.so.6.0.4 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend          (RPATH from file /home/anaconda/anaconda3/l
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libyaml-0.so.2 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/../..            (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/_yaml.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/x86_64/libyaml-0.so.2
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/haswell/libyaml-0.so.2
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/../../tls/x86_64/libyaml-0.so.2
     13084:       trying file=/
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/haswell/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/haswell/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../tls/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../haswell/x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../haswell/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../x86_64/libjpeg.so.9
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libjpeg.so.9
     13084:
     13084:     find library=libtiff.so.5 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../..          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/_imaging.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libtiff.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libjpeg.so.9
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/../../../libtiff.so.5
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/PIL/_imaging.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libmkl_rt.so [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/../../../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packag
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     find library=libgfortran.so.4 [0]; searching
     13084:      search path=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../x86_64:/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../..          (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/haswell/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../tls/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../haswell/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../x86_64/libgfortran.so.4
     13084:       trying file=/home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../libgfortran.so.4
     13084:
     13084:     find library=libquadmath.so.0 [0]; searching
     13084:      search path=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/x86_64:/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib                (RPATH from file /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/../../../../libgfortran.so.4)
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/x86_64/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/haswell/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/x86_64/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/tls/libquadmath.so.0
     13084:       trying file=/home/rdonnelly/mc/conda-bld/compilers_linux-64_1534627447954/work/gcc_built/x86_64-conda_cos6-linux-gnu/lib/../lib/haswell/x86_64/libquadmath.so.0
     13084:       try
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_fitpack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/dfitpack.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_bspl.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/_ppoly.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/interpolate/interpnd.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/ckdtree.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/qhull.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/_lib/messagestream.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/reduce.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce_axis.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/move.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/writers.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling init: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so
     13084:
     13084:
     13084:     calling fini: /home/anaconda
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/grp.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensor
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/fast_tensor_util.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/array.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/libuuid.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/unicodedata.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryptography/hazmat/bindings/_constant_time.abi3.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/_cffi_backend.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/.libs_cffi_backend/libffi-bce22613.so.6.0.4 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/cryp
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/lib-dynload/../../libz.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-
     13084:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/_lib/messagestream.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_voronoi.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_distance_wrap.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/spatial/_hausdorff.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/conversion.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/np_datetime.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/nattype.cpyth
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/nonreduce_axis.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/bottleneck/move.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/tslibs/offsets.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     1308
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /home/anaconda/anaconda3/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so [0]
     13084:
     13084:
     13084:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
     13084:

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64::/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Apr 19 15:35:28 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.116                Driver Version: 390.116                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |
| 47%   77C    P2   153W / 250W |   7177MiB / 12194MiB |     92%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      2059      G   /usr/lib/xorg/Xorg                            16MiB |
|    0     28211      C   python3                                     2299MiB |
|    0     28214      C   python3                                     1275MiB |
|    0     28216      C   python3                                     1275MiB |
|    0     28217      C   python3                                     2299MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7```"
27984,Failed to load the native TensorFlow runtime. (ImportError: DLL load failed),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version: 1.13.1
- Python version: 3.6.8 x64
- Installed using virtualenv? pip? conda?: pip install tensorflow-gpu
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 9.0 / cudnn-9.0-windows10-x64-v7.5.0.56.zip 
- GPU model and memory: GeForce 610M



**Describe the problem**
We want run OpenNMT-tf.
Install tensorflow.
Run simple test. Get error.
Help wanted.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python -c ""import tensorflow as tf; print(tf.__version__)""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

python -c ""import tensorflow as tf; print(tf.__version__)""
Traceback (most recent call last):
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\venv113\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\venv113\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Не найден указанный модуль.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\venv113\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\venv113\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\venv113\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\venv113\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\venv113\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Не найден указанный модуль.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
"
