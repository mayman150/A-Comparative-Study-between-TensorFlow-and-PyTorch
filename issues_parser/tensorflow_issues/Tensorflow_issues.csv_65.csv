Issue Number,Issue Title,Issue Body
8860,Server terminated abruptly when build from source,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Initially I encountered the problem described as in https://github.com/tensorflow/tensorflow/issues/8731

After modified configure file as in https://github.com/tensorflow/tensorflow/commit/fba05c300bf6840e76787680ed7fd1239cdb9ad0. This error happened

### Environment info
Operating System: CentOS Linux release 7.3.1611 (Core)

Installed version of CUDA and cuDNN: CUDA 7.5 and cuDNN 5.0
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): 
$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   322936 Dec  6 11:05 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Dec  6 11:05 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 asb  ttic       13 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 asb  ttic       17 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
-rwxr-xr-x 1 asb  ttic 59909104 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 asb  ttic 58775484 Apr 22  2016 /usr/local/cuda/lib64/libcudnn_static.a

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
$ git rev-parse HEAD
83be34c47c7ed81c62bd61a908fde017e301b578

2. The output of `bazel version`
$ bazel version
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

./configure
Please specify the location of python. [Default is /home-nfs/mingdachen/anaconda2/envs/py35gpu/bin/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to use jemalloc as the malloc implementation? [Y/n]
jemalloc enabled
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N]
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N]
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages]

Using python library path: /home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N]
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5
Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]:
INFO: $TEST_TMPDIR defined: output root default is '/scratch/mingda/'.
Extracting Bazel installation...
.........
INFO: All external dependencies fetched successfully.
Configuration finished

$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package &> ../log

### What other attempted solutions have you tried?
I have tried different branches, including r1.0, r1.1 and master

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
[log.txt](https://github.com/tensorflow/tensorflow/files/884405/log.txt)

The log file of /scratch/mingda/_bazel_mingdachen/b48312abf86569e91b44fe1197c1c461/server/jvm.out is empty"
8859,code auto completion error,"i'm using pycharm. And can't get code completion when using tf.contrib.
but when I write tensorflow.contrib import learn. it has a right behavior.

I guess it happened because of the lasy loading of tf.contrib.

can anyone help me?"
8858,Machine restarts when running TensorFlow with GPU,"A simple Python program which runs a few TensorFlow computations consequently crashes when running on GPU.

Code:

```
from __future__ import print_function
import numpy as np
import tensorflow as tf
from tensorflow.python.client import timeline

def train_model(run_number):
    image_size = 28
    num_labels = 10
    batch_size = 16
    layer1_neuron_count = 16384

    graph = tf.Graph()
    
    with graph.as_default():
        tf_valid_dataset = tf.constant(valid_dataset)

        # Variables.
        weights0 = tf.Variable(
            tf.truncated_normal([image_size * image_size, layer1_neuron_count]))
        biases0 = tf.Variable(tf.zeros([layer1_neuron_count]))

        weights1 = tf.Variable(
            tf.truncated_normal([layer1_neuron_count, num_labels]))
        biases1 = tf.Variable(tf.zeros([num_labels]))

        valid_layer0 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
        valid_prediction = tf.matmul(valid_layer0, weights1) + biases1
    
    with tf.Session(graph=graph) as session:
        tf.global_variables_initializer().run()

        print('Validation')
        
        session.run(valid_prediction)
            
        print('Validation done')

valid_dataset = np.random.uniform(-1, 1, (10000, 784)).astype(dtype=np.float32)
valid_labels = np.random.uniform(0, 1, (10000, 10)).astype(dtype=np.float32)

for i in range(10):
    print(""Run #{}"".format(i))
    train_model(i)
```

It should run the same computation 10 times, recreating a graph and a session every time. 
Works fine when I run it on CPU. When running on GPU, it fails on running computation for 2nd, 3rd or 4th session.

Console output:

```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Run #0
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 5.83GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
Validation
Validation done
Run #1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
Validation
Validation done
Run #2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
Validation
Validation done
Run #3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
Validation
```

Then the machine just restarts.
There are no relevant messages in syslog before the restart.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/39122984/system-auto-reboot-when-tensorflow-model-is-too-large
http://stackoverflow.com/questions/41237115/computer-restarts-with-large-mini-batches-in-tensorflow

When running other TensorFlow programs, I noticed that sometimes such crashes happen when I use large tensors. Issues above seem to be related, at least symptoms are similar.


### Environment info
GPU: GeForce GTX 980 Ti
Operating System: Ubuntu 16.04.2 LTS

Installed version of CUDA and cuDNN: CUDA 8.0.61, cuDNN 7.5
Output of `ls -l /usr/local/cuda/lib64/libcud*`:

`-rw-r--r-- 1 root root   556000 Mar 30 18:05 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root root   415432 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Mar 30 18:05 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       18 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 root root 70364814 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn_static.a`

TensorFlow:
1. ""pip install tensorflow-gpu"". Version 1.0.1
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:

`I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.1`



### What other attempted solutions have you tried?

Tried to reinstall Ubuntu/CUDA/cuDNN/TensorFlow, didn't help.

"
8857,Utility for repeatedly running tensors on queued input and accumulating the results,"I've written a utility function in TensorFlow that I've found quite helpful for loading Tensors with queue based inputs into memory, e.g., for looking at input data stored in the form of TF-records files or for looking at inference results. I've found it especially useful for interactively exploring the results of saved models from IPython notebooks, i.e., doing inference on small datasets or small portions of big datasets.

Here's what the API looks like:
```python
from typing import Mapping, Optional, Dict

def run_repeatedly(
    batched_tensors: Mapping[object, tf.Tensor],
    checkpoint_dir: Optional[str] = None,
    max_num_batches: Optional[int] = None) -> Dict[object, np.ndarray]:
  """"""Repeatedly run tensors until they are exhausted.

  Args:
    batched_tensors: dict of tensors to evaluate, each of which should have a
      first axis corresponding to a batch of examples.
    checkpoint_dir: optional path to checkpoint to load.
    max_num_batches: optional maximum number of batches to run.

  Returns:
    A dict of numpy.ndarray objects containing the result of evaluating
    batched_tensors and concatenated across batches along the first axis.

  Raises:
    ValueError: if checkpoint_dir has no valid checkpoint
  """"""
  # the implementation makes use of tf.contrib.metrics.streaming_concat
  # and a tf.train.Supervisor: it handles all the boilerplate around starting
  # up a session.
```

And a few usage example:

- Previewing features loaded from files:
```
tensors = tf.contrib.learn.read_batch_features(....)
arrays = run_repeatedly(tensors, max_num_batches=10)
```
- For running inference on a full dataset while also accumulating input tensors (useful for debugging):
```
tensors = tf.contrib.learn.read_batch_features(....)
predictions = make_predictions(tensor_inputs, ...)
tensors.update(predictions)
arrays = run_repeatedly(tensors, checkpoint_dir=path_to_saved_model)
```

**Does something like this belong somewhere in core TensorFlow, or maybe one of the contrib libraries?**

This is somewhat similar to `Estimator.predict` from `tf.contrib.learn`, but with a few key differences:
- It's more flexible, not expecting inputs in the form of a `tf.learn` model.
- It automatically concatenates across batches. In practice, I find this highly useful, because I can often store the results of a model in memory even though I don't have enough memory to run inference on everything at once."
8854,tensorboard: view graph from saved_model.pb file [feature request],"Plot a graph from just a saved_model.pb file. 

Currently tensorboard only works given a training folder containing checkpoints and summary events. Understanding the output graph is important, especially if you don't have access to the training output files."
8853,Warning: TensorFlow library wasn't compiled to use SSE instructions,"Installed nightly build using 
>pip3 install tensorflow-1.1.0rc0-cp35-cp35m-win_amd64.whl 
on Windows 7 Professional x64.  Simple run gives warning that the libs were not compiled to use SSE, SSE2, SSE3, SSE4.1, SSE4.2 and AVX instructions even though available. 

c:\python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> print( tf.__version__)
1.1.0-rc0
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
2017-03-30 15:20:06.126686: W c:\tf_jenkins\home\workspace\nightly-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_
tions, but these are available on your machine and could speed up CPU computations.
<<lines ommitted>
>>> print(sess.run(hello))
b'Hello, TensorFlow!'"
8852,Allow SavedModelBuilder to overwrite existing directory,"Currently, SavedModelBuilder throws an exception when called with a directory that already exists. It will be helpful to add a flag `overwrite` when initializing SavedModelBuilder, which allows it to overwrite the contents of the directory when .save() is called. 

When training a model every couple of hours on fresh data, it will be easier to overwrite existing models than having to implement housekeeping code around cleaning old models or writing code to move around new model after training."
8850,Error in running text_classification_character_rnn.py,"I am running the exact example given in the repo for text classification using rnn. I am getting the following error. 
TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.
Example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_rnn.py

Would you please let me know how can I fix it. 

```
Detail error. 
TypeError                                 Traceback (most recent call last)
<ipython-input-5-90c7a682f760> in <module>()
     23 
     24 # Train and predict
---> 25 classifier.fit(x_train, y_train, steps=100)
     26 y_predicted = [
     27   p['class'] for p in classifier.predict(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
    278             _call_location(), decorator_utils.get_qualified_name(func),
    279             func.__module__, arg_name, date, instructions)
--> 280       return func(*args, **kwargs)
    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    282         func.__doc__, date, instructions)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    408     _verify_input_args(x, y, input_fn, None, batch_size)
    409     if x is not None:
--> 410       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
    411       return self
    412 

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, batch_size, steps, max_steps, monitors)
   1351                         steps=steps,
   1352                         max_steps=max_steps,
-> 1353                         monitors=all_monitors)
   1354     return self
   1355 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
    278             _call_location(), decorator_utils.get_qualified_name(func),
    279             func.__module__, arg_name, date, instructions)
--> 280       return func(*args, **kwargs)
    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(
    282         func.__doc__, date, instructions)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    424       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))
    425 
--> 426     loss = self._train_model(input_fn=input_fn, hooks=hooks)
    427     logging.info('Loss for final step: %s.', loss)
    428     return self

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, hooks)
    932       features, labels = input_fn()
    933       self._check_inputs(features, labels)
--> 934       model_fn_ops = self._call_legacy_get_train_ops(features, labels)
    935       ops.add_to_collection(ops.GraphKeys.LOSSES, model_fn_ops.loss)
    936       all_hooks.extend([

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_legacy_get_train_ops(self, features, labels)
   1001 
   1002   def _call_legacy_get_train_ops(self, features, labels):
-> 1003     train_ops = self._get_train_ops(features, labels)
   1004     if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature
   1005       return train_ops

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)
   1160       `ModelFnOps` object.
   1161     """"""
-> 1162     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
   1163 
   1164   def _get_eval_ops(self, features, labels, metrics):

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)
   1131     if 'model_dir' in model_fn_args:
   1132       kwargs['model_dir'] = self.model_dir
-> 1133     model_fn_results = self._model_fn(features, labels, **kwargs)
   1134 
   1135     if isinstance(model_fn_results, model_fn_lib.ModelFnOps):

<ipython-input-4-813de742180a> in char_rnn_model(features, target)
      6 
      7     cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)
----> 8     _, encoding = tf.contrib.rnn.static_rnn(cell, byte_list, dtype=tf.float32)
      9 
     10     logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.pyc in static_rnn(cell, inputs, initial_state, dtype, sequence_length, scope)
    195             state_size=cell.state_size)
    196       else:
--> 197         (output, state) = call_cell()
    198 
    199       outputs.append(output)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.pyc in <lambda>()
    182       if time > 0: varscope.reuse_variables()
    183       # pylint: disable=cell-var-from-loop
--> 184       call_cell = lambda: cell(input_, state)
    185       # pylint: enable=cell-var-from-loop
    186       if sequence_length is not None:

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in __call__(self, inputs, state, scope)
     90         r, u = array_ops.split(
     91             value=_linear(
---> 92                 [inputs, state], 2 * self._num_units, True, 1.0, scope=scope),
     93             num_or_size_splits=2,
     94             axis=1)

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)
    749       res = math_ops.matmul(args[0], weights)
    750     else:
--> 751       res = math_ops.matmul(array_ops.concat(args, 1), weights)
    752     if not bias:
    753       return res

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in concat(values, axis, name)
   1032   return gen_array_ops._concat_v2(values=values,
   1033                                   axis=axis,
-> 1034                                   name=name)
   1035 
   1036 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in _concat_v2(values, axis, name)
    517   """"""
    518   result = _op_def_lib.apply_op(""ConcatV2"", values=values, axis=axis,
--> 519                                 name=name)
    520   return result
    521 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    462                                 (prefix, dtype.name))
    463               else:
--> 464                 raise TypeError(""%s that don't all match."" % prefix)
    465             else:
    466               raise TypeError(""%s that are invalid."" % prefix)

TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.
```

 "
8848,"How to have a ""static"" like variable inside a function","The StackOverflow website does not work for logging in. Sorry, I have to post my question here.

I want a simple test. A simple function F(x), inside which mean and variance are calculated from the input x. Meanwhile, I also want to keep two ""static"" like variables (like in C) avg_mean, avg_variance. So that every time F is called avg_mean and avg_variance are updated based on their previous values. 

Also, I want to have two sets of ""avg_mean, avg_variance"" for different scopes. My test codes are as below, but the avg_mean and avg_variance are only the values calculated from the last call and does not include the influence from the first call. If I remove the two ""reuse_variables()"" lines, the program does not run.

Could anyone help what should I do? By the way, please help withOUT using tf.contrib libs please, because those are not fully supported in Windows now. Thank you.

import tensorflow as tf

def getsta(x):
  print('getsta start...')
  params_shape = [x.get_shape()[-1]]

  decay=0.9
  
  mean = tf.get_variable(
          'mean', [1], tf.float32,
          initializer=tf.constant_initializer(0.0, tf.float32))
  variance = tf.get_variable(
          'howvariance', [1], tf.float32,
          initializer=tf.constant_initializer(1.0, tf.float32))

  avg_mean = tf.get_variable(
          'avg_mean', [1], tf.float32,
          initializer=tf.constant_initializer(0.0, tf.float32))
  avg_variance = tf.get_variable(
          'avg_variance', [1], tf.float32,
          initializer=tf.constant_initializer(0.0, tf.float32))

  mean, variance = tf.nn.moments(x, [0], name='moments')

  avg_mean -= (1.0 - decay) * (avg_mean - mean)
  avg_variance -= (1.0 - decay) * (avg_variance - variance)

  return x, mean, variance, avg_mean, avg_variance

def main(argv=None):
  x1 = tf.constant([1,2,3,4], tf.float32)
  x2 = tf.constant([5,6,7,8], tf.float32)
  x3 = tf.constant([1,3,5,7], tf.float32)
  x4 = tf.constant([4,8,12,16], tf.float32)

  with tf.variable_scope(""AAA"") as scopeA:
    y1, mean1, variance1, avg_mean1, avg_variance1 = getsta(x1)
    scopeA.reuse_variables()
    y1, mean1, variance1, avg_mean1, avg_variance1 = getsta(x2)
  with tf.variable_scope(""BBB"") as scopeB:
    y2, mean2, variance2, avg_mean2, avg_variance2 = getsta(x3)
    scopeB.reuse_variables()
    y2, mean2, variance2, avg_mean2, avg_variance2 = getsta(x4)

  sess = tf.InteractiveSession()
  sess.run(tf.global_variables_initializer())
  print(sess.run([y1, mean1, variance1, avg_mean1, avg_variance1]))
  print(sess.run([y2, mean2, variance2, avg_mean2, avg_variance2]))

if __name__ == '__main__':
  tf.app.run(main=main)
"
8847,cannot run test_session on warpctc,"python 3.5 ;  gtx1080*2

```
InvalidArgumentError (see above for traceback): Cannot assign a device to node 'CTCLoss': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: CTCLoss = CTCLoss[_kernel=""WarpCTC"", ctc_merge_repeated=true, preprocess_collapse_repeated=false, _device=""/device:GPU:0""](Const_3, Const, Const_1, CTCLoss/sequence_length)]]
```
### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN:  8.0.44 5

1. A link to the pip package you installed: 9.0.1
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.1 

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
https://github.com/baidu-research/warp-ctc/blob/master/tensorflow_binding/tests/test_ctc_loss_op.py


### What other attempted solutions have you tried?

after some search, I flip the flag to  `allow_soft_placement=True`, however I get
```
anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py"", line 248, in prepare_config
    config.allow_soft_placement = False
AttributeError: 'NoneType' object has no attribute 'allow_soft_placement'
```

And if I replace `self.test_session` to `tf.Session` and remove the `use_gpu` and `force_gpu` option
the error will disappear, don't know what happend"
8846,Crashing when trying distributed implementation,"Hi, I start saying that I'm a new Tensorflow user :-)

I was trying to test the potential speed-up due to a distributed implementation vs. a sequential one, so I came up with the following:

1. A script sequentially computing matmul of two matrices with themselves and then adding them up
2. A script that distributes the two computations to different devices and then sum them up in a third device

The problem is the following: if I run the code below with matrices' dimensions of 100x100, it works. If I run the same with dimensions 1000x1000, the following error occurs:

> tensorflow.python.framework.errors_impl.InternalError: {""created"":""@1490890419.097000000"",""description"":""RST_STREAM"",""file"":""C:\tf_jenkins\home\workspace\release-win\DEVICE\cpu\OS\windows\cmake_build\grpc\src\grpc\src\core\ext\transport\chttp2\transport\frame_rst_stream.c"",""file_line"":107,""http2_error"":1}

I can't figure it out, so any help would be appreciated...

Here is the distributed code (after 3 local Servers have been created):

```
import tensorflow as tf
import time

cluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223"", ""localhost:2224""]})

with tf.device(""/job:local/task:1""):
    print('task 1')
    x1 = tf.Variable(tf.zeros([1000,1000])+10, dtype=tf.float32, name=None)
    _matmul_1 = tf.matmul(x1,x1)

with tf.device(""/job:local/task:0""):
    print('task 0')
    x0 = tf.Variable(tf.zeros([1000,1000])+10, dtype=tf.float32, name=None)
    _matmul_0 = tf.matmul(x0, x0)

with tf.device(""/job:local/task:2""):
    print('task 2')
    _matmul_ = tf.add(_matmul_0,_matmul_1)

time4 = time.clock()
sess = tf.Session(""grpc://localhost:2222"")
print('session')
init = tf.global_variables_initializer()
sess.run(init)
print(sess.run(_matmul_))
time5 = time.clock()
print(time5-time4)
```"
8845,Tensorboard not showing data on Windows,"I have a simple example of a MNIST classifier that works well for the dataset. However, Tensorboard is unable to read the data from the saved logs. I have inspected the data using `tensorboard --inspect --logdir...` and the files seem to be in order.

I have tested this and a much simpler model on a different machine, and got the same results. Tensorboard runs and the interface is accessible, but no data is shown (I tested with Google Chrome and Edge).

The code is as follows:
```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

import tensorflow as tf
sess = tf.InteractiveSession()

with tf.name_scope('input'):
    x = tf.placeholder(tf.float32, shape=[None, 784])
    y_ = tf.placeholder(tf.float32, shape=[None, 10])

def variable_summaries(var):
  with tf.name_scope('summaries'):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    tf.summary.scalar('max', tf.reduce_max(var))
    tf.summary.scalar('min', tf.reduce_min(var))
    tf.summary.histogram('histogram', var)

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1, name='weights')
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape, name='bias')
  return tf.Variable(initial)

def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name='conv')

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')

with tf.name_scope('conv1'):
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    variable_summaries(W_conv1)
    variable_summaries(b_conv1)

    x_image = tf.reshape(x, [-1, 28, 28, 1], name='reshape')
    tf.summary.image('image', x_image, 3)

    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name='relu')
    h_pool1 = max_pool_2x2(h_conv1)
    variable_summaries(h_conv1)
    variable_summaries(h_pool1)

with tf.name_scope('conv2x'):
    W_conv2x = weight_variable([5, 5, 32, 64])
    b_conv2x = bias_variable([64])
    variable_summaries(W_conv2x)
    variable_summaries(b_conv2x)

    h_conv2x = tf.nn.relu(conv2d(h_pool1, W_conv2x) + b_conv2x, name='relu')
    h_pool2x = max_pool_2x2(h_conv2x)
    variable_summaries(h_conv2x)
    variable_summaries(h_pool2x)

with tf.name_scope('conv2y'):
    W_conv2y = weight_variable([3, 3, 32, 64])
    b_conv2y = bias_variable([64])
    variable_summaries(W_conv2y)
    variable_summaries(b_conv2y)

    h_conv2y = tf.nn.relu(conv2d(h_pool1, W_conv2y) + b_conv2y, name='relu')
    h_pool2y = max_pool_2x2(h_conv2x)
    variable_summaries(h_conv2y)
    variable_summaries(h_pool2y)

with tf.name_scope('concat'):
    h_pool2 = tf.concat([h_pool2x, h_pool2y], 3)
    variable_summaries(h_pool2)

with tf.name_scope('fc1'):
    W_fc1 = weight_variable([7 * 7 * (64*2), 1024])
    b_fc1 = bias_variable([1024])
    variable_summaries(W_fc1)
    variable_summaries(b_fc1)

    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * (64*2)])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='relu')
    variable_summaries(h_pool2_flat)
    variable_summaries(h_fc1)

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name='dropout')
variable_summaries(h_fc1_drop)

with tf.name_scope('fc2'):
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    variable_summaries(W_fc2)
    variable_summaries(b_fc2)

    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
    variable_summaries(y_conv)

learning_rate = tf.placeholder(tf.float32, shape=[])
tf.summary.scalar('learning_rate', learning_rate)

with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)
    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)
    variable_summaries(cross_entropy)

correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
tf.summary.scalar('accuracy', accuracy)

merged = tf.summary.merge_all()
train_writer = tf.summary.FileWriter('summaries/train', sess.graph)
test_writer = tf.summary.FileWriter('summaries/test', sess.graph)

tf.global_variables_initializer().run()

for i in range(101):
  batch = mnist.train.next_batch(100)
  summary, _, acc = sess.run([merged, train_step, accuracy], feed_dict={x:batch[0], y_: batch[1], learning_rate:0.02, keep_prob: 0.5})
  train_writer.add_summary(summary, i)
  #train_writer.add_graph(sess.graph, i)
  print(""step %d, training accuracy %g"" % (i, acc))
  if i % 100 == 0:
    #This is just a test model, the accuracy is not important
    batch = mnist.test.next_batch(100)
    summary, acc = sess.run([merged, accuracy], feed_dict={x:batch[0], y_: batch[1], learning_rate:0, keep_prob: 1.0})
    test_writer.add_summary(summary, i)
    print(""step %d, test accuracy %g"" % (i, acc))

train_writer.close()
test_writer.close()
```

The logs are provided below.
[summaries.zip](https://github.com/tensorflow/tensorflow/files/883033/summaries.zip)

System information:
Windows 10
CUDA 8.0, cuDNN 5.0
Tensorflow 1.0.1
Python 3.5 64-bit

"
8843,Executor failed to create kernel for FFT even though using GPU,"### Environment info
Operating System:
Linux

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
-rw-r--r-- 1 root root   556000 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root   415432 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       18 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 root root 70364814 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn_static.a

```
If installed from binary pip package, provide:

1. A link to the pip package you installed: 
https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf

sess = tf.InteractiveSession()
b = tf.convert_to_tensor([[1.], [1.], [1.]], dtype=tf.float32)
input = tf.complex(b, tf.zeros_like(b))
x = tf.fft(input)
x.eval()
```


### Logs or other output that would be helpful

```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 750 Ti
major: 5 minor: 0 memoryClockRate (GHz) 1.202
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)
E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Not found: No registered 'FFT' OpKernel for CPU devices compatible with node FFT = FFT[_device=""/job:localhost/replica:0/task:0/gpu:0""](Complex)
        .  Registered:  device='GPU'

         [[Node: FFT = FFT[_device=""/job:localhost/replica:0/task:0/gpu:0""](Complex)]]

```
"
8842,AttributeError: 'Tensor' object has no attribute 'get' at DynamicRnnEstimator.fit(...). The same code works well with LinearRegressor.,"TensorFlow version: **1.1**

The code:

```
import random
import tensorflow as tf
from tensorflow.contrib.learn.python.learn.estimators import constants
from tensorflow.contrib.learn.python.learn.estimators.dynamic_rnn_estimator import PredictionType

xData = []
yData = []
for _ in range(10000):
    x = random.random()
    xData.append(x)
    y = 2 * x
    yData.append(y)


xc = tf.contrib.layers.real_valued_column("""")
estimator = tf.contrib.learn.DynamicRnnEstimator(problem_type = constants.ProblemType.LINEAR_REGRESSION,
                                                 prediction_type = PredictionType.SINGLE_VALUE,
                                                 sequence_feature_columns = [xc],
                                                 context_feature_columns = None,
                                                 num_units = 5,
                                                 cell_type = 'lstm', 
                                                 optimizer = 'SGD',
                                                 learning_rate = '0.1')

def get_train_inputs():
  x = tf.constant(xData)
  y = tf.constant(yData)

  return x, y

estimator.fit(input_fn=get_train_inputs, steps=1000) 
```

> Got:  AttributeError: 'Tensor' object has no attribute 'get' here


The same code works for **LinearRegressor** instead of **DynamicRnnEstimator**.

> 
> WARNING:tensorflow:From E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dynamic_rnn_estimator.py:724: regression_target (from tensorflow.contrib.layers.python.layers.target_column) is deprecated and will be removed after 2016-11-12.
> Instructions for updating:
> This file will be removed after the deprecation date.Please switch to third_party/tensorflow/contrib/learn/python/learn/estimators/head.py
> WARNING:tensorflow:Using temporary folder as model directory: C:\Users\pavel\AppData\Local\Temp\tmpzy68t_iw
> Traceback (most recent call last):
>   File ""C:/Users/pavel/PycharmProjects/rnnEstimator/main.py"", line 31, in <module>
>     estimator.fit(input_fn=get_train_inputs, steps=1000)
>   File ""E:\Python35\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
>     return func(*args, **kwargs)
>   File ""E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 430, in fit
>     loss = self._train_model(input_fn=input_fn, hooks=hooks)
>   File ""E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 927, in _train_model
>     model_fn_ops = self._get_train_ops(features, labels)
>   File ""E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1132, in _get_train_ops
>     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
>   File ""E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 1103, in _call_model_fn
>     model_fn_results = self._model_fn(features, labels, **kwargs)
>   File ""E:\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dynamic_rnn_estimator.py"", line 516, in _dynamic_rnn_model_fn
>     sequence_length = features.get(sequence_length_key)
> AttributeError: 'Tensor' object has no attribute 'get'"
8841,Gradient of reduce_prod not available on GPU,"The following example fails to colocate the values:
```python
with tf.device('/gpu:2'):
    x = tf.placeholder(tf.float32, shape=[None, 100])
    weight_dense_1 = tf.Variable(tf.zeros([100, 10]))
    dense_1_out = tf.matmul(x, weight_dense_1)
    y = tf.reduce_prod(tf.cast(tf.shape(dense_1_out), tf.float32))
    grad = tf.gradients(y, [weight_dense_1], colocate_gradients_with_ops=True)
```
A bunch of warnings like this is displayed:
```
WARNING:tensorflow:Tried to colocate gradients_1/Prod_1_grad/Rank with an op Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.
```
The symptom is similar to #3397. Using CPU or specifying all input dimensions solves the problem. But the cause seems different.

Gradient of `Prod` operation is defined in [python/ops/math_grad.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/math_grad.py#L102-L143). There, the operation is forced to run on CPU (see 182fef1b55640906637e4bf0d205e508c24549e7), mentioning the `listdiff()` operation is CPU-only.
I tried remove the forcing line and run this. It yields a kind explanation:
```
InvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/range_1': Could not satisfy explicit device specification '/device:GPU:2' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
InvertPermutation: GPU CPU 
Transpose: GPU CPU 
ConcatV2: GPU CPU 
Pack: GPU CPU 
Cumprod: GPU CPU 
ListDiff: CPU 
Shape: GPU CPU 
    ...(many GPU CPU ops)
Reshape: GPU CPU 
Gather: CPU 
```

Two operations used here, namely [Gather](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/gather_op.cc) and [ListDiff](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/listdiff_op.cc), are defined on CPU-only. As some of the operations needed in calculating the gradient are CPU-only, by the colocation rule, they get grouped into CPU-only.

This also occurs when using `moments()` or [`sufficient_statistics()`](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_impl.py#L495-L541) (the former calls the latter). There, when some of the axes of the tensor are unknown (like batch size), the total number of values (which is needed for the mean and variance) is calculated by `reduce_prod()` on `shape()`.
When the value of mean or variance is differentiated in some way (which is the case in batch normalization), a colocation between tensors named like `gradients/moments/sufficient_statistics/count_grad/Rank` and `moments/sufficient_statistics/count` fails.

Though `listdiff()` is renamed later on Python interface to `setdiff1d()`, it's still named `ListDiff` internally.
`gather()` operation is defined on GPU too, but only on float types.

It seems there hasn't been any issue on this. Would it mean that `Prod()` op is not differentiated in most of the cases?
How this can be solved? I'm not sure if the `setdiff1d()` operation is needed.
For me, this occured when using `moments()`, where the reciprocal of number of values is multiplicated to the sum of values. I think this is unnecessary, as it can be done with `reduce_mean()`. Is it right?

### Environment info
Operating System: **Ubuntu 16.04**.
Installed version of CUDA and cuDNN: **CUDA 8.0.61** / **cuDNN 5.1.10**.
pip3-installed `tensorflow-gpu==1.0.1`; all links here pointed to `r1.0`, but the problematic parts are the same as `master`.
`python -c ""import tensorflow; print(tensorflow.__version__)""` yields: `1.0.1`."
8840,BasicDecoder error,"Hi,
I am trying to use BasicDecoder for a sequence-to-sequence translation model and I get error:
`
InvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [1] [ but saw shape: ] [64] [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/All/_2029, rnn/Assert/Assert/data_0, rnn/stack/_2031, rnn/Assert/Assert/data_2, rnn/Shape_1/_2033)]] [[Node: rnn/while/Identity_12/_2727 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:5"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_4293_rnn/while/Identity_12"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:5""](^_clooprnn/while/multi_rnn_cell/cell_5/lstm_cell/zeros/_208)]]`
I can't figure it out, sequence_length tensor should be batch-sized and it is and error messege suggests that it should be [1]. This would be a case when a list of Tensors would be used instead of one fat tensor [time, batch_size, 1]

My target_vocab_size is 500K
size of RNN = 64, for testing
dec_inp : <tf.Tensor 'embedded_inputs_1:0' shape=(100, ?, 64) dtype=float32>
decoder seq len is batch-sized (64) <tf.Tensor 'decoder_seq_len:0' shape=(?,) dtype=int32>

>>> tf.__version__
'1.1.0-rc0'
>>>


```

W_target_emb = tf.Variable(tf.random_uniform([target_vocab_size, size], -1.0, 1.0), name=""W_target_emb"")

half = tf.constant(0.5)
dec_inp = tf.cast(tf.stack(self.decoder_inputs), tf.float32)
dec_inp = tf.reshape(dec_inp,[encoder_max_size, -1, size], name = ""embedded_inputs"")
if not forward_only:
	#helper = seq2seq.TrainingHelper(inputs = target_embedded_chars, sequence_length = self.decoder_seq_len, time_major=True)
	helper = seq2seq.ScheduledEmbeddingTrainingHelper(inputs = dec_inp,
													  sequence_length = self.decoder_seq_len,
													 embedding = W_target_emb,
													 sampling_probability = half,
													 time_major=True)
	
else:
	helper = seq2seq.GreedyEmbeddingHelper(dec_inp, 
										   start_tokens=self.decoder_inputs[0],
										   end_token=data_utils.EOS_ID)
	
decoder_cell = LSTMBlockCell(num_units=size)
decoder_cell = MultiRNNCell([DeviceWrapper(ResidualWrapper(decoder_cell),device=""/gpu:%d"" % i) for i in range(8) ])

my_decoder = seq2seq.BasicDecoder(
		cell=decoder_cell,
		helper=helper,
		initial_state=encoder_final_state)


decoder_outputs, decoder_state = seq2seq.dynamic_decode(my_decoder, output_time_major=False, parallel_iterations=32,
			   swap_memory = True)
```"
8838,Bazel test failure in TF  (Linking error ) on RHEL 7.3,"### Environment info
Operating System: Rhel:7.3 (ppc64le)

####################################################
I could able to build the TF (version 1.0.1) successfully  , however I am getting following error for the command ,` bazel test  -c opt //tensorflow/...`

```
ERROR: /root/Sandip/Bazel/new/with_patches/tensorflow/tensorflow/compiler/xla/tests/BUILD:953:1: Linking of rule '//tensorflow/compiler/xla/tests:broadcast_simple_test_cpu_parallel' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/broadcast_simple_test_cpu_parallel '-Wl,-rpath,$ORIGIN/../../../../_solib_ppc/' -Lbazel-out/local-opt/bin/_solib_ppc -pthread ... (remaining 9 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
/usr/bin/ld: bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/broadcast_simple_test_cpu_parallel: hidden symbol `pthread_atfork' in /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS) is referenced by DSO
/usr/bin/ld: final link failed: Bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/compiler/xla/tests:broadcast_simple_test_cpu_parallel failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 4.953s, Critical Path: 4.18s

```

Any solution  ?
"
8836,AttributeError: 'module' object has no attribute 'rnn',"I had a version of ptb_word_lm.py which worked in 0.11. Now I am trying to use the same code in version 1.0.1. I am facing the following error:

outputs, state = tf.nn.rnn(cell, inputs,initial_state=self._initial_state)
AttributeError: 'module' object has no attribute 'rnn'

Please help soon !!"
8835,[windows 7] Not using CUDA. No errors or messages indicating dlls have been loaded in log. ,"Issue: after upgrading from 0.12 to 1.0.1, I lost GPU support. On 0.12, there were log entries indicating successful loading of cuda/cudnn dlls and log entries if dlls weren't found. On 1.0.1, I don't see any log entries when importing tensorflow, but GPU is no longer listed amongst available devices. 

Can't provide much useful information without anything in the logs. Has logging been disabled, or is it getting written to a file now? How can I diagnose this?

---

Operating System: windows 7

Installed version of CUDA and cuDNN: CUDA 8.0 cuDNN 6.0

Output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.1

What other attempted solutions have you tried?
Reinstalled tf, reinstalled cudnn

"
8834,java tensorflow api :  Malformed TF_STRING tensor; too short to hold number of elements ,"First, when i tried LabelImage [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java) with inception model ver 5, everything was good.

Then i tried with an older inception model ( ver3 ) and i saw that both model have different input and output.

In ver5, our input tensor name is : ""Input"" , with dtype = FLOAT
In ver 3, our input tensor name is ""DecodeJpeg/contents"" , with dtype = STRING.

So i change LabelExample example with new name for both input and output : `Tensor result = s.runner().feed(""input"", string_tensor_image).fetch(""output"")` >> `s.runner().feed(""DecodeJpeg/contents"", image).fetch(""softmax"")` .

Also, i changed Image tensor to STRING type : 

    Tensor float_tensor_image = s.runner().fetch(output.op().name()).run().get(0);
    byte[] bytes = new byte[res.numBytes()];
    ByteBuffer buffer = ByteBuffer.wrap(bytes);
    res.writeTo(buffer);
    Tensor string_tensor_image = Tensor.create(DataType.STRING,res.shape(),buffer);

It looked good when i printed both tensor :
 
    FLOAT tensor with shape [1, 224, 224, 3]
    STRING tensor with shape [1, 224, 224, 3]

But after feeding to the graph, i get this error : 
`Exception in thread ""main"" java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements.`

I have tried everything i can, but no results. How can i fix it ?"
8833,DynamicAttentionWrapper expects own state on the 0-th step,"Using Tensorflow 1.1rc0
AFAIK the [DynamicAttentionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py) wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the __call__()

However, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)

I got the following error
```python
Traceback (most recent call last):
  File ""/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py"", line 54, in __init__
    self.model = Model(self.tok_chr.dim)
  File ""/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py"", line 164, in __init__
    outputs, _ = dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_sl)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 278, in dynamic_decode
    swap_memory=swap_memory)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2623, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2456, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2406, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 231, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py"", line 140, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py"", line 530, in __call__
    cell_inputs = self._cell_input_fn(inputs, state.attention)
AttributeError: 'tuple' object has no attribute 'attention'
```
The error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax

The following snippet shows the use of the wrapper. This was coded after [this](https://www.tensorflow.org/versions/r1.1/api_guides/python/contrib.seq2seq#Dynamic_Decoding) explanation

```python
encoder_outputs, encoder_state = core_rnn.static_rnn(
                encoder_cell, encoder_inputs, dtype=dtype, sequence_length=self.SL_enc)

#Some other code

attention_size = 10
attn_obj = BahdanauAttention(num_units=attention_size,
                                             memory=attention_states,
                                             memory_sequence_length=self.SL_enc,
                                             normalize=True,
                                             name='BahdanauAttentionObject')

wrapped_cell = DynamicAttentionWrapper(cell_dec_fw,
                                                       attn_obj,
                                                       D,
                                                       output_attention=False,
                                                       name='DynAttnWrap')

sampler = ScheduledEmbeddingTrainingHelper(decoder_inputs,
                                                        sequence_length=self.SL_dec,
                                                        embedding=embedding_in,
                                                        sampling_probability=self.samp_prob)
decoder = BasicDecoder(wrapped_cell,
                                       sampler,
                                       encoder_state)
outputs, _ = dynamic_decode(decoder)
```"
8832,[tensorflow.org] Wrong footer position,"In a few pages of tensorflow.org (like https://www.tensorflow.org/install/install_windows), the footer appears at the wrong place.

The fix is to move 
```
<footer class=""devsite-utility-footer"">...</footer>
<footer class=""devsite-footer-linkboxes nocontent devsite-footer-linkboxes-all-backup"">...</footer>
```
after
```
<div class=""devsite-main-content clearfix"">...</div>
```"
8830,Couldn't open CUDA library libcupti.so.8.0,"Hi,

I've installed 1.0 on my ubuntu 16.04 machine and deployed tf-1.0 with success, at least before I run the example shipped with tf, e.g, [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)

When running mnist_with_summaries.py, I got the very problem posted in #5282. The discussion is mainly about the missing `libcupti.so` and most of them suggested to install or symlink the lib. But both approach didn't work for me.... Since the `so` file exists when installing CUDA-8.0 under the directory `/usr/local/cuda/extra/CUPTI/lib64/`, so I thought what I need to do is just link the library under the directory `/usr/local/cuda/lib64/`. However, problem still exists after the linking. And the `libcupti-dev` in `apt-get` is old (v7.5), which fails also. 

Anyone could give me some advice? Thanks in advance. 

@drpngx @gunan @K-Wu"
8829,Tensorboard smoothing is not smooth at all.,"I have Tensorflow v1.0.1 installed on Linux.

""Smoothed"" Tensorboard looks not smooth at all compared to the original graph. For some reason it manages to find some interesting loss function behavior features that simply do not exist in the original loss data.

![image](https://cloud.githubusercontent.com/assets/1829149/24485892/f269a388-14bb-11e7-8b4c-b21d54e4c4be.png)

"
8828,Suport cuDNN v6.0,"cuDNN v6.0 has been released. There are some cool new features:
* Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a
change in API.
* `cudnnConvolutionBiasActivationForward` allows for the execution of a single kernel fusing convolution, bias and activation operations

Full release notes:
>Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a
change in API. Previously unused “upscale” fields in the Convolution Descriptor
have been repurposed to allow user specification of dilation factors along each
dimension. Support for dilation is present in the following code paths :
Forward : CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,
Backward Data: CUDNN_CONVOLUTION_BWD_DATA_ALGO_0 and
Backward Filter: CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0
 The new API cudnnConvolutionBiasActivationForward allows for the execution of
a single kernel fusing convolution, bias and activation operations. At present, only
per channel bias and RELU activation are supported.
 Inference on 8 bit integer data is now supported, leveraging the 4 element dot
product instruction (IDP4A) of Pascal GPUs with CUDA capabilities 6.1. Two tensor
layouts are supported for this feature: CUDNN_TENSOR_NHWC with INT8 data
type and CUDNN_TENSOR_NCHW_VECT with INT8x4 data type.
 RNN now supports 3 algorithms
o CUDNN_RNN_ALGO_STANDARD :
 Same functionality as in CUDNN v5.1
o CUDNN_RNN_ALGO_PERSIST_STATIC :
 This algorithm relies on the usage of persistent CUDA kernels which
are pre-compiled to fit different GPUs.
 This algorithm is available only on Pascal GPUs.
o CUDNN_RNN_ALGO_PERSIST_DYNAMIC :
This algorithm also relies on the usage of persistent CUDA kernels
but these kernels are compiled at runtime using nvrtc. In some cases
this results in a significant performance benefit.
 This algorithm is also available only on Pascal GPUs and is supported
only on Linux and Windows.
 Support for 1D-FFT convolutions has been added
 New API routine cudnnReduceTensor has been added, supporting 8 reduction
operations
 Activation mode CUDNN_ACTIVATION_ELU is now supported.
 A deterministic max pooling mode CUDNN_POOLING_MAX_DETERMINISTIC
has been added.
 Significant performance improvement for softmax layers for mode
CUDNN_SOFTMAX_MODE_CHANNEL has been achieved when low batch
number is used.
 Significant performance improvements have been added for cudnnAddTensor
when spatial dimensions are set to 1."
8827,bugs with cifar10 while doing evaluation,"I slightly modify the cifar10 code, which is modifying length of width and height not to have same length.

height = 16
width = 32

While I did eval, it showed an error log cifar10_input.py at ""  float_image.set_shape([height, width, 3])"" this line.
I debugged it, I found a strange thing. Above two line from this, I found this ""  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,**width, height**)"". 
The reason why I felt strange is the position of second and third param of resize_image_with_crop_or_pad is not same to compare any other functions. (normally height is second and width is third. or first and second.) I mean the order is strange.

So I think that ""  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, **height, width**)"" may be correct.

"
8823,GRU cell error in sequence to sequence models tutorial,"Hello everyone,
I got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error

Thanks in advance
![seq](https://cloud.githubusercontent.com/assets/20130992/24481010/92cc0f3e-149c-11e7-8244-7b20f513efdd.png)
"
8822,AttributeError: 'module' object has no attribute 'GRUCell',"Hello everyone,
I got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error

Thanks in advance
![seq](https://cloud.githubusercontent.com/assets/20130992/24480978/67188b74-149c-11e7-89a5-63d69c3a8412.png)
"
8821,GRU cell error- sequence to sequence models tutorial,"Hello everyone,
I got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error

Thanks in advance
![seq](https://cloud.githubusercontent.com/assets/20130992/24480912/0820eb52-149c-11e7-8734-c5861c14d73e.png)
"
8820,Feature Request: Accelerate TensorFlow core on FPGA - How?,"Consider the two following hardware scenarios:
1) Linux running on x86 w/ FPGA fabric connected via PCIe
2) Linux running on Arm A53 with AXI i/f to FPGA fabric (Think Xilinx Zynq)

How could the tensorflow core be accelerated for these scenarios? 
FPGA vendors do offer OpenCL binaries for running OpenCL APIs to parallelize computations.

Forgive me, I am a bit ignorant, still learning in this area, but I am intrigued by the future possibility of this, and would love to help in anyway I can."
8819,Does tf.train.MonitoredTrainingSession() support GRPC?,"I am not sure if the feature for `MonitoredTrainingSession()` working with `grpc` is existing or not. If not, can we have a feature request here? I also posted details on StackOverflow: http://stackoverflow.com/questions/43103763/does-tf-train-monitoredtrainingsession-support-grpc. Thanks!
"
8818,Bug when using `tf.contrib.metrics.streaming_mean_iou`,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
ls -l /usr/local/cuda-8.0/lib/libcud*
-rw-r--r-- 1 root root   560184 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart_static.a
lrwxrwxrwx 1 root root       13 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so -> libcudnn.so.5
-rwxr-xr-x 1 root root 61453024 Nov 13 12:10 /usr/local/cuda-8.0/lib/libcudnn.so.4
-rwxr-xr-x 1 root root 61453024 Nov 13 12:10 /usr/local/cuda-8.0/lib/libcudnn.so.4.0.7
lrwxrwxrwx 1 root root       17 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 78065952 Jan 12 11:03 /usr/local/cuda-8.0/lib/libcudnn.so.5.0.5
-rwxr-xr-x 1 root root 79337624 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`0.12.1`
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).


I'm getting a strange error when trying to compute the intersection over union using tensorflows tf.contrib.metrics.streaming_mean_iou.
This was the code I was using before which works perfectly fine

```
import tensorflow as tf
label = tf.image.decode_png(tf.read_file('/path/to/label.png'),channels=1)
label_lin = tf.reshape(label, [-1,])
weights = tf.cast(tf.less_equal(label_lin, 10), tf.int32)
mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(label_lin, label_lin,num_classes = 11,weights = weights)
init = tf.local_variables_initializer()
sess.run(init)
sess.run([update_op])
```
However when I use a mask like this

```
mask = tf.image.decode_png(tf.read_file('/path/to/mask_file.png'),channels=1)
mask_lin = tf.reshape(mask, [-1,])
mask_lin = tf.cast(mask_lin,tf.int32)
mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(label_lin, label_lin,num_classes = 11,weights = mask_lin)
init = tf.local_variables_initializer()
sess.run(init)
sess.run([update_op])
```
It keeps on failing after an irregular number of iterations showing this error:

`*** Error in `/usr/bin/python': corrupted double-linked list: 0x00007f29d0022fd0 ***`

I checked the shape and data type of both mask_lin and  weights. They are the same, so I cannot really see what is going wrong here.
Also the fact that the error comes after calling update_op an irregular number of times is strange. Maybe TF empties the mask_lin object after calling several sess.run()'s ?
Or is this some TF bug? But then again why would it work with weights..."
8817,Conditionally trainable variables and stochastic depth neural networks,"I came across with a task where I would like to apply stochastic depth _regularization_ technique using Tensorflow (https://arxiv.org/pdf/1603.09382.pdf). Tensorflow doesn't provide enough settings to implement this one. I found closed issue  #1784 which is similar to this request, where guys finished the discussion with claim that [ `tf.cond` | `tf.select` ] primitives are enough for this task. But if you carefully read the paper it says that during training the depth changes for both directions: forward and backward propagation steps. Therefore number of tranable W parameters of the network changes too. The core conception of the Tensorflow is building computation graph before session of training is run. Currently, I can not create dynamic computation graph, so that depending on a boolean value W parameters of a layer were not engaged in optimisation process.

If `tf.Variable` accepted `trainable` parameter as a boolean tensor apart from built-in boolean value it would solve the problem. In this case, it would mean that Tensorflow operates natively with dynamic computational graphs, which in fact very powerful tool.

I would appreciate any suggestions and ideas, so that this question was closed for good and all.

@vrv, @martinwicke, @aselle"
8815,tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform,"I built tensorflow from source on Ubuntu 14.04 LTS.

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

ubuntu@ip-10-0-1-152:~/tensorflow$ ls -l /usr/local/cuda
lrwxrwxrwx 1 root root 19 Mar 27 16:02 /usr/local/cuda -> /usr/local/cuda-7.0

If installed from binary pip package, provide:

ubuntu@ip-10-0-1-152:~/tensorflow$ ls -l /home/ubuntu/cuda
total 8
drwxrwxr-x 2 ubuntu ubuntu 4096 Mar 21 20:34 include
drwxrwxr-x 2 ubuntu ubuntu 4096 Mar 21 20:34 lib64

1. The commit hash (`git rev-parse HEAD`)
ubuntu@ip-10-0-1-152:~/tensorflow$ git rev-parse HEAD
baa85cbf5e51a21f58bc28ef9eedc122e6118eb8
2. The output of `bazel version`
ubuntu@ip-10-0-1-152:~/tensorflow$ bazel version
...........................................................................................................
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 3522.680s, Critical Path: 2077.85s
ubuntu@ip-10-0-1-19:~/tensorflow$ 
ubuntu@ip-10-0-1-19:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 
Wed Mar 29 17:58:57 UTC 2017 : === Using tmpdir: /tmp/tmp.fQQIv2RgvY
~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow
~/tensorflow
/tmp/tmp.fQQIv2RgvY ~/tensorflow
Wed Mar 29 17:58:58 UTC 2017 : === Building wheel
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
~/tensorflow
Wed Mar 29 17:59:29 UTC 2017 : === Output wheel file is in: /tmp/tensorflow_pkg


ubuntu@ip-10-0-1-19:~/tensorflow$ sudo -H pip install --upgrade /tmp/tensorflow_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl 
tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.

Is this a clue?

sudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl 
protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.
"
8811,Tensorflow Variables are Not Initialized using Between-graph Replication,"I am running an issue for Distributed Tensorflow code with ""Between-Graph Replication"", which does not initialize Tensorflow Variables for some worker nodes. I posted the issue on StackOverflow: http://stackoverflow.com/questions/43084960/tensorflow-variables-are-not-initialized-using-between-graph-replication. My concern is if this is an issue/bug for `MonitoredTrainingSession` which uses `SessionManager` in Tensorflow. Thanks!
"
8810,"Cannot create a tensor of shape (?), but can create a tensor of shape (?, 1)","I tried to create a tensor of dynamic shape.

```
shape = tf.shape(boxes) # boxes' shape is unknown 
batch_inds = tf.zeros((shape[0]), dtype=tf.int32, name='batch_inds')     # fails
batch_inds = tf.zeros((shape[0], 1), dtype=tf.int32, name='batch_inds')  # works
batch_inds = tf.reshape(batch_inds, [-1])                     
tf.image.crop_and_resize(images, boxes, batch_inds,
                                     [pooled_height, pooled_width],
                                     method='bilinear',
                                     name='Crop')
```
hmmm.. it seems a little bit clumsy to write code this way."
8809,an update for the tf.contrib.learn Quickstart example is needed,"Hi all,

just tried to start the script from here: https://www.tensorflow.org/get_started/tflearn

found one issue for python3 users:


```python
import urllib
raw = urllib.urlopen(IRIS_TRAINING_URL).read()

```
 this returns:


```bash
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-32-2005abd64a8b> in <module>()
----> 1 raw = urllib.urlopen(IRIS_TRAINING_URL).read()

AttributeError: module 'urllib' has no attribute 'urlopen'

```

the solution:

```python
import urllib.request as ur
raw = ur.urlopen(IRIS_TRAINING_URL).read()
```

But, the main reason for this issue-ticket is **warning messages** like:

```
WARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported. 
```

or this

```
WARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
```

or like this:

```
WARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
INFO:tensorflow:Starting evaluation at 2017-03-29-15:28:31
INFO:tensorflow:Evaluation [1/1]
INFO:tensorflow:Finished evaluation at 2017-03-29-15:28:32
INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.966667, auc = 0.998333, global_step = 4000, loss = 0.0793921
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
```

So, I believe, the introduction tutorials should be free from warnings. 

I can try to fix the warnings, if someone is interesting. 

I'm using Python 3.5.2 |Anaconda 4.3.1 (x86_64)| (default, Jul  2 2016, 17:52:12) .
"
8807,error running tensorflow trained model c++,"Hello,

I am working on Tensorflow on c++ with other network. I trained facenet on MS-Celeb-1M then i created my graph.pb. I modified the example provided here : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image in order to test my network.
In main.cpp:    
    `string graph = ""data/graph1.pb"";`
    `string output_layer = ""InceptionResnetV1/Repeat/block35_5/Relu"";`

I get this error if I test : 
    

> Running model failed: Invalid argument: You must feed a value for placeholder tensor 'phase_train'     with dtype bool [[Node: phase_train = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0    /cpu:0""]()]]
"
8806,Installation: Permission error,"`(tensorflow) wermarter@Werma:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl
Collecting tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl
Collecting protobuf>=3.1.0 (from tensorflow-gpu==1.0.1)
  Using cached protobuf-3.2.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting numpy>=1.11.0 (from tensorflow-gpu==1.0.1)
  Using cached numpy-1.12.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting six>=1.10.0 (from tensorflow-gpu==1.0.1)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting wheel>=0.26 (from tensorflow-gpu==1.0.1)
  Using cached wheel-0.29.0-py2.py3-none-any.whl
Collecting setuptools (from protobuf>=3.1.0->tensorflow-gpu==1.0.1)
  Using cached setuptools-34.3.3-py2.py3-none-any.whl
Collecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)
  Using cached appdirs-1.4.3-py2.py3-none-any.whl
Collecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)
  Using cached packaging-16.8-py2.py3-none-any.whl
Collecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)
  Using cached pyparsing-2.2.0-py2.py3-none-any.whl
Installing collected packages: six, appdirs, pyparsing, packaging, setuptools, protobuf, numpy, wheel, tensorflow-gpu
Exception:
Traceback (most recent call last):
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/wheel.py"", line 329, in clobber
    os.utime(destfile, (st.st_atime, st.st_mtime))
PermissionError: [Errno 1] Operation not permitted
`
I've tried sudo -H but it said `tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.`. "
8805,valgrind helloworld.py throws 7805 errors,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Several issues report memory leaks, but only for specific uses of Tensorflow:
https://raw.githubusercontent.com/aymericdamien/TensorFlow-Examples/master/examples/1_Introduction/helloworld.py
https://github.com/tensorflow/tensorflow/issues/700
https://github.com/tensorflow/tensorflow/issues/4151
http://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session

### Environment info
Operating System:
Linux, Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
❯ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   556000 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 26 18:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 26 18:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root   415432 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rw-r--r-- 1 root root 79337624 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
`pip install tensorflow`

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
(env) ❯ python -c ""import tensorflow; print(tensorflow.__version__)""
1.0.1
```

If installed from source, provide 
**I'm listing both because I encountered the problem using both source and pip package.**

1. The commit hash (`git rev-parse HEAD`)
```
~/tensorflow r1.0*
❯ git rev-parse HEAD
e895d5ca395c2362df4f5c8f08b68501b41f8a98

```

2. The output of `bazel version`
```
❯ bazel version
............
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```
valgrind python helloworld.py
```
Here, `helloworld.py` refers to https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/helloworld.py.

While there are certain cases in which memory violations are not a problem, I am trying to track down a segfault from using Tensorflow with Ros and Gazebo. It's very difficult to know whether one of the memory issues already present in Tensorflow is  responsible.

Final summary is as follows:
```
==18112== HEAP SUMMARY:
==18112==     in use at exit: 8,356,021 bytes in 99,634 blocks
==18112==   total heap usage: 775,798 allocs, 676,164 frees, 352,616,777 bytes allocated
==18112== 
==18112== LEAK SUMMARY:
==18112==    definitely lost: 154,618 bytes in 82 blocks
==18112==    indirectly lost: 0 bytes in 0 blocks
==18112==      possibly lost: 1,745,007 bytes in 32,420 blocks
==18112==    still reachable: 6,456,396 bytes in 67,132 blocks
==18112==         suppressed: 0 bytes in 0 blocks
==18112== Rerun with --leak-check=full to see details of leaked memory
==18112== 
==18112== For counts of detected and suppressed errors, rerun with: -v
==18112== Use --track-origins=yes to see where uninitialised values come from
==18112== ERROR SUMMARY: 7713 errors from 159 contexts (suppressed: 0 from 0)
```

### What other attempted solutions have you tried?
None.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
This [gist](https://gist.github.com/lobachevzky/0a7319f9cb5df23e32c1cc173210e768) contains the full output of `valgrind python helloworld.py`.
"
8804,why use unknown batch_size in BasicDecoder class?,"I found that in the source code:[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py](basic_decoder.py)
```python
  @property
  def batch_size(self):
    return self._helper.batch_size

  def _rnn_output_size(self):
    size = self._cell.output_size
    if self._output_layer is None:
      return size
    else:
      # To use layer's compute_output_shape, we need to convert the
      # RNNCell's output_size entries into shapes with an unknown
      # batch size.  We then pass this through the layer's
      # compute_output_shape and read off all but the first (batch)
      # dimensions to get the output size of the rnn with the layer
      # applied to the top.
      output_shape_with_unknown_batch = nest.map_structure(
          lambda s: tensor_shape.TensorShape([None]).concatenate(s),
          size)
      layer_output_shape = self._output_layer._compute_output_shape(  # pylint: disable=protected-access
          output_shape_with_unknown_batch)
return nest.map_structure(lambda s: s[1:], layer_output_shape)
```

As above, since we can get the batch size by calling self._helper.batch_size, why it is set to be None in
`lambda s: tensor_shape.TensorShape([None]).concatenate(s)`?  Why the batch size is still unknown? Couldn't we set to be `lambda s: tensor_shape.TensorShape([self._helper.batch_size]).concatenate(s)`?"
8803,how to read more than one tfrecord files once?,"I want to do validation during training, but how to read two tfrecord files once, one is for training and annother one is for validation"
8802,Wheels on PyPI violate manylinux1 specification / PEP513,"Follow-up issue to #5033.

As already pointed out by @jjhelmus, the Linux wheels on PyPI do not conform to the manylinux1 specification provided in [PEP513](https://www.python.org/dev/peps/pep-0513/). To quote @jjhelmus's comment from #5033:

> Specifically, they contain shared libraries which reference versioned symbols which do not meet the GLIBC <= 2.5 and GLIBCXX <= 3.4.9 requirements specified in the PEP.

It would be great if the wheels could be fixed to match the specification."
8801,[Tensorflow]RuntimeError: Graph is finalized and cannot be modified.,"### Environment info
Operating System:
Ubuntu 14.04  
cuda 8.0 + cudnn 5.1

I'm running a Cifar-10 tutorial from [https://www.tensorflow.org/tutorials/deep_cnn]
And I modified the code a lot.
Specifically, I want the model can evaluate simultaneously while training, thus I can get a curve of precision. And I don't have ideal how the cifar10_eval.py and checkpoints work out. So I added an evaluation function to train( ) in cifar10_train.py, and I tried to run the evaluation function every 100 global_step. And then I got this:

        RuntimeError: Graph is finalized and cannot be modified.

Here is my evaluation function in cifar10.py:

      def evaluation():
        images_e, labels_e = inputs(eval_data=True)
        logits_e = inference(images_e)
        correct_pred = tf.equal(tf.argmax(logits_e, 1), tf.argmax(labels_e, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
        print('%s: accuracy @ 1 = %.3f' % (datetime.now(), accuracy))
        tf.summary.scalar('accuracy', accuracy)

And here is the call (the last few lines) :

       def after_run(self, run_context, run_values):
        if self._step % FLAGS.log_frequency == 0:
          current_time = time.time()
          duration = current_time - self._start_time
          self._start_time = current_time

          loss_value = run_values.results
          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration
          sec_per_batch = float(duration / FLAGS.log_frequency)

          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '
                        'sec/batch)')
          print (format_str % (datetime.now(), self._step, loss_value,
                               examples_per_sec, sec_per_batch))
        if self._step % EVAL_STEP == 0: 
          print('evaluation\n')
          cifar10.evaluation()

I don't know how to solve it. I do searched the Internet before but there is no similar situation. Is this caused by I called inference() twice in one global_step or else? Any suggestions would be helpful. Thank you!
"
8800,Facing error while building APK ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### Currently trying to run tensorflow android code downloaded from Github facing following error
Error:Execution failed for task ':buildNativeMake'.
A problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh''


### Environment info
Operating System: 64 bit
Android Studio : 2.3
Bazel: 0.3.0
python: 3.5
JDK: 8

Please get back to us today ASAP
Thanking you in advance.

[build_gradle.txt](https://github.com/tensorflow/tensorflow/files/878702/build_gradle.txt)



"
8799,Ensuring positive definite covariance matrix in tensorflow,"Hi all,

I have the problem how to ensure psd property of covariance matrix in tensorflow. so I map the rnn outputs to the $\mu$ and $\Sigma$ of a MVN. So here I have to make the covariance psd, I tried to use matrix exponential, but it is not implemented in tensorflow, and I tried SVD, but then my loss function is not differentiable. So I tried to use $LL^{T} + \alpha |$, but still the program has bugs that the Sigma matrix is not invertible. Any suggestions for this problem? Thanks in advance!


```
def get_lossfunc(mu, Sigma,input_data):
    loss = 0
    for i in range(len(mu)):
        muC = tf.squeeze(mu[i])
        SigmaC = Sigma[i]
        inputC = input_data[i]
        SigmaC = tf.matmul(SigmaC, tf.transpose(SigmaC)) + tf.eye(10)
#         s,u,v = tf.svd(Sigma[0])
#         SigmaC = tf.matmul(tf.matmul(u,tf.diag(tf.exp(s))),v)
        dist = tf.contrib.distributions.MultivariateNormalFull(muC, SigmaC)
        loss += -tf.log(dist.pdf(inputC))
#         print loss
    return loss
```"
8798,Not found error : Key tower/fully_connected/weights not found in checkpoint,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/6263

### Environment info
Operating System: Google Cloud Platform

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

python eval.py --eval_data_pattern='gs://youtube8m-ml-us-east1/1/frame_level/validate/validate*.tfrecord' --train_dir=$BUCKET_NAME/LstmModel --run_once=True

-----------------------------------------------------------------------------------------------------------

Caused by op u'save/RestoreV2_2', defined at:
  File ""eval.py"", line 332, in <module>
    app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""eval.py"", line 328, in main
    evaluate()
  File ""eval.py"", line 309, in evaluate
    saver = tf.train.Saver(tf.global_variables())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1040, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1070, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 675, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()
NotFoundError (see above for traceback): Key tower/fully_connected/weights not found in checkpoint
         [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_2/tensor_names, sav
e/RestoreV2_2/shape_and_slices)]]
"
8797,Currently trying to run tensorflow android code downloaded from Github facing following error as Error:Execution failed for task ':buildNativeMake'. A problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh'' Please reply asap,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8796,Distributed mode hangs on in local mode,"Looks like the workers do not start their server when using the estimator API and running in local-distributed mode. The minimalist code down helps reproduce: runs fine with only one process, but when launched with `TF_CONFIG={""cluster"": {""ps"":[""localhost:5040""], ""worker"":[""localhost:5041""]}, ""task"":{""type"":""ps"",""index"":0}}` (resp. `worker`), it hangs on. After investigating, looks like in
`tensorflow/contrib/learn/python/learn/experiment.py l.250-258`:

    # Start the server, if needed. It's important to start the server before
    # we (optionally) sleep for the case where no device_filters are set.
    # Otherwise, the servers will wait to connect to each other before starting
    # to train. We might as well start as soon as we can.
    config = self._estimator.config
    if (config.environment != run_config.Environment.LOCAL and
        config.environment != run_config.Environment.GOOGLE and
        config.cluster_spec and config.master):
      self._start_server()
the server is not started when the environment is local, no matter distributed or not. When I force the _start_server() to be executed though, everything works just fine.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I wrote a stackoverflow thread here: http://stackoverflow.com/questions/43076035/tensorflow-minimalist-program-fails-on-distributed-mode

### Environment info
Operating System: Windows-64 bit

Installed version of CUDA and cuDNN: none, only on CPU
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed: from install in the website (`pip install --upgrade tensorflow`)
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

    import numpy as np
    import tensorflow as tf
    from tensorflow.contrib.learn.python.learn import learn_runner
    from tensorflow.contrib import layers

    DATA_SIZE=10
    DIMENSION=5

    def generate_input_fn():
        def _input_fn():
            mid = int(DATA_SIZE/2)
            data = np.array([np.ones(DIMENSION) if x < mid else -np.ones(DIMENSION) for x in range(DATA_SIZE)])
            labels = ['0' if x < mid else '1' for x in range(DATA_SIZE)]        
            table = tf.contrib.lookup.string_to_index_table_from_tensor(tf.constant(['0', '1']))
            label_tensor = table.lookup(tf.convert_to_tensor(labels, dtype=tf.string))
            return dict(zip(['features'], [tf.convert_to_tensor(data, dtype=tf.float32)])), label_tensor
        return _input_fn

    def build_estimator(model_dir):
        features = layers.real_valued_column('features', dimension=DIMENSION)
        return tf.contrib.learn.LinearClassifier(
            feature_columns=[features],
            model_dir=model_dir)

    def generate_exp_fun():
        def _exp_fun(output_dir):
            return tf.contrib.learn.Experiment(
                build_estimator(output_dir),
                train_input_fn=generate_input_fn(),
                eval_input_fn=generate_input_fn(),
                train_steps=1000)
        return _exp_fun

    if __name__ == '__main__':
        tf.logging.set_verbosity(tf.logging.DEBUG)
        learn_runner.run(generate_exp_fun(), 'job_dir')

### What other attempted solutions have you tried?

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
If server is not started, hangs on with log: `INFO:tensorflow:Create CheckpointSaverHook.`"
8795,"[CPU Performance Issue] Why don't hugepages, prefetch.","
Hugepages and Prefetch are considered as two main techniques for performance improvement.

https://wiki.debian.org/Hugepages
Intel® 64 and IA-32 Architectures Optimization Reference Manual (http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html)

Just don't understand why TF does not use hugepages and prefetch techniques for CPU version.


Thanks for great TF.

"
8794, 'module' object has no attribute 'learn',"I'm trying tensorflow with some examples, when I run https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py, it raise the exception below:

--------------------------------------------------------------------------------------------------
File ""/Users/cxm/cifar10-train.py"", line 124, in <module>
  tf.app.run()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
  _sys.exit(main(_sys.argv[:1] + flags_passthrough))
File ""/Users/cxm/cifar10-train.py"", line 120, in main
  train()
File ""/Users/cxm/cifar10-train.py"", line 62, in train
  global_step = tf.contrib.framework.get_or_create_global_step()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 35, in __getattr__
  contrib = importlib.import_module('tensorflow.contrib')
File ""/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py"", line 37, in import_module
  __import__(name)
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 29, in <module>
  from tensorflow.contrib import factorization
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/factorization/__init__.py"", line 24, in <module>
  from tensorflow.contrib.factorization.python.ops.gmm import *
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 32, in <module>
  from tensorflow.contrib.learn.python.learn import graph_actions
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py"", line 83, in <module>
  from tensorflow.contrib.learn.python.learn import *
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py"", line 23, in <module>
  from tensorflow.contrib.learn.python.learn import *
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 24, in <module>
  from tensorflow.contrib.learn.python.learn import datasets
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py"", line 27, in <module>
  from tensorflow.contrib.learn.python.learn.datasets import base
File ""/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 28, in <module>
  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
File ""/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 36, in __getattr__
  return getattr(contrib, item)

AttributeError: 'module' object has no attribute 'learn'

----------------------------------------------------------------------------------------------
I run tensorflow on maOS Serria 10.12.3, python 2.7 I install tensorflow by pip install,
besides, when I run some other example of tensorflow.contrib, I, met the same exception, Any help? Thanks!
"
8793,python StagingArea documentation missing,"StagingArea (and perhaps other structures in [data_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py)) are not present in the python documentation. They are however mentioned in the 1.0.0 and 1.1.0 [release notes](https://github.com/tensorflow/tensorflow/blob/346bd59b3c99055c3fdcbc0d0b0710418f48c5c6/RELEASE.md)

### What other attempted solutions have you tried?

A search of the docs for [""StagingArea""](https://www.tensorflow.org/s/results/?q=StagingArea) doesn't reveal any of the docstrings present in [data_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py).

[Stage](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/stage) and Unstage are present in the C++ documentation."
8790, crosstool_wrapper_driver_is_not_gcc failed: error executing command,"I am installing tensorflow 1.0.0-rc0 with GPU, the environment is:
Operating System: Ubuntu 14.04.4 LTS
gcc version: 4.7
bazel:0.4.5
python 3.6
CUDA 8.0
 cuDNN:5.1.5
cuDNN is installed at /home/scs4450/CaffeInstall/cuda ranther than /usr/local/cuda-8.0
when i ./configure my tensorflow, i get the error like this:
scs4450@scs4450:~/tensorflow-1.0.0-rc0$ bazel build --copt=-march=native -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

INFO: Found 1 target...
ERROR: /home/scs4450/.cache/bazel/_bazel_scs4450/a3c5204dbd511f84e92c40ad5244a5e1/external/protobuf/BUILD:334:1: C++ compilation of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/scs4450/.cache/bazel/_bazel_scs4450/a3c5204dbd511f84e92c40ad5244a5e1/execroot/tensorflow-1.0.0-rc0 && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/lib64 \
    PATH=/home/scs4450/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: unrecognized command line option '-fno-canonical-system-headers'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.692s, Critical Path: 0.16s"
8789,tensorflow can not be imported in python,"Tensorflow  (CPU) can not be imported in python 2.7 

Operating System: Linux; output of uname -a
Linux laptop 4.4.14 #2 SMP Fri Jun 24 13:38:27 CDT 2016 x86_64 Intel(R) Core(TM) i3-4005U CPU @ 1.70GHz GenuineIntel GNU/Linux

Installed version of CUDA and cuDNN: NONE 

Installed within virtualenv, via pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl

Output from python while attempting to import tensorflow:
-------------------------------------------------------------------------
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: /opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicodeUCS4_AsUTF8String


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error


"
8787,TFRecord integration with Keras API,"Since the Keras-2 API is now directly in TensorFlow, I think it would be very useful if there were a mechanism directly in TensorFlow to supply TFRecords to Keras, such as in a call to `model.fit()`, or with equivalent functionality to `flow_from_directory`.

One key implementation detail is with the way `model.compile()` works as [detailed in a comment on this topic in keras](https://github.com/fchollet/keras/issues/5358#issuecomment-282117525). As discussed in the same comment, numpy arrays have been determined to be the primary way of interacting with Keras, so this should not be an upstream Keras request because TFRecords currently require TensorFlow, so other backends couldn't be supported easily.

New Pull Request (includes examples):
https://github.com/fchollet/keras/pull/6928

(outdated) Starter code:
https://github.com/farizrahman4u/keras-contrib/pull/27

Related Keras issues:
https://github.com/fchollet/keras/issues/5356
https://github.com/fchollet/keras/issues/5368
https://github.com/fchollet/keras/issues/5325"
8786,Problem with processing/displaying RGB images,"I just switched to a build of the r1.1 branch of tensorflow (due to a fixed bug in tfdbg present in 1.0) and without any changes to my model, RGB images saved with tf.summary.image are showing up weird in tensorboard, whereas they showed up fine in all previous versions of tensorflow/tensorboard.

Here's an example of what I mean: http://i.imgur.com/xFoEYzO.png

Those should be regular MSCOCO color images, but now the colors are all over the place.

Before being displayed, the images are adjusted as follows:

        distorted_image = tf.cast(result.image_raw, tf.float32)
        distorted_image = tf.reshape(distorted_image,
                                     [self.width, self.height, 3])
        distorted_image = tf.image.random_brightness(distorted_image,
                                                     max_delta=35)
        distorted_image = tf.image.random_contrast(distorted_image,
                                                   lower=0.4, upper=1.4)
        distorted_image = tf.image.random_hue(distorted_image, max_delta=0.01)
        float_image = tf.image.per_image_standardization(distorted_image)


and then the bounding boxes are drawn with: `tf.image.draw_bounding_boxes`

the images are logged using `tf.summary.image('images', preview_images, max_outputs=16)`

I don't know if the issue is exclusive to tensorboard (conv layer activations seem to be displayed just fine, though the first layer conv activations look like they are working on the 'broken' image data as well, so it looks like it's not a tensorboard bug).

The images are read from a TFRecord file (which wasn't changed) and supplied with a shuffle_batch

Have any of the image summary/image adjustment ops changed in a way that I'm not aware of, or is this a bug? It looks like it could have something to do with normalization of the image.
"
8785,uninstall tensorflow from windows,"Hello,
I installed python and tensorflow in windows 10. How can I uninstall it?

Thanks in advance"
8782,"tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].","I am running the carpedm20/attentive-reader-tensorflow at https://github.com/carpedm20/attentive-reader-tensorflow. This is a Tensorflow implementation of Google DeepMind's Teaching Machines to Read and Comprehend.


When I run the command on my terminal:  python main.py --dataset cnn

I get the following error. Can some one suggest how do I fix it?

{'batch_size': 32,
 'checkpoint_dir': 'checkpoint',
 'data_dir': 'data',
 'dataset': 'cnn',
 'decay': 0.95,
 'epoch': 25,
 'forward_only': False,
 'learning_rate': 5e-05,
 'model': 'LSTM',
 'momentum': 0.9,
 'vocab_size': 100000}
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 [*] Building Deep LSTM...
initial state is 
Tensor(""zeros:0"", shape=(32, 768), dtype=float32, device=/device:CPU:0)
 [*] Loading vocab from data/cnn/cnn.vocab100000 ...
 [*] Loading vocab finished.
Printing self.emb
Tensor(""emb/read:0"", shape=(264560, 256), dtype=float32, device=/device:CPU:0)
printing self.inputs
Tensor(""Placeholder:0"", shape=(32, 1000), dtype=int32, device=/device:CPU:0)
printing embed_inputs
Tensor(""embedding_lookup:0"", shape=(1000, 32, 256), dtype=float32, device=/device:CPU:0)
Traceback (most recent call last):
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""/Users/skreddy/anaconda/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 53, in <module>
    tf.app.run()
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 48, in main
    FLAGS.data_dir, FLAGS.dataset)
  File ""/Users/skreddy/TensorFlow/PycharmProjects/Teaching_Machines_to_Read and_Comprehend/deep_lstm.py"", line 105, in train
    self.prepare_model(data_dir, dataset_name, vocab_size)
  File ""/Users/skreddy/TensorFlow/PycharmProjects/Teaching_Machines_to_Read and_Comprehend/deep_lstm.py"", line 76, in prepare_model
    initial_state=self.initial_state)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 489, in dynamic_rnn
    for input_ in flat_input)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 489, in <genexpr>
    for input_ in flat_input)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1288, in transpose
    ret = gen_array_ops.transpose(a, perm, name=name)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3841, in transpose
    result = _op_def_lib.apply_op(""Transpose"", x=x, perm=perm, name=name)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].
"
8780,Cannot import tensorflow in python ,"I get the following error when I try to import tensorflow in  python 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named tensorflow

Python Version:-
Python 2.7.10 (default, Jul 30 2016, 19:40:32) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

Tensor Flow Version :- 
Name: tensorflow
Version: 1.0.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/site-packages
Requires: wheel, protobuf, numpy, six, mock

Protobuf
Name: protobuf
Version: 3.2.0
Summary: Protocol Buffers
Home-page: https://developers.google.com/protocol-buffers/
Author: protobuf@googlegroups.com
Author-email: protobuf@googlegroups.com
License: New BSD License
Location: /usr/local/lib/python2.7/site-packages
Requires: setuptools, six

Numpy 
Name: numpy
Version: 1.12.1
Summary: NumPy: array processing for numbers, strings, records, and objects.
Home-page: http://www.numpy.org
Author: NumPy Developers
Author-email: numpy-discussion@scipy.org
License: BSD
Location: /usr/local/lib/python2.7/site-packages
Requires: 

working on MAC sierra . Please let me know what could be the issue. "
8779,Contrintuitive get_variable dtype checks,"When reusing variables get_variable checks if dtypes are consistent, but it seems to compare the wrong dtypes.

Example:
```
a = tf.get_variable('a', initializer=tf.zeros(2, tf.int32))
with tf.variable_scope('', reuse=True):
  tf.get_variable('a', initializer=tf.zeros(2, tf.int32)) # raises ValueError
  tf.get_variable('a', initializer=tf.zeros(2, tf.int16)) # raises ValueError

b = tf.get_variable('b', initializer=tf.zeros(2))
with tf.variable_scope('', reuse=True):
  tf.get_variable('b', initializer=tf.zeros(2, tf.int32)) # works
```
Here the TF will compare dtype of existing variable `a` (tf.int32) against the dtype of the next get_variable call, which is tf.float32 (which is the default value for dtype argument of get_variable).

The resulting situations where get_variable raises ValueError seems counterintuitive."
8778,Getting started: anaconda and tensorflow,"Hi,

when you install tensorflow from anaconda, it does not provide any models as they are (probably) not part of the pip package. Could you update the documentation so it's states that you need to download the models manually?

Thanks!
"
8777,scope issue of '_linear' method in 'tensorflow.python.ops.rnn_cell_impl' module,"
In 'core_rnn_cell_impl' module _linear method, the arg 'scope' is not used in the method, easily causing variable conflicts. eg:
with tf.variable_scope(scope):
        zw = core_rnn_cell_impl._linear(self.hyper_output, self.hyper_embedding_size, False, scope=scope + ""z"")        
        alpha = core_rnn_cell_impl._linear(zw, dimensions, False, scope=scope + ""alpha"")
zw, alpha(forgive me for the indent in format) although defined in their specific scope, they will collide each other for using the same variable name.

I look up the 'core_rnn_cell_impl' code and I find the 'scope' arg has not been truly used.
This is the 1.0.0 version:
scope = vs.get_variable_scope()
with vs.variable_scope(scope) as outer_scope:

This is the 0.12.0 version:
with vs.variable_scope(scope or ""Linear""):

Please check if it's truly the problem, thanks a lot !"
8776,tf.case giving unexpected result in TF 1.0.1,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I posted this SO question on 2017-03-10 which was never answered: http://stackoverflow.com/questions/42728235/tensorflow-why-is-tf-case-giving-me-the-wrong-result

### Environment info
Operating System: `Linux 312e492cd9df 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`

Installed version of CUDA and cuDNN: none

Installed from: I'm running this on official tensorflow-devel Docker image for 1.0.1 (`gcr.io/tensorflow/tensorflow:1.0.1-devel`)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import tensorflow as tf

global_step = tf.Variable(0, dtype=tf.int64)
train_op = tf.assign(global_step, global_step + 1)

learning_rate = tf.Variable(0.1, dtype=tf.float32, name='learning_rate')

# Update the learning_rate tensor conditionally
# When global_step == 2, update to 0.01
# When global_step == 4, update to 0.001
case_tensors = [
    (tf.equal(global_step, 2), tf.constant(0.01, dtype=tf.float32)),
    (tf.equal(global_step, 4), tf.constant(0.001, dtype=tf.float32)),
]
cases = [(pred, lambda: fn_tensor) for pred, fn_tensor in case_tensors]
update = tf.case(cases, default=lambda: learning_rate)
updated_learning_rate = tf.assign(learning_rate, update)

print tf.__version__
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for _ in xrange(6):
        print sess.run([global_step, case_tensors, learning_rate, update, updated_learning_rate])
        sess.run(train_op)
```

### What other attempted solutions have you tried?

None

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

The above code prints the following output:
```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
1.0.1
[0, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1, 0.1]
[1, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1, 0.1]
[2, [(True, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]
[3, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]
[4, [(False, 0.0099999998), (True, 0.001)], 0.001, 0.001, 0.001]
[5, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]
```

I expect that the learning rate should get set to `0.0099999998` when the global step reaches 2.  However, even though the predicate for global_step==2 evaluates to True, the learning rate does not get set to `0.0099999998`, but rather gets set to `0.001` instead."
8775,Attention Decoder,"Hi,
@ebrevdo Eugene speaks here https://www.youtube.com/watch?v=RIR_-Xlbp7s (29:50) about a class AttentionDecoder and says it's under development (as of Tensorflow Summit date), however I can't find this class anywhere in the master/r1.1/r1.0 tensorflow code. Do you know when it will be added?"
8774,can not import tensorflow ,"I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: zsx-All-Series
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
"
8773,Can I use tensorflow on Windows10 with c++ and gpu support,"I wish to use tensorflow on windows10 with c++ and gpu support. Is there any pre build SDK for windows? Or I need to compile from sources?

when I tried to compile on windows with cmake and MSVC, smaple trainer cannot build correctly because of some header files missing, such as ""graph.pb.h"". Is there any solutions?

ENV:
windows10 vs2015 cmake3.6.2"
8772,Symbol not found: _SSLCreateContext in Python REPL,"I have installed tensorflow 1.0.1 (cpu version) on Mac as following .
 
`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.1-py2-none-any.whl`

At the end of installation, I got the following message:
`Successfully installed tensorflow-1.0.1`

But unable to import the libraries in python.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:
Mac version:
```
Software:
    System Software Overview:
      System Version: Mac OS X 10.7.5 (11G63)
      Kernel Version: Darwin 11.4.2
      64-bit Kernel and Extensions: Yes

```
Python version: 
```
$ python
Python 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) 
[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 

```
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
$ ls -l /path/to/cuda/lib/libcud*
ls: /path/to/cuda/lib/libcud*: No such file or directory
```
If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
$ python -c ""import tensorflow; print(tensorflow.__version__)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Symbol not found: _SSLCreateContext
  Referenced from: /Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```
"
8771,Tensorflow 1.0 RNN weights already exists,"i  got a error  
```
`  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 639, in _get_single_variable
    name, """".join(traceback.format_list(tb))))
ValueError: Variable lstm_def/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:
```

```python
this is my code:
 `def lstm(batch):      
    w_in=weights['in']
    b_in=biases['in']
    input=tf.reshape(X,[-1,input_size])  
    input_rnn=tf.matmul(input,w_in)+b_in
    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit]) 
    
    #cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)  
    with tf.variable_scope('cell_def'):
    	cell=tf.contrib.rnn.BasicLSTMCell(rnn_unit)
    	init_state=cell.zero_state(batch,dtype=tf.float32)
    with tf.variable_scope('lstm_def'):
    	output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32) 

    #output_rnn,final_states=tf.nn.rnn_cell.LSTMCell(cell, input_rnn,initial_state=init_state, dtype=tf.float32)
    #with tf.variable_scope('lstm'):
    #output_rnn,final_states=tf.contrib.rnn.static_rnn(cell, input_rnn,initial_state=init_state,dtype=tf.float32)
    output=tf.reshape(output_rnn,[-1,rnn_unit]) 
    w_out=weights['out']
    b_out=biases['out']
    pred=tf.matmul(output,w_out)+b_out
    return pred,final_states
```"
8770,tf.contrib.seq2seq.sequence_loss with tf.nn.sampled_softmax_loss,"There maybe an incompatible matmul when use `tf.contrib.seq2seq.sequence_loss` together with `tf.nn.sampled_softmax_loss`. `sampled_softmax_loss` need a rank 2 tensor as its label, however in `sequence_loss`, the label has been reshape to [-1], which will raise an error:
ValueError: Shape must be rank 2 but is rank 1 for 'sequence_loss/sampled_softmax_loss/MatMul_1' (op: 'MatMul') with input shapes: [50], [?,20].

```
    batch_size = 5
    max_step = 10
    dim = 20
    vocab_size = 100

    logits = tf.constant(np.random.randn(batch_size, max_step, dim),
                         tf.float32)
    targets = tf.constant(np.random.randint(vocab_size, size=(batch_size, max_step)),
                         tf.int32)
    target_weights = tf.constant(np.ones((batch_size, max_step)), tf.float32)
    proj_w = tf.constant(np.random.randn(vocab_size, dim), tf.float32)
    proj_b = tf.constant(np.zeros(vocab_size), tf.float32)

    def _sampled_loss(labels, logits):
        labels = tf.cast(labels, tf.int64)
        logits = tf.cast(logits, tf.float32)
        return tf.cast(
                        tf.nn.sampled_softmax_loss(
                            proj_w,
                            proj_b,
                            labels,
                            logits,
                            num_sampled=20,
                            num_classes=vocab_size),
                        tf.float32)

    softmax_loss_f = _sampled_loss

    loss = tf.contrib.seq2seq.sequence_loss(
                    logits,
                    targets,
                    target_weights,
                    softmax_loss_function=softmax_loss_f)

    sess = tf.Session()
    print sess.run(loss)
```
This error can be fixed if the I change line 81 in contrib/seq2seq/python/ops/loss.py:
`crossent = softmax_loss_function(labels=array_ops.reshape(targets, [-1, 1]), logits=logits_flat)`

tensorflow version: 1.01"
8769,AttributeError: module 'tensorflow' has no attribute 'confusion_matrix',"I am using GPU version of tensorflow 0.12.1 and python3.5 for building a CNN network. 
When I am trying to compute confusion matrix it gives me the error. 

`confusion = tf.confusion_matrix(labels = y_, prediction = y, num_classes = model.No_Classes)`

y_ is the input labels that I am giving to the network and y is the output of my CNN network. model.No_Classes = 4 an integer value.

is there package missing in this version ?
"
8764,configure hangs on download and aborts,"Hi,
I am trying to compile tensorflow from source.
when I run ./configure, things start to be downloaded, but generaly, it will hang for ~10s at a large download (>~30MB), and aborts with such error:
`ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:449:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:elemental_ir_emitter'.
ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:176:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:ir_emitter'.
ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:32:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:cpu_compiler'.
ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/python/BUILD:2571:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:swig', but this target could not be found because of: no such package '@swig//': Error downloading [http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://pilotfiber.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/swig/swig-3.0.8.tar.gz: Tried to reconnect at offset 7,935,828 but server didn't support it.
ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/python/BUILD:2571:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:templates', but this target could not be found because of: no such package '@swig//': Error downloading [http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://pilotfiber.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/swig/swig-3.0.8.tar.gz: Tried to reconnect at offset 7,935,828 but server didn't support it.
ERROR: Evaluation of query ""deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.
`

I once managed to download manually the incriminated files from a browser, and then decompress them manually in the temporary folder where bazel can find it, but it is rather cumbersome to do so.

### Environment info
Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: cuda8.0.44, cudnn5.1.5 (but this happens even when I ./configure without gpu support)

The commit hash (12a98726e769e988f6368a029ec2f5b0ac3ccbd4), but it has been doing this for as long as I have tried to compile from source (v < 0.10).

bazel version:
`Build label: 0.4.5`
`Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar`
`Build time: Thu Mar 16 12:19:38 2017 (1489666778)`
`Build timestamp: 1489666778`
`Build timestamp as int: 1489666778`

"
8762,`tf.device()` issue,"I posted my issue about In-graph replication for Distributed Tensorflow on StackOverflow: http://stackoverflow.com/questions/43036201/in-graph-replication-for-distributed-tensorflow. I wonder if there is a bug in terms of `tf.device()`, as I expect each process has its own log instead of print out message that belongs to other block of `tf.device()` like my example there. Thanks!"
8760,[bug] tf.shape,"Please expose the `optimize` flag of the `array_ops.shape_internal` for `tf.shape`.

The current `tf.shape` use `optimize=True` as default, but sometimes we do not want `tf.shape` return the constant. The resulting graph can not be imported by the android runtime."
8759,Simple add tensor example fails,"### Environment info
Operating System: Ubuntu 17.04

Installed version of CUDA and cuDNN:

```bash
stefano@stefano-PC:~$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root   556000 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root   415432 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 84163560 Mär 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 84163560 Mär 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 84163560 Mär 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 root root 70364814 Mär 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

>>> import tensorflow; print(tensorflow.__version__)
1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
import tensorflow as tf

sess = tf.Session()

a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b  # + provides a shortcut for tf.add(a, b)

print(sess.run(adder_node, {a: 3, b:4.5}))
print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))

```

Output:

```
stefano@stefano-PC:~/Dokumente/Programming/Python/TensorflowCoreTutorial$ /home/stefano/tensorflow/bin/python3 /home/stefano/Dokumente/Programming/Python/TensorflowCoreTutorial/src/main.py
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.7085
pciBusID 0000:22:00.0
Total memory: 5.93GiB
Free memory: 52.19MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:22:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 52.19M (54722560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 46.97M (49250304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
0.0
[ 0.  0.]
```

Expected result should be

```
7.5
[ 3.  7.]
```
It seems, tensorflow reads uninitialized memory, because sometimes it prints some random numbers."
8756,Error installing from source,"I'm trying to update tensorflow from the latest source. I'm running Ubuntu 14.04, with cuda 8.0 and cudnn 5.1.5. I was previously running r0.11 with no problem.

I pulled the new source. After running ./config and enabling cuda, I try to build with bazel:

> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

and I'm getting the following error:

> ERROR: /home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
> 	File ""/home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD"", line 4
> 		error_gpu_disabled()
> 	File ""/home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
> 		fail(""ERROR: Building with --config=c..."")
> ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
> ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD.

Note where it says ""Please re-run ./configure and enter 'Y' at the prompt to build with GPU support."" There is no such prompt when I run ./configure. I type 'Y' when it prompts if I want to build with cuda support and I give the correct cuda and cudnn versions and paths."
8755,tf.contrib.seq2seq.sequence_loss documentation incorrect,"The function returns a tensor with rank = 0, 1, 2 depending on its arguments. This is inconsistent with its docstring's description of the return:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L58

I will submit a PR soon to fix the doc.



NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

irrelevant

### Environment info
Operating System: 

irrelevant

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8754,"tf.nn.max_pool works incorrectly with option:  data_format=""NCHW"" on GPU","The max_pooling operation seems to be ignorant of the data_format option of ""NCHW"" and treat the input data as ""NHWC""

>>> with tf.device('/gpu:0'):
...     a=tf.zeros((100,3,224,224))
...     b=tf.nn.max_pool(a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=""SAME"",data_format=""NCHW"")
... 
>>> b
<tf.Tensor 'MaxPool:0' shape=(100, 2, 112, 224) dtype=float32>

I think the shape of the 'b' variable should be (100,3,112,112) instead of (100, 2, 112, 224). The max pooling operations seems to operate on the wrong axis.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None 
### Environment info
Operating System:
Ubuntu LTS 16.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   558720 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn_static.a


If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.1


### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
with tf.device('/gpu:0'):
...     a=tf.zeros((100,3,224,224))
...     b=tf.nn.max_pool(a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=""SAME"",data_format=""NCHW"")
... 
b
<tf.Tensor 'MaxPool:0' shape=(100, 2, 112, 224) dtype=float32>

### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8753,TFDBG 'PyFunc' is not found in partition graphs.,"When using tensorflow debugger, the program crashes with the following message :


> Traceback (most recent call last):
>   File ""farn_train.py"", line 470, in <module>
>     train()
>   File ""farn_train.py"", line 448, in train
>     _, loss_value = sess.run([train_op, loss])
>   File ""/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 470, in run
>     run_end_resp = self.on_run_end(run_end_req)
>   File ""/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 275, in on_run_end
>     self._dump_root, partition_graphs=partition_graphs)
>   File ""/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 524, in __init__
>     self._load_partition_graphs(partition_graphs, validate)
>   File ""/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 782, in _load_partition_graphs
>     self._validate_dump_with_graphs()
>   File ""/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 928, in _validate_dump_with_graphs
>     datum.node_name)
> ValueError: Node name 'PyFunc' is not found in partition graphs.
> 

### Environment info
Operating System:
Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

> rw-r--r-- 1 root root   560184 10月 31 11:03 /usr/local/cuda/lib64/libcudadevrt.a
> lrwxrwxrwx 1 root root       16 10月 31 11:03 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
> lrwxrwxrwx 1 root root       19 10月 31 11:03 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
> -rwxr-xr-x 1 root root   394472 10月 31 11:03 /usr/local/cuda/lib64/libcudart.so.8.0.27
> -rw-r--r-- 1 root root   737516 10月 31 11:03 /usr/local/cuda/lib64/libcudart_static.a
> -rwxr-xr-x 1 root root 79337624 11月  5 08:55 /usr/local/cuda/lib64/libcudnn.so
> -rwxr-xr-x 1 root root 79337624 11月  5 08:55 /usr/local/cuda/lib64/libcudnn.so.5
> -rwxr-xr-x 1 root root 79337624 11月  5 08:55 /usr/local/cuda/lib64/libcudnn.so.5.1.5
> -rw-r--r-- 1 root root 69756172 11月  5 08:55 /usr/local/cuda/lib64/libcudnn_static.a


If installed from binary pip package, provide:

1. A link to the pip package you installed: 
https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 
'1.0.1'


"
8752,"ERROR,I tried to compile Tensorflow source code in 'compute_21' ","I tried to compile Tensorflow source code in 'compute_21' 

### My computer environment info
    Ubuntu 16.04
    CUDA 8.0
    CUDNN 5.1
    GCC 4.9


The commit hash : a23f5d7f7757623a4ea8c6e1d743d178a0c561c5
 Build label: 0.4.5



### tried solutions 
https://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196542169
Replace the loop on L253--256 with the following:
`nvccopts += r'-gencode=arch=compute_20,\""code=sm_21,compute_20\"" '`

### Error Logs 
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h(519): error: no instance of function template ""Eigen::internal::SumReducer<T>::reducePacket [with T=Eigen::half]""

10 errors detected in the compilation of ""/tmp/tmpxft_00000f04_00000000-7_l2loss_op_gpu.cu.cpp1.ii"".
ERROR: /home/bingco/tensorflow/tensorflow/core/kernels/BUILD:2611:1: output 'tensorflow/core/kernels/_objs/l2loss_op_gpu/tensorflow/core/kernels/l2loss_op_gpu.cu.pic.o' was not created.
ERROR: /home/bingco/tensorflow/tensorflow/core/kernels/BUILD:2611:1: not all outputs were created or valid."
8751,Loading new operation - Already Exists Error,"# Problem
When loading a new op using `load_op_library`, the ***AlreadyExistsError*** is thrown.
Here is the simple code:
```python
import os
import h5py
import csv
import time
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
select_module = tf.load_op_library('./pixel_selector.so')
....
```

# Environment Info
TF v1.0.1 installed from source on Linux Ubuntu 14.04 with NVIDIA GPU Titan X and Python 2.7.6
"
8749,s ufw,Sorry about this. No clue how this got posted. :(
8746,Gradients are not averaged when using GradientDescentOptimizer,"When using the GradientDescentOptimizer the weights are updated using `var.device(d) -= grad * lr();`. The gradients are not averaged as we could expect. That why my model was working fine with a mini-batch gradient descent but not with the batch gradient descent.

The trick is to use `GradientDescentOptimizer(1/m * 0.01)` where `m` is the size of the batch.

The corresponding StackOverflow issue is here: http://stackoverflow.com/questions/43027109/tensorflow-weights-increasing-when-using-the-full-dataset-for-the-gradient-desce
"
8745,TFRecordWriter doesn't throw an error when disk partition gets out of space,"## Problem
When tensorflow.python_io.TFRecordWriter(path) is initialized with a path leading to a device out of free storage space, one can still write() and close() it without receiving an error or exception. The resulting file is empty (0B).

## Environment info
TF v.1.0.1 installed from pip3 (package tensorflow-gpu) with Python 3.4.2 on a Linux server.

## Minimal example
    writer = tf.python_io.TFRecordWriter(output_file)
    for i in files_in_shard:
      # ....
      # [prepare record]
      writer.write(example.SerializeToString())
    writer.close()
Working example e.g. https://github.com/tensorflow/models/blob/master/inception/inception/data/build_image_data.py"
8744,DC-ASGD(Delay Compensated Asynchronous Stochastic Gradient Descent)?,"DC-ASGD is Microsoft's very useful algorithm for distributed asynchronous training. Compared with the ordinary ASGD algorithm, DC-ASGD has no significant loss in speed, but can get almost the same effect as Sequential SGD. As far as I know, other mainstream deep learning open source tools have implemented this algorithm, such as: CNTK, Mxnet, Paddle and so on. But in Tensorflow I have not found similar modules.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
[https://github.com/Microsoft/CNTK/issues/1295](https://github.com/Microsoft/CNTK/issues/1295)
[https://github.com/PaddlePaddle/Paddle/issues/185](https://github.com/PaddlePaddle/Paddle/issues/185)
[https://github.com/dmlc/mxnet/pull/3614](https://github.com/dmlc/mxnet/pull/3614)

### What other attempted solutions have you tried?
I tried to implement this algorithm in Tensorflow by myself.  I do not have enough ability to do this now.

### The link address of the paper
[Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning](https://arxiv.org/abs/1609.08326)"
8743,Issue when installing from source,"Operating System: Ubuntu 16.04
gcc version: 5.3.0
CUDA: 8.0
cuDNN: v5.1.10 
commit hash: `bbe056e5a0ab81b67fcb6053400812b3d5805fc7`
bazel version: 0.4.5

I've installed CUDA and cuDNN correctly, and I'm trying to install TF from source. After performing the configure, when running the command: 
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

an error shows that:
`INFO: From Compiling external/nccl_archive/src/libwrap.cu.cc:
gcc: error trying to exec 'as': execvp: No such file or directory`
`ERROR: /home/machine/.cache/bazel/_bazel_machine/3ded171561e3ac30c3410854211a1a62/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/libwrap.cu.pic.o' was not created.`
`ERROR: /home/machine/.cache/bazel/_bazel_machine/3ded171561e3ac30c3410854211a1a62/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.`
`INFO: Elapsed time: 120.107s, Critical Path: 57.71s`

It seems that there's something wrong with my gcc. However, I've reinstalled it and the error still occurs.
I wanna know if it is the issue of cross-platform options."
8742,Broken Link in README.md,"In https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/README.md,
the link of _Get the recommended Bazel version listed in os_setup.html_ which is pointing to https://www.tensorflow.org/versions/master/get_started/os_setup#source is broken."
8741,temperature will be higher when I using two different cards in the same machine,"Hey, guys, I am trying to train a CNN model. My machine has 2 K40. When I run my model in the second card, the temperature  will be higher than running in the first card, and the training will be slower and slower.


"
8738,configure error,"1. git clone https://github.com/tensorflow/tensorflow
2. cd tensorflow
3. ./configure

Since our server has no GPU, we configure cuda option with no but error occurs by cuda.

`ERROR: /home/kesl/kms_tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl:828:18: unexpected keyword 'environ' in call to repository_rule(implementation: function, *, attrs: dict or NoneType = None, local: bool = False).
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'third_party/gpus/cuda_configure.bzl' has errors.
` 

But if I configure same thing with branch r1.0 version, it works."
8734,"Tensorflow GPU ""no known devices"" after update from version 0.12.1 to 1.0.1 - everything was fine before","### Environment info

Operating System:

Ubuntu Linux 16.04

Installed version of CUDA and cuDNN: 

CUDA 8.0.44
cuDNN 5.1.10
Driver version 367.57.0

Output of `ls -l /path/to/cuda/lib/libcud*`:

-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 root root 70364814 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn_static.a

If installed from binary pip package, provide:

A link to the pip package you installed: I don't know what you mean by that :(

Tensorflow versions:
0.12.1 before the issue
1.0.1 caused the issue

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

On an AWS g2.2xlarge instance with
    * Ubuntu 16.04, 
    * CUDA and cuDNN versions as listed above
    * tensorflow-gpu 0.12.1 installed using pip
run the example code from the [tensorflow ""Using GPUs"" website](https://www.tensorflow.org/tutorials/using_gpu).

The output for me is:

```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0
I tensorflow/core/common_runtime/direct_session.cc:255] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0

MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
b: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] b: (Const)/job:localhost/replica:0/task:0/gpu:0
a: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:827] a: (Const)/job:localhost/replica:0/task:0/gpu:0
[[ 22.  28.]
 [ 49.  64.]]
```

Now update Tensorflow

`pip install tensorflow-gpu --upgrade`

and run the example code again. Now the output for me is:

```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Device mapping: no known devices.
I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:

MatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0
b: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] b: (Const)/job:localhost/replica:0/task:0/cpu:0
a: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] a: (Const)/job:localhost/replica:0/task:0/cpu:0
[[ 22.  28.]
 [ 49.  64.]]
```

It's not even giving me any hints as to what might be the problem, it just says ""no known devices"". 

How come the GRID K520 was a known device **just** before I updated Tensorflow and now it isn't anymore?

Nothing else happened in the meantime apart from updating Tensorflow, no other changes to the system were made in any way (at least not from my side).

### Logs or other output that would be helpful

nvidia-smi output:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |
| N/A   39C    P0    38W / 125W |      0MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```"
8731,Error from 'bazel build' after r1.1.0: TensorFlow is not configured to build with GPU support,"### Environment info
Operating System: Ubuntu Desktop 16.04

Installed version of CUDA and cuDNN:  8.0, 5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
lrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a
-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a
-rw-r--r-- 1 root root 69756172 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn_static.a
lrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so
```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): 
bbe056e5a0ab81b67fcb6053400812b3d5805fc7 (master branch)

2. The output of `bazel version`:

```
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
```

----------------------------------------------------------------------

### If possible, provide a minimal reproducible example. 


* __Summary__: bazel build says TensorFlow is not configured with GPU support even though ./configure confirmed CUDA support will be enabled. Main error output seems to be (full error shown further down):
```
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. 
```
Please note that I've done this many times so (hopefully) I don't think I'm overlooking some common mistake on my end. Details below.

* __./configure info__: Here are the non-default (i.e. not just hitting enter) configuration flags I give to ./configure and its corresponding outputs:

```
./configure 
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Do you wish to use jemalloc as the malloc implementation? [Y/n] 
jemalloc enabled
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
  /home/brandon/bin/caffe/python
  /usr/local/boost_1_63_0
Using python library path: /usr/local/lib/python3.5/dist-packages
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5
Please specify a list of comma-separated Cuda compute capabilities you want to build with: 6.1 
```

* __Error from bazel build__: Then I run bazel build as follows with the error message below. 

```
 bazel build --config=opt --config=cuda --local_resources 8192,4.0,1.0 //tensorflow/tools/pip_package:build_pip_package
ERROR: /home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
	File ""/home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD"", line 4
		error_gpu_disabled()
	File ""/home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/error_gpu_disabled.bzl"", line 3, in error_gpu_disabled
		fail(""ERROR: Building with --config=c..."")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD.
INFO: Elapsed time: 0.199s
```

-------------------------------------------------------------------------------

I'm running a GTX 1080. If it's useful as a sanity-check, I've included a portion of nvidia-smi below:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |
|  0%   45C    P8    17W / 240W |    347MiB /  8110MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

Any idea why bazel thinks TF configured without GPU support? Also, I should note that ./configure did not run initially, but then I ran ``sudo apt-get upgrade bazel``` and that fixed the issue.  __My best guess is this is related to ./configure not playing nice with the new bazel upgrade__ or something similar. This was the first time I've had to upgrade bazel in awhile just to run ./configure. Thanks for any help/suggestions. "
8729,conv1d_transpose not implemented,"I would like to perform a conv1d_transpose, but I can’t see any implementation in tensorflow.

I guess that inserting zeros between each values and then applying a regular conv1d would do the job ?

(principle explained hereafter with conv2d_tranpose : https://github.com/tensorflow/tensorflow/issues/4306)

But I can't find a way to implement this kind of operation with tensorflow :

import numpy as np
arr = np.arange(1, 10)                 # array([1, 2, 3, 4, 5, 6, 7, 8, 9])
np.insert(arr, slice(1, None), 0)  # array([1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9])

Any ideas ?

Thank you"
8727,NameError: name 'core' is not defined,"when i run a command in the terminal
```
c:\python
>>> import tensorflow as tf 
```
it show the error 
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""E:\Tools\Python35\lib\site-packages\tensorflow\__init__.py"", line 51, in <module>
    del core
NameError: name 'core' is not defined
```
i have a searching but don't find an answer, i would happy that someone gives me the answer
"
8726,Retraining inception tutorial bazel error,"Hi , I am following along this tutorial https://www.tensorflow.org/tutorials/image_retraining , I performed all the steps and they went fine even training but when I entered the command below for evaluating the model , I got an error after few minutes 

```
bazel build tensorflow/examples/label_image:label_image && \
bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \
--output_layer=final_result \
--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg
```

```
ERROR: /home/saurabh/saurabh/tensorflow/tensorflow/core/kernels/BUILD:1468:1: C++ compilation of rule '//tensorflow/core/kernels:resource_variable_ops' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 106 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
No space left on device
No space left on device
No space left on device
No space left on device
No space left on device
Target //tensorflow/examples/label_image:label_image failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 201.779s, Critical Path: 150.97s
saurabh@saurabh-MS-7923:~/saurabh/tensorflow$ 

```"
8725,semantic about softmax_cross_entropy_with_logits,"In [https://www.tensorflow.org/get_started/mnist/beginners](url), it says:

> we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b)

In [https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits](url), it says

> logits: Unscaled log probabilities.

Here is the question, take the following code for example:

```
z = tf.matmul(x, W) + b)
cost = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=the_lables)
```

**z** is not log probability, why pass it to the parameter **logits**?
"
8724,Error occurred in Tensorflow version 1.0.1 date: 26th March 2017.,"I just reinstalled Tensorflow on my laptop this morning and got this problem. 
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'what the hell?'

![tensorflow](https://cloud.githubusercontent.com/assets/10723417/24329241/5ffd36fe-123b-11e7-8298-c957a13b3141.PNG)

"
8723,Disturbing Grammatical error...,"Grammatical error with this sentence...
""In this lab, we will be using transfer learning, which means we are starting with a model that has been already trained on another problem""
Correction: ""has already been trained on another problem""


"
8721,Issue while executing https://github.com/suriyadeepan/easy_seq2seq,"I tried running execute.py from https://github.com/suriyadeepan/easy_seq2seq however, I'm facing trouble with this.

Not sure if I'm doing something wrong, but a suggestion or a solution would be great help.

This is the dump from my Windows CMD prompt:

`C:\Users\myPC\Downloads\easy_seq2seq-master\easy_seq2seq-master>python execu
te.py`

`Traceback (most recent call last):`

`  File ""execute.py"", line 31, in <module>`
`    import seq2seq_model`
`  File ""C:\Users\myPC\Downloads\easy_seq2seq-master\easy_seq2seq-master\seq2
seq_model.py"", line 28, in <module>`
`    from tensorflow.models.rnn.translate import data_utils`
`ImportError: No module named 'tensorflow.models'`
"
8720,split not working correctly,"```python
import tensorflow as tf
import numpy as np

model_input = tf.placeholder(tf.float32, shape=(1, 10))
model_split = tf.split(model_input, [4], axis=1)[0]

input = np.zeros((1, 10))
session = tf.Session()
split = session.run(model_split, feed_dict = { model_input: input })

# the size of the evaluated tensor does not match the size of the tensor
assert split.shape == (1, 10)
assert model_split.get_shape() == (1, 4)

model_b = tf.zeros((1, 4))

# this is okay adding things of the same shape
session.run(model_b+model_b, feed_dict = { model_input: input })

# check that model_b and model_split have the same shape
assert model_b.get_shape() == model_split.get_shape()

# this line crashes because the shape of model_split + model_b are not compatible.
# although the assert just checked that the sized match. This is the error message:
#          InvalidArgumentError (see above for traceback): Incompatible shapes: [1,10] vs. [1,4]
session.run(model_split + model_b, feed_dict = { model_input: input })
```

NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Windows

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
See #2 below

If installed from binary pip package, provide:

1. A link to the pip package you installed:
How do I get this?
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

(C:\Users\television2\Anaconda3) C:\cygwin64\home\television2\nn\hierarchy>python -c ""import tensorflow; print(tensorflow.__version__)""
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
1.0.0

"
8718,Incorrect reference for tf.learn in Linear Model tutorials,"This [page](https://www.tensorflow.org/tutorials/linear) and [this](https://www.tensorflow.org/tutorials/wide) mentions `tf.learn` multiple times. 
However, looking at the links and source code, I believe that `tf.learn` should actually be either `tf.contrib` or `tf.contrib.learn`, not `tf.learn`.

(I guess `tf.learn` was the old name, but got renamed but the tutorial is still outdated?)"
8717,*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***,"I run : python train.py --train_data_pattern='/vol/vssp/msos/yx/audioset/audioset_v1_embeddings/unbal_train/*.tfrecord' --frame_features=True --model=DbofModel --feature_names=""audio_embedding"" --feature_sizes=""128"" --train_dir=tmp_model/frame_level_DbofModel_unbal

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
INFO:tensorflow:/job:master/task:0: Tensorflow version: 1.0.1.
INFO:tensorflow:/job:master/task:0: No checkpoint file found. Building a new model.
INFO:tensorflow:Using batch size of 1024 for training.
*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***
Aborted (core dumped)


Distributor ID: Ubuntu
Description:    Ubuntu 14.04.5 LTS
Release:        14.04
Codename:       trusty
"
8715,RecordReader reads disk without buffer,"tensorflow::io::RecordReader calls RandomAccessFile::Read() directly, without go through IoBuffer.
And RandomAccessFile::Read() will call pread(2) or ReadFile OS API.
If there is an IoBuffer between them, it could reduce a lot of syscalls. 

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System:
Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
None

if installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
c7b80d51da4fb6d51ea54a0bdf2601afa379d60c

2. The output of `bazel version`
(compiled by cmake and vs 2017)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
None


### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful

"
8713,Configure should work offline,"Currently, `configure` set some environment variables and invoke `bazel fetch`. When the fetch fails, due to network issue for example, the environment variables are also lost. I tried to `source ./configure`, but the shell died instantly. The only way I found to workaround this is to replace `bazel fetch` with `bash` in `configure`, then manually run `bazel fetch` a few times until it succeed. IMHO, `configure` should do the configuration separately instead of as part of `bazel fetch`."
8712,major memory allocation/copying overhead in Android Inference Library,"There seems to be a major overhead in [`TensorFlowInferenceInterface.feed(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java#L257)
When running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.

So I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in [`Tensor.create(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L128) which includes first [allocating the memory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L204) and then copying over from the passed Buffer.

Although this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.

The chart is really packed with `FloatBuffer.put(...)`

Why isn't it possible to use the just newly created Buffer directly?
Or why isn't the final Buffer filled directly from the Array?"
8710,Random crashing in c2c fft,"I'm having issues with the c2c fft. I would think this was more an issue with my code except that it seems to be non-deterministic. As a result I'm having a hard time getting a minimal example. I'm working on a recurrent neural net, in which the recurrent cell involves an fft (and an ifft), and the crash seems to occur more often as I increase the depth (number of recurrent cells chained together), or as I increase the size of the tensor being fft'd. The problem doesn't seem to scale with the product of these two, though, so I don't immediately think it's a memory issue: rather, it happens only if both are above some sort of weak cutoffs. (40 and 80 causes problems for me consistently.) This is on a GPU with 1.7GB of memory, and not much else in the computation graph, so I don't think I'm hitting constraints there. It's also very odd that the error is so random: I can typically run through 100 batches (each of 1 element) before it crashes, although this number seems random. I find the error message pretty inscrutable -- it reads:

```
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_fft.cc:169] failed to create cuFFT batched plan:2
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_fft.cc:111] failed to run cuFFT routine cufftSetStream: 1
Traceback (most recent call last):
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: c2c fft failed : in.shape=[2,110]
         [[Node: stack_wrapper_14/FFT = FFT[_device=""/job:localhost/replica:0/task:0/gpu:0""](stack_wrapper_14/Pad/_535)]]
         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2694_stack_wrapper_34/shift_exp"", tensor_type=DT_COMPLEX64, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""stacknn_test.py"", line 300, in <module>
    results = sess.run([merged, loss], feed_dict={xs: x, y_true: y})
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: c2c fft failed : in.shape=[2,110]
         [[Node: stack_wrapper_14/FFT = FFT[_device=""/job:localhost/replica:0/task:0/gpu:0""](stack_wrapper_14/Pad/_535)]]
         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2694_stack_wrapper_34/shift_exp"", tensor_type=DT_COMPLEX64, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'stack_wrapper_14/FFT', defined at:
  File ""stacknn_test.py"", line 254, in <module>
    (cell_output, state) = cell(inp, state)
  File ""stacknn_test.py"", line 162, in __call__
    tf.pad(state_stack, [[0,0], [0, self._stack_size]])
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 812, in fft
    result = _op_def_lib.apply_op(""FFT"", input=input, name=name)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Timeroot\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): c2c fft failed : in.shape=[2,110]
         [[Node: stack_wrapper_14/FFT = FFT[_device=""/job:localhost/replica:0/task:0/gpu:0""](stack_wrapper_14/Pad/_535)]]
         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2694_stack_wrapper_34/shift_exp"", tensor_type=DT_COMPLEX64, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```

and I could only find one stack overflow question about this: http://stackoverflow.com/questions/41720751/ignoring-warning-unable-to-load-cudnn-dso-effecting-fft-computation which ended up only getting some comment about the cudnn DSO. I don't have the cudnn DSO either, but if /that's/ what's causing this problem, I would be surprised.

Using Tensorflow version 1.0.1, the Windows GPU build. Python version 3.5.3. The machine has a single GPU with compute capabilities 3.0. It has no problem loading cublas64_80.dll, cufft64_80.dll,  nvcuda.dll, and curand64_80.dll, but I don't have a cudnn dso."
8709,gcc: error: unrecognized command line option '-fno-canonical-system-headers',"when I run :
`bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package`

INFO: Found 1 target...
ERROR: /home/qs/.cache/bazel/_bazel_qs/081cd1dbca77dcff65c775e7e860e873/external/farmhash_archive/BUILD.bazel:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/qs/.cache/bazel/_bazel_qs/081cd1dbca77dcff65c775e7e860e873/execroot/tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64/:/usr/lib32/:/usr/lib/x86_64-linux-gnu/::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \
    PATH=/bin:/usr/bin:/home/qs/mysoft/jdk1.8/bin \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d '-frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o' -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/farmhash_archive/src/farmhash.cc -o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
gcc: error: unrecognized command line option '-fno-canonical-system-headers'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1.003s, Critical Path: 0.49s

How to solve this problem?"
8708,Permissions error when building with bazel,"### Environment info
Operating System:
MacOS Sierra 10.12.3
CUDA Version 8.0.62
CUDNN version 5.1.10
Bazel Version: Build label: 0.4.5

git HEAD is c7b80d51da4fb6d51ea54a0bdf2601afa379d60c

**Seems to be a permissions issue of some sort where it can't execute?**

Not sure what permissions to change so just tried to rebuild with bazel clean but I don't think that's the issue. Looked up a bunch of similar cases but they are out of date.
### Logs 
Original Command was 
```
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
Error after running for 3556s was
```
ERROR: /Users/Kevin/tensorflow/tensorflow/cc/BUILD:388:1: Executing genrule //tensorflow/cc:remote_fused_graph_ops_genrule failed: bash failed: error executing command
  (cd /private/var/tmp/_bazel_Kevin/e120589e94215f409fa40a9ac20b2fce/execroot/tensorflow && \
  exec env - 
  ... 
  ...


```"
8706,Broken links in TensorBoard README,"In https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md,
links for both ""TensorBoard: Visualizing Learning"" and ""TensorBoard: Graph Visualization"" lead to ""page not found"".  "
8705,Which is TensorFlow's best seq2seq? r1.0 or r1.1? Both are much different!!,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8704,TensorFlow drops the first batch?,"### Description

I'm trying to understand how TF generates batches from `TFRecord` file format, how to implement the basic idea of evaluate the whole validation dataset after a full epoch so I've done a small experiment.

Basically I've added an `image/id` key in the `TFRecord` file that looks like this:
```
'image/id': tf.FixedLenFeature(shape=[], dtype=tf.int64)
'id': slim.tfexample_decoder.Tensor('image/id')
```

The values of these ids are just numbers increasing from 0 to `num_samples - 1`. Then I'm reading the data like this:

```
...
data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset, shuffle=False)
raw_image, instance_id, label = data_provider.get(items=['image', 'id', 'label'])
...
num_threads = 1
images, instance_ids, labels = tf.train.batch(tensors=[image, instance_id, label],
                                                      batch_size=batch_size,
                                                      num_threads=num_threads,
                                                      capacity = batch_size,
                                                      allow_smaller_final_batch=False)        
...
```

I've used `shuffle=False`, `num_threads=1` and `capacity=batch_size` to ensure we are reading the instances in the validation set in order.

I've then defined a `train_step_fn` that evaluates a mini-batch from the validation set after each step. I'm doing this to check and test we are not returning instances with duplicate ids in the same batch. The code looks like this:

```
    def train_step_fn(session, *args, **kwargs):
        total_loss, should_stop = train_step(session, *args, **kwargs)
        curr_global_step = tf.train.global_step(session, global_step)
        curr_epoch = curr_global_step / validation_every_n_steps
        image, val_ids, val_loss, accuracy = session.run([validation_images, validation_ids, total_validation_loss, validation_accuracy])
        print(val_ids)
        def float_formatter(x): return ""%.4f"" % x
        tf.logging.info('after global step {} (epoch {}): validation loss = {}, validation accuracy = {}'
                            .format(curr_global_step, curr_epoch, float_formatter(val_loss), float_formatter(accuracy)))
        return [total_loss, should_stop]
```

Most of the output I'm seeing makes sense but the output from the first batch looks weird.

```
INFO:tensorflow:Starting Session.
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global step 1: loss = 4.1099 (3.70 sec/step)
[17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]
INFO:tensorflow:after global step 1 (epoch 0): validation loss = 4.6337, validation accuracy = 0.0000
INFO:tensorflow:global step 2: loss = 4.2118 (0.52 sec/step)
[33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]
INFO:tensorflow:after global step 2 (epoch 0): validation loss = 3.9538, validation accuracy = 0.0625
INFO:tensorflow:global step 3: loss = 4.0593 (0.75 sec/step)
[49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64]
INFO:tensorflow:after global step 3 (epoch 0): validation loss = 4.0773, validation accuracy = 0.0000
INFO:tensorflow:global step 4: loss = 3.7749 (0.64 sec/step)
[65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80]
INFO:tensorflow:after global step 4 (epoch 0): validation loss = 4.0847, validation accuracy = 0.0625
INFO:tensorflow:global step 5: loss = 3.7973 (0.68 sec/step)
[81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96]
INFO:tensorflow:after global step 5 (epoch 0): validation loss = 3.5591, validation accuracy = 0.1875
INFO:tensorflow:global step 6: loss = 3.5584 (0.72 sec/step)
[ 97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112]
INFO:tensorflow:after global step 6 (epoch 0): validation loss = 3.6849, validation accuracy = 0.1250
INFO:tensorflow:global step 7: loss = 3.5303 (0.70 sec/step)
[113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128]
INFO:tensorflow:after global step 7 (epoch 0): validation loss = 3.7812, validation accuracy = 0.1875
INFO:tensorflow:global step 8: loss = 3.2698 (0.85 sec/step)
[129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]
INFO:tensorflow:after global step 8 (epoch 0): validation loss = 3.3004, validation accuracy = 0.1875
INFO:tensorflow:global step 9: loss = 3.4842 (0.94 sec/step)
INFO:tensorflow:global_step/sec: 0.915008
[145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]
INFO:tensorflow:after global step 9 (epoch 0): validation loss = 4.0377, validation accuracy = 0.1875
INFO:tensorflow:global step 10: loss = 3.5635 (1.48 sec/step)
[178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193]
INFO:tensorflow:after global step 10 (epoch 0): validation loss = 3.7854, validation accuracy = 0.1875
INFO:tensorflow:global step 11: loss = 3.1773 (0.65 sec/step)
[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209]
```

I'm assuming the first mini-batch returned should be `[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]` instead of `[17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]`. Plus the fact that we are starting from 17 instead of 16 also bothers me a bit. It seems for some unknown reason TF is dropping a batch.

I understand this probably won't affect the model training / validation process at all but I'm still pointing this out in case this is hiding a more serious root cause."
8703,Debug tool not fully installed in Windows version,"First of All: WINDOWS environment!

I have installed the tensorflow 1.0 (Python 3.5, Cuda 8.0, Cudnn 5) in Windows using the official command pip3 install --upgrade tensorflow-gpu. Although there are some minor bugs, I have fixed them by myself, and the tensorflow mostly works OK. However, when using the tensorflow debug tool, I find it failed. 

The reason is that the python files in folder tensorflow\python\debug are not generated from the most updated proto files. For example, in the generated ""debug_pb2.py"", there are no ""global_step"" for Message DebugOptions. I checked the most updated tensorflow codes, the debug.proto does have that ""global_step"". 

I have done some manual modifications of the debug_pb2.py to add necessary parameters. Now the error is 

File ""C:\Users\XXX\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\debug\lib\debug_data.py"", line 519, in __init__
    raise IOError(""Dump root directory %s does not exist"" % dump_root)
OSError: Dump root directory C:\Users\XXX\AppData\Local\Temp\tfdbg_8u5q2tve does not exist

I have already run the codes under the administrator mode. 

My questions are:
1. Whether there is any method to just re-generate the debug tool related files using the most updated proto files stored in my local disk?
2. Is there any way to rebuild the tensorflow using the online source codes, which seem to be for Linux?
3. If not possible, is there anyway to manual fix the above error?

Thank you.
"
8702,[bug] bazel cudnn-version-check is broken on python 2.7 / ubuntu 14.04,"When I ./configure on /r1.1 or /r1.0, I get the following error:

```
Auto-Configuration Error: cuDNN version detected from /usr/local/cuda-8.0/include/cudnn.h (      5.      1. 5) does not match TF_CUDNN_VERSION (5.1.5)
```
For what it is worth, I counted the space in cudnn.h, and they match the number of spaces in that error message above. Not sure if related.
```
#define CUDNN_MAJOR      5
#define CUDNN_MINOR      1
#define CUDNN_PATCHLEVEL 5
```


```
$ ./configure
Please specify the location of python. [Default is /usr/bin/python]: 
Please specify optimization flags to use during compilation [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] 
jemalloc enabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
/usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5
Please specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
..........
ERROR: package contains errors: tensorflow/core/kernels/cloud.
ERROR: error loading package 'tensorflow/core/kernels/cloud': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl"", line 815
		_create_cuda_repository(repository_ctx)
	File ""/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl"", line 728, in _create_cuda_repository
		_get_cuda_config(repository_ctx)
	File ""/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl"", line 584, in _get_cuda_config
		_cudnn_version(repository_ctx, cudnn_install_base..., ...)
	File ""/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl"", line 311, in _cudnn_version
		auto_configure_fail(""cuDNN version detected from %s ...))
	File ""/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl"", line 93, in auto_configure_fail
		fail(""
%sAuto-Configuration Error:%s ...))

Auto-Configuration Error: cuDNN version detected from /usr/local/cuda-8.0/include/cudnn.h (      5.      1. 5) does not match TF_CUDNN_VERSION (5.1.5)
```

### Environment info
Operating System: Ubuntu 14.04 LTS

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`
```
r1.0, r1.1

Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
```

"
8701,InvalidArgumentError using tf.learn and eval,"See https://github.com/google/seq2seq/issues/103 for details and user logs.

TLDR; I'm using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue, as subsequent runs seem to consistently increase the shape size:

```
InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [1280,36240] and labels shape [6272]

InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2304,36240] and labels shape [6272]

InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,36240] and labels shape [6272]
```

Evaluation works independently when there is no training in progress. It also doesn't happen when using the CPU only.

I personally have run into similar issues before then multiple processes were trying to share the GPU, but that shouldn't be the case here."
8698,Graph Transforms | quantize_nodes | Android TensorFlow Inference | native error,"I am using the Graph Transforms Tool (built with Bazel) to prepare my graph for Android TensorFlow Inference.

When using `--transforms='quantize_weights'` everything works just fine.
But with `--transforms='quantize_weights quantize_nodes'` I get this:

```
I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
I/TensorFlowInferenceInterface: TensorFlow native methods already loaded
A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 22562 (obin.loremipsum)
        
        [ 03-24 19:19:10.194   375:  375 W/         ]
        debuggerd: handling request: pid=22562 uid=10173 gid=10173 tid=22562
```

I don't have any clue on what has happened.
I wasn't able to find anything related online."
8697,Lower libc version on tensorflow:1.1.0-rc0-devel,"Building libtensorflow.so with the Docker 1.10-devel-rc0 image results in a binary that requires glibc 2.23. Since that is aggressive for many current machines, might it make sense to reduce the dependency to eg 2.17? "
8696,TensorFlow hangs during training while using with tf.device('/device:CPU:0'):,"### Description

I'm trying to fine-tune an Inception-V1 model and my latest implementation is based on [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py). I've noticed that sometimes TF hangs (might be similar to Issue [#2788](https://github.com/tensorflow/tensorflow/issues/2788)) and I'm trying to figure out why. 

The following snippet is my `load_batch` function. I've also used this `session_config=tf.ConfigProto(operation_timeout_in_ms=60000)` to throw a `DeadlineExceededError` when things timed out.

I've noticed that if I remove the `with tf.device('/device:CPU:0')` line, I can run the model for a whole night without any issue. If I add this line back I'll get a `DeadlineExceededError` fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.

```
def load_batch(dataset, batch_size, height, width):
    dataset_basename = os.path.basename(dataset.data_sources)
    with tf.device('/device:CPU:0'):
        with tf.name_scope(name=dataset_basename):
            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)
            raw_image, label = data_provider.get(items=['image', 'label'])
            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))
            image = tf.cast(raw_image, tf.float32) / 255.0
            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)
            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))
            # TensorFlow recommendation:
            # min_after_dequeue + (num_threads + a small safety margin) * batch_size
            # https://www.tensorflow.org/programmers_guide/reading_data
            num_threads = 8
            images, labels = tf.train.batch(tensors=[image, label],
                                            batch_size=batch_size,
                                            num_threads=num_threads,
                                            capacity=(num_threads + 2) * batch_size,
                                            allow_smaller_final_batch=True)
    return images, labels
```

The error message is:
```
INFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)
2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed
INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'>, Timed out waiting for notification
INFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%
INFO:tensorflow:Finished training! Saving model to disk.
Traceback (most recent call last):
  File ""fine-tune.py"", line 215, in <module>
    run()
  File ""fine-tune.py"", line 211, in run
    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 752, in train
    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 960, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 788, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 234, in _run
    sess.run(enqueue_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 786, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 994, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1044, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1064, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification
```

Since I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.

This is the TensorBoard information from `/batch/fraction_of_320_full`:

<img width=""643"" alt=""image"" src=""https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png"">
"
8693,CUDA 8.0 with Xcode 8.0,I am not able to install CUDA 8.0 with Xcode 8.0 version. Everywhere i found the solution to downgrade the version of Xcode. Is there any other solution without downgrading the version of Xcode.
8692,CUDA 8.0 installation with Xcode 8,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8691,Problems to visualize Tensorboard ,"Hi guys

I have problems to visualize tensorboard on my browser. ( Google chrome )

At the moment I did 3 python files.  ( 2 models and input_data )

Everything works fine, and I don't have any errors with the terminal ( Docker --> IPython ---> Terminal )

I already install pandas, urllib3, sklearn, etc etc

The terminal say me, you can sulf on Tensorboad at 0.0.0.0:6006 ( example )

I try to launch the ip on my browser but I don't see anything.

---- Step by step ---

Step 1
[My file](http://imgur.com/h39C8nd)

Step 2

[Terminal](http://imgur.com/LeT1muD)

Step 3

[Browser](http://imgur.com/6ZQSm2T) 

I tried with different browers, disable the firewall but nothing :(

P.s I'm noob and don't judge me for this question.

Best regards,


"
8688, could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available,"When I used the tesla K40c train a VGG16, there was a problem leading the kernel died, restarting. Can some one help me resolve it?
The retails are 

- I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:01:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3461a00
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: GeForce GTX 750 Ti
major: 5 minor: 0 memoryClockRate (GHz) 1.1105
pciBusID 0000:03:00.0
Total memory: 978.50MiB
Free memory: 678.75MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: GeForce GTX 750 Ti, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: GeForce GTX 750 Ti, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
E tensorflow/stream_executor/cuda/cuda_driver.cc:1147] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available
F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed
E tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available


"
8687,[HDFS]OutOfRangeError cause java runtime error,"### Environment info
TensorFlow: v1.0.0 and v1.0.1
JDK: 1.8.0_111
Hadoop: 2.6.0
GCC: 4.8.2

### OutOfRangeError
I think the `tf.errors.OutOfRangeError` is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues. 

### How to reproduce 
I followed the guide of [reading data](https://www.tensorflow.org/programmers_guide/reading_data).
I used the recommended mnist example [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data).  

Firstly, convert MNIST dataset by this script: [convert_to_records.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py)

Secondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is `hdfs://host:port/tfrecords/mnist-data/`.

Then, train the MNIST network by this script: [fully_connected_reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py). And feed the `train_dir` argument with `hdfs://host:port/tfrecords/mnist-data/`.

Finally, you can get the train log and error log as below:
### Train log
```shell
Step 0: loss = 2.29 (1.795 sec)
Step 100: loss = 1.99 (0.045 sec)
Step 200: loss = 1.62 (0.045 sec)
Step 300: loss = 1.39 (0.043 sec)
Step 400: loss = 1.01 (0.048 sec)
Step 500: loss = 0.77 (0.043 sec)
Step 600: loss = 0.65 (0.045 sec)
Step 700: loss = 0.60 (0.043 sec)
Step 800: loss = 0.65 (0.043 sec)
Step 900: loss = 0.46 (0.043 sec)
```

### Error log
```shell
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /home/mind/projects/tf_hdfs/hs_err_pid43645.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
Aborted (core dumped)
```

### Ask for a better way
I know it's hard to avoid the `OutOfRangeError` because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception. "
8685,Unable to restore inception v3 retrained model,"I'm retraining the inception v3 model on my image dataset. I use tf.Saver() to store the model at the time of best validation accuracy. But when I'm using the stored model to do the validation again, I don't get the same accuracy. 

Please let me know if I'm missing anything specifically with inception v3 model. I want store the entire inception v3 graph along with newly added layers, so that I can reuse it normally.

code snippet : 
saver.save(sess=sess, save_path=save_path + ""best_valid"", global_step=i)

Thanks,
Rohit
"
8682,Retrained inception v3 model from retrain.py takes 8-10 seconds to classify an image vs 1-2 seconds taken by sample TFClassify application,"Hi,

I am using tensorflow image classification in my project. 

Initially i built and installed Camera Demo from GitHub link : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android

Case 1: On running this example application (**TFClassify**) it is taking 1 or 2 seconds to do classification and shows result.

_**(On the other hand)**_

Case 2: If I use my retrained inception V3 model (which I have retrained using retrain.py) in same example application by placing the retrained model and labels fie in assets package it is taking 8- 10 seconds to classify per image.

Example application is using inception 5h model i guess and retrain.py is using inception V3 - I am not sure that the difference in classification time is because of this difference in models used, if so I could not find a way to use inception 5h model in retrain.py, it is giving an error :

_ValueError: Requested return_element 'pool_3/_reshape:0' not found in graph_def._
 

Is there a way to reduce the image classification time for the retrained inception V3 model? 

Thanks   
 "
8680,Any roadmap for supporting windows 10 GPU for Python 3.6.1?,"Any plan or expected date?

for windows 10 x64 + python 3.6.1 x64 + CUDA"
8678,wget issue with links in workspace.bzl,"Hello,

I'm trying to build from the most recent master branch as of today and am getting this error during the ./configure step:

```ERROR: Evaluation of query ""deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))"" failed: errors were encountered while computing transitive closure.```

```every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:swig',```
```every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:templates'```

The links from a preliminary glance take much longer than expected albeit being relatively small.

I've already looked at these but the links are outdated.
https://github.com/tensorflow/tensorflow/issues/5432
https://github.com/tensorflow/tensorflow/issues/8619
https://github.com/tensorflow/tensorflow/issues/4312
...
And a lot more that don't seem effective

### Environment info
Operating System:
MacOS Sierra 10.12.3
CUDA Version 8.0.61
CUDNN version 5.1.10
Bazel Version: Build label: 0.4.5


ls -l /usr/local/cuda/lib/libcud* **output**

-rwxr-xr-x    1 root     wheel  13504 Jan 24 14:58 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x@  1 root     wheel  45 Jan 11 20:33 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x@  1 root     wheel  50 Jan 11 20:33 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x@  1 root     wheel  46 Jan 11 20:33 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x@  1 root     wheel  49  Jan 11 20:33 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
-rwxr-xr-x@ 1 Kevin   staff    82210088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn.5.dylib
-rwxr-xr-x@ 1 root     wheel  82210088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn.dylib
-rw-r--r--@  1 Kevin  staff    66197088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn_static.a

Tensorflow git version c7b80d5  

##Attempted Solution
Lots of changing the links to older versions. Also tried building Bazel by source for 0.4.3 and 0.4.5.
Been stuck on this for a couple of hourst. Following any previous solution seems to not work as other issues arise (Such as using Bazel 0.4.3). 

My very strong suspicion is that the links are down. 
```
wget http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz
```
will take an obscenely long amount of time and as a result timeout. My internet is fine (university) and all else is fine. My GPU is a NVIDIA 1080 GTX connected to my mac via thunderbolt cable. "
8676,Tensorflow causes whole python to crash with error Python frozen importlib._bootstrap,"If I try to run a computer vision script python crashes with no error message at all.
If I run with `python -m trace` python console goes in loop with a message repeated continuously that is unreadable because lines are printed endless too fastly. I can see some line related to `sre_compile.py` `text_format.py` `inspect.py` and `charmap codec can't encode \u2713` in `cp1252.py` and `from tensorflow.contrib.keras`

If I try to redirect the output to a file, the file content is
 
```
<frozen importlib._bootstrap_external>(376): <frozen importlib._bootstrap_external>(1258): <frozen importlib._bootstrap_external>(1255): <frozen importlib._bootstrap_external>(1256):  --- modulename: _bootstrap_external, funcname: _path_join
<frozen importlib._bootstrap_external>(59): <frozen importlib._bootstrap_external>(60):  --- modulename: _bootstrap_external, funcname: <listcomp>
...again and again
```

This isn't the first time that a tensorflow issue cause the whole python to go crazy.
I have tried both today nightly build, past days nightly build, stable version, no way to run and since causes python crash It's very hard to debug. 

Tensorflow stable and nightly 1.0.1 on windows 10 x64.


To reproduce this issue run `evaluate.py` scipt of TensorBox project"
8674,wrong libjpeg version for 1.1.0-rc0 gcr docker image ,"While running the Mandelbrot Set on the latest docker image on gcr.io, there is the following error:

IOError: encoder error -2 when writing image file

In the console: there is real error message:
Wrong JPEG library version: library is 90, caller expects 80

It shall be the latest image as of Mar 23.
16715e3f77e1 
tagged: 1.1.0-rc0  latest

Full command is:
$ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow

The same code works fine on the Docker Hub image.
$ docker run -it -p 8888:8888 tensorflow/tensorflow"
8671,regarding best approach to update tensorflow without affecting other installed packages,"We have maintained a virtual environment to hold all the deep learning-related development projects. In this environment, we have installed `tensorflow, keras, theano` and quite a lot other` python` libraries for project purposes.

Currently, I would like to update `tensorflow `from `0.12.0-rc0` to `1.0`. To keep the current development environment, which is shared by other users, I don’t want to change the current virtual environment at all.

I am thinking of the following options,

1) Create another virtual environment, have `tensorflow `with the latest version of `1.0`

2) Are there any ways to just copy or link all the other python packages in the old environment to the new virtual environment? I just do not want to re-install everything.

Thank you for the advice!"
8670,Outputs of a model are different depending on the batch size (tested for inception models),"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/1299

### Environment info
Operating System: ArchLinux
CUDA: 8.0.61
cuDNN: 5.1.10
TensorFlow: 1.0.1 (both with and without GPU support for python 2.7)

I  have also tested it with the latest nightly build:
http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-cp27-none-linux_x86_64.whl

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Minimun script for testing: https://gist.github.com/jorgemf/e1f1a97ad39c02ace3576d1248327d2a

I have tested it with the inception_v1 and inception_v4 models (commented in the code)

### What other attempted solutions have you tried?

None

### Logs or other output that would be helpful.

Logs for inception_v1 for the GPU and CPU based on my previous script.  You should expect the same output regardless of the batch size. Output must be the same for all the items of the batch as the input is the same image (which doesn't happen for one of the cases of this logs, I can upload the image and the checkpoint to reproduce that exactly case if required)

Please note that in this examples the errors in the output or the values of the layers are really small. You could think it is due to any weird conversion python does, but I have trained models based on inception_v1 where the outputs varies from 0.4 to 1.04.

The logs shows the output of the model (num_classes=1) and whether the layers outputs are exactly the same as with previous batch sizes or are different. Note that for some layers the outputs are the same regardless of the batch size, you start seeing errors with deeper layers. I only show few layers as example.

GPU
```
batch size: 1
        outputs: [  5.03433178e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        OK Mixed_5c
batch size: 2
        outputs: [  5.03433233e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        ERROR Mixed_4f  Different values than batch_sizes 1.
        ERROR Mixed_5c  Different values than batch_sizes 1.
batch size: 10
        outputs: [  5.03433233e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        ERROR Mixed_4f  Different values than batch_sizes 1. Same values as batch_sizes 2.
        ERROR Mixed_5c  Different values than batch_sizes 1, 2.
batch size: 16
        outputs: [  5.03433233e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        ERROR Mixed_4f  Different values than batch_sizes 1, 2, 10.
        ERROR Mixed_5c  Different values than batch_sizes 1, 2, 10.
batch size: 20
        outputs: [  5.03433287e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        ERROR Mixed_4f  Different values than batch_sizes 1, 16. Same values as batch_sizes 2, 10.
        ERROR Mixed_5c  Different values than batch_sizes 1, 2, 16. Same values as batch_sizes 10.
```

CPU
```
batch size: 1
        outputs: [  5.03432962e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        OK Mixed_5c
batch size: 2
        outputs: [  5.03433016e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        ERROR Mixed_5c  Different values than batch_sizes 1.
batch size: 10
        outputs (ERROR: outputs must have the same value): [  5.03433178e-13   5.03433178e-13   5.03433178e-13   5.03433178e-13
   5.03433178e-13   5.03433178e-13   5.03433178e-13   5.03433178e-13
   5.03433016e-13   5.03433016e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2.
batch size: 16
        outputs: [  5.03433178e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2, 10.
batch size: 20
        outputs: [  5.03433178e-13]
        OK inputs
        OK Conv2d_1a_7x7
        OK MaxPool_3a_3x3
        OK Mixed_3b
        OK Mixed_4b
        OK Mixed_4f
        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2, 10, 16.
```
"
8669,Input ... was not previously added to ShapeRefiner,"When calling `TF_GraphImportGraphDef`, some graphs result in the error ""Input (...) was not previously added to ShapeRefiner"". 

I'm still working on a MWE of a graph that causes this, but just wanted to double-check first that this does indeed indicate a bug. So far the graphs that trigger this involve back edges, although I'm not sure yet if that's necessary or sufficient. "
8667,"tf.nn.softmax -- the ""name"" argument is passed to the internal flattened tensor","Hi, if I use tf.nn.softmax with the `name` argument, I expect the output tensor to be given this name.
However, the output tensor is always a Reshape op named `Reshape_...`, and the passed name refers to an internal (flattened) tensor.

Steps to reproduce (TF v1.0.1):
```
In [1]: import tensorflow as tf
In [2]: a = tf.nn.softmax(tf.ones([2, 8, 20]), name=""softmax_custom_name"")
In [3]: print a
Tensor(""Reshape_1:0"", shape=(2, 8, 20), dtype=float32)
In [4]: print tf.get_default_graph().get_tensor_by_name(""softmax_custom_name:0"")
Tensor(""softmax_custom_name:0"", shape=(16, 20), dtype=float32)
```

Here is the relevant code from the master branch:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1501
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1519

Is this the expected behaviour? In this case it would probably make sense to mention it in the documentation?

Otherwise I'd submit a fix."
8666,`dynamic_rnn`'s docstring refers to a non-existing `rnn` function,"Here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L392

The docstring of the `dynamic_rnn` function refers to a non-existing `rnn` function.
I don't know if it got deleted or moved somewhere else."
8665,freeze_graph not initializing tables,"I am not sure if this is an actual bug or if its expected but undocumented behavior.

I have a model that uses multiple lookup tables created via string_to_index. I freeze the model like so:
`bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/tmp/tf/graph.pbtxt 
--input_checkpoint=/tmp/tf/model.ckpt-0 --output_graph=/tmp/ticker_classifier.pb 
--output_node_names=sigmoid --initializer_nodes=init_all_tables`

However when the model is reloaded and I attempt to run it I get an error ""Table not initialized."" I get exactly the same resulting file whether I specify initializer_nodes or not. The behavior I was expecting was for the model to contain the lookup tables in a ready to use state for inference but I don't know if that is an unreasonable expectation.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I have not seen any issues related to this. I previously posted about this here http://stackoverflow.com/questions/42916383/how-to-properly-freeze-a-tensorflow-graph-containing-a-lookuptable

### Environment info
Operating System: MacOS and Linux (CentOS 7)

Installed version of CUDA and cuDNN: None

If installed from source, provide 
1. The commit hash (`git rev-parse HEAD`) 07bb8ea2379bd459832b23951fb20ec47f3fdbd4
2. Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I have been unable to make a small example but I can spend more time on it if needed.

### What other attempted solutions have you tried?

The workaround is to add init_all_tables to the output_nodes and then run init_all_tables before feeding the session examples for inference. This does have the side effect of needing to distribute the source files for the tables to the same path on all nodes that was originally used for training.
"
8664,SyntaxError: name '_googletest_temp_dir' is used prior to global declaration,"I am following tutorial https://www.tensorflow.org/tutorials/image_retraining

The code successfully complied with Bazel but failed to run with command

`bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`

With a very strange ERROR:

> Traceback (most recent call last):
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 79, in <module>
    import tensorflow as tf
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 104, in <module>
    from tensorflow.python.platform import test
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 38, in <module>
    from tensorflow.python.framework import test_util as _test_util
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 46, in <module>
    from tensorflow.python.platform import googletest
  File ""/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 118
    global _googletest_temp_dir
SyntaxError: name '_googletest_temp_dir' is used prior to global declaration

When I run the **retrain.py** directly with python, the error did not show up. It only happens when I use bazel. Can anyone explain why?"
8661,graph_editor.graph_replace produces WARNING,"Since version 1.0.0 the method `tensorflow.contrib.graph_editor.graph_replace` raises the following warning:

WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.

 

### Minimal reproducible example 
```python 
import tensorflow as tf
import tensorflow.contrib.graph_editor as ge
a = tf.constant(1)
b = tf.constant(2)
c = tf.constant(3)
d = a +  b
e = ge.graph_replace([d], {a: c})
```

### Output log
INFO:tensorflow:Copying op: add
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
INFO:tensorflow:Finalizing op: add

"
8660,Error when creating a summary inside a tf.while_loop in a multi-GPU setup (like CIFAR-10),"Why diagnosing a problem with batch normalized RNNs, I tried to add a summary of the population statistics in my batch normalized cell, but then when I try to run the merged summaries I get the following error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Merge/MergeSummary' has inputs from different frames. The input 'tower_2/rnn/while/multi_rnn_cell/cell_4/gru_cell/gates/r/rnn/multi_rnn_cell/cell_4/gru_cell/gates/r/pop_var_0' is in frame 'tower_2/rnn/while/tower_2/rnn/while/'. The input 'tower_3/rnn/while/multi_rnn_cell/cell_4/gru_cell/gates/r/rnn/multi_rnn_cell/cell_4/gru_cell/gates/r/pop_var_0' is in frame 'tower_3/rnn/while/tower_3/rnn/while/'.

This is on TensorFlow 1.0. I'll try to get a reduced testcase."
8659,My tensorboard can not show anything. ,(The code didn't show any errors and my Brower is firefox)What's wrong with it ?
8658,Saving best models instead of most recent models with tf.train.Saver.,"Most of the time I want to save the best models instead of the most recent models. Doing so using tf.train.Saver requires to choose when to save a model and to delete the worst model (which might not be the oldest) ""manually"".

A method to save the N best models (according to some user defined value) would be nice."
8657,Numeric stability CPU vs GPU,"### Numerical stability CPU vs GPU
My lab partner and I found that in some cases, our scripts would run into numerical instability issues when using GPU, while they would run fine using a CPU. To minimally reproduce this issue, we've come up with the following script:
```python
import tensorflow as tf
import numpy as np

if __name__ == ""__main__"":

    t = tf.placeholder(tf.float32, [])
    division = tf.constant(1, dtype='float32') / tf.pow(1.5, -t)

    sess = tf.Session()
    for count in range(100000000):
        val = sess.run(division, feed_dict={t: count})

        if np.isnan(val) or np.isinf(val) or val < 0:
            print(count)
            break

```
If we run this with CUDA_VISIBLE_DEVICES="""", we see that the script reaches 219 steps before being either `nan` or `inf`. If we use GPU instead, we see that the script reaches **only 216** steps before being either `nan` or `inf`.

### Operating system
Ubuntu 16.04

Installed version of CUDA and cuDNN:
```
-rw-r--r-- 1 root root   560184 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 root root   394472 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 root root   737516 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```
Tensorflow version (from binary):
0.12.0"
8656,Inconsistent behaviour between CPU and GPU gradient step operation,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: Linux Mint 17.2 Rafaela

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
ls -l /usr/local/cuda/lib64/libcud*
/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
 /usr/local/cuda/lib64/libcudart.so.8.0.61
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so
/usr/local/cuda/lib64/libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5.1.10
/usr/local/cuda/lib64/libcudnn_static.a

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.1.0-rc0
This bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
git rev-parse HEAD: 49380d638bdc2983722c9a2831ca74770dc6ba43
2. The output of `bazel version`
Build label: 0.4.5
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = """"

import tensorflow as tf
a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run([train_op,a]))
print(sess.run(a))
```
The two print statements evaluate to 
```
[None, 3.0]
3.0
```

When allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:

```
[None, 1.0]
3.0
```
So apparently when using the GPU, the Variable `a` is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent. 

### What other attempted solutions have you tried?
A few things I have observed:
```
import os
# os.environ[""CUDA_VISIBLE_DEVICES""] = """"

import tensorflow as tf

with tf.device(""/cpu:0""):
    a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init_op)
print(sess.run([train_op,a]))
print(sess.run(a))
```
evaluates to 
```
[None, 3.0]
3.0
```

The following code
```
import os
# os.environ[""CUDA_VISIBLE_DEVICES""] = """"

import tensorflow as tf

a = tf.Variable(1.0)
loss = (a-2.0)**2
optimizer = tf.train.GradientDescentOptimizer(1.0)
train_op = optimizer.minimize(loss)
init_op = tf.global_variables_initializer()
with tf.control_dependencies([train_op]):
    a = tf.identity(a)
sess = tf.Session()
sess.run(init_op)
print(sess.run(a))
```
evaluates to 
`3.0`. This seems to ""enforce"" the behaviour of CPU-only computation when using the GPU.


Also the initial example has been run on three different machines, all with the same TitanX GPU Model.

What am I missing? Any help would be greatly appreciated.

Matthias
"
8655,Bazel does not support execution of Python interpreters via labels yet,"I following tutorial https://www.tensorflow.org/tutorials/image_retraining
when I try to use the retrainer I got this error
Bazel does not support execution of Python interpreters via labels yet error

What can I do ?
![screen shot 2017-03-23 at 6 10 04 pm](https://cloud.githubusercontent.com/assets/18695558/24242844/0fa487aa-0ff4-11e7-9cf6-3fe8cadde0ae.png)


My setting:
Python 2.7.13
Tensorflow Release 1.0.1
bazel Build label: 0.4.5-homebrew
"
8654,AttributeError: module 'tensorflow' has no attribute 'layers',"Hi there,

The source code of tutorial
* https://www.tensorflow.org/tutorials/layers
* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py

cannot run on my MacAir.

It just reported: AttributeError: module 'tensorflow' has no attribute 'layers'
Is that a version problem?"
8652,AttributeError: 'NoneType' object has no attribute 'TF_NewStatus,"In the latest sources the issue: https://github.com/tensorflow/tensorflow/issues/3388 is fixed in the thensorflow/python/client/session.py, But I now receive the following error instead:
```
Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fb2e08cee48>>
Traceback (most recent call last):
  File ""$HOME/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 595, in __del__
AttributeError: 'NoneType' object has no attribute 'TF_NewStatus'

```
I compiled the model manually using the latest version sources and using Bazel 0.4.5. The same seems to be true as with the issue 3388: The error is not always popping up and the run seems to have ended normally. However, now and then to above error message is displayed. "
8651,how to add conditions to tf.Print,"can I add a certain condition to the tf.Print op?
something like print after certain iterations or something similar
I have tf.Print to print out the losses during training, therefore there is too many lines I couldn't possibly read"
8649,Who can tell me the way to use tf.confusion_matrix()?,"I want to craete a confusion matrix to get the result of test data performance, I found that there is a function 'tf.confusion_matrix()', but I don't know how to use it, anybody can tell me ?"
8648,"No module named tensorflow occurs, Windows 7  no GPU involved","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Same problem on Linux or MacOS.

### Environment info
Operating System: Windows 7

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
No

Tensorflow is well installed to a Windows 7 before posting this question. 
D:\Python35\Scripts>pip show tensorflow
---
Metadata-Version: 2.0
Name: tensorflow
Version: 1.0.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
Installer: pip
License: Apache 2.0
Location: d:\python35\lib\site-packages
Requires: numpy, six, protobuf, wheel
Classifiers:
  Development Status :: 4 - Beta
  Intended Audience :: Developers
  Intended Audience :: Education
  Intended Audience :: Science/Research
  License :: OSI Approved :: Apache Software License
  Programming Language :: Python :: 2.7
  Topic :: Scientific/Engineering :: Mathematics
  Topic :: Software Development :: Libraries :: Python Modules
  Topic :: Software Development :: Libraries
Entry-points:
  [console_scripts]
  tensorboard = tensorflow.tensorboard.tensorboard:main
You are using pip version 8.1.1, however version 9.0.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' comm
and.


Packages are all well installed (just for me cause this is the first day to Tensorflow.
Also, I've tried to install A ipython and it seems running well.
[1]: import tensorflow as tf
[2]: (I don't know what to do here and give an exit).


"
8647,Try to call tf.select but get AttributeError: 'module' object has no attribute 'select',"Here is the traceback information:
Traceback (most recent call last):
  File ""guidedBack.py"", line 149, in <module>
    grads = K.gradients(model.layers[-2].output[0, 0], model.layers[-6].layers[-2].output)[0]
  File ""/N/u/zehzhang/myTensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2108, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File ""/N/u/zehzhang/myTensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 482, in gradients
    in_grads = grad_fn(op, *out_grads)
  File ""guidedBack.py"", line 25, in _GuidedReluGrad
    return tf.select(0. < grad, gen_nn_ops._relu_grad(grad, op.outputs[0]), tf.zeros(grad.get_shape()[1:]))
AttributeError: 'module' object has no attribute 'select'

Anyone has any idea?"
8645,errors meet after django  call several times saver.restore,"

 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()



NotFoundError (see above for traceback): Key wd1/Adam_7 not found in checkpoint
	 [[Node: save_3/RestoreV2_133 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save_3/Const_0, save_3/RestoreV2_133/tensor_names, save_3/RestoreV2_133/shape_and_slices)]]

W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc4/Adam_9 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc3/Adam_11 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc3/Adam_10 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_9 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_8 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_7 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_6 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_10 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_11 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_11 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_10 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_9 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_8 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_7 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_6 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_11 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_10 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_9 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_8 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_7 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_6 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_6 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_7 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_8 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_9 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_11 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_10 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_5 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_4 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_3 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_5 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_4 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_3 not found in checkpoint


"
8644,unknown op: BestSplits (windows 10 + python 3.5.3),"After install tensorflow on windows 10 with python 3.5.3, I try to run hello world. but got some message

```bash
PS C:\Users\huzhifeng> python --version
Python 3.5.3
PS C:\Users\huzhifeng> python -m pip  install -U tensorflow
Collecting tensorflow
  Downloading tensorflow-1.0.1-cp35-cp35m-win_amd64.whl (14.7MB)
    100% |████████████████████████████████| 14.7MB 40kB/s
Collecting protobuf>=3.1.0 (from tensorflow)
  Downloading protobuf-3.2.0-py2.py3-none-any.whl (360kB)
    100% |████████████████████████████████| 368kB 73kB/s
Collecting numpy>=1.11.0 (from tensorflow)
  Downloading numpy-1.12.1-cp35-none-win_amd64.whl (7.7MB)
    100% |████████████████████████████████| 7.7MB 61kB/s
Collecting six>=1.10.0 (from tensorflow)
  Using cached six-1.10.0-py2.py3-none-any.whl
Collecting wheel>=0.26 (from tensorflow)
  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)
    100% |████████████████████████████████| 71kB 46kB/s
Collecting setuptools (from protobuf>=3.1.0->tensorflow)
  Downloading setuptools-34.3.2-py2.py3-none-any.whl (389kB)
    100% |████████████████████████████████| 399kB 119kB/s
Collecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow)
  Downloading appdirs-1.4.3-py2.py3-none-any.whl
Collecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow)
  Downloading packaging-16.8-py2.py3-none-any.whl
Collecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow)
  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56kB)
    100% |████████████████████████████████| 61kB 93kB/s
Installing collected packages: six, appdirs, pyparsing, packaging, setuptools, protobuf, numpy, wheel, tensorflow
  Found existing installation: setuptools 28.8.0
    Uninstalling setuptools-28.8.0:
      Successfully uninstalled setuptools-28.8.0
Successfully installed appdirs-1.4.3 numpy-1.12.1 packaging-16.8 protobuf-3.2.0 pyparsing-2.2.0 setuptools-34.3.2 six-1.
10.0 tensorflow-1.0.1 wheel-0.29.0
PS C:\Users\huzhifeng> tensorflow
PS C:\Users\huzhifeng> python
Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!');
>>> sess = tf.Session();
>>> print(sess.run(hello));
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('
op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'Hello, TensorFlow!'
>>> print(sess.run(hello));
b'Hello, TensorFlow!'
>>> print(sess.run(hello));
b'Hello, TensorFlow!'
```

"
8642,Graph with tf.cond can not be serialized correctly after calling remove_training_nodes.,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#3667 may be the same problem? But that thread is closed without a clear answer.

### Environment info
Operating System:

- Tensorflow 1.0.0
- Python 2.7
- Ubuntu 14.04

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
CUDA 8.0
cuDNN 5.1.10
*This problem occurs in both GPU and CPU environment.

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
The following code is to produce a `.pb` file that can be used for `tf.import_graph_def`.

```
import tensorflow as tf
from tensorflow.python.framework import graph_util

input_jpeg = tf.placeholder(tf.string, name='DecodeJpeg/contents')
image = tf.image.decode_jpeg(input_jpeg, channels=3)
image = tf.image.convert_image_dtype(image, dtype=tf.float32)

height = tf.constant(224)
width = tf.constant(224)
input_h = tf.shape(image)[0]
input_w = tf.shape(image)[1]

def resize_t(image_t, h=height, w=width):
  image2 = tf.expand_dims(image_t, axis=0)
  return image2

image_checked = tf.cond(
    (tf.less(input_h, height)), 
    lambda:resize_t(image), lambda:image)

shape_t = tf.shape(image_checked, name='shape_t')

with tf.Session() as sess:
  # freeze the graph and export
  output_graph_def = sess.graph_def
  output_graph_def = graph_util.convert_variables_to_constants(
        sess, sess.graph_def, ['shape_t'])
  output_graph_def = graph_util.remove_training_nodes(output_graph_def)
  with open('out.pb', 'wb') as f:
    f.write(output_graph_def.SerializeToString())
```

The `out.pb` file can be generated as expected, but when I import it using the code below:

```
import tensorflow as tf
with open('out.pb', 'rb') as f:
  graph_content = f.read()
graph_def = tf.GraphDef()
graph_def.ParseFromString(graph_content)
_ = tf.import_graph_def(graph_def, name='')
```

It failed with Message:
```
Traceback (most recent call last):
  File ""im.py"", line 6, in <module>
    _ = tf.import_graph_def(graph_def, name='')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 342, in import_graph_def
    % (input_name,)))
ValueError: graph_def is invalid at node u'cond/ExpandDims/dim': More inputs specified ('cond/Switch:1') than the op expects..
```
### What other attempted solutions have you tried?
when I removed the `graph_util.remove_training_nodes` call, everything works well.
So it seems that something goes wrong while removing the Identity nodes from the graph.


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
Exploring the `.pbtxt` generated with and without `graph_util.remove_training_nodes`,some Identity nodes are removed and the input of ExpandDims/dim is modified to connect to the output of Switch.(maybe?)
Part of `.pbtxt` file content without `graph_util.remove_training_nodes`:
```
^Kcond/Switch^R^FSwitch^Z^DLess^Z^DLess*^G
^AT^R^B0

1
^Mcond/switch_t^R^HIdentity^Z^Mcond/Switch:1*^G
^AT^R^B0

'
^Lcond/pred_id^R^HIdentity^Z^DLess*^G
^AT^R^B0

M
^Scond/ExpandDims/dim^R^EConst^Z^N^cond/switch_t*^K
^Edtype^R^B0^C*^R
^Evalue^R       B^G^H^C^R^@:^A^@
h
^Vcond/ExpandDims/Switch^R^FSwitch^Z^Mconvert_image^Z^Lcond/pred_id*^G
^AT^R^B0^A*
^F_class^R^V
^T^R^Rloc:@convert_image
a
^Ocond/ExpandDims^R
ExpandDims^Z^Xcond/ExpandDims/Switch:1^Z^Scond/ExpandDims/dim*

^DTdim^R^B0^C*^G
^AT^R^B0^A

```

Part of `.pbtxt` file content with `graph_util.remove_training_nodes`:
```
^Kcond/Switch^R^FSwitch^Z^DLess^Z^DLess*^G
^AT^R^B0

L
^Scond/ExpandDims/dim^R^EConst^Z^Mcond/Switch:1*^R
^Evalue^R       B^G^H^C^R^@:^A^@*^K
^Edtype^R^B0^C
`
^Vcond/ExpandDims/Switch^R^FSwitch^Z^Mconvert_image^Z^DLess*^G
^AT^R^B0^A*
^F_class^R^V
^T^R^Rloc:@convert_image
a
^Ocond/ExpandDims^R
ExpandDims^Z^Xcond/ExpandDims/Switch:1^Z^Scond/ExpandDims/dim*

^DTdim^R^B0^C*^G
^AT^R^B0^A
```"
8641,Cannot build Android demo - Bazel BUILD failed,"### Description

I've successfully built the [Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) a couple days ago but I've failed to build the app today. The error message looks like this:

    ERROR: /Users/hao.hu/GitHub/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3870:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: Process exited with status 1 [sandboxed].
    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]
              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                               ^~~~~~~~~~~~~~~~~~
    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function 'tensorflow::SplitVOpCPU<int, long long>::Compute' requested here
      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}
               ^
    tensorflow/core/kernels/split_v_op.cc:355:19: note: in instantiation of member function 'tensorflow::SplitVOpCPU<int, long long>::SplitVOpCPU' requested here
    TF_CALL_ALL_TYPES(REGISTER_SPLIT_LEN);
                  ^
    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue
              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                               ^~~~~~~~~~~~~~~~~~
                               static_cast<int>( )
    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]
              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                               ^~~~~~~~~~~~~~~~~~
    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function  'tensorflow::SplitVOpCPU<float, long long>::Compute' requested here
      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}
               ^
    tensorflow/core/kernels/split_v_op.cc:355:19: note: in instantiation of member function 'tensorflow::SplitVOpCPU<float, long long>::SplitVOpCPU' requested here
    TF_CALL_ALL_TYPES(REGISTER_SPLIT_LEN);
                      ^
    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue
              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                               ^~~~~~~~~~~~~~~~~~
                               static_cast<int>( )
    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]
              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                               ^~~~~~~~~~~~~~~~~~
    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function 'tensorflow::SplitVOpCPU<tensorflow::bfloat16, long long>::Compute' requested here
      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}
               ^
    tensorflow/core/kernels/split_v_op.cc:356:1: note: in instantiation of member function 'tensorflow::SplitVOpCPU<tensorflow::bfloat16, long long>::SplitVOpCPU' requested here
    REGISTER_SPLIT_LEN(bfloat16);
    ^
    tensorflow/core/kernels/split_v_op.cc:353:3: note: expanded from macro 'REGISTER_SPLIT_LEN'
      REGISTER_SPLIT(type, int64);
      ^
    tensorflow/core/kernels/split_v_op.cc:349:27: note: expanded from macro 'REGISTER_SPLIT'
                              SplitVOpCPU<type, len_type>);
                              ^
    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue
          prefix_dim_size, split_sizes_vec[i], suffix_dim_size};
                           ^~~~~~~~~~~~~~~~~~
                           static_cast<int>( )
    3 errors generated.
    Use --strategy=CppCompile=standalone to disable sandboxing for the failing actions.
    Target //tensorflow/examples/android:tensorflow_demo failed to build
    Use --verbose_failures to see the command lines of failed build steps.
    INFO: Elapsed time: 468.550s, Critical Path: 454.15s"
8640,"Java:  The TensorFlow library wasn't compiled to use AVX / SSE / FMA instructions, but these are available on your machine and could speed up CPU computations","I'm building a small project using the current java binding of TensorFlow - [tensorflow-java ](https://github.com/loretoparisi/tensorflow-java)
When running the examples of inception network I get this warning about the platform cpu instruction set that could be used

```
2017-03-23 01:17:20.243925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-23 01:17:20.243980: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-23 01:17:20.243986: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-03-23 01:17:20.243990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-23 01:17:20.243994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
```

Is it possibile to build the jar [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) enabling SSE, AVX and FMA instruction set?"
8633,Please provide an example how to use a model trained from scratch for image classification,"The following documentation of TensorFlow-Slim contains how to train a model from scratch, but it's not explained how to use the resulting checkpoint files for image classification.
https://github.com/tensorflow/models/tree/master/slim#training-a-model-from-scratch

Although it's possible to load the pre-trained models (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models) and use it for image classification with the example given in https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb, it seems not possible to simply use the checkpoint files generated by ""training from scratch"" in the same way.

Any example on how to use the newly generated checkpoints for image classification (for example with inception) would be appreciated."
8630,NAN returning for cost and optimizer for tensorflow.train.GradientDescentOptimizer (updated code),"Been working on this all day now and totally at a loss...  I keep getting Nan values for my cost function right away and i cannot tell why.  Any help much appreciated at this point....

I am running the following code with tensorflow GradientDescentOptmizer...

```
#!/usr/bin/env python

import tensorflow as tf
import numpy as np
import os
from   os.path import isfile, join
import argparse
import sys
import glob
import pandas as pd
import csv
import re
import tempfile
import urllib
import matplotlib.pyplot as plt

# Start basic regression
tf.reset_default_graph()

rng = np.random

# Parameters
learning_rate = 0.0001
training_epochs = 1000
display_step = 50

logs_path = '/tmp/tensorflow_logs/example'

# Bad Training Data
train_Y = np.asarray([  59.8000,   60.5000,   60.9000,   61.0000,   61.5000,   64.0000,   64.5000,
                        64.8000,   67.8000,   71.2000,   72.0000,   78.9000,   79.2000,   81.0000,
                        82.6000,   84.0000,   84.0000])
train_X = np.asarray([600., 760., 802., 568., 679., 865., 1103., 865., 896., 1068.,
                        769., 1062., 1123., 1081., 1137., 1137., 1137.])
# Good Training Data
#train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
#                     7.042,10.791,5.313,7.997,5.654,9.27,3.1])
#train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
#                     2.827,3.465,1.65,2.904,2.42,2.94,1.3])

print(train_X.dtype)
print(train_Y.dtype)
print(str(train_X))
print(str(train_Y))

n_samples = train_X.shape[0]

print(""Samples = %d"" % n_samples)

# tf Graph Input
X = tf.placeholder(""float"")
Y = tf.placeholder(""float"")

# Set model weights
W = tf.Variable(1.0, name=""weight"")
b = tf.Variable(1.0, name=""bias"")

# Construct a linear model
with tf.name_scope('Model'):
    pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
with tf.name_scope('Loss'):
    cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
with tf.name_scope('SGD'):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initializing the variables
init = tf.global_variables_initializer()

# summaries
tf.summary.scalar(""loss"",cost)
merged_summary_op = tf.summary.merge_all()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    # op to write logs to Tensorboard
    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            #print(""x=%f y=%f"" % (x,y))
            op, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={X: x, Y: y})

            summary_writer.add_summary(summary, epoch)

            # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c, summmary = sess.run([cost, merged_summary_op], feed_dict={X: train_X, Y:train_Y})
            summary_writer.add_summary(summary, epoch)                
              
            print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(c), \
            ""W="", sess.run(W), ""b="", sess.run(b))

    print(""Optimization Finished!"")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print(""Training cost="", training_cost, ""W="", sess.run(W), ""b="", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Bad Test Data
    test_Y = np.asarray([ 48.2000,  56.5000,  57.0000,  59.5000,  15.0000,  17.8000,  43.5000,  50.2000])
    test_X = np.asarray([ 549.,  710.,  568.,  825., 414.,  439.,  460.,  614. ])

    # Good Test Data
    #test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    #test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])


    print(""Testing... (Mean square loss Comparison)"")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print(""Testing cost="", testing_cost)
    print(""Absolute mean square loss difference:"", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()`
```

And get the following result...

> float64
> float64
> [  600.   760.   802.   568.   679.   865.  1103.   865.   896.  1068.
>    769.  1062.  1123.  1081.  1137.  1137.  1137.]
> [ 59.8  60.5  60.9  61.   61.5  64.   64.5  64.8  67.8  71.2  72.   78.9
>   79.2  81.   82.6  84.   84. ]
> Samples = 17
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
> Epoch: 0050 cost= nan W= nan b= nan
> Epoch: 0100 cost= nan W= nan b= nan
> Epoch: 0150 cost= nan W= nan b= nan
> Epoch: 0200 cost= nan W= nan b= nan
> Epoch: 0250 cost= nan W= nan b= nan
> Epoch: 0300 cost= nan W= nan b= nan
> Epoch: 0350 cost= nan W= nan b= nan
> Epoch: 0400 cost= nan W= nan b= nan
> Epoch: 0450 cost= nan W= nan b= nan
> Epoch: 0500 cost= nan W= nan b= nan
> Epoch: 0550 cost= nan W= nan b= nan
> Epoch: 0600 cost= nan W= nan b= nan
> Epoch: 0650 cost= nan W= nan b= nan
> Epoch: 0700 cost= nan W= nan b= nan
> Epoch: 0750 cost= nan W= nan b= nan
> Epoch: 0800 cost= nan W= nan b= nan
> Epoch: 0850 cost= nan W= nan b= nan
> Epoch: 0900 cost= nan W= nan b= nan
> Epoch: 0950 cost= nan W= nan b= nan
> Epoch: 1000 cost= nan W= nan b= nan
> Optimization Finished!
> Training cost= nan W= nan b= nan 
> 
> Testing... (Mean square loss Comparison)
> Testing cost= nan
> Absolute mean square loss difference: nan
> And get the following output...
> 

if I uncomment out the alternate array values for X amd Y everything works fine...

> float64
> float64
> [  3.3     4.4     5.5     6.71    6.93    4.168   9.779   6.182   7.59
>    2.167   7.042  10.791   5.313   7.997   5.654   9.27    3.1  ]
> [ 1.7    2.76   2.09   3.19   1.694  1.573  3.366  2.596  2.53   1.221
>   2.827  3.465  1.65   2.904  2.42   2.94   1.3  ]
> Samples = 17
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
> Epoch: 0050 cost= 8.580235481 W= 0.846042 b= 0.978224
> Epoch: 0100 cost= 5.489832401 W= 0.72321 b= 0.960839
> Epoch: 0150 cost= 3.522622824 W= 0.625209 b= 0.946956
> Epoch: 0200 cost= 2.270416975 W= 0.547022 b= 0.935869
> Epoch: 0250 cost= 1.473346949 W= 0.484644 b= 0.927012
> Epoch: 0300 cost= 0.965984821 W= 0.434878 b= 0.919934
> Epoch: 0350 cost= 0.643029690 W= 0.395175 b= 0.914275
> Epoch: 0400 cost= 0.437456042 W= 0.363499 b= 0.909749
> Epoch: 0450 cost= 0.306603283 W= 0.338229 b= 0.906127
> Epoch: 0500 cost= 0.223311335 W= 0.318069 b= 0.903225
> Epoch: 0550 cost= 0.170292616 W= 0.301985 b= 0.900899
> Epoch: 0600 cost= 0.136546493 W= 0.289155 b= 0.899031
> Epoch: 0650 cost= 0.115067258 W= 0.278921 b= 0.89753
> Epoch: 0700 cost= 0.101395160 W= 0.270757 b= 0.89632
> Epoch: 0750 cost= 0.092692636 W= 0.264245 b= 0.895344
> Epoch: 0800 cost= 0.087154001 W= 0.259051 b= 0.894554
> Epoch: 0850 cost= 0.083628759 W= 0.254909 b= 0.893912
> Epoch: 0900 cost= 0.081385054 W= 0.251606 b= 0.893389
> Epoch: 0950 cost= 0.079956941 W= 0.248972 b= 0.892961
> Epoch: 1000 cost= 0.079047829 W= 0.246872 b= 0.892606
> Optimization Finished!
> Training cost= 0.0790478 W= 0.246872 b= 0.892606 
> 
> Testing... (Mean square loss Comparison)
> Testing cost= 0.0796623
> Absolute mean square loss difference: 0.000614464


I am using tensorflow 1.01 libraries, and running on a mac OS Sierra...

Banging my head against the wall trying to figure this out. As you can see I tried to setup tensorboad to help, but seems I have limited success, get a graph but so far has not been any help debugging this...
"
8628,Build configuration error: TensorFlow is not configured to build with GPU support.,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Commit ""ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69"" fails to configure for me, the commit before that succeeds. Might be related to issue #8619 which was closed, although for me using bazel 0.4.5 does not work.

Fails with ""git checkout ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69""
http://pastebin.com/FHt0r5kS
Succeeds with ""git checkout 450386c513a4c68d6b77f9b475dc39b442775a86""
http://pastebin.com/3vGHik0G

### Environment info
Operating System: Ubuntu 16.04.2 LTS

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
jbunk@desktop:~/libraries$ cat /usr/local/cuda/version.txt 
CUDA Version 8.0.61
jbunk@sherman:~/libraries/Tensorflow-Github$ ls /usr/local/cuda/lib64/libcudn*
/usr/local/cuda/lib64/libcudnn.so  /usr/local/cuda/lib64/libcudnn.so.5  /usr/local/cuda/lib64/libcudnn.so.5.1.10  /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

Commit ""ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69""
```
jbunk@desktop:~/libraries$ bazel version
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

### What other attempted solutions have you tried?

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

Fails with ""git checkout ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69""
http://pastebin.com/FHt0r5kS
Succeeds with ""git checkout 450386c513a4c68d6b77f9b475dc39b442775a86""
http://pastebin.com/3vGHik0G"
8627,HDF5 library version mismatched error with latest Tensorflow build for windowsx64,"Due to a bug still present in Tensorflow 1.0.1 release for windows (see https://github.com/tensorflow/tensorflow/issues/8336)
I have installed the latest nightly build tensorflow cpu win 22-mar-2017 2.25.00 as suggested

Unfortunately with this version, now kernel crashes for a different reason and I get this error:

```
Warning! ***HDF5 library version mismatched error***
The HDF5 header files used to compile this application do not match
the version used by the HDF5 library to which this application is linked.
Data corruption or segmentation faults may occur if the application continues.
This can happen when an application was compiled by one version of HDF5 but
linked with a different version of static or shared HDF5 library.
You should recompile the application or check your shared library related
settings such as 'LD_LIBRARY_PATH'.
You can, at your own risk, disable this warning by setting the environment
variable 'HDF5_DISABLE_VERSION_CHECK' to a value of '1'.
Setting it to 2 or higher will suppress the warning messages totally.
Headers are 1.8.15, library is 1.8.18
      SUMMARY OF THE HDF5 CONFIGURATION
      =================================

General Information:
-------------------
                   HDF5 Version: 1.8.18
                  Configured on: 2017-03-06
                  Configured by: Visual Studio 14 2015 Win64
                 Configure mode: CMAKE 3.7.2
                    Host system: Windows-10.0.14393
              Uname information: Windows
                       Byte sex: little-endian
                      Libraries:
             Installation point: C:/Program Files/HDF5

Compiling Options:
------------------
               Compilation Mode:
                     C Compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
                         CFLAGS: /DWIN32 /D_WINDOWS /W3
                      H5_CFLAGS:
                      AM_CFLAGS:
                       CPPFLAGS:
                    H5_CPPFLAGS:
                    AM_CPPFLAGS:
               Shared C Library: YES
               Static C Library: YES
  Statically Linked Executables: OFF
                        LDFLAGS: /machine:x64
                     AM_LDFLAGS:
                Extra libraries: X:inclib-vc14-x64/zlib.lib;X:/inclib-vc14-x64/libsz.lib;X:/inclib-vc14-x64/libaec.lib
                       Archiver:
                         Ranlib:
              Debugged Packages:
                    API Tracing: OFF

Languages:
----------
                        Fortran: OFF
               Fortran Compiler:
          Fortran 2003 Compiler:
                  Fortran Flags:
               H5 Fortran Flags:
               AM Fortran Flags:
         Shared Fortran Library: YES
         Static Fortran Library: YES

                            C++: ON
                   C++ Compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
                      C++ Flags: /DWIN32 /D_WINDOWS /W3 /GR /EHsc
                   H5 C++ Flags:
                   AM C++ Flags:
             Shared C++ Library: YES
             Static C++ Library: YES

Features:
---------
                  Parallel HDF5: OFF
             High Level library: ON
                   Threadsafety: OFF
            Default API Mapping: v18
 With Deprecated Public Symbols: ON
         I/O filters (external):  DEFLATE DECODE ENCODE
                            MPE:
                     Direct VFD:
                        dmalloc:
Clear file buffers before write: ON
           Using memory checker: OFF
         Function Stack Tracing: OFF
      Strict File Format Checks: OFF

```
The script crashes when` tf.contrib.layers.convolution2d` is invoked

I'm using windows 10 x64 and I have installed all available update released for the others libraries

Here the list of packages of my setup
```
alabaster (0.7.10)
anaconda-clean (1.0)
anaconda-client (1.6.2)
anaconda-navigator (1.5)
anaconda-project (0.4.1)
appdirs (1.4.3)
argcomplete (1.8.2)
asn1crypto (0.22.0)
astroid (1.4.9)
astropy (1.3.1)
Babel (2.3.4)
backports.shutil-get-terminal-size (1.0.0)
beautifulsoup4 (4.5.3)
bitarray (0.8.1)
blaze (0.10.1)
bleach (2.0.0)
bokeh (0.12.4)
boto (2.46.1)
Bottleneck (1.2.0)
cffi (1.10.0)
chardet (2.3.0)
chest (0.2.3)
click (6.7)
cloudpickle (0.2.2)
clyent (1.2.2)
colorama (0.3.7)
comtypes (1.1.3)
conda (4.3.14)
conda-build (2.1.7)
conda-verify (2.0.0)
configobj (5.0.6)
contextlib2 (0.5.4)
cryptography (1.8.1)
cycler (0.10.0)
Cython (0.25.2)
cytoolz (0.8.2)
dask (0.14.1)
datashape (0.5.4)
decorator (4.0.11)
dill (0.2.6)
docutils (0.13.1)
dynd (c328ab7)
entrypoints (0.2.2)
et-xmlfile (1.0.1)
fastcache (1.0.2)
filelock (2.0.7)
Flask (0.12)
Flask-Cors (3.0.2)
gevent (1.2.1)
glue-core (0.10.1)
glue-vispy-viewers (0.7.2)
glueviz (0.10.1)
greenlet (0.4.12)
h5py (2.7.0)
HeapDict (1.0.0)
html5lib (0.999999999)
idna (2.5)
imagesize (0.7.1)
ipykernel (4.5.2)
ipython (5.3.0)
ipython-genutils (0.2.0)
ipywidgets (6.0.0)
isort (4.2.5)
itsdangerous (0.24)
jdcal (1.3)
jedi (0.10.0)
Jinja2 (2.9.5)
jsonschema (2.6.0)
jupyter (1.0.0)
jupyter-client (5.0.0)
jupyter-console (5.1.0)
jupyter-core (4.3.0)
lazy-object-proxy (1.2.2)
llvmlite (0.16.0)
locket (0.2.0)
lxml (3.7.3)
MarkupSafe (1.0)
matplotlib (2.0.0)
mccabe (0.6.1)
menuinst (1.4.4)
mistune (0.7.4)
mpmath (0.19)
multipledispatch (0.4.9)
nb-anacondacloud (1.2.0)
nb-conda (2.0.0)
nb-conda-kernels (2.0.0)
nbconvert (5.1.1)
nbformat (4.3.0)
nbpresent (3.0.2)
networkx (1.11)
nltk (3.2.2)
nose (1.3.7)
notebook (4.4.1)
numba (0.31.0)
numexpr (2.6.2)
numpy (1.12.1)
numpydoc (0.6.0)
odo (0.5.0)
olefile (0.44)
opencv-python (3.2.0.6)
openpyxl (2.4.5)
packaging (16.8)
pandas (0.19.2)
pandocfilters (1.4.1)
partd (0.3.7)
path.py (10.1)
pathlib2 (2.2.1)
patsy (0.4.1)
pep8 (1.7.0)
pickleshare (0.7.4)
Pillow (4.0.0)
pip (9.0.1)
pkginfo (1.4.1)
ply (3.10)
prompt-toolkit (1.0.13)
protobuf (3.2.0)
psutil (5.2.0)
py (1.4.33)
pyasn1 (0.2.3)
pycosat (0.6.2)
pycparser (2.17)
pycrypto (2.6.1)
pycurl (7.43.0)
pyflakes (1.5.0)
Pygments (2.2.0)
pylint (1.6.5)
PyOpenGL (3.1.0)
pyOpenSSL (16.2.0)
pyparsing (2.2.0)
pyreadline (2.1)
pytest (3.0.7)
python-dateutil (2.6.0)
pytz (2016.10)
pywin32 (220)
PyYAML (3.12)
pyzmq (16.0.2)
QtAwesome (0.4.4)
qtconsole (4.2.1)
QtPy (1.2.1)
requests (2.13.0)
rope-py3k (0.9.4.post1)
scikit-image (0.12.3)
scikit-learn (0.18.1)
scipy (0.19.0)
seaborn (0.7.1)
setuptools (34.3.2)
simplegeneric (0.8.1)
singledispatch (3.4.0.3)
six (1.10.0)
snowballstemmer (1.2.1)
sockjs-tornado (1.0.3)
Sphinx (1.5.3)
spyder (3.1.3)
SQLAlchemy (1.1.6)
statsmodels (0.8.0)
sympy (1.0)
tables (3.3.0)
tensorflow (1.0.1)
testpath (0.3)
toolz (0.8.2)
tornado (4.4.2)
traitlets (4.3.2)
unicodecsv (0.14.1)
wcwidth (0.1.7)
webencodings (0.5)
Werkzeug (0.12.1)
wheel (0.29.0)
widgetsnbextension (2.0.0)
win-unicode-console (0.5)
wrapt (1.10.10)
xlrd (1.0.0)
XlsxWriter (0.9.6)
xlwings (0.10.4)
xlwt (1.2.0)
```

"
8626,The reuse flag of tf.variable_scope doesn't work with tf.contrib.layers ?,"For example, I defined a simple network
```python
import tensorflow as tf
import tensorflow.contrib.layers as tcl

def model(x, reuse=None):
    with tf.variable_scope('foo', reuse=reuse) as scope:
        tcl.conv2d(x, 3, 3, 1)
```
And then, I try to call the model function twice **without** setting reuse=True for second call
```python
x1 = tf.placeholder(tf.float32, (10,10,3))
x2 = tf.placeholder(tf.float32, (10,10,3))
model(x1)
model(x2)
```
When using with tf.get_variable(), this will cause a error. However, it's not the case for tf.contrib.layers?
I tried to print out all the nodes in the graph with following codes
```python
for n in tf.get_default_graph().as_graph_def().node:
    print(n.name)
```
I got this result:
```
Placeholder
Placeholder_1
foo/Conv/weights/Initializer/random_uniform/shape
foo/Conv/weights/Initializer/random_uniform/min
foo/Conv/weights/Initializer/random_uniform/max
foo/Conv/weights/Initializer/random_uniform/RandomUniform
foo/Conv/weights/Initializer/random_uniform/sub
foo/Conv/weights/Initializer/random_uniform/mul
foo/Conv/weights/Initializer/random_uniform
foo/Conv/weights
foo/Conv/weights/Assign
foo/Conv/weights/read
foo/Conv/biases/Initializer/Const
foo/Conv/biases
foo/Conv/biases/Assign
foo/Conv/biases/read
foo/Conv/convolution/Shape
foo/Conv/convolution/dilation_rate
foo/Conv/convolution/ExpandDims/dim
foo/Conv/convolution/ExpandDims
foo/Conv/convolution/ExpandDims_1/dim
foo/Conv/convolution/ExpandDims_1
foo/Conv/convolution/Conv2D
foo/Conv/convolution/Squeeze
foo/Conv/BiasAdd
foo/Conv/Relu
foo_1/Conv/convolution/Shape
foo_1/Conv/convolution/dilation_rate
foo_1/Conv/convolution/ExpandDims/dim
foo_1/Conv/convolution/ExpandDims
foo_1/Conv/convolution/ExpandDims_1/dim
foo_1/Conv/convolution/ExpandDims_1
foo_1/Conv/convolution/Conv2D
foo_1/Conv/convolution/Squeeze
foo_1/Conv/BiasAdd
foo_1/Conv/Relu
```
It seems that there is only one copy of weights, instead of two.
So, even if ```reuse``` is not set to ```True```, the weights can still be shared ?

Is this the correct behavior?
Or did I miss anything?

I'm using Python3, Tensorflow 1.0.1, Ubuntu 16.04."
8625,"ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients,","I found a error when I try to train my models using the GradientDescentOptimizer(). There are my train code.
```
    y_prob = tf.sigmoid(layer_dropped)

    # Set a thresholds value
    y_pred = tf.floor(y_prob)

    # Loss Operation
    loss_op = tf.reduce_mean(tf.abs(y_true - y_pred))
    optimizer_op = tf.train.GradientDescentOptimizer(0.005).minimize(loss_op)
```

If I do not use the `tf.floor()` function in my loss operation, the training is OK. Just like this:
```
loss_op = tf.reduce_mean(tf.abs(y_true - y_prob))
optimizer_op = tf.train.GradientDescentOptimizer(0.005).minimize(loss_op)
```"
8624,Constant folding does not work across devices?,"I've been trying to understand tensorflow internals recently. I found in `tensorflow/core/common_runtime/direct_session.cc`, if I understand it correctly, that constant folding only take place at [#L1051](https://github.com/tensorflow/tensorflow/blob/ef56133461079f28b61b5a83a62685051408aadb/tensorflow/core/common_runtime/direct_session.cc#L1051) after graph partitioning, so constants won't propagate through device boundary.

This is also evidenced by a simple experiment:
```
with tf.device('gpu'):
    a = tf.constant(0)
with tf.device('cpu'):
    b = a + 1
    c = b + 1
```
Resulting computation time graph is
![graph-run](https://cloud.githubusercontent.com/assets/10446514/24199102/62c291ae-0f43-11e7-8689-a595ded2c7ed.png)
Whereas placing all ops on GPU gives a fully shaded graph.

Did I miss something? Or is there any consideration not to run constant folding before partitioning the graph?"
8623,TensorFlow install issue ,"Hi Team .,

While installing TensorFlow on Windows10 I'm facing the below error., 
>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl

****tensorflow-1.0.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.****

Please let me know how to overcome this issue.

Thanks
Pavan"
8622,virtualenv install python issues.,"While installing virtualenv for tensorflow, I get the following error.

The executable /Users/xxx/tensorflow/bin/python is not functioning ERROR: It thinks sys.prefix is
'/Users/xxx' (should be '/Users/xxx/tensorflow')

It seems it is common and related to having python3 or conda installed. But I could not find a known solution.

On my OS Sierra 10.12.3 machine, I have Conda 1.4.3 (python 3.6) installed (came with standard Conda install). 

Thanks.




"
8620,Add more Android Examples,"Hi,

The only Android example is currently for image recognition, but there's a lot more that can be done with Machine Learning on handsets.

Could more examples be added, using Tensorflow to recognize patterns from motion sensors such as accelerometer for example?

[See related question here on StackOverflow](http://stackoverflow.com/questions/42901282/train-android-app-to-recognize-sensor-patterns-using-machine-learning)"
8619,compilation error on latest master,"Seems to have been caused by ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69
```
$ bazel build -c opt //tensorflow/examples/android:tensorflow_demo
ERROR: /Users/yonits/dev/joytunes/tensorflow/third_party/gpus/cuda_configure.bzl:828:18: unexpected keyword 'environ' in call to repository_rule(implementation: function, *, attrs: dict or NoneType = None, local: bool = False).
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'third_party/gpus/cuda_configure.bzl' has errors.
```"
8617,Error when using lookup table from tf.contrib.keras,"I'm trying to build a simple model that includes string input and lookup in a lookup table, and embedding.

I get an error that appears to caused by using non keras layers in a keras model

the code:
```
import tensorflow.contrib.keras as keras
import tensorflow

input_layer = keras.layers.Input(shape=(1,), name='input', dtype='string')
lut = tensorflow.contrib.lookup.HashTable(tensorflow.contrib.lookup.KeyValueTensorInitializer(['a', 'b', 'c'], [1, 2, 3]), default_value=-1)
lut_layer = lut.lookup(input_layer)
embed = keras.layers.Embedding(input_dim=10, output_dim=5)(lut_layer)

model = keras.models.Model(inputs=[input_layer], outputs=[embed])


```
the error:
```
Traceback (most recent call last):
  File ""/Users/ophir/dev/ophir/tf_keras_2.py"", line 10, in <module>
    model = keras.models.Model(inputs=[input_layer], outputs=[embed])
  File ""/Users/ophir/anaconda3/envs/tf_master/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/topology.py"", line 1640, in __init__
    build_map_of_graph(x, seen_nodes, depth=0)
  File ""/Users/ophir/anaconda3/envs/tf_master/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/topology.py"", line 1631, in build_map_of_graph
    next_node = layer.inbound_nodes[node_index]
AttributeError: 'NoneType' object has no attribute 'inbound_nodes'

Process finished with exit code 1
```

using tensorflow on macos, built from source revision: 88998663605ca3b728d6eec298717c8a8558639f"
8612,"make_test_graphs.py fails to open file in binary mode, breaks in py3","The file //tensorflow/compiler/aot/tests/make_test_graphs.py attempts to write binary data to files without opening the files in binary mode, which raises errors in Python 3. This can be fixed by opening the files in binary mode, i.e. 

```
74     with open(saver_file, 'wb') as f:
75         f.write(saver.as_saver_def().SerializeToString())
```

I'm not sure if making this change will break Python 2. I will try and submit a pull request later in the week.
"
8611,"TensorBoard not working, "," I run it as https://www.tensorflow.org/get_started/summaries_and_tensorboard here described, but the browser can't display it ,what's wrong with it ? 
![0x22a3](https://cloud.githubusercontent.com/assets/15243563/24180783/edbb7ff2-0ef1-11e7-852b-a579773aa8be.jpg)
"
8610,Tensorboard Embedding Sprite Image Aspect Ratio,"I tried to use tensorboard embedding visualization with sprite images. 
It works well except two things:
1) Even though I set the image width and height as:
embedding.sprite.single_image_dim.extend([my_width, my_height])
and the projector_config.pbtxt file has:
sprite {
    image_path: ""sprite.png""
    single_image_dim: my_width
    single_image_dim: my_height
}
Tensorboard **resizes the images to be square**, though it can get correct part from the sprite image, except that:
2) Every last image of each row is not correct. This happens when I have lots of images (like 200 images with size 70*100).

Seems like it only support square images?

"
8609,Problem with 3-D matmul,"I want to do a multiplication with two 3-D tensors, as defined:
a = tf.random_uniform(shape = [5,3,3])
b = tf.ones(shape = [5,3,1])
c = tf.matmul(a,b)
but I can't get the right answer as described in the tf.matmul function
https://www.tensorflow.org/api_docs/python/tf/matmul
a:
[[[ 0.05892992  0.12031484  0.7328496 ]
  [ 0.70889461  0.19382548  0.74017155]
  [ 0.90600419  0.91791809  0.19239831]]

 [[ 0.81325758  0.82476532  0.08050001]
  [ 0.48782277  0.21102762  0.22384059]
  [ 0.92273653  0.27739847  0.09880614]]

 [[ 0.6034205   0.83058596  0.74381268]
  [ 0.848786    0.59108317  0.19264364]
  [ 0.16111362  0.97805214  0.53506196]]

 [[ 0.26249301  0.67511797  0.16425216]
  [ 0.91532063  0.62129664  0.26845133]
  [ 0.48103786  0.38934362  0.7424022 ]]

 [[ 0.69902301  0.23881793  0.7409364 ]
  [ 0.91846371  0.71484613  0.45291376]
  [ 0.20789778  0.31036663  0.40649498]]]
b = 
[[[ 1.]
  [ 1.]
  [ 1.]]

 [[ 1.]
  [ 1.]
  [ 1.]]

 [[ 1.]
  [ 1.]
  [ 1.]]

 [[ 1.]
  [ 1.]
  [ 1.]]

 [[ 1.]
  [ 1.]
  [ 1.]]]
c = 
[[[ 2.07270455]
  [ 1.61431181]
  [ 1.29296649]]

 [[ 1.85375774]
  [ 1.27382767]
  [ 2.0612402 ]]

 [[ 1.18541944]
  [ 0.6504941 ]
  [ 0.26519179]]

 [[ 1.86258638]
  [ 2.32511139]
  [ 0.82957554]]

 [[ 2.26229262]
  [ 1.60952091]
  [ 1.65814292]]]
"
8608,gradient_override_map not working for tf.matmul,"### `gradient_override_map` not working for `tf.matmul`

I want to mask the gradient of `tf.matmul`, but I found that the `gradient_override_map` didn't work while it worked for `tf.square`.

### Gradient override for `tf.matmul`: failed

```python
import tensorflow as tf

@tf.RegisterGradient(""CustomMatmul"")
def _custom_matmul_grad(op, grad):
    mask = tf.eye(3)
    return [tf.multiply(grad, mask)]

with tf.Graph().as_default() as g:
    w = tf.Variable(tf.eye(3), dtype=tf.float32)
    x = tf.constant([1,2,3], dtype=tf.float32, shape=[3,1])
    with g.gradient_override_map({""Matmul"": ""CustomMatmul""}):
            logits = tf.matmul(w, x, a_is_sparse=True, name=""Matmul"")
    loss = logits
    optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)
    grad_val = optimizer.compute_gradients(loss)
    # I do not use the following method because my actual code has many other gradients
    # mask = tf.eye(3)
    # grad_val = [(tf.multiply(g, mask), v) for g, v in grad_val]
    train_op = optimizer.apply_gradients(grad_val)

with tf.Session(graph=g) as session:
    session.run(tf.global_variables_initializer())
    _, g_v, w = session.run([train_op, grad_val, w])

    for g, v in g_v:
        print(g)
        print('*' * 80)
        print(v)    
```

The output:
```
[[ 1.  2.  3.]
 [ 1.  2.  3.]
 [ 1.  2.  3.]]

[[ 0.89999998 -0.2        -0.30000001]
 [-0.1         0.80000001 -0.30000001]
 [-0.1        -0.2         0.69999999]]
```

The expected output:
```
[[ 1.  0.  0.]
 [ 0.  2.  0.]
 [ 0.  0.  3.]]

[[ 0.89999998  0.          0.        ]
 [ 0.          0.80000001  0.        ]
 [ 0.          0.          0.69999999]]
```

### Gradient override for `tf.square`: worked

```python
import tensorflow as tf

@tf.RegisterGradient(""CustomSquare"")
def _custom_square_grad(op, grad):
  return tf.constant([101.0])

with tf.Graph().as_default() as g:
  c = tf.Variable([5.0], dtype=tf.float32)
  s_1 = tf.square(c)  # Uses the default gradient for tf.square.
  with g.gradient_override_map({""Square"": ""CustomSquare""}):
    s_2 = tf.square(c, name='Square')

  optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)
  grad_val = optimizer.compute_gradients(s_2)
  train_op = optimizer.apply_gradients(grad_val)

with tf.Session(graph=g) as session:
  session.run(tf.global_variables_initializer())

  _, g_v = session.run([train_op, grad_val])

  for g, v in g_v:
      print(g)
      print('*' * 80)
      print(v)
```

The output:
```
[ 101.]

[-5.10000038]
```

### Environment info
Operating System:
Ubuntu 16.04
TensorFlow:
1.0.1
"
8604,Creating variables in a `while_loop`,"I understand why this fails given the way control inputs are inserted in non-Variable ops created in while loops, but maybe the limitations should be documented somewhere:

```python
import tensorflow as tf
i = tf.constant(0)
def body(i):
  w = tf.Variable(tf.constant(1))
  return [i+w]
loop = tf.while_loop(lambda i: tf.less(i,5), body, [i])
s = tf.Session()
s.run(tf.global_variables_initializer())
```  

```

InvalidArgumentError: The node 'while/w/Assign' has inputs from different frames. The input 'while/j' is in frame 'while/while/'. The input 'while/w' is in frame ''.
```
"
8602,Multiple simultaneous distributed-TF runs on single machine,"I am running multiple distributed-TF sessions simultaneously on a single machine. I am setting up the configuration as described below. However, I am not getting the expected parallelism speedup.

I am running 4 different runs simultaneously, each of them with following config:
```
# Run - 1
tf.train.ClusterSpec({
	""ps"": [""localhost:5000""]
	""worker"": [
		""localhost:5001"",
		""localhost:5002"",
		""localhost:5003""],
	})

# Run - 2
tf.train.ClusterSpec({
	""ps"": [""localhost:6000""]
	""worker"": [
		""localhost:6001"",
		""localhost:6002"",
		""localhost:6003""],
	})

# Run - 3
tf.train.ClusterSpec({
	""ps"": [""localhost:7000""]
	""worker"": [
		""localhost:7001"",
		""localhost:7002"",
		""localhost:7003""],
	})

# Run - 4
tf.train.ClusterSpec({
	""ps"": [""localhost:8000""]
	""worker"": [
		""localhost:8001"",
		""localhost:8002"",
		""localhost:8003""],
	})
```

The command that I use to setup each server looks **exactly** like this for each worker and parameter server respectively:
```
# worker: index varies from 0 to 2
server = tf.train.Server(clusterSpec, job_name=""worker"", task_index=0,
                                 config=tf.ConfigProto(intra_op_parallelism_threads=1,
                                 inter_op_parallelism_threads=2))

### ps
server = tf.train.Server(clusterSpec, job_name=""ps"", task_index=0,
                                 config=tf.ConfigProto(device_filters=[""/job:ps""]))
```

While setting up the network variables, the device is setup **exactly** like this for all runs:
```
ps_device = ""/job:ps""
worker_device = ""/job:worker/task:0/cpu:0""
worker_device = ""/job:worker/task:1/cpu:0""
worker_device = ""/job:worker/task:2/cpu:0""
```

There are `80` cores and enough memory. Each job has `3 workers and 1 ps`. In some fixed amount of time, I get following performance:
```
1 run on machine: x iterations
4 runs on machine: approx. x/2 iterations
6 runs on machine: approx. x/3 iterations
```

The number of cores are enough such that I would expect 4 simultaneous runs to go as fast as single run, but it is no where close. Is there some mistake in my usage of the distributed-tf api ? I suspect there might be issue in the way I am using cpu device for worker: `cpu:0`. But as I read from docs, it seems to mention that `cpu:0` would do automatic scheduling. Does this hold even if I am running 4 different runs of distributed-tf on single machine? Should I be setting up `cpu:1 , cpu:2, cpu:3, cpu:4` for workers of 4 different runs ? Any help would be greatly appreciated. 

Thanks in advance !!"
8596,tf.nn.conv3d ignoring first spacial dimension,"Im using tensorflow 1.0.0 (python 3.5) on OSX(10.11.6 El Capitan), but the same problem occurred with tf version 1.0.1. also got this error when I tried to run the module with the google ml-engine.

# **Relevant code**



````
def conv3d(inputs,weights, biases,layer_name,act=tf.nn.relu,padding='VALID'):
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            tf.add_to_collection(tf.GraphKeys.WEIGHTS, weights)
            variable_summaries(weights)
        with tf.name_scope('biases'):
            variable_summaries(biases)
        with tf.name_scope('convWx_plus_b'):
            preactivate = tf.nn.conv3d(inputs,weights,strides=[1,1,1,1,1],padding=padding) + biases
            #preactivate = tf.nn.convolution(inputs,weights,padding,data_format='NDHWC')
            tf.summary.histogram('preactivate', preactivate)
            activation = act(preactivate) 
            tf.summary.histogram('activation', activation)
    return activation
````        

```
def weight_variable(shape,dtype=np.float32,partition_info=None):
    shape[shape==None] = 1
    n = np.prod(shape)
    w = (np.random.randn(n) * np.sqrt(2./n)).astype(np.float32)
    return tf.Variable(w.reshape(shape),trainable=True)
```

```
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial,trainable=True)

```


```
def maxpool(inputs,padding='VALID'):
    return tf.nn.max_pool3d(inputs,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding=padding)
```

```
def ll_model(inputs):
    
    input_layer = tf.reshape(inputs,[-1,65,65,65,2])
    
    W_conv1 = weight_variable([3,3,3,2,16])
    b_conv1 = bias_variable([16])
    conv1 = conv3d(input_layer,W_conv1,b_conv1,'conv1')

    print(conv1.get_shape().as_list())

    pad = tf.pad(conv1,[[0,0],[1,0],[1,0],[1,0],[0,0]],mode='CONSTANT')
    print(pad.get_shape().as_list())
    maxpool1 = maxpool(pad)
    print(maxpool1.get_shape().as_list())

    W_conv2 = weight_variable([3,3,3,16,24])
    b_conv2 = bias_variable([24])
    conv2 = conv3d(maxpool1,W_conv2, b_conv2,'conv2',padding=""SAME"")
    print(conv2.get_shape().as_list())
    W_conv3 = weight_variable([3,3,3,24,28])
    b_conv3 = bias_variable([28])
    conv3 = conv3d(conv2,W_conv3,b_conv3,'conv3',padding=""SAME"")
    print(conv3.get_shape().as_list())
    W_conv4 = weight_variable([4,4,4,28,34])
    b_conv4 = bias_variable([34])
    conv4 = conv3d(conv3,W_conv4,b_conv4,'conv4')
    print(conv4.get_shape().as_list())
    W_conv5 = weight_variable([5,5,5,34,42])
    b_conv5 = bias_variable([42])
    conv5 = conv3d(conv4,W_conv5,b_conv5,'conv5')
    print(conv5.get_shape().as_list())
    W_conv6 = weight_variable([5,5,5,42,50])
    b_conv6 = bias_variable([50])
    conv6 = conv3d(conv5,W_conv6,b_conv6,'conv6')
    print(conv6.get_shape().as_list())
    W_conv7 = weight_variable([5,5,5,50,50])
    b_conv7 = bias_variable([50])
    conv7 = conv3d(conv6,W_conv7,b_conv7,'conv7')
    print(conv7.get_shape().as_list())
    W_conv8 = weight_variable([1,1,1,50,2])
    b_conv8 = bias_variable([2])
    conv8 = conv3d(conv7,W_conv8, b_conv8,'output',act=depth_softmax)
    
    return conv8
```


the error happens when compiling the model. conv8 is supposed to have the shape [None,17,17,17,2].

# The Error:

```
python task.py                                                                       
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 ins
tructions, but these are available on your machine and could speed up CPU computations.                     
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 ins
tructions, but these are available on your machine and could speed up CPU computations.                     
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instru
ctions, but these are available on your machine and could speed up CPU computations.                        
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instr
uctions, but these are available on your machine and could speed up CPU computations.                       
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instru
ctions, but these are available on your machine and could speed up CPU computations.                        
[None, 65, 63, 63, 16]                                                                                      
[None, 66, 64, 64, 16]                                                                                      
[None, 33, 32, 32, 16]                                                                                      
[None, 33, 32, 32, 24]                                                                                      
[None, 33, 32, 32, 28]                                                                                      
[None, 33, 29, 29, 34]                                                                                      
[None, 33, 25, 25, 42]                                                                                      
[None, 33, 21, 21, 50]                                                                                      
[None, 33, 17, 17, 50]                                                                                      
Traceback (most recent call last):                                                                          
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
common_shapes.py"", line 670, in _call_cpp_shape_fn_impl                                                     
    status)                                                                                                 
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/contextlib.py"", line 66, in __exit__      
    next(self.gen)                                                                                          
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
errors_impl.py"", line 469, in raise_exception_on_not_ok_status                                              
    pywrap_tensorflow.TF_GetCode(status))                                                                   
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 17 and 33 fo
r 'SquaredDifference' (op: 'SquaredDifference') with input shapes: [?,17,17,17,2], [?,33,17,17,2].          
                                                                                                            
During handling of the above exception, another exception occurred:                                         
                                                                                                            
Traceback (most recent call last):                                                                          
  File ""task.py"", line 33, in <module>                                                                      
    loss = mean_square_error(y,ll_model(x))                                                                 
  File ""/Users/vhasfclanga/tflow_trainer/trainer/functions.py"", line 80, in mean_square_error               
    return tf.reduce_sum(tf.squared_difference(a,b)) / N                                                    
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_ma
th_ops.py"", line 2754, in squared_difference                                                                
    result = _op_def_lib.apply_op(""SquaredDifference"", x=x, y=y, name=name)                                 
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
op_def_library.py"", line 763, in apply_op                                                                   
    op_def=op_def)                                                                                          
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
ops.py"", line 2397, in create_op                                                                            
    set_shapes_for_outputs(ret)                                                                             
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
ops.py"", line 1757, in set_shapes_for_outputs                                                               
    shapes = shape_func(op)                                                                                 
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
ops.py"", line 1707, in call_with_requiring                                                                  
    return call_cpp_shape_fn(op, require_shape_fn=True)                                                     
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
common_shapes.py"", line 610, in call_cpp_shape_fn                                                           
    debug_python_shape_fn, require_shape_fn)                                                                
  File ""/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/
common_shapes.py"", line 675, in _call_cpp_shape_fn_impl                                                     
    raise ValueError(err.message)                                                                           
ValueError: Dimensions must be equal, but are 17 and 33 for 'SquaredDifference' (op: 'SquaredDifference') wi
th input shapes: [?,17,17,17,2], [?,33,17,17,2].
```

As you can see, these are the shapes of the tensors resulting from each convolutional layer.

[None, 65, 63, 63, 16]                                                                                      
[None, 66, 64, 64, 16]                                                                                      
[None, 33, 32, 32, 16]                                                                                      
[None, 33, 32, 32, 24]                                                                                      
[None, 33, 32, 32, 28]                                                                                      
[None, 33, 29, 29, 34]                                                                                      
[None, 33, 25, 25, 42]                                                                                      
[None, 33, 21, 21, 50]                                                                                      
[None, 33, 17, 17, 50]     


Notice the first spacial dimension is only effected by pooling and padding layers, and is totally ignored by convolutional layers. It's strange to me because everything should be symmetric across spatial dimensions.

I've tried using tf.nn.convolution as seen in my conv3d wrapper, that yielded the same result. I've tried switching up the padding, that also didn't work. I tried using the higher level functions in tf.layers to construct the model, that also didn't work. The fact that none of these methods worked makes me think this must be a programming error on my part, but the error is coming from a simple propagation of tensor shapes, starting with placeholders

```
with tf.name_scope('inputs'):
    x = tf.placeholder(tf.float32,shape=[None,65,65,65,2],name='features')
    tf.summary.histogram('feature-hist', x)
with tf.name_scope('ground_truth'):
    y = tf.placeholder(tf.float32,shape=[None,17,17,17,2],name='targets')
    tf.summary.histogram('target-hist', y)

```
So I'm not sure where I could have possibly gone wrong.

Also, this exact model structure resulted in the correct output shape when used with the Estimator + ModelFnOps API.

the error can be reproduced by using one of the built in loss functions and running

`tf.some_loss_function(y,ll_model(x))`

the function I was using was 
```
def mean_square_error(a,b):
    shape = a.get_shape().as_list()
    shape[shape==None] = 1
    N = np.prod(shape)
    return tf.reduce_sum(tf.squared_difference(a,b)) / N

```
Does anyone know if this is a bug or programming error on my part?

Thanks in advance,
Alex"
8594,tf.case doesn't return a tensor with the given name,"With TF 1.0.1.
```python
>> print(tf.case([(tf.constant(True, dtype=tf.bool), lambda: tf.constant(1))], lambda: tf.constant(2), name='aa'))
Tensor(""aa/If_1/Merge:0"", shape=(), dtype=int32)
```
I expect to see ""aa:0"".

Similar to #6741 "
8593,dtype of methods in tensorflow.contrib.distributions does not depend on self.dtype,"In master (f4a0c2c) (and versions such as r1.0), the docstring in `log_prob()` claims to return a tensor which has the same dtype as the distributions' dtype. This is not true for the built-in distributions. For example:
```python
>>> from tensorflow.contrib import distributions as ds
>>>>
>>> x = ds.Bernoulli(p=0.5)
>>> x.dtype
tf.int32
>>> x.log_prob(1)
<tf.Tensor 'Bernoulli_1/log_prob/Neg:0' shape=() dtype=float32>
```

This inconsistency also occurs for many methods beyond `log_prob()`."
8591,TensorBoard fails grouping summaries by names,"Hi. Using Tensorboard, I found that it fails grouping scalar summaries by its name.

As I know, summaries are grouped by their name scopes, which are delimited by slash('/').
However, for some cases, the group to be bound together would be separated.

Here's a reproducible code that causes the same error, and the resulting tensorboard page.

```
import tensorflow as tf
import os

if not os.path.exists('./temp'):
    os.mkdir('./temp')
summary_writer = tf.summary.FileWriter('./temp', None)

summary = tf.Summary()
summary.value.add(tag='train/accuracy', simple_value=1.0)
summary.value.add(tag='train/cross_entropy', simple_value=1.0)
summary.value.add(tag='train/learning_rate', simple_value=1.0)
summary.value.add(tag='train_image/foo', simple_value=1.0)

for i in range(10):
    summary_writer.add_summary(summary, i)
summary_writer.flush()
```

![image](https://cloud.githubusercontent.com/assets/13655756/24163367/b4673d48-0ead-11e7-8478-b0733581b90e.png)

### Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: No cuda related, I guess.
TensorFlow version: Only confirmed in the nightly build, Linux-GPU version. But I think it will occur in other versions as well.

Thank you for your hard work. :^)"
8589,contrib.crf + XLA doesn't work,"The combination of tf.contrib.crf + XLA seems not working (sometimes hang forever).

Following code is for reproducing errors.
The code is from standard example - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf
The only difference is XLA part. 

When XLA is enabled, loss is not reducing.
In more large network(real NLP applications), it hangs forever after graph building. 

My settings is 
- tensorflow 1.0.1
- python 3.6 (anaconda 4.3.0 )
- cuda 7.5 
 

```python
import numpy as np
import tensorflow as tf

# Data settings.
num_examples = 10
num_words = 20
num_features = 100
num_tags = 5

# Random features.
x = np.random.rand(num_examples, num_words, num_features).astype(np.float32)

# Random tag indices representing the gold sequence.
y = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)

# All sequences in this example have the same length, but they can be variable in a real model.
sequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)


# to enable XLA
tf_config = tf.ConfigProto()
tf_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1


# Train and evaluate the model.
with tf.Graph().as_default():
  with tf.Session(config=tf_config) as session:
    # Add the data to the TensorFlow graph.
    x_t = tf.constant(x)
    y_t = tf.constant(y)
    sequence_lengths_t = tf.constant(sequence_lengths)

    # Compute unary scores from a linear layer.
    weights = tf.get_variable(""weights"", [num_features, num_tags])
    matricized_x_t = tf.reshape(x_t, [-1, num_features])
    matricized_unary_scores = tf.matmul(matricized_x_t, weights)
    unary_scores = tf.reshape(matricized_unary_scores,
                              [num_examples, num_words, num_tags])

    # Compute the log-likelihood of the gold sequences and keep the transition
    # params for inference at test time.
    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(
        unary_scores, y_t, sequence_lengths_t)

    # Add a training op to tune the parameters.
    loss = tf.reduce_mean(-log_likelihood)
    
    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)


    # Train for a fixed number of iterations.
    session.run(tf.global_variables_initializer())
    for i in range(1000):
      tf_unary_scores, tf_transition_params, _ = session.run(
          [unary_scores, transition_params, train_op])
      if i % 100 == 0:
        correct_labels = 0
        total_labels = 0
        for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y,
                                                          sequence_lengths):
          # Remove padding from the scores and tag sequence.
          tf_unary_scores_ = tf_unary_scores_[:sequence_length_]
          y_ = y_[:sequence_length_]

          # Compute the highest scoring sequence.
          viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(
              tf_unary_scores_, tf_transition_params)

          # Evaluate word-level accuracy.
          correct_labels += np.sum(np.equal(viterbi_sequence, y_))
          total_labels += sequence_length_
        accuracy = 100.0 * correct_labels / float(total_labels)
        print(""Accuracy: %.2f%%"" % accuracy)
```

it generates following outputs
```sh
2017-03-22 00:40:01.110489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:06:00.0
Total memory: 11.25GiB
Free memory: 11.16GiB
2017-03-22 00:40:01.110631: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b8c6d0
2017-03-22 00:40:01.337723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:07:00.0
Total memory: 11.25GiB
Free memory: 11.16GiB
2017-03-22 00:40:01.337815: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b90150
2017-03-22 00:40:01.544460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 2 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:84:00.0
Total memory: 11.25GiB
Free memory: 11.16GiB
2017-03-22 00:40:01.544569: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b93e00
2017-03-22 00:40:01.750146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 3 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:85:00.0
Total memory: 11.25GiB
Free memory: 11.16GiB
2017-03-22 00:40:01.750302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 2
2017-03-22 00:40:01.750319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 3
2017-03-22 00:40:01.750346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 2
2017-03-22 00:40:01.750358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 3
2017-03-22 00:40:01.750369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 0
2017-03-22 00:40:01.750380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 1
2017-03-22 00:40:01.750479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 0
2017-03-22 00:40:01.750519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 1
2017-03-22 00:40:01.750597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3
2017-03-22 00:40:01.750608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y N N
2017-03-22 00:40:01.750614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y N N
2017-03-22 00:40:01.750619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   N N Y Y
2017-03-22 00:40:01.750625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   N N Y Y
2017-03-22 00:40:01.750643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)
2017-03-22 00:40:01.750650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0)
2017-03-22 00:40:01.750656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:84:00.0)
2017-03-22 00:40:01.750662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:85:00.0)
2017-03-22 00:40:03.347959: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 4 visible devices
2017-03-22 00:40:03.347994: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 48 visible devices
2017-03-22 00:40:03.358665: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4b47600 executing computations on platform Host. Devices:
2017-03-22 00:40:03.358682: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-03-22 00:40:03.359012: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 4 visible devices
2017-03-22 00:40:03.359026: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 48 visible devices
2017-03-22 00:40:03.367439: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4b6fcb0 executing computations on platform CUDA. Devices:
2017-03-22 00:40:03.367459: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2017-03-22 00:40:03.367468: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7
2017-03-22 00:40:03.367475: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (2): Tesla K80, Compute Capability 3.7
2017-03-22 00:40:03.367482: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (3): Tesla K80, Compute Capability 3.7
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
Accuracy: 15.26%
```



"
8588,Got an unexpected keyword argument 'fix_global_step_increment_bug',"I get this error while trying to run the [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py).

```
TypeError: __init__() got an unexpected keyword argument 'fix_global_step_increment_bug'
```

It appears to be coming from - [ea963dd048749df525d2a1f19c31d7abcdc3268e](https://github.com/tensorflow/tensorflow/commit/ea963dd048749df525d2a1f19c31d7abcdc3268e).

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None

### Environment info
Operating System: MacOS 10.11.5

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

1. A link to the pip package you installed: official python package tensorflow.
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I just ran this code - [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)

```
 ~$ python wide_n_deep_tutorial.py --model_type=wide_n_deep
```

### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
Traceback (most recent call last):
  File ""wide_n_deep_tutorial.py"", line 234, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/Users/sdua/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""wide_n_deep_tutorial.py"", line 197, in main
    FLAGS.train_data, FLAGS.test_data)
  File ""wide_n_deep_tutorial.py"", line 185, in train_and_eval
    m = build_estimator(model_dir, model_type)
  File ""wide_n_deep_tutorial.py"", line 132, in build_estimator
    fix_global_step_increment_bug=True)
TypeError: __init__() got an unexpected keyword argument 'fix_global_step_increment_bug'
```"
8587,Android Example Doesn't Build: Could not get unknown property 'assembleRelease',"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

http://stackoverflow.com/questions/39602587/could-not-get-unknown-property-assemblerelease-for-project/39720618
http://stackoverflow.com/questions/39590549/after-update-to-android-studio-2-2-gradle-plugin-2-2-0-could-not-get-unknown

### Environment info
Operating System:
Ubuntu 16.10
Installed version of CUDA and cuDNN: 
Toolkit 8.0.1 cuDNN 5.1

If installed from binary pip package, provide:

https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp36-cp36m-linux_x86_64.whl
1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
The default android example code. Specifically `build.gradle`
When trying to run the code on an emulator, for example, there is a config error:
`Could not get unknown property 'assembleRelease for root project 'android' of type org.gradle.api Project` (Get this twice)
Links to:

```
afterEvaluate {
    assembleDebug.dependsOn copyNativeLibs
    assembleRelease.dependsOn copyNativeLibs
}
```
from build.gradle lines 143-146

### What other attempted solutions have you tried?
It looks like this is a conflict with gradle of some sorts. My knowledge is minimal with gradle/adroid studio. Some online threads say it should be included in a task but when putting 
`task afterEvaluate{ ...}` there is a similar error and this doesn't work either.

This post from a googler implies that your code is written as it should be though.
https://code.google.com/p/android/issues/detail?id=219732#32

At my wits end with this. For reference, I am able to build with bazel from the terminal. This seems to be a gradle/AS induced issue.
### Logs or other output that would be helpful
```
Information:Gradle tasks [:assembleDebug]

Error:(145, 1) A problem occurred configuring root project 'android'.
> Failed to notify project evaluation listener.
   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.
   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.
```"
8586,TensorFlow upgrade to 1.0.1 ,"I upgraded my server from 1.0.0 (ubuntu):

pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl
But I get the following discrepancy:

(tensorflow)$ pip list | grep tensorflow
tensorflow (1.0.0)
(tensorflow)$ python -c 'import tensorflow as tf; print(tf.__version__)'
1.0.1"
8585,Consistent dtypes for discrete tensorflow.contrib.distributions?,"In master (https://github.com/tensorflow/tensorflow/commit/f4a0c2c0f1bbff6e9b4d5d4a0796e7645a974321), the following is true:
+ Bernoulli, Categorical, and OneHotCategorical accept dtype as an argument, defaulting to `tf.int32`.
+ Binomial and Geometric have dtype given by its `probs` parameter, which is returned by the `get_logits_and_probs` utility function. It errors unless `probs` is float, meaning they must have dtype float.
+ Deterministic has dtype given by its `loc` parameter, which can have either dtype float or int.
+ Poisson has dtype given by its `rate` parameter, which can have either dtype float or int.

It seems like the default dtype for discrete distributions with fixed and bounded support is int. And the dtype for discrete distributions whose support depends on a parameter, or whose support is unbounded, varies between always being float or being either float or int."
8584,TF for Xeon Phi,"Hi,

Does tensorflow support xeon phi? 

Thanks !
Afshin"
8583,Bernoulli's probability mass function produces output beyond its support,"Bernoulli's (log) probability mass function is valid for values between 0 and 1, even though its support is only {0, 1}. This is a result of [its implementation](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/distributions/python/ops/bernoulli.py#L150) relying on `tf.sigmoid_cross_entropy_with_logits(logits, targets)`, where `targets` does not have to be {0, 1}.

Is this intended behavior?

```python
from tensorflow.contrib import distributions as ds

x = ds.Bernoulli(p=0.5)
y = x.log_prob(0.5)

sess = tf.Session()
sess.run(y)
## -0.69314718
```

```python
print(tf.__version__)
## 1.0.1
```"
8582,Greater than fifteen minutes to run a C++ example code using Tensorflow libraries using bazel,"Why does it take greater than fifteen minutes to run a C++ example code using the Tensorflow libraries using bazel?

https://www.tensorflow.org/api_guides/cc/guide

"
8581,tf.read_file doesn't support non ASCII characters in filename,"Windows 2003 server R2, if input file names contain non ASCII characters (such as Chinese), exception will raise.

     [[Node: ReadFile_17 = ReadFile[_device=""/job:localhost/replica:0/task:0/cpu:0""](unstack_5:2)]]

Caused by op 'ReadFile_17', defined at:
  File ""train.py"", line 660, in <module>
    tf.app.run()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 657, in main
    train()
  File ""train.py"", line 430, in train
    num_preprocess_threads=num_preprocess_threads)
  File ""input.py"", line 216, in distorted_inputs
    num_readers = FLAGS.num_readers)
  File ""input.py"", line 256, in batch_inputs
    image_buffer = tf.read_file(filename)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 203, in read_file
    result = _op_def_lib.apply_op(""ReadFile"", filename=filename, name=name)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2334, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Can not get size for: 01_瑜_057_14734860
91829.jpg : \u03f5\u0373\udcd5\u04b2\udcbb\udcb5\udcbd\u05b8\udcb6\udca8\udcb5\udcc4\udcce\u013c\udcfe\udca1\udca3

         [[Node: ReadFile_17 = ReadFile[_device=""/job:localhost/replica:0/task:0/cpu:0""](unstack_5:2)]]
"
8579,OutOfMemoryError: Java heap space on Tensorflow tests execution,"Executing Tensorflow test suite using command: 

    bazel test //tensorflow/...

throws Out of memory issue: 

    INFO: Found 1886 targets and 1155 test targets...
    INFO: Elapsed time: 291.775s, Critical Path: 0.91s
    java.lang.OutOfMemoryError: Java heap space
            at com.google.devtools.build.skyframe.SkyKey.create(SkyKey.java:57)
            at com.google.devtools.build.lib.skyframe.ArtifactSkyKey.key(ArtifactSkyKey.java:43)
            at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.toKeys(ActionExecutionFunction.java:576)
            at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.compute(ActionExecutionFunction.java:158)
            at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)
            at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
            at java.lang.Thread.run(Thread.java:745)
    Java heap space
    
    bazel ran out of memory and crashed.


I have tried below options still issue persists. 

> export _JAVA_OPTIONS=""-Xms1024m -Xmx1024m""
> 
> export JVM_ARGS=""-Xmx1024m -XX:MaxPermSize=256m""
> 
> export JVM_ARGS=""-XX:PermSize=64M -XX:MaxPermSize=256m""

Also, changed the file from bazel code: scripts/bootstrap/compile.sh

`"" run ""${JAVAC}"" -J-Xms1g -J-Xmx1g -classpath ""${classpath}"" -sourcepath ""${sourcepath}""`""


Machine configurations: Ubuntu distribution, openjdk8, RAM 16G
TensorFlow : master

"
8576,Tensorflow Install issue on windows 10,"I tried to install TensorFlow on my PC with Windows 10 OS, and I've installed with Conda Command

C:> conda create -n tensorflow 
Below is the error .. please let me know how can I install .. Thanks.

      File ""C:\ProgramData\Anaconda3\lib\site-packages\conda\core\index.py"", line 391, in fetch_repodata
        with open(cache_path, 'w') as fo:
    **PermissionError: [Errno 13] Permission denied: 'C:\\ProgramData\\Anaconda3\\pkgs\\cache\\2116b818.json'**
"
8575,error when attempting to run image_retraining,"### Environment info
Operating System: Ubuntu Linux 16.04 LTS

Installed from source
1. Tensorflow version: 1.0.1
2. The output of `bazel version`: 
```
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Link to the tutorial: https://www.tensorflow.org/tutorials/image_retraining
```
bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos
```

### Logs or other output that would be helpful
```
Traceback (most recent call last):
  File ""/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 1052, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 768, in main
    maybe_download_and_extract()
  File ""/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py"", line 305, in maybe_download_and_extract
    tarfile.open(filepath, 'r:gz').extractall(dest_directory)
  File ""/usr/lib/python3.5/tarfile.py"", line 1996, in extractall
    numeric_owner=numeric_owner)
  File ""/usr/lib/python3.5/tarfile.py"", line 2038, in extract
    numeric_owner=numeric_owner)
  File ""/usr/lib/python3.5/tarfile.py"", line 2108, in _extract_member
    self.makefile(tarinfo, targetpath)
  File ""/usr/lib/python3.5/tarfile.py"", line 2156, in makefile
    copyfileobj(source, target, tarinfo.size, ReadError)
  File ""/usr/lib/python3.5/tarfile.py"", line 241, in copyfileobj
    buf = src.read(BUFSIZE)
  File ""/usr/lib/python3.5/gzip.py"", line 274, in read
    return self._buffer.read(size)
  File ""/usr/lib/python3.5/_compression.py"", line 68, in readinto
    data = self.read(len(byte_view))
  File ""/usr/lib/python3.5/gzip.py"", line 480, in read
    raise EOFError(""Compressed file ended before the ""
EOFError: Compressed file ended before the end-of-stream marker was reached
```"
8574,Problems with saver and restore: Failed to find any matching files,"This is the code:

```

save_path = model_path+""%s.ckpt""%model_name
if flag == ""initial_train"":
    print(""Training new model"")
    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
        train_writer =  tf.summary.FileWriter('/tmp/log/train_tanh2_reg_100_%s'%beta_reg, sess.graph) # activate TensorBoard
        val_writer =  tf.summary.FileWriter('/tmp/log/val_reg_tanh2_100_%s'%beta_reg, sess.graph) 
        sess.run(tf.global_variables_initializer())
        print(""The model will be saved in "",save_path)
        training_loop(num_epochs)
        flag = None
else:
    print(""Training pretrained model"")
    # Restore from previous model
    print(""Restore from: "" + save_path)
    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
        train_writer =  tf.summary.FileWriter('/tmp/log/train_tanh2_reg_100_%s'%beta_reg, sess.graph) # activate TensorBoard
        val_writer =  tf.summary.FileWriter('/tmp/log/val_reg_tanh2_100_%s'%beta_reg, sess.graph) 
        saver.restore(sess,save_path)
        print(""Model loaded"")
        training_loop(nump_epochs)
train_writer.close()
val_writer.close()
```
and these are the ckpt files that have been created:

checkpoint
prezi.ckpt-0.data-00000-of-00001
prezi.ckpt-0.index
prezi.ckpt-0.meta
prezi.ckpt-1.data-00000-of-00001
prezi.ckpt-1.index
prezi.ckpt-1.meta
prezi.ckpt-2.data-00000-of-00001
prezi.ckpt-2.index
prezi.ckpt-2.meta
prezi.ckpt-3.data-00000-of-00001
prezi.ckpt-3.index
prezi.ckpt-3.meta

But still I get this error:

**Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/carol/tensorflowLF/clasification/prezi.ckpt**

Thanks for your help in advance :)"
8573,bazel-bin command not found,"I am having issues while building pip package.I am building tensorflow with cpu configurations Steps i followed

bazel release= 0.4.5, ubuntu -version = 14.04 LTS
1. bazel clean
2. ./configure 
3. bazel build --config op
```
kush@kush-Lenovo-B40-80:~/machine_learning/deeplearning/tensorflow$ bazel build --config=op
Warning: ignoring LD_PRELOAD in environment.
WARNING: Config values are not defined in any .rc file: op
INFO: Found 0 targets...
INFO: Elapsed time: 2.365s, Critical Path: 0.03s

```
4. Now when i try to run 
`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`
```
kush@kush-Lenovo-B40-80:~/machine_learning/deeplearning/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package
bash: bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory
```

But then i tried sudo it gives 
`sudo: bazel-bin/tensorflow/tools/pip_package/build_pip_package: command not found
`

bazel is installed in my system. i have checked it. 

I also tried giving full path in bazel-bin

`sudo: /usr/bin/bazel-bin/tensorflow/tools/pip_package/build_pip_package: command not found
`

i have bazel release = 0.4.5
Can anyone help me on this?
"
8572,Image Labeler Fails with Gifs?,"The image_label example assumes that decode_gif (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L105) will return a 3-d tensor ([height, width, channels]) in the same way that decode_jpeg or decode_png would. However decode_dif returns a 4-d shape ([frames, height, width, channels], https://www.tensorflow.org/api_docs/python/tf/image/decode_gif). This will cause ResizeBilinear to fail.

This could be fixed by pulling the first frame from the gif, or by choosing to not expand dimensions on the gif (and classifying each frame).

"
8571,shape of state in the output of dynamic_rnn is undetermined,"short code:
```
sequence_lengths, (sequences, labels) = bucket_by_sequence_length(
                                              input_length=sequence_length,
                                              tensors=[sequence, label],
                                              batch_size=72,
                                              bucket_boundaries=[80, 160, 240, 320, 400],
                                              dynamic_pad=True)
lstm_cell = core_rnn_cell.BasicLSTMCell(num_units=56)
lookup = tf.nn.embedding_lookup(......)
_, state = tf.nn.dynamic_rnn(
                      cell=lstm_cell,
                      inputs=lookup,
                      sequence_length=sequence_lengths,
                      dtype=tf.float32)
```
then `state.h` and `state.c` should be of shape `[72, 56]`, but actually the first dim(batch_size) is undetermined in program, is it a bug?"
8570,Generating sentences using the RNN model,"How do we generate sentences with the trained rnn model in:
[https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb](url)

I found some people doing it here and there but nothing that is well documented with a working code, at least not ones that are compatible with TF 1.0

I found this answer on stackoverflow but I keep getting errors with it
[http://stackoverflow.com/a/38729349/7568128](url)

Here is a copy of the file I'm working with 
[ptb_word_lm2 (copy).txt](https://github.com/tensorflow/tensorflow/files/856981/ptb_word_lm2.copy.txt)

"
8569,Incorrect results from tfprof,"I was profiling a few matrix multiplications with tfprof but couldn't make sense of the output. The per-op timing results are hard to believe to be true.

Code:
```
import tensorflow as tf
import tensorflow.contrib.tfprof as tfprof

dim = 6000
n = 5
with tf.device('/gpu:0'):
    X, Y, Z, _X, _Y = [], [], [], [], []
    for i in range(n):
        X.append(tf.random_uniform([dim, dim], 0, 10, name='X' + str(i)))
        Y.append(tf.random_uniform([dim, dim], 0, 10, name='Y' + str(i)))
        _X.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))
        _Y.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))
        Z.append(tf.matmul(_X[i], _Y[i]))
    W = tf.ones([dim, dim])
    for i in range(n):
        W = tf.matmul(W, Z[i])

sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))
sess.run(tf.global_variables_initializer())
X_, Y_ = sess.run([X, Y])

run_metadata = tf.RunMetadata()
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
W_ = sess.run(W,
              {_i: i_ for _i, i_ in zip(_X + _Y, X_ + Y_)},
              options=run_options,
              run_metadata=run_metadata)

tfprof.model_analyzer.print_model_analysis(
    tf.get_default_graph(),
    run_meta=run_metadata,
    tfprof_options=tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)
```

Output:
```
_TFProfRoot (0B/1296.00MB, 0us/778.18ms)
  MatMul (144.00MB/144.00MB, 162.51ms/162.51ms)
  MatMul_1 (144.00MB/144.00MB, 145.03ms/145.03ms)
  MatMul_2 (144.00MB/144.00MB, 145.07ms/145.07ms)
  MatMul_3 (144.00MB/144.00MB, 162.69ms/162.69ms)
  MatMul_4 (144.00MB/144.00MB, 162.57ms/162.57ms)
  MatMul_5 (144.00MB/144.00MB, 241us/241us)
  MatMul_6 (144.00MB/144.00MB, 21us/21us)
  MatMul_7 (144.00MB/144.00MB, 16us/16us)
  MatMul_8 (144.00MB/144.00MB, 10us/10us)
  MatMul_9 (144.00MB/144.00MB, 19us/19us)
  ones (144.00MB/144.00MB, 6us/6us)
```

Graph:
![image](https://cloud.githubusercontent.com/assets/425637/24126577/0f5aafbe-0d8c-11e7-9b3e-92a5df5548cc.png)

**As shown in the profiling output, MatMul_[0-4] take 1,000 -10,000x longer than MatMul_[5-9]. However, they are all matrix multiplications of the same size!**

### Environment info
Operating System: Ubuntu 14.04.5

Installed version of CUDA and cuDNN: 
```
-rw-r--r-- 1 root root 556000 Mar  1 12:58 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Mar  1 12:58 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Mar  1 12:58 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root root 415432 Mar  1 12:58 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root 775162 Mar  1 12:58 /usr/local/cuda/lib64/libcudart_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
`tensorflow_gpu-1.0.1-cp27-cp27mu-manylinux1_x86_64.whl`

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
`1.0.1`
"
8567,Is there a way to do partial_run with queues?,"I'm reading a bunch of image files using the FIFO queue provided by `string_input_producer`. Some of these files have bad format and cannot be decoded properly. Therefore I would want to print the names of these files when the related functions from `tf.image` throw `tf.errors.InvalidArgumentError` exceptions.

My graph definition looks like this:

    ...
    filename_queue = tf.train.string_input_producer(files, num_epochs=1, shuffle=True)
    image_reader = tf.WholeFileReader()
    key, image_file = image_reader.read(filename_queue)
    is_png = tf.py_func(has_png_ext, [key], [tf.bool])
    image = tf.cond(pred=is_png[0],
                    fn1=lambda: tf.image.decode_png(image_file),
                    fn2=lambda: tf.image.decode_jpeg(image_file))
    ...

And then I'm fetching the output like this:

    ...
    with tf.Session() as sess:
        sess.run(init_op)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        try:
            while not coord.should_stop():
                try:
                    nodes = sess.run([key, image])
                    nodes_results.append(nodes)
                    num_files += 1
                except tf.errors.InvalidArgumentError:
                    num_failed += 1
                    num_failed_dir += 1
    ...

Initially I've tried to print the value of `key` in the `except` block but this won't work. Then I've tried a bunch of ideas and none of them seems to be working. I've been trying to see if [partial_run](https://www.tensorflow.org/api_docs/python/tf/Session#partial_run) could help but it takes a `feed_dict` input instead of a native queue input.

So in short, my question is, is there a way to make partial_run work while using native queue inputs? Or is there a better way to solve my problem - output the file names when `tf.image.decode_*` throw exceptions?"
8566,avg_pool() is quietly returning garbage.,"I'm seeing an issue with various networks where, depending on image input size, it either works, crashes with a CUDA_ERROR_ILLEGAL_ADDRESS error, or, most worryingly, runs without issue but visually has strange artifacts (see attached). Does not happen if running on CPU.

OS: Ubuntu 16.04. Running docker image based on tensorflow/tensorflow:1.0.1-gpu, with nvidia-docker 17.03.0-ce. GPUs: 2x Titan X (Pascal). Was also able to replicate running native on similar machine with same OS and hardware.

Possibly related github issues:  #6509, #4425, #3422 but these seem to be generally fixed by upgrading to same or earlier tensorflow or cudnn versions.

ls -l /usr/local/nvidia/lib64/libcud*
lrwxrwxrwx 1  999  999      17 Nov 21 20:42 /usr/local/nvidia/lib64/libcuda.so -> libcuda.so.370.28
lrwxrwxrwx 1  999  999      17 Nov 21 20:42 /usr/local/nvidia/lib64/libcuda.so.1 -> libcuda.so.370.28
-rw-r--r-- 2 root root 8219624 Sep  2  2016 /usr/local/nvidia/lib64/libcuda.so.370.28

This code can demonstrate the issues. Changing run_type changes the effect, hopefully each option is clear. Problem here starts after avg_pool, but I've seen with other outputs, e.g. activation functions.
```python
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

run_type = ""gpu_artifacts""  # ""gpu_works"" ""gpu_artifacts"" ""gpu_crashes"" ""cpu""
if run_type == ""gpu_works"":
    num_gpus = 1
    num_rows = 900
elif run_type == ""gpu_crashes"":
    num_gpus = 1
    num_rows = 950
elif run_type == ""gpu_artifacts"":
    num_gpus = 1
    num_rows = 1000
elif run_type == ""cpu"":
    num_gpus = 0
    num_rows = 950

num_cols = 1900
xx, yy = np.meshgrid(np.linspace(0, 2 * np.pi, num_cols), np.linspace(0, 2 * np.pi, num_rows))
faux_img = np.zeros((num_rows, num_cols, 3))
faux_img[:, :, 0] = np.sin(xx) + np.cos(yy)
faux_img[:, :, 1] = np.cos(xx) + np.cos(yy)
faux_img[:, :, 2] = np.cos(xx) + np.sin(yy)

plt.figure()
plt.imshow(faux_img)
plt.title(""input"")

input_shape = faux_img.shape
input_shape = np.append([-1], input_shape)
inputs = np.array([faux_img.copy().ravel()])

x_in_ravel = tf.placeholder(tf.float32, [None, np.prod(input_shape[1::])], name=""x_in_ravel"")
x_in = tf.reshape(x_in_ravel, input_shape)
filt_vals = np.random.randn(5, 5, 3, 30).astype(np.float32)
W_0 = tf.Variable(tf.constant(filt_vals))
strides = [1, 1, 1, 1]
conv_out = tf.nn.conv2d(x_in, W_0, strides=strides, padding=""VALID"", name=""conv1"")
pool_out = tf.nn.avg_pool(conv_out, ksize=[1, 9, 9, 1], strides=strides, padding=""VALID"", name=""pool1"")

config = tf.ConfigProto(device_count={'GPU': num_gpus})
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
outs_conv, outs_pool = sess.run([conv_out, pool_out], feed_dict={x_in_ravel: inputs})

filt_out_fig, filt_out_ax = plt.subplots()
pool_out_fig, pool_out_ax = plt.subplots()
filt_out_ax.imshow(outs_conv[0, :, :, 15])
filt_out_ax.set_title(""conv2d output, looks normal"")
pool_out_ax.imshow(outs_pool[0, :, :, 15])
pool_out_ax.set_title(""avg_pool output, striping artifacts"")

plt.show()
```

Output for run_type = ""gpu_crashes"":
```
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x37cb5b0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 10.97GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1
```

Output plots for run_type = ""gpu_artifacts"":
![input](https://cloud.githubusercontent.com/assets/3643255/24121814/e47dd59c-0d76-11e7-9b35-4ee1327c0230.png)
![conv_out](https://cloud.githubusercontent.com/assets/3643255/24121821/e806480c-0d76-11e7-839d-b94da94a3c75.png)
![pool_out](https://cloud.githubusercontent.com/assets/3643255/24121824/e9f23176-0d76-11e7-931f-62bb49bf2abd.png)


"
8564,`tf.test.compute_gradient` gives error when computation involves TensorArrays,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

``` python
a = tf.TensorArray(tf.float32, size=1)
x = tf.ones(5)
a = a.write(0, x)
y = a.stack()

grads = tf.gradients(y, x)

with tf.Session() as sess:
    print(""y"")
    print(sess.run(y))

    print(""grad"")
    print(sess.run(grads))

    # gives error:
    tf.test.compute_gradient(x, (5,), y, (5,))
```

The gradient calculation (`sess.run(grads)`) works fine, it is just something to do with how the gradients are being calculated within `tf.test.compute_gradients`.  Here is the stack trace:
```
File ""...\tensorflow\python\ops\gradient_checker.py"", line 312, in compute_gradient
    dx, dy = _compute_dx_and_dy(x, y, y_shape)
  File ""...\tensorflow\python\ops\gradient_checker.py"", line 193, in _compute_dx_and_dy
    grads = gradients.gradients(y, x, dy)
  File ""...\tensorflow\python\ops\gradients_impl.py"", line 560, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""...\tensorflow\python\ops\gradients_impl.py"", line 368, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""...\tensorflow\python\ops\gradients_impl.py"", line 560, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""...\tensorflow\python\ops\tensor_array_grad.py"", line 158, in _TensorArrayGatherGrad
    grad_source = _GetGradSource(grad)
  File ""...\tensorflow\python\ops\tensor_array_grad.py"", line 74, in _GetGradSource
    "", got: %s"" % op_or_tensor.name)
ValueError: Expected op/tensor name to start with gradients (excluding scope), got: Identity:0
```"
8561,CTC_Greedy_Decoder outputs sum of logits and not logprobs,"## CTC_Greedy_Decoder outputs sum of logits and not logprobs

I have been writing my own CTC_Greedy_Decoder function in order to decode the output of a RNN trained with the CTC loss. To check my version, I decided to compare my outputs with TensorFlow's ones. We had the same first output (the decoded label) but we had big differences on second (the logprobs).

After some debugging, I figured out that TF's GreedyDecoder outputs the opposite of the sum of the higher logit of each column. Whereas, in the documentation it is written that we should expect logprobs. 

### How to fix:
- Change the documentation from logprobs to logits
- Instead of summing the logits, it would be  better to sum the higher logit of each column minus the logsumexp of the column. With that fix, the output will be logprobs as written in the documentation.

### Environment info
Operating System: Ubuntu 16.04
TensorFlow: 1.0.1 running on CPU


"
8560,GPU PoolAllocator never satisfied with the eviction rate. Can we limit its allocated size?,"I have a tensorflow network where I call `sess.run()` with **tensors of widly changing sizes**. _(to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M])_. Additionally, I use some `tf.where` calls which generate variable sized tensors to be transferred to the GPU.

Everything works fine and training goes nicely and quickly, but eventually it seems that the **`pool_allocator` is never satisfied with the eviction rate it gets and constantly try to increase its size**. 

```
2017-03-20 18:24:07.895271: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 590087 get requests, put_count=590089 evicted_count=2000 eviction_rate=0.00338932 and unsatisfied allocation rate=0.0140911
2017-03-20 18:24:07.895342: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 69492 to 76441
```

Eventually, it **fills the whole machine RAM** (256GB....) and **kills itself by running out-of-memory** (in less than 1h...).

This behavior only happens when running on GPU. I checked that no ops are added during training through calling `graph.finalize()`.

I also tried using `tcmalloc` as proposed [here](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429), and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in `top` was multiple GB. As far as I understand, the `pool_allocator` uses his own `malloc` system so it would not show in `tcmalloc` profiler right?

[tcmalloc profiler output](https://github.com/tensorflow/tensorflow/files/855907/output_1.pdf)

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

- [How to interpret PoolAllocator messages](http://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow)
- [How to debug a memory leak in TF](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429)
- The question I asked [here](http://stackoverflow.com/questions/42861956/gpu-poolallocator-explodes-the-cpu-memory) on SO, without much success.

### Environment info
Operating System:
Ubuntu 14.04
CUDA 8.0 + cuDNN 5.1

python 3.5 nighty build

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

I honestly tried, but it seems that if I simplify the system the problem goes away."
8559,forward pass projection,"Hi,

Is there any way to define a layer that for forward-pass calculates its output by projecting the weight values into a different value range, but for the back-propagation uses the real weight values? i.e., as an example from the [mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py) file, if for the hidden layer 1 forward pass we could have `hidden1 = tf.nn.relu(tf.matmul(images, projection_func(weights)) + biases)`, but for the gradient calculation during back-propagation the weights were updated without `projection_func()`.

This would be useful to implement BNNs, to apply functions such as the ones mentioned by @Jony101K [here](https://github.com/itayhubara/BinaryNet/issues/9#issuecomment-243705218).

Thanks.
Alexandre"
8558,Mention Julia API bindings,"In several places, like [here](https://www.tensorflow.org/api_docs/), API bindings for other languages are mentioned. It would be nice to mention the [Julia bindings](https://github.com/malmaud/TensorFlow.jl), which are now fairly stable and are actively used by many people.

It appears much of the website content comes from this repo but I can't find that page to make a PR. Would greatly appreciate a pointer to the right place for this.

cc @malmaud"
8557,Error building TensorFlow with local LLVM repository. ( Using local_repository extension in bazel ),"Essentially, I want to run TensorFlow with a custom LLVM repository and not the llvm-mirror that bazel pulls from. 

I made the following changes:

1. Changed the ```temp_workaround_http_archive``` rule in ```//tensorflow/workspace.bzl``` to:

          native.local_repository (
              name = ""llvm"",
              path = ""/git/llvm/"",
          )

Where ```/git/llvm``` is my local llvm repository.

2. In ```/git/llvm``` I added the file ```WORKSPACE``` containing:

        workspace( name = ""llvm"" )


Error log:

    bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
    ERROR: /git/tensorflow/tensorflow/tools/pip_package/BUILD:81:1: no such package '@llvm//': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:licenses'.
    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
    INFO: Elapsed time: 0.219s


Installed TensorFlow from source. Here is the version info:

    $ git rev-parse HEAD
    4c3bb1aeb7bb46bea35036433742a720f39ce348

    $ bazel version
    Build label: 0.4.5
    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
    Build time: Thu Mar 16 12:19:38 2017 (1489666778)
    Build timestamp: 1489666778
    Build timestamp as int: 1489666778


Now, obviously a ```llvm.build``` file is required, I just need to know how and where to place it. Also, how can I build tools like Polly from the bazel build files?

Thanks in advance!

"
8556,Tenor flow distributed support for spectrum lsf ,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8554,Floating Point Exception/SIGFPE in tf.map_fn over an empty tensor,"Working on a custom implementation of Faster-RCNN, it runs fine for ~150 - 300 batches but then I get a Floating Point Error, apparently in ConcatGPUImpl.

Source is here: https://github.com/Panaetius/woipv (src/models/train_model.py and src/models/model.py, it's a bit of a mess still since it's a work in progress), reproducible as of commit 6eb1e3c5e818919b64a0a981abb98c2f3bc3dea1

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: CUDA 8.0, cuDNN 5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

-rw-r--r-- 1 root root   558720 Okt  4 23:15 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Okt  4 23:15 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn_static.a


If installed from binary pip package, provide:

1. A link to the pip package you installed: official python 3 tensorflow-gpu package
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0



### Logs or other output that would be helpful
gdb:

Thread 49 ""python"" received signal SIGFPE, Arithmetic exception.
[Switching to Thread 0x7fff10bb8700 (LWP 8227)]
0x00007fffcaab91de in void tensorflow::ConcatGPUImpl<float, int>(Eigen::GpuDevice const&, tensorflow::CudaDeviceArrayStruct<float const*, 8> const&, tensorflow::CudaDeviceArrayStruct<int, 8> const&, bool, int, tensorflow::TTypes<float, 2, long>::Matrix*) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so


Backtrace:

#0  0x00007fffcaab91de in void tensorflow::ConcatGPUImpl<float, int>(Eigen::GpuDevice const&, tensorflow::CudaDeviceArrayStruct<float const*, 8> const&, tensorflow::CudaDeviceArrayStruct<int, 8> const&, bool, int, tensorflow::TTypes<float, 2, long>::Matrix*) () from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#1  0x00007fffcaaaf45c in void tensorflow::(anonymous namespace)::ConcatGPUCall<float, int>(tensorflow::OpKernelContext*, std::vector<std::unique_ptr<tensorflow::TTypes<float, 2, long>::ConstMatrix, std::default_delete<tensorflow::TTypes<float, 2, long>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<float, 2, long>::ConstMatrix, std::default_delete<tensorflow::TTypes<float, 2, long>::ConstMatrix> > > > const&, tensorflow::TTypes<float, 2, long>::Tensor*) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#2  0x00007fffc9b14c16 in tensorflow::TensorArrayPackOrGatherOp<Eigen::GpuDevice, float, false>::Compute(tensorflow::OpKernelContext*) () from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007fffcad155b2 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007fffcad56183 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007fffcad569fa in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007fffcb09a960 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007fffcb099c10 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
---Type <return> to continue, or q <return> to quit---
#8  0x00007fffc7f2c260 in ?? () from /home/zenon/anaconda3/envs/tensorflow/bin/../lib/libstdc++.so.6
#9  0x00007ffff76d16fa in start_thread (arg=0x7fff10bb8700) at pthread_create.c:333
#10 0x00007ffff6aefb5d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

"
8552,SyncReplicasOptimizer' object has no attribute 'get_clean_up_op',"```
python /models/slim/train_image_classifier.py \
  --worker_replicas=2 \
  --num_ps_tasks=1 \
  --sync_replicas=True \
  --replicas_to_aggregate=2 \
  --task=0 \
  --dataset_name=imagenet  \
  --train_dir=/tmp/train_logs  \
  --dataset_split_name=train \
  --dataset_dir=/imagenet/dataEgs \
  --model_name=inception_v3

````

Run /models/slim/train_image_classifier.py 

```
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2
INFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.
Traceback (most recent call last):
  File ""/models/slim/train_image_classifier.py"", line 583, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/models/slim/train_image_classifier.py"", line 579, in main
    sync_optimizer=optimizer if FLAGS.sync_replicas else None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 733, in train
    cleanup_op = sync_optimizer.get_clean_up_op()
AttributeError: 'SyncReplicasOptimizer' object has no attribute 'get_clean_up_op'

```

I confirm that more than one demos will encounter this issue if using distributed training under tensorflow 1.0.1 image. 

Hope some improvements for distributed training to demo how to use distributed training. 

Thanks. "
8551,  tf.image.decode_image doesn't return tensor's shape,"I try to use ""image.decode_image"" to read image file as tensor, but this function returns a tensor without ""shape"".
As said in the doc:
>  Their input and output are all of variable size. If you need fixed size images, pass the output of the decode Ops to one of the cropping and resizing Ops.
But actually it cannot be passed to resizing op without shape.

Is there a way to use import image as a normal tensor?

Codes: I ran it in ipy notebook
```
fn = '8.png'
image_contents = tf.read_file(fn)
image = tf.image.decode_image(image_contents, channels=1)
image

image.shape

image.eval().shape

img_resize = tf.image.resize_images(image, [28,28])
```
Output:
<tf.Tensor 'decode_image/cond_jpeg/Merge:0' shape=<unknown> dtype=uint8>

TensorShape(None)

(250, 250, 1)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-b913c79cb137> in <module>()
----> 1 img_resize = tf.image.resize_images(image, [28,28])

/home/tianwei/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/image_ops_impl.pyc in resize_images(images, size, method, align_corners)
    641   images = ops.convert_to_tensor(images, name='images')
    642   if images.get_shape().ndims is None:
--> 643     raise ValueError('\'images\' contains no shape.')
    644   # TODO(shlens): Migrate this functionality to the underlying Op's.
    645   is_batch = True

ValueError: 'images' contains no shape.

"
8550,"Android ""TF Stylize"" demo crash with ""input_max_range must be larger than input_min_range""","I have compiled and installed the demo app according to the instruction in the [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md).  The ""TF Detect"" and ""TF Classify"" apps are working as expected.  However, starting the ""TF Stylize"" app, it always crashes in about 3 seconds.  The prebuilt apk crashes in the same way.  Here is the android logcat recorded:

> $ adb -s b80360c7 logcat  | grep Tensor
I/TensorFlowInferenceInterface( 6801): Checking to see if TensorFlow native methods are already loaded
I/TensorFlowInferenceInterface( 6801): TensorFlow native methods not found, attempting to load via tensorflow_inference
I/TensorFlowInferenceInterface( 6801): Successfully loaded TensorFlow native methods (RunStats error may be ignored)
I/TensorFlowInferenceInterface( 6801): Model load took 540ms, TensorFlow version: 1.0.1
I/TensorFlowInferenceInterface( 6801): Successfully loaded model from 'file:///android_asset/stylize_quantized.pb'
E/TensorFlowInferenceInterface( 6801): Failed to run TensorFlow inference with inputs:[input, style_num], outputs:[transformer/expand/conv3/conv/Sigmoid]
E/TensorFlowInferenceInterface( 6801): Inference exception: java.lang.IllegalArgumentException: input_max_range must be larger than input_min_range.
E/TensorFlowInferenceInterface( 6801):   [[Node: transformer/contract/conv1/Relu_eightbit_quantize_transformer/contract/conv1/InstanceNorm/batchnorm/add_1 = QuantizeV2[T=DT_QUINT8, mode=""MIN_FIRST"", _device=""/job:localhost/replica:0/task:0/cpu:0""](transformer/contract/conv1/InstanceNorm/batchnorm/add_1, transformer/contract/conv1/Relu_eightbit_min_transformer/contract/conv1/InstanceNorm/batchnorm/add_1, transformer/contract/conv1/Relu_eightbit_max_transformer/contract/conv1/InstanceNorm/batchnorm/add_1)]]
E/AndroidRuntime( 6801):        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.getTensor(TensorFlowInferenceInterface.java:486)
E/AndroidRuntime( 6801):        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeIntoFloatBuffer(TensorFlowInferenceInterface.java:332)
E/AndroidRuntime( 6801):        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeFloat(TensorFlowInferenceInterface.java:287)

As can be inferred from the logcat, the problem happens in `tensorflow/core/kernels/quantize_op.cc` line 71.  The app is using a quantized model.  I found that `input_min_range` is set to be `inf`, and `input_max_range` is `-inf`, which is obviously wrong.

What should be done to fix it?

The crash appears on Samsung Galaxy S4, but does not appears on emulator with Intel CPU.

[Edit]
With more experiment I found that sometimes the apps works without crashing, about 1 out of 10 trials."
8549,How to  use MMI in seq2seq model?,
8547,Incorrect hessian of a quadratic function,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/5876
https://github.com/tensorflow/tensorflow/issues/7403

### Environment info
Operating System:
mac 10.12.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
No, it is a CPU version.

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
 python -c ""import tensorflow; print(tensorflow.__version__)""
1.0.1
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Hi,

I am testing hessian computation:

```
import tensorflow as tf
import matplotlib as plt
import numpy as np
import math
```

```
def getHessian(dim):
    g = tf.Graph()
    with g.as_default():
        # First create placeholders for inputs: A, b, and c.
        A = tf.placeholder(tf.float32, shape=[dim, dim])
        b = tf.placeholder(tf.float32, shape=[dim, 1])
        c = tf.placeholder(tf.float32, shape=[1])
        # Define our variable
        x = tf.Variable(np.float32(np.repeat(1,dim).reshape(dim,1)))
        # Construct the computational graph for quadratic function: f(x) = 1/2 * x^t A x + b^t x + c
        fx = 0.5 * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + c
        
        # Get gradients of fx with repect to x
        dfx = tf.gradients(fx, x)[0]
        # Compute hessian
        for i in range(dim):
            dfx_i = tf.slice(dfx, begin=[i,0] , size=[1,1])
            ddfx_i = tf.gradients(dfx_i, x)[0] # whenever we use tf.gradients, make sure you get the actual tensors by putting [0] at the end
            if i == 0: hess = [ ddfx_i ]
            else: hess.append(ddfx_i) 

        hess = tf.stack(hess,axis=0)
        hess = tf.squeeze(hess)
        init_op = tf.initialize_all_variables()
    
        with tf.Session() as sess:
            sess.run(init_op)
            # We need to feed actual values into the computational graph that we created above. 
            feed_dict = {A: np.float32(np.vstack([[1,2,3],[4,5,6],[7,8,9]])), 
                         b: np.float32(np.repeat(3,dim).reshape(dim,1)) , c: [1]}
            # sess.run() executes the graph. Here, ""hess"" will be calculated with the values in ""feed_dict"".
            print(sess.run(hess, feed_dict))


```

```
getHessian(3)
```

```
WARNING:tensorflow:From <ipython-input-2-ed7e9735185e>:31: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
[[ 1.  3.  5.]
 [ 3.  5.  7.]
 [ 5.  7.  9.]]
```

This is obviously wrong.

### What other attempted solutions have you tried?
I tried `np.float64`, it didn't help. 

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8536,pool allocator spending more than 18h to allocate and still not finished,"Hey guys, I've been having this issue which I don't really understand if it's related to memory issues or not. I've tested it on cpu and the code runs fine without any issues. On gpu I get the following messages:

```
2017-03-18 14:21:51,379 INFO - I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
2017-03-18 14:21:51,379 INFO - name: Tesla K80
2017-03-18 14:21:51,379 INFO - major: 3 minor: 7 memoryClockRate (GHz) 0.8235
2017-03-18 14:21:51,380 INFO - pciBusID 0000:00:1e.0
2017-03-18 14:21:51,380 INFO - Total memory: 11.17GiB
2017-03-18 14:21:51,380 INFO - Free memory: 11.11GiB
2017-03-18 14:21:51,381 INFO -  DMA: 0
2017-03-18 14:21:51,381 INFO -  Y
2017-03-18 14:21:51,394 INFO - Creating TensorFlow device (/gpu:0) -> 
(device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0) 2017-03-18 14:22:06,005
PoolAllocator: After 2898 get requests, put_count=2334 evicted_count=1000 
eviction_rate=0.428449 and unsatisfied allocation rate=0.574189
2017-03-18 14:22:06,025 INFO - Raising pool_size_limit_ from 100 to 110
2017-03-18 14:48:17,797 INFO -  PoolAllocator: After 2898 get requests, put_count=3604
 evicted_count=2000 eviction_rate=0.554939 and unsatisfied allocation rate=0.452381
2017-03-18 14:48:17,800 INFO - Raising pool_size_limit_ from 193 to 212
2017-03-18 15:30:38,803 INFO - PoolAllocator: After 2898 get requests, put_count=2881 
evicted_count=1000 eviction_rate=0.347102 and unsatisfied allocation rate=0.364734
2017-03-18 15:30:38,806 INFO - Raising pool_size_limit_ from 449 to 493
2017-03-18 16:46:39,551 INFO - PoolAllocator: After 8694 get requests, put_count=8682 
evicted_count=1000 eviction_rate=0.115181 and unsatisfied allocation rate=0.128479
2017-03-18 16:46:39,554 INFO - Raising pool_size_limit_ from 1158 to 1273
```
It's been like this for 18h and still no output or training has started, is this a known issue. Why does it take so long for the memory allocator to finish?"
8535,Feature request: add support for float16/float64 to tf.contrib.layers.batch_norm(),"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

`tf.contrib.layers.batch_norm` does not support `float16` due to defaulting to `dtype=tf.float32` for `get_variable()`.

https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/python/layers/normalization.py#L141

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

- Ubuntu 16.04 Docker container
- Ubuntu 16.10 Docker host
- Dockerfile base `nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04`
- CUDA 8.0.61
- CUDNN 5.1.10
- NVidia driver version 378.13

```sh
$ uname -a
Linux 97fca57d7bb6 4.8.0-41-generic #44-Ubuntu SMP Fri Mar 3 15:27:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root    556000 Jan 26 23:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root    415432 Jan 26 23:48 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root    775162 Jan 26 23:48 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 jake users       13 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 jake users       18 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 jake users 84163560 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 jake users 70364814 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn_static.a

$ conda -V
conda 4.3.14
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

```sh
$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.1
```

Conda `environment.yml`:

```yaml
name: root

channels:
- defaults
- conda-forge
- menpo

dependencies:
- ipyparallel==5.2.0
- opencv3=3.1.0
- pip:
  - keras==1.2.2
  - pydicom==0.9.9
  - tensorflow-gpu==1.0.1
  - tflearn==0.3
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```py
import tensorflow as tf
from tensorflow.contrib.layers import batch_norm
# from normalization import batch_norm
with tf.Session() as sess, tf.device('/cpu'):
  inputs = tf.convert_to_tensor([1., 2.], dtype=tf.float16)
  norm = batch_norm(inputs)
  sess.run(tf.global_variables_initializer())
  sess.run(norm)
```

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-10-f094f32eca33> in <module>()
      1 a = tf.convert_to_tensor([1., 2.], dtype=tf.float16)
----> 2 tf.contrib.layers.batch_norm(a).eval()

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)
    175       current_args = current_scope[key_func].copy()
    176       current_args.update(kwargs)
--> 177     return func(*args, **current_args)
    178   _add_op(func)
    179   setattr(func_with_args, '_key_op', _key_op(func))

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py in batch_norm(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope)
    516           _scope=sc,
    517           _reuse=reuse)
--> 518       outputs = layer.apply(inputs, training=is_training)
    519 
    520       # Add variables to collections.

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, **kwargs)
    301       Output tensor(s).
    302     """"""
--> 303     return self.__call__(inputs, **kwargs)
    304 
    305 

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, **kwargs)
    271             self.build(input_shapes)
    272           self._built = True
--> 273         outputs = self.call(inputs, **kwargs)
    274 
    275         # Apply activity regularization.

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/normalization.py in call(self, inputs, training)
    191       if not self.updates:
    192         mean_update = moving_averages.assign_moving_average(
--> 193             self.moving_mean, mean, self.momentum, zero_debias=False)
    194         variance_update = moving_averages.assign_moving_average(
    195             self.moving_variance, variance, self.momentum, zero_debias=False)

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)
     70         update_delta = _zero_debias(variable, value, decay)
     71       else:
---> 72         update_delta = (variable - value) * decay
     73       return state_ops.assign_sub(variable, update_delta, name=scope)
     74 

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _run_op(a, *args)
    675     def _run_op(a, *args):
    676       # pylint: disable=protected-access
--> 677       return getattr(ops.Tensor, operator)(a._AsTensor(), *args)
    678     # Propagate __doc__ to wrapper
    679     try:

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    791     with ops.name_scope(None, op_name, [x, y]) as name:
    792       if not isinstance(y, sparse_tensor.SparseTensor):
--> 793         y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    794       return func(x, y, name=name)
    795 

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    635       name=name,
    636       preferred_dtype=preferred_dtype,
--> 637       as_ref=False)
    638 
    639 

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    700 
    701         if ret is None:
--> 702           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    703 
    704         if ret is NotImplemented:

/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    573     raise ValueError(
    574         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--> 575         % (dtype.name, t.dtype.name, str(t)))
    576   return t
    577 

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: 'Tensor(""BatchNorm_2/Reshape_1:0"", shape=(2,), dtype=float16)'
```

### What other attempted solutions have you tried?

Use a modified `tensorflow/python/layers/normalization.py` which passes `dtype=inputs.dtype.base_dtype`.
"
8532,ImportError: cannot import name pywrap_tensorflow,"After installation of TensorFlow by official tutorial, I occured these import error messages below:
```Shell
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

/home/yuens/Downloads/code/tensorflow/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/home/yuens/Downloads/code/tensorflow/tensorflow/python/__init__.py in <module>()
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     73 
     74 # Protocol buffers

ImportError: Traceback (most recent call last):
  File ""tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name pywrap_tensorflow


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

## Tried
I can't open this page(404 error): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error"
8531,Add support for non-scalar string tensors in java,"Currently, string tensors are not supported in java:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L90

Implementing this  would be very helpful in scenarios of training a model in one language, and serving from a different one, as it allows moving some of the logic to the tensor flow model."
8530,import error after installing tensorflow from source,"i have build tensorflow from source and got a lot of warning but it was done without errors.But now when I import tensorflow it is giving the error mentioned below:

`p_rakash@prakash008:~$ python3
Python 3.5.2 |Anaconda 4.3.1 (64-bit)| (default, Jul  2 2016, 17:53:06) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name 'pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
ImportError: cannot import name 'pywrap_tensorflow'`_

any suggestion..?
I have a nvidia 720m gpu and python 3.5.2 from anaconda and I hav installed the gpu version of tensorflow."
8529,Compile Tensorflow 1.0 with GPU on RHEL6 / SL6,"Dear all,
i managed to compile Tensorflow 0.12.1 with GPU option few months ago on our computational grid based on SL6 but moving to version 1.0 is still an issue with trying to activate the GPU support.
I tried to merge all the advise i could find here but still no success.
So here is a summary:

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

This issue is actually close to the ones as #7118, #2109, #2413, etc.
My compilation attempts always show similar error but dealing with different packages. Here is the last example:
`ERROR: /gpfs/MUST-SHARE/univ_home/alben/.cache/bazel/_bazel_alben/07a9bf809031fe756469f64b1512dee9/external/grpc/BUILD:71:1: undeclared inclusion(s) in rule '@grpc//:gpr':
this rule is missing dependency declarations for the following files included by 'external/grpc/src/core/lib/support/thd_windows.c':
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 20.467s, Critical Path: 9.90s
`

**Please note that Tensorflow compiles fine if GPU support is not activated. Then, i suspect a missing include option in third_party/gpus/crosstool/CROOSTOOL.tpl or third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl since such options disappeared from version 0.12 such as `cxx_builtin_include_directory: ""/usr/local/cuda-8.0/include/"" `but i need some detailled information to fix it.**

 
### Environment info
Operating System: 
`RedHat Scientific Linux 6 with kernel 2.6.32-642.13.1.el6.x86_64
`
Regarding libc , gcc versions, i rely on the devtoolsets-4.
Using the following gcc version:
```
 gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/var/opt/rh/devtoolset-4/root/usr/bin/../libexec/gcc/x86_64-redhat-linux/5.3.1/lto-wrapper
Target: x86_64-redhat-linux
Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-4/root/usr --mandir=/opt/rh/devtoolset-4/root/usr/share/man --infodir=/opt/rh/devtoolset-4/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --disable-libgcj --with-default-libstdcxx-abi=gcc4-compatible --with-isl=/builddir/build/BUILD/gcc-5.3.1-20160406/obj-x86_64-redhat-linux/isl-install --enable-libmpx --with-mpc=/builddir/build/BUILD/gcc-5.3.1-20160406/obj-x86_64-redhat-linux/mpc-install --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
Thread model: posix
gcc version 5.3.1 20160406 (Red Hat 5.3.1-6) (GCC) 
```
Regarding system variables, here are the specific tunings:
```
set path=(/univ_home/UNIVSOFT/COMMON/python/2.7.6/bin $path)
set path=(/usr/local/cuda-8.0/bin $path)
set path=(/uds_data/listic/install/bazel/output $path)
set path=(/opt/rh/devtoolset-4/root/usr/bin $path)
setenv CC /opt/rh/devtoolset-4/root/usr/bin/gcc

setenv JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/
setenv GRPC_JAVA_PLUGIN /gpfs/MUST-DATA/listic/install/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_64.exe

setenv CUDA_HOME /usr/local/cuda-8.0
setenv INCLUDE_PATH /usr/local/cuda-8.0/include
setenv LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH

setenv LD_LIBRARY_PATH /gpfs/MUST-DATA/listic/install/caffe_must/cudnn_51/cuda/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/opt/rh/devtoolset-4/root/usr/lib:$LD_LIBRARY_PATH
#set path=(/uds_data/listic/install/gcc/build6.2/bin $path)
setenv EXTRA_BAZEL_ARGS '-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'


```
Installed version of CUDA and cuDNN: Cuda 8.0 and CuDNN 5.1
```
ls /usr/local/cuda-8.0/lib64/libcud*
/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudart.so
/usr/local/cuda-8.0/lib64/libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a


ls /univ_home/UNIVSOFT/COMMON/cudnn/lib64
libcudnn.so  libcudnn.so.5  libcudnn.so.5.1.5  libcudnn_static.a  libm.so

```
Tensorflow is being compiled from source 

1. The commit hash (`git rev-parse HEAD`)
```
git rev-parse HEAD
e895d5ca395c2362df4f5c8f08b68501b41f8a98
```
2. The output of `bazel version`
I actually tried with the following bazel versions : 0.4.3, 0.4.4 and 0.4.5 and always got the same errors.
```
bazel version
Build label: 0.4.3-2017-03-18 (@1d2fb1f)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sat Mar 18 15:58:41 2017 (1489852721)
Build timestamp: 1489852721
Build timestamp as int: 1489852721

```

Here are my local code changes to target devtoolsets-4 compiling tools:

```
git diff
diff --git a/tensorflow/core/platform/default/build_config.bzl b/tensorflow/core
index 48ef8df..be831d5 100644
--- a/tensorflow/core/platform/default/build_config.bzl
+++ b/tensorflow/core/platform/default/build_config.bzl
@@ -8,7 +8,7 @@ load(""//tensorflow:tensorflow.bzl"", ""if_not_mobile"")
 WITH_GCP_SUPPORT = False
 WITH_HDFS_SUPPORT = False
 WITH_XLA_SUPPORT = False
-WITH_JEMALLOC = True
+WITH_JEMALLOC = False
 
 # Appends a suffix to a list of deps.
 def tf_deps(deps, suffix):
diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl
index 3788eed..fa5b670 100644
--- a/tensorflow/tensorflow.bzl
+++ b/tensorflow/tensorflow.bzl
@@ -751,7 +751,7 @@ def tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[]
   )
 
 def tf_extension_linkopts():
-  return []  # No extension link opts
+  return [""-lrt""]  # No extension link opts
 
 def tf_extension_copts():
   return []  # No extension c opts
diff --git a/third_party/gpus/crosstool/CROSSTOOL.tpl b/third_party/gpus/crossto
index b77a45c..746817f 100644
--- a/third_party/gpus/crosstool/CROSSTOOL.tpl
+++ b/third_party/gpus/crosstool/CROSSTOOL.tpl
@@ -42,10 +42,10 @@ toolchain {
   target_system_name: ""local""
   toolchain_identifier: ""local_linux""
 
-  tool_path { name: ""ar"" path: ""/usr/bin/ar"" }
-  tool_path { name: ""compat-ld"" path: ""/usr/bin/ld"" }
-  tool_path { name: ""cpp"" path: ""/usr/bin/cpp"" }
-  tool_path { name: ""dwp"" path: ""/usr/bin/dwp"" }
+  tool_path { name: ""ar"" path: ""/opt/rh/devtoolset-4/root/usr/bin/ar"" }
+  tool_path { name: ""compat-ld"" path: ""/opt/rh/devtoolset-4/root/usr/bin/ld"" }
+  tool_path { name: ""cpp"" path: ""/opt/rh/devtoolset-4/root/usr/bin/cpp"" }
+  tool_path { name: ""dwp"" path: ""/opt/rh/devtoolset-4/root/usr/bin/dwp"" }
   # As part of the TensorFlow release, we place some cuda-related compilation
   # files in @local_config_cuda//crosstool/clang/bin, and this relative
   # path, combined with the rest of our Bazel configuration causes our
@@ -56,21 +56,23 @@ toolchain {
   cxx_flag: ""-std=c++11""
   linker_flag: ""-Wl,-no-as-needed""
   linker_flag: ""-lstdc++""
-  linker_flag: ""-B/usr/bin/""
+  linker_flag: ""-lm""
+  linker_flag: ""-lrt""
+  linker_flag: ""-B/opt/rh/devtoolset-4/root/usr/bin/""
 
 %{gcc_host_compiler_includes}
-  tool_path { name: ""gcov"" path: ""/usr/bin/gcov"" }
+  tool_path { name: ""gcov"" path: ""/opt/rh/devtoolset-4/root/usr/bin/gcov"" }
 
   # C(++) compiles invoke the compiler (as that is the one knowing where
   # to find libraries), but we provide LD so other rules can invoke the linker.
-  tool_path { name: ""ld"" path: ""/usr/bin/ld"" }
+  tool_path { name: ""ld"" path: ""/opt/rh/devtoolset-4/root/usr/bin/ld"" }
 
-  tool_path { name: ""nm"" path: ""/usr/bin/nm"" }
-  tool_path { name: ""objcopy"" path: ""/usr/bin/objcopy"" }
+  tool_path { name: ""nm"" path: ""/opt/rh/devtoolset-4/root/usr/bin/nm"" }
+  tool_path { name: ""objcopy"" path: ""/opt/rh/devtoolset-4/root/usr/bin/objcopy""
   objcopy_embed_flag: ""-I""
   objcopy_embed_flag: ""binary""
-  tool_path { name: ""objdump"" path: ""/usr/bin/objdump"" }
-  tool_path { name: ""strip"" path: ""/usr/bin/strip"" }
+  tool_path { name: ""objdump"" path: ""/opt/rh/devtoolset-4/root/usr/bin/objdump""
+  tool_path { name: ""strip"" path: ""/opt/rh/devtoolset-4/root/usr/bin/strip"" }
 
   # Anticipated future default.
   unfiltered_cxx_flag: ""-no-canonical-prefixes""
diff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_no
index b7d6cc6..e78b70c 100755
--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.t
+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.t
@@ -51,7 +51,7 @@ GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')
 
 CURRENT_DIR = os.path.dirname(sys.argv[0])
 NVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'
-LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc')
+LLVM_HOST_COMPILER_PATH = ('/opt/rh/devtoolset-4/root/usr/bin/gcc')
 PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)
 NVCC_VERSION = '%{cuda_version}'
 
(END) 
```
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Here is my configuration step:
```
./configure
Please specify the location of python. [Default is /univ_home/UNIVSOFT/COMMON/python/2.7.6/bin/python]: 
Please specify optimization flags to use during compilation [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] n
jemalloc disabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] 
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] 
No XLA support will be enabled for TensorFlow
Found possible Python library paths:
  /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages]

Using python library path: /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /opt/rh/devtoolset-4/root/usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: /univ_home/UNIVSOFT/COMMON/cudnn/
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
.
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
...............
INFO: All external dependencies fetched successfully.
Configuration finished
```

Next, compiling:

```
bazel build --linkopt='-lrt' -c opt --config=cuda --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
INFO: Found 1 target...
ERROR: /gpfs/MUST-SHARE/univ_home/alben/.cache/bazel/_bazel_alben/07a9bf809031fe756469f64b1512dee9/external/grpc/BUILD:71:1: undeclared inclusion(s) in rule '@grpc//:gpr':
this rule is missing dependency declarations for the following files included by 'external/grpc/src/core/lib/support/tmpfile_windows.c':
  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 12.842s, Critical Path: 3.24s

```
This time the error highlights grpc inclusion errors, it used to be related to @nasm or others. depending on the trials.


### What other attempted solutions have you tried?
Tried bazel version 0.4.3, 0.4.4, 0.4.5 with the same results.
Tried Tensorflow R1.0, v1.0.1, same results

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8528,There is a learning_rate parameter in the Adadelta implementation although there isn't such one in the original paper,"Hi,
I see in the documentation that `tf.train.AdadeltaOptimizer` has a `learning_rate` parameter, but in the original paper that parameter was eliminated from the update rule [see equation 14 in the paper](http://arxiv.org/pdf/1212.5701v1.pdf) (or step 5 in the pseudo code of the algorithm).
Is that a bug in the implementation? 
If you deliberately add a learning_rate parameter, I think that the API documentation should state explicitly that this is NOT the exact implementation of the algorithm as in the paper."
8527,configure script shouldn't ask interactive questions when its output isn't a terminal,It should use the default values.
8526,"""Feed"" cannot run in Windows","Environment:
Windows 10
CPU: i7 6700HQ
GPU: 960M
Driver: 368.71
CUDA: 8.0  
CUDNN: 5
Python 3.5.2
TensorFlow-GPU 1.0.1
-------------------------------------------------------------
When I try to run this    

```Python
import tensorflow as tf

input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.add(input1, input2)  

with tf.Session() as sess:
    print(sess.run([output], feed_dict={input1:32.2, input2:13.35}))
```

My program will crash and show that Python has stopped working.  

This is trace:
```
E:\PyCode>python test.py
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 960M
major: 5 minor: 0 memoryClockRate (GHz) 1.176
pciBusID 0000:02:00.0
Total memory: 4.00GiB
Free memory: 3.35GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)
Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #0: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #1: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #2: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #3: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #4: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #5: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #6: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #7: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #8: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #9: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #10: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #11: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #12: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #13: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #14: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #15: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #16: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #17: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #18: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #19: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #20: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #21: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #22: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #23: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #24: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #25: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #26: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #27: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #28: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #29: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #30: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #31: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #32: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #33: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #34: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #35: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #36: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #37: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #38: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #39: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #40: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #41: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #42: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #43: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #44: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #45: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #46: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #47: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #48: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #49: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #50: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #51: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #52: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #53: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #54: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #55: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #56: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #57: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #58: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #59: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #60: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #61: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #62: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #63: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #64: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #65: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #66: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #67: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #68: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #69: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #70: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #71: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #72: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #73: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #74: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #75: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #76: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #77: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #78: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #79: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #80: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #81: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #82: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #83: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #84: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #85: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #86: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #87: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #88: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #89: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #90: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #91: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #92: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #93: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #94: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #95: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #96: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #97: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #98: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #99: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #100: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #101: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #102: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #103: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #104: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #105: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #106: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #107: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #108: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #109: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #110: CUDA driver version is insufficient for CUDA runtime version
Failed to initialize CUDA device #111: CUDA driver version is insufficient for CUDA runtime version
```
But, if change test.py as this
```Python
import tensorflow as tf

with tf.Session() as sess:
    with tf.device('/cpu:0'):
        input1 = tf.placeholder(tf.float32)
        input2 = tf.placeholder(tf.float32)
        output = tf.add(input1, input2)  
        print(sess.run([output], feed_dict={input1:32.2, input2:13.35}))
```
It will OK 
```
(py35) E:\PyCode>python test.py
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 960M
major: 5 minor: 0 memoryClockRate (GHz) 1.176
pciBusID 0000:02:00.0
Total memory: 4.00GiB
Free memory: 3.35GiB
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0:   Y
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
[45.550003]
```

"
8525,Tensorflow first time run error.,"
I installed tensorflow throuh pip3 in windows. And it installed successfully.
Then i run a simple program to check if it is working or not. I got this error.
Python version 35. 
CODE :  
```
import tensorflow as tf  
hello = tf.constant('Hello, TensorFlow!')  
sess = tf.Session()  
print(sess.run(hello))
```  

Output :

> E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') fo
r unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_t
ype: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""')
 for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for
unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_ty
pe: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""')
for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""'
) for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') fo
r unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') fo
r unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""
') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""C
PU""') for unknown op: UpdateFertileSlots
b'Hello, TensorFlow!'"
8524,Asking for TF's official implementation of the inception v3 model's distributed training,"From the recorded video of TF dev summit held about one month ago, I got to know the newest distributed TF 1.0 is able to achieve 57x speed up for the inception v3 model on a server of 8 nodes/ 8 gpus and the codes will be released. Since I am far away from getting such a good scaling performance using my own distributed implementation, I have been looking forward to seeing and studying from the training codes from TF. Could anyone tell me the progress now? Thanks!"
8523,Usage of sparse_average_precision_at_k when dense labels are available,"Is there any example for usage of sparse_average_precision_at_k?
"
8521,Add tf.repeat equivalent to np.repeat,"In some applications, we need to repeat Tensors, so hopefully there could be a build-in function tf.repeat which is equivalent to np.repeat. 

Example:
```
 np.repeat([1, 2, 3], 2)
>> array([1, 1, 2, 2, 3, 3])
```
"
8519,error while building tensorflow from source,"_I am trying to install tensorflow from source and getting this error while building tensorflow from bazel using the command:_ 

`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
`
_and the error is:_

`ERROR: /home/prakash/.cache/bazel/_bazel_prakash/8d51bbac70ad4d48b6548f0de307c16b/external/nccl_archive/BUILD.bazel:33:1: error while parsing .d file: /home/prakash/.cache/bazel/_bazel_prakash/8d51bbac70ad4d48b6548f0de307c16b/execroot/tensorflow/bazel-out/local_linux-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce.cu.pic.d (No such file or directory).


nvcc fatal   : Unsupported gpu architecture 'compute_21'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 360.810s, Critical Path: 345.61s`

_I have a Nvidia 720M graphic chip on my laptop is that causing the issue .? any advice would be helpful.
the configuration i did was this:_

`prakash@prakash008:~/Downloads/tensorflow$ ./configure 
Please specify the location of python. [Default is /home/prakash/anaconda3/bin/python]: 
Please specify optimization flags to use during compilation [Default is -march=native]: 
Do you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] n
jemalloc disabled on Linux
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n
No XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  /home/prakash/anaconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/prakash/anaconda3/lib/python3.6/site-packages]

Using python library path: /home/prakash/anaconda3/lib/python3.6/site-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] n
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5
Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 3.5,2.1`"
8518,tf.contrib.seq2seq Documentation,"Where can I find documentation for `tf.contrib.seq2seq`?

I get a 404 error on [attention_decoder_fn.py].(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py)

Namely, I want to hack the `attention_decoder_fn_inference` or `dynamic_rnn_decoder` function during inference in order to sample the outputs of each cell based on output probabilities before feeding into the new cell. The aforementioned functions seem to be a black box that predict the most probable class at each cell.

Thanks"
8517,"CUDA_ERROR_DEINITIALIZED running CIFAR10_multi_gpu on CUDA8.0 with cuDNN v5, built from pip","## Environment info

### Hardware Platform: 
Google Cloud Compute Engine
4 vCPUs, 15 GB memory
4 * NVIDIA Tesla K80

### Software Platform: 
Operating System: Ubuntu 16.04
cuDNN v5
CUDA toolkit 8.0
CUDA Compute Capability 3.7

### Tensor-flow:
Installed from pip 
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl
(Installing from source also doesn't work)
## Steps to reproduce

python3 cifar10_multi_gpu_train.py --num_gpus=4

Error occurs after training.

## What have you tried?

1. restart machine and rerun several times;
2. --num_gpus=2 --> Segmentation fault
3. Reinstall Tensor-flow from source

## Logs or other output that would be helpful
Log file is attached.
[Terminal Saved Output.txt](https://github.com/tensorflow/tensorflow/files/852192/Terminal.Saved.Output.txt)
"
8516,Missing pip install file for Python3 GPU version for MacOS platform,"Receive the following error when attempting to install GPU TF 1.0.1

`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl`

`Collecting tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl
  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl
  Could not install requirement tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl
Could not install requirement tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl for URL https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl`

### Environment info
Operating System:  MacOS 10.12.3

Python 3.5.2 |Anaconda 4.3.1 (x86_64)| (default, Jul  2 2016, 17:52:12) 
[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin


"
8515,Gradients Tutorial feature requests.,"
I need to modify the tensorflow source for specific ops. When add one more input to the `REGISTER_OP` from

    .Input(""feature: T"")
    .Input(""labels: T"")
to 

    .Input(""feature: T"")
    .Input(""labels: T"")
    .Input(""my_input: T"")

the forward version of corresponding codes do work, but the backpropagation version of corresponding codes seem to need further modification. Since the organization of TF source codes seems too complicated to me, I don't what to modify for the next step.

Here is the Error I got:
  
        grads = optimizer.compute_gradients(total_loss)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 345, in compute_gradients
        colocate_gradients_with_ops=colocate_gradients_with_ops)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 488, in gradients
        _VerifyGeneratedGradients(in_grads, op)
      File ""/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 255, in _VerifyGeneratedGradients
        ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
    ValueError: Num gradients 2 generated for op name: ""softmax_cross_entropy_loss/xentropy""
    op: ""SoftmaxCrossEntropyWithLogits""
    input: ""softmax_cross_entropy_loss/Reshape""
    input: ""softmax_cross_entropy_loss/Reshape_1""
    input: ""Reshape_3""
    attr {
      key: ""T""
      value {
        type: DT_FLOAT
      }
    }
     do not match num inputs 3

Any help will be appreciated. Thanks.
"
8514,caffe/tensorflow/mxnet: converter from python to c++,"https://github.com/rmekdma/python_misc/blob/master/caffe/caffe_ftr.py

I need to do some system integration in C++. Given the above caffe code,

how can I track the C++ code being used behind each caffe/tensorflow/mxnet function in python?

Can I generate the C++ code so that I can compile and run?

What is the python wrapper used by caffe/tensorflow/mxnet? ctype, boot:python or swig?

Similarly, it will be interesting to know whether I can do the same from python to c++ in tensorflow , mxnet or other python binding code, if it is easier to do it in tensorflow.

http://stackoverflow.com/questions/42869355/caffe-tensorflow-mxnet-how-to-convert-python-to-c"
8513,ImportError: cannot import name 'moving_averages' ,"I have a fresh install on AWS EC2 p2.xlarge.

The TensorFlow install method was pip:
```
conda list  | grep -E ""keras|teano|tensorflow""
tensorflow-gpu            1.0.1                     <pip>
```

Here is the error:
```
/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in <module>()
      1 import tensorflow as tf
----> 2 from tensorflow.python.training import moving_averages
      3 from tensorflow.python.ops import tensor_array_ops
      4 from tensorflow.python.ops import control_flow_ops
      5 from tensorflow.python.ops import functional_ops

ImportError: cannot import name 'moving_averages'
```
Thank you for your support!"
8511,Documentation for serving canned estimators,"Hey, 

the documentation on how to serve canned estimators with tensorflow serving is somewhat confusing. The docstring for export_savedmodel says to provide a function returning an InputFnOps which isn't documented anywhere. In the source it says it moved to estimator/export.py and InputFnOps was renamed to ServingInputReceiver. ~~However, this does not exist at all.~~ [I might have been confused by multiple checked out versions here, I suppose]

Digging around in the tests for estimator it all is becoming a bit clearer but it would be great if there were more documentation on this topic (also from the serving side as I now have my model exported but cannot get any predictions out of it :( )

Thanks!

Christian

Mentions @martinwicke and @ispirmustafa"
8510,Error running TF just after install,"Hello, guys.
I got an error just after install Python 3.5.3 and Tensorflow in a Windows 10 system and it fails in the very first test, the one suggested in Tensorflow install instructions.
The messages I got are here:

Python 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> hello=tf.constant('Hello')
>>> sess=tf.Session()
>>> print(sess.run(hello))
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'Hello'
>>>

Do you know how to fix this issue.

Thanks in advance,

Daniel Vermes
"
8509,"Hello, guys.","NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8508,Expose feature importances in TensorForestEstimator,"TensorForestEstimator can log/print feature importances but they are not accessible.

Could you expose a feature_importances property in TensorForestEstimator or expose the graph builder object instantiated by it so we can access the feature_importances() method?"
8507,LocalCLIDebugWrapperSession hides exception messages,"### Environment info
Operating System: osx

Installed version of CUDA and cuDNN: no

If installed from binary pip package, provide:

1. A link to the pip package you installed: nightly, python3, today
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`:1.0.1

```python
import tensorflow as tf
with tf.device('/gpu:0'):
    x = tf.get_variable('W', shape=[10])

sess = tf.Session()
from tensorflow.python import debug as tf_debug
sess = tf_debug.LocalCLIDebugWrapperSession(sess)
sess.run(tf.global_variables_initializer())
```

My machine doesn't have GPU, and the above code throws:
```
OSError: Dump root directory /var/folders/3t/1kq225bs04j3k_2rbdcn2g34g86_w8/T/tfdbg_fw0uylcr does not exist
```
But it should throw the device error, like when debug session is not used:
```
Cannot assign a device to node 'W': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
```"
8506,A Strange Behavior in TensorFlow about Conv2d Creation and Weight Optimization,"To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.

It seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?


Version 1
---
`sigma = 0.1`
`x = tf.placeholder(tf.float32, (None, 32, 32, 3))`

`conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))`
`conv1_b = tf.Variable(tf.zeros(6))`
`conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b`


Version 2
---

`def weights(dims, mu = 0, sigma = 0.1):`
&nbsp;&nbsp;&nbsp;&nbsp;`w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))`
&nbsp;&nbsp;&nbsp;&nbsp;`return w`

`def bias(dims):`
&nbsp;&nbsp;&nbsp;&nbsp;`b = tf.Variable(tf.zeros(dims))`
&nbsp;&nbsp;&nbsp;&nbsp;`return b`

`def conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):`
&nbsp;&nbsp;&nbsp;&nbsp;`w = weights(dims)`
&nbsp;&nbsp;&nbsp;&nbsp;`b = bias(dims[-1])`
&nbsp;&nbsp;&nbsp;&nbsp;`conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b`
&nbsp;&nbsp;&nbsp;&nbsp;`return conv2`
this line is the only line that I put in my network architecture: so I created my conv2d layers like this:

`conv2 = conv(x, [5, 5, 3, 6]) `"
8505,tutorials/rnn/translate/translate.py Errors,"Hello,

I am trying to run the ""translate.py"" example from the sequence to sequence tutorial but it fails.

The tensorflow version is today nightly built (tensorflow-1.0.1-cp35-cp35m-linux_x86_64.whl). The model git containing the tutorial  is at the latest commit on the master branch.

I am using a linux OS.

The error message is:

ValueError: Shape must be rank 2 but is rank 1 for 'model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/MatMul_1' (op: 'MatMul') with input shapes: [?], [?,1024]

Many thanks!

Alexandre Nanchen
"
8504,Cannot reuse variables by tf.layers.conv2d,"I am trying to make two conv layers share the same weights, however, it seems the API does not work. 
```
import tensorflow as tf

x = tf.random_normal(shape=[10, 32, 32, 3])

conv1 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=None, name='conv')
print(conv1.name)
conv2 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=True, name='conv')
print(conv2.name)
```

gives

```
conv/BiasAdd:0
conv_2/BiasAdd:0
```

--------------------------------Update-------------------------------
According to a [post](https://stackoverflow.com/questions/42862300/tensorflow-reuse-variable-with-tf-layers-conv2d), the weights are already sharing, with different layer names, since the computation not sharing. However, is it feasible to consider a better naming strategy so that it is easier to see from names that different layers are sharing the same weights ?"
8503,Can aggregate all gradients from local wokers on local GPUs before push to parameter server ？,"Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph， before pushing gradients to all pservers？ 

If OK， can show us some examples？ 

BTW： 

* with r1.0.1 version, inception training with distributed nodes will crash for several old APIs，and some more unpredictable reason. 
  > TypeError: __init__() got an unexpected keyword argument 'replica_id' . 

* some ps error comes if we use non-default DNS IP when multiple NICS exist.  

"
8502,iOS - No OpKernel was registered to support Op 'ExtractImagePatches' with these attrs,"Hi everyone,

I am trying to load a graph inside iOS that was generated by another software into a .pb file.  It was tested on a Ubuntu machine, so I know the model works correctly.
When trying to use it with iOS, the project builds correctly and then I have the following error:

```
Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'ExtractImagePatches' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: ExtractImagePatches = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 2, 2, 1], padding=""VALID"", rates=[1, 1, 1, 1], strides=[1, 2, 2, 1]](concat)]]
```

I know other issues have been posted related to this; however I was unable to locate a solution.

I am using TensorFlow 1.0.1 and Python 3.6.0
I can run the iOS examples with no problem.

iOS seems not to be able to find `ExtractImagePatches` Op.

If anyone could help with this issue, it would be greatly appreciated.


"
8501,seq2seq.py: encoder_inputs from embedding_attention_seq2seq to static_rnn wrong?,"The encoder_inputs to the embedding_attention_seq2seq function is stated as ""A list of 1D int32 Tensors of shape **[batch_size]**.""

Which is a shape of **[input_size, batch_size]**

This same input is then passed to **core_rnn.static_rnn** directly where **static_rnn** expects an input shape of **[batch_size, input_size]** instead.

```
with variable_scope.variable_scope(
      scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:
    dtype = scope.dtype
    # Encoder.
    encoder_cell = core_rnn_cell.EmbeddingWrapper(
        cell,
        embedding_classes=num_encoder_symbols,
        embedding_size=embedding_size)
    encoder_outputs, encoder_state = core_rnn.static_rnn(
        encoder_cell, encoder_inputs, dtype=dtype)
```

This seems like an error or am I unclear?

Thanks"
8500,"TensorFlow 1.0 on Windows: OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits","Just installed TensorFlow 1.0 on Windows 10 (x64) on a Surface Pro 4 following the instructions from here: https://www.tensorflow.org/install/install_windows (e.g. pip3 install --upgrade tensorflow).

I'm unable to validate my installation, though - here's what I'm running into:

```~\Projects> python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf;
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
>>> print(sess.run(hello))
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
b'Hello, TensorFlow!'
>>>
```

There seem to be several similar reports that indicate this is fixed, but as far as I can tell, this is still effectively broken when you install a clean version of TensorFlow on Windows. Is installing a nightly build the only way to get past this?"
8498,Upgrade Protobuf,"We are building TensorFlow master using bazel on 64 bit platform.

We need a fix for 64 bit platform as mentioned in #1044 in `tensorflow/third_party/protobuf/3.0.0/src/google/protobuf/stubs/atomicops_internals_generic_gcc.h`. 

This fix is available in protobuf with commit id [c59473d ](https://github.com/google/protobuf/commit/c59473d53eafadd126502657e5c5c33e952b67ed)or higher.
Will it be possible to pick this or higher commit of protobuf in TensorFlow?

"
8497,Tensorflow Retrain Model performance,"I am using [Tensorflow for poet](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) guide for train own model. I have create retrained_graph.pb and retrained_labels.txt. While I use it in application then I get error that 


`Caused by: java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization(). at org.tensorflow.Graph.importGraphDef(Native Method) at org.tensorflow.Graph.importGraphDef(Graph.java:118) `

After That  further train model for application use [Tensorflow for mobile ](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/)  blog and create **optimized_graph.pb, rounded_graph.pb, mmapped_graph.pb** files.

optimized_graph.pb and rounded_graph.pb file work in android application without any error.
While use mmapped_graph.pb I get error that **Failed to initialize: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef**

Performance of application is not good while use optimized_graph.pb and rounded_graph.pb file.While application camera screen not contain any flower photos otherwise random flower name show with high confidence rate. Any way to detect only flower and remain blank when not flowers.

![screenshot](https://cloud.githubusercontent.com/assets/25680329/24044730/8edab30c-0b42-11e7-8209-a5fab382b81c.png)
"
8496,[Feature request] np.random.choice analogue,"TF has no analogue to `np.random.choice` function that chooses random element from tensor (optionally according to provided probabilities). It's especially useful in RL problems when you use epsilon-greedy exploration strategy.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
[These](http://stackoverflow.com/questions/41123879/numpy-random-choice-in-tensorflow) [two](http://stackoverflow.com/questions/37757986/weighted-random-tensor-select-in-tensorflow) solve the problem using `tf.multinomial`, but maybe it will be convenient to include `tf.random_choice` in TF API?

It could be implemented through `tf.multinomial` , for example."
8495,"""grep: AVX: No such file or directory"" -> importing tensorflow libraries in python, on mac ","When I try to import tensorflow in python, under python2.7 or 3 I get the following error output, it loads fine, but the errors are annoying.. (Also running in bash terminal if that matters...)

grep: AVX: No such file or directory
grep: 10.9.: No such file or directory

I thought it might be an issue with the specific version of grep on the mac os so I updated to the latest grep using brew, and now I get this error...

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
grep: AVX: No such file or directory
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
grep: 10.9.: No such file or directory

which is going in the opposite direction I was hoping to. Is anyone else getting this? Is there a fix? Much appreciated any help on this, have no clue if this is just something I need to live with on a mac, or if I can find a way to fix it.

Should also mention i am using version 1.0.1 of tensorflow which I got running this command from above... python -c ""import tensorflow; print(tensorflow.version)""

Installed tensorflow using
pip3 install --upgrade tensorflow
for python 3 for instance."
8494,"Segfault: ""Check failed: num_elements_ <= kMaxElements""","I'm running the current nightly build as of today (2017-03-17). I'm working with very large, sparse matrices, and my program seems to have hit a variable overflow. It crashed with the following log entry:

    2017-03-17 10:48:34.554715: F tensorflow/core/framework/tensor_shape.cc:212] Check failed: num_elements_ <= kMaxElements (1141286132736 vs. 1099511627776)
    Aborted (core dumped)

I'm guessing this is because my sparse matrix is too large to handle for tensorflow. It would be great if this could somehow be fixed. Please refer to #7783 for more details about my code (that was another overflow issue I encountered).




"
8493,Implementation of LSTMCell inconsistent with the refered paper.,"This is an extended issue of the existing issue #8469 .

In the comments, it said that the LSTMCell is implemented based on this [paper](https://research.google.com/pubs/archive/43905.pdf). 
In this paper, the several gates are computed like `sum_i(args[i] * W[i])`. Here, the args are list of input and m_state. More details can be found in the mentioned paper.
However, in the code the gates are computed as:
```
      # i = input_gate, j = new_input, f = forget_gate, o = output_gate
      lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)
      i, j, f, o = array_ops.split(
          value=lstm_matrix, num_or_size_splits=4, axis=1)
```
Diving into the `_linear` function, showing that:
```
      res = math_ops.matmul(array_ops.concat(args, 1), weights)
```
The `args `are firstly concatenated, and then `matmul `with the weights. In this implementation, the parameter number of the weights is much larger than what said in the paper. This is totally different from what said in the paper.


"
8492,Gradient by a function issue,"I find out an inconsistency in tf.gradients function.

```
import tensorflow as tf

x = tf.get_variable(""x"", shape=(1,))
x2 = x * 2
x4_issue = (x * 2) * 4
x4 = x2 * 4

issue_grads = tf.gradients(x4_issue, x2)[0]
grads = tf.gradients(x4, x2)[0]

print issue_grads
print grads
```


 I would expect the two prints have the same output because the expressions are equivalent.
However, output that I'm getting is following:

```
None
Tensor(""gradients_1/mul_3_grad/Reshape:0"", shape=(1,), dtype=float32)
```

"
8491,Convolution and CuDNN version TF v1.0,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Did not find much.

### Environment info
Operating System:

Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0-64-generic x86_64)

Installed version of CUDA and CuDNN: 

```
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/stubs/libcuda.so
/usr/local/cuda-8.0/lib64/libcudart.so.8.0
/usr/local/cuda-8.0/lib64/libcudadevrt.a
/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
/usr/local/cuda-8.0/lib64/libcudart.so
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.5
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-8.0/extras/Debugger/include/libcudacore.h
/usr/local/cuda-8.0/extras/Debugger/lib64/libcudacore.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcuda.7
/usr/local/cuda-8.0/doc/man/man7/libcuda.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cudnn-5.0/lib64/libcudnn.so.5.0.5
/usr/local/cudnn-5.0/lib64/libcudnn.so
/usr/local/cudnn-5.0/lib64/libcudnn.so.5
/usr/local/cudnn-5.0/lib64/libcudnn_static.a
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib64/stubs/libcuda.so
/usr/local/cuda-7.5/lib64/libcudart.so.7.5
/usr/local/cuda-7.5/lib64/libcudadevrt.a
/usr/local/cuda-7.5/lib64/libcudart.so
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/extras/Debugger/include/libcudacore.h
/usr/local/cuda-7.5/extras/Debugger/lib64/libcudacore.a
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/doc/man/man7/libcuda.7
/usr/local/cuda-7.5/doc/man/man7/libcuda.so.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5
/usr/local/cuda-7.5/lib/libcudadevrt.a
/usr/local/cuda-7.5/lib/libcudart.so
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/lib/i386-linux-gnu/libcuda.so.370.28
/usr/lib/i386-linux-gnu/libcuda.so
/usr/lib/i386-linux-gnu/libcuda.so.1
/usr/lib/x86_64-linux-gnu/libcuda.so.370.28
/usr/lib/x86_64-linux-gnu/libcudart_static.a
/usr/lib/x86_64-linux-gnu/stubs/libcuda.so
/usr/lib/x86_64-linux-gnu/libcuda.so
/usr/lib/x86_64-linux-gnu/libcudart.so.7.5
/usr/lib/x86_64-linux-gnu/libcudadevrt.a
/usr/lib/x86_64-linux-gnu/libcuda.so.1
/usr/lib/x86_64-linux-gnu/libcudart.so
/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18
```

If installed from binary pip package, provide:

```
pip install tensor-gpu --upgrade
```
Using TF v1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Model compiles and runs when avoiding function call:

```
tf.nn.conv2d()
```

### What other attempted solutions have you tried?

Using tensorflow 0.12rc.However, this changes a bunch of our implementations due to API changes. Have not tried downgrading to CuDNN v5.0.

### Error Log:

```
E tensorflow/stream_executor/cuda/cuda_dnn.cc:390] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5110 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 
Aborted (core dumped)
```


Thanks! Really enjoying TF v1.0.1

"
8490,Tools for generating the website is not available in this public github repo ,"Could the website generator tools for tensorflow.org be changed to  open source tools here. It will be good and appreciated!
Many people want to get the offline version of tensorflow documentation, not only the API documentation, I think they can use the  tools to generate the website from markdown files, and  to offline html files. It will be helpful.
And I think if the tool become a open source tool. It will be more useful and powerful.  thanks:)"
8489,I cant download it !,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8487,init_fn Error of tf.train.Scaffold,"In my experiment the **init_fn** of the **tf.train.Scaffold** can not  work correctly. It seems that this is caused by the **lambda** usage in the **__init__** of **tf.train.Scaffold**



### Environment info
Operating System: ubuntu 16.04

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root root    556000 Jan 26 23:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root        16 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root        19 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root    415432 Jan 26 23:48 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root    775162 Jan 26 23:48 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 1000 users       13 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 1000 users       18 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 1000 users 84163560 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 1000 users 70364814 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn_static.a


All the experiments run in a **nvidia-docker container** run with **tensorflow/tensorflow:devel-latest-gpu** image

1. A link to the pip package you installed: tensorflow/tensorflow:devel-latest-gpu
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
with tf.Graph().as_default():
    images = tf.placeholder(shape=[32, 224, 224, 3], dtype=tf.float32)`
    logits = tf.layers.conv2d(images, 3, 1)
    all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
    my_init_fn = slim.assign_from_checkpoint_fn('', all_vars)
    scaffold = tf.train.Scaffold(init_fn=my_init_fn)
    with tf.train.MonitoredTrainingSession(
        checkpoint_dir='',
        scaffold=scaffold,
        ) as session:
        pass
```
### What other attempted solutions have you tried?
It seems that the problem may be solved by replacing the 
```python
 scaffold = tf.train.Scaffold(init_fn=my_init_fn)
```
with 
```python
scaffold = tf.train.Scaffold()
scaffold._init_fn = my_init_fn
```

### Logs or other output that would be helpful
```
Traceback (most recent call last):
  File ""start_job.py"", line 219, in <module>
    train_param=get_train_param(train_args)
  File ""/root/py_libs/slim_toolbox/training/train_core.py"", line 150, in train_network
    config=get_default_sess_config()) as session:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 315, in MonitoredTrainingSession
    return MonitoredSession(session_creator=session_creator, hooks=all_hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 601, in __init__
    session_creator, hooks, should_recover=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 434, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 767, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 772, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 494, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 375, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 264, in prepare_session
    init_fn(sess)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 128, in <lambda>
    self._init_fn = lambda sess: init_fn(self, sess)
TypeError: callback() takes exactly 1 argument (2 given)
```
"
8486,Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs.,"### Environment info
Operating System:

Mac 

 I hava look Mobile and Embedded TensorFlow (TensorFlow Dev Summit 2017) video on yotube.

In the video , I hava learn some function to reduce tensorflow so file size on Android.

I do here
```
Prints a header file to be used with SELECTIVE_REGISTRATION.

Example usage:
print_selective_registration_header \
--graphs=path/to/graph.pb > ops_to_register.h

Then when compiling tensorflow, include ops_to_register.h in the     include
 search path and pass -DSELECTIVE_REGISTRATION  - see
 core/framework/selective_registration.h for more details.
```

the *.pb file is myself , then I get the ops_to_register.h file here

```c++
  #ifndef OPS_TO_REGISTER
  #define OPS_TO_REGISTER
  constexpr inline bool ShouldRegisterOp(const char op[]) {
      return false
     || (strcmp(op, ""Add"") == 0)
     || (strcmp(op, ""Const"") == 0)
     || (strcmp(op, ""Conv2D"") == 0)
     || (strcmp(op, ""Exp"") == 0)
     || (strcmp(op, ""Identity"") == 0)
     || (strcmp(op, ""Max"") == 0)
     || (strcmp(op, ""MaxPool"") == 0)
     || (strcmp(op, ""NoOp"") == 0)
     || (strcmp(op, ""Placeholder"") == 0)
     || (strcmp(op, ""RealDiv"") == 0)
     || (strcmp(op, ""Relu"") == 0)
     || (strcmp(op, ""Reshape"") == 0)
     || (strcmp(op, ""Sub"") == 0)
     || (strcmp(op, ""Sum"") == 0)
     || (strcmp(op, ""_Recv"") == 0)
     || (strcmp(op, ""_Send"") == 0)
     ;
    }
  #define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

  const char kNecessaryOpKernelClasses[] = "",""
""BinaryOp< CPUDevice, functor::add<float>>,""
""ConstantOp,""
""Conv2DOp<CPUDevice, float>,""
 ""UnaryOp< CPUDevice, functor::exp<float>>,""
  ""IdentityOp,""
 ""ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>,""
""MaxPoolingOp<CPUDevice, float>,""
 ""NoOp,""
  ""PlaceholderOp,""
 ""BinaryOp< CPUDevice, functor::div<float>>,""
  ""ReluOp<CPUDevice, float>,""
""ReshapeOp,""
""BinaryOp< CPUDevice, functor::sub<float>>,""
 ""ReductionOp<CPUDevice, float, Eigen::internal::SumReducer<float>>,""
 ""RecvOp,""
""SendOp,""
 ;
#define SHOULD_REGISTER_OP_KERNEL(clz)            
   (strstr(kNecessaryOpKernelClasses, "","" clz "","") != nullptr)

 #define SHOULD_REGISTER_OP_GRADIENT false
 #endif
```

I put ops_to_register.h in tensorflow/tensorflow/core/framework dir.

then I do:   

`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a    --copt=""-DSELECTIVE_REGISTRATION""`


In my android studio project , initializeTensorFlow() My slef .pb file ,but I got the error:


```
tensorflow_inference_jni.cc:145 Could not create TensorFlow graph: Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs.  Registered devices: [CPU], Registered kernels:
            <no registered kernels>

          	 [[Node: add_1 = Add[T=DT_FLOAT](Conv2D, Reshape)]]
```




"
8485,AOT compilation doesn't work on MacOS,"I was trying to reproduce this [example](https://www.tensorflow.org/versions/master/experimental/xla/tfcompile) on how to use AOT compilation. My graph is made by calling [make_test_graphs.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/make_test_graphs.py) and I used [test_graph_tfmatmul.config.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/test_graph_tfmatmul.config.pbtxt) as my config file. I failed at '**Step 2: Use tf_library build macro to compile the subgraph**'. 
Here is the error message:
error:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: file: bazel-out/local-opt/genfiles/test_aot/test_graph_tfmatmul.o is not an object file (not allowed in a library)

However, if I `bazel build` the same `tf_library` on a GCE instance, it compiles successfully. The only difference is that the Tensorflow on my GCE instance is cuda enabled. I think there may be a bug in AOT compilation in CPU-only mode.

### Environment info
Operating System: macOS 10.12
Processor: intel 'haswell'.
Tensorflow version: 1.0.1 stable release
bazel version: 0.4.4
"
8484,"sparse_softmax_cross_entropy_with_logits returns NaN instead of raising an error when the class int is not in the range [0, logit_size) when run on GPU","If you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits() labels that are not inside the number of classes  (that is the length of the logits, or the length of the second dimension of the logits if using the first dimension for batching), you get a loss of NaN and then everything in the model becomes NaN. I spent quite some time debugging this in my model and I believe that this should not be silent, there should be at least a warning at execution time.

More detail I just discovered: if you run it on CPU an InvalidAgrumentError is raised, if you run it on GPU, you get NaN, so you may want to fix only the GPU implementation to behave like the CPU one.

Environment info

Operating System: Ubuntu 16.04 64bit
Installed version of CUDA and cuDNN: cuda 8.0 cudnn 5.1
Tensorflow version: 1.0.0

If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Run it with CUDA_VISIBLE_DEVICE="""" to see the difference on CPU and GPU.

```
import numpy as np
import tensorflow as tf

classes = 5
num_datapoints = 100

xs = np.random.rand(num_datapoints,4)
ys = np.random.randint(classes + 1, size=num_datapoints)

graph = tf.Graph()
with graph.as_default():
    x = tf.placeholder(tf.float32, shape=[None, 4])
    y = tf.placeholder(tf.int32, shape=[None, ])

    w = tf.Variable(tf.random_normal([4, classes]))
    b = tf.Variable(tf.random_normal([classes]))

    logits = tf.matmul(x, w) + b

    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))
    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)


with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    for step in range(201):
        _, loss_val = session.run(
            [optimizer, loss],
            feed_dict={x: xs, y: ys})
        print(""step {step} - loss: {loss_val:.6f}"".format(step=step, loss_val=loss_val))
```
"
8483,Errors polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED,"I am running Tensor-flow on Google compute engine instances and I met with this problem when the training period is finished (after step 9990).
```
2017-03-17 00:24:23.021298: step 9980, loss = 0.72 (242.4 examples/sec; 0.264 sec/batch)
2017-03-17 00:24:34.693612: step 9990, loss = 0.79 (208.4 examples/sec; 0.307 sec/batch)
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1
Aborted (core dumped)
```
Can anyone identify this problem and let me know how to fix it?"
8482,[Windows - Bazel] ERROR: Executing genrule @llvm//:datatypes_gen failed,"Windows 10
Bazel 0.4.5
Visual C++ 2015
Msys2 v20160205
Python 3.5 (python.org distribution)

I am getting these genrule errors when building.

Configuring:
```
Adriano@Adriano MSYS /c/tensorflow
$ ./configure
Please specify the location of python. [Default is /c/Users/Adriano/AppData/Local/Programs/Python/Python35/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  C:\Users\Adriano\AppData\Local\Programs\Python\Python35
  C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Adriano\AppData\Local\Programs\Python\Python35]

Using python library path: C:\Users\Adriano\AppData\Local\Programs\Python\Python35
Junction created for util\python\python_include <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35\include
Junction created for util\python\python_lib <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35
Junction created for third_party\py\numpy\numpy_include <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\site-packages\numpy\core\include
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1
Please specify the location where cuDNN 5.1 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 5.0
.......................................................................................
INFO: All external dependencies fetched successfully.
Configuration finished
```

Building:
```

$ export BUILD_OPTS='--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'

Adriano@Adriano MSYS /c/tensorflow
$ bazel build -c opt $BUILD_OPTS --config=win-cuda tensorflow/tools/pip_package:build_pip_package
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package.
INFO: Found 1 target...
INFO: From Compiling tensorflow/core/util/version_info.cc [for host]:
Warning: Unmatched arguments: -Iexternal/gemmlowp -fno-exceptions -msse3
INFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc [for host]:
Warning: Unmatched arguments: -Iexternal/gemmlowp -fno-exceptions -msse3 -msse4.2
Slow read: a 67403059-byte read from C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/local_config_cuda/cuda/include/sobol_direction_vectors.h took 9044ms.
INFO: From Compiling external/farmhash_archive/src/farmhash.cc [for host]:
C:\tools\msys64\tmp\_bazel_adriano\x1e5egqw\execroot\tensorflow\external\farmhash_archive\src\farmhash.cc(394): note: see reference to function template instantiation 'T util::DebugTweak<uint64_t>(T)' being compiled
        with
        [
            T=uint64_t
        ]
C:\tools\msys64\tmp\_bazel_adriano\x1e5egqw\execroot\tensorflow\external\farmhash_archive\src\farmhash.cc(1879): note: see reference to function template instantiation 'T util::DebugTweak<uint32_t>(T)' being compiled
        with
        [
            T=uint32_t
        ]
ERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:162:1: Executing genrule @llvm//:datatypes_gen failed: bash.exe failed: error executing command
  cd C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/execroot/tensorflow
  SET PATH=C:\tools\msys64\usr\bin;C:\Users\Adriano\AppData\Local\Programs\Python\Python35\;C:\bazel\;c;C:\tools\msys64\Program Files\Java\jdk1.8.0_112\;C:\tools\msys64\usr\local\bin;C:\tools\msys64\usr\bin;C:\tools\msys64\usr\bin;C:\tools\msys64\opt\bin;C:\Windows\System32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\tools\msys64\usr\bin\site_perl;C:\tools\msys64\usr\bin\vendor_perl;C:\tools\msys64\usr\bin\core_perl
  C:/tools/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/third_party/llvm/expand_cmake_vars ""HAVE_DIRENT_H=1"" ""HAVE_DLFCN_H=1"" ""HAVE_ERRNO_H=1"" ""HAVE_EXECINFO_H=1"" ""HAVE_FCNTL_H=1"" ""HAVE_INTTYPES_H=1"" ""HAVE_PTHREAD_H=1"" ""HAVE_SIGNAL_H=1"" ""HAVE_STDINT_H=1"" ""HAVE_SYS_IOCTL_H=1"" ""HAVE_SYS_MMAN_H=1"" ""HAVE_SYS_PARAM_H=1"" ""HAVE_SYS_RESOURCE_H=1"" ""HAVE_SYS_STAT_H=1"" ""HAVE_SYS_TIME_H=1"" ""HAVE_SYS_TYPES_H=1"" ""HAVE_TERMIOS_H=1"" ""HAVE_UNISTD_H=1"" ""HAVE_ZLIB_H=1"" ""HAVE_BACKTRACE=1"" ""HAVE_DLOPEN=1"" ""HAVE_FUTIMES=1"" ""HAVE_GETCWD=1"" ""HAVE_GETPAGESIZE=1"" ""HAVE_GETRLIMIT=1"" ""HAVE_GETRUSAGE=1"" ""HAVE_GETTIMEOFDAY=1"" ""HAVE_INT64_T=1"" ""HAVE_ISATTY=1"" ""HAVE_LIBEDIT=1"" ""HAVE_LIBPTHREAD=1"" ""HAVE_LIBZ=1"" ""HAVE_MKDTEMP=1"" ""HAVE_MKSTEMP=1"" ""HAVE_MKTEMP=1"" ""HAVE_PREAD=1"" ""HAVE_PTHREAD_GETSPECIFIC=1"" ""HAVE_PTHREAD_MUTEX_LOCK=1"" ""HAVE_PTHREAD_RWLOCK_INIT=1"" ""HAVE_REALPATH=1"" ""HAVE_SBRK=1"" ""HAVE_SETENV=1"" ""HAVE_SETRLIMIT=1"" ""HAVE_SIGALTSTACK=1"" ""HAVE_STRERROR=1"" ""HAVE_STRERROR_R=1"" ""HAVE_STRTOLL=1"" ""HAVE_SYSCONF=1"" ""HAVE_UINT64_T=1"" ""HAVE__UNWIND_BACKTRACE=1"" ""ENABLE_BACKTRACES=1"" ""LLVM_BINDIR=/dev/null"" ""LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0"" ""LLVM_ENABLE_ABI_BREAKING_CHECKS=0"" ""LLVM_ENABLE_THREADS=1"" ""LLVM_ENABLE_ZLIB=1"" ""LLVM_HAS_ATOMICS=1"" ""LLVM_INCLUDEDIR=/dev/null"" ""LLVM_INFODIR=/dev/null"" ""LLVM_MANDIR=/dev/null"" ""LLVM_NATIVE_TARGET=1"" ""LLVM_NATIVE_TARGETINFO=1"" ""LLVM_NATIVE_TARGETMC=1"" ""LLVM_NATIVE_ASMPRINTER=1"" ""LLVM_NATIVE_ASMPARSER=1"" ""LLVM_NATIVE_DISASSEMBLER=1"" ""LLVM_ON_UNIX=1"" ""LLVM_PREFIX=/dev/null"" ""LLVM_VERSION_MAJOR=0"" ""LLVM_VERSION_MINOR=0"" ""LLVM_VERSION_PATCH=0"" ""LTDL_SHLIB_EXT=.so"" ""PACKAGE_NAME=llvm"" ""PACKAGE_STRING=llvm tensorflow-trunk"" ""PACKAGE_VERSION=tensorflow-trunk"" ""RETSIGTYPE=void"" ""LLVM_HOST_TRIPLE=x86_64-unknown-linux_gnu"" ""LLVM_DEFAULT_TARGET_TRIPLE=x86_64-unknown-linux_gnu"" ""LLVM_NATIVE_ARCH=X86"" ""HAVE_MALLOC_H=1"" ""HAVE_LINK_H=1"" ""HAVE_MALLINFO=1"" ""HAVE_FUTIMENS=1""< external/llvm/include/llvm/Support/DataTypes.h.cmake > bazel-out/host/genfiles/external/llvm/include/llvm/Support/DataTypes.h: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1
Traceback (most recent call last):
  File ""C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""bazel-out\host\bin\third_party\llvm\expand_cmake_vars\__main__.py"", line 168, in <module>
  File ""bazel-out\host\bin\third_party\llvm\expand_cmake_vars\__main__.py"", line 115, in Main
  File ""bazel-out\host\bin\third_party\llvm\expand_cmake_vars\__main__.py"", line 98, in CreateModuleSpace
  File ""C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\zipfile.py"", line 1009, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '\\\\?\\bazel-out\\host\\bin\\third_party\\llvm\\expand_cmake_vars'
ERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:183:1: declared output 'external/llvm/include/llvm/Config/abi-breaking.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: C:/tools/msys64/tmp/_bazel_adriano/x1e5egqw/external/llvm/BUILD:176:1: declared output 'external/llvm/include/llvm/Config/llvm-config.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 205.884s, Critical Path: 69.60s
FAILED: Build did NOT complete successfully
```"
8481,Cannot import CudnnRNN from contrib.cudnn_rnn in 1.0.1,"For some reason all the ""__init__.py"" in python directory of _contrib.cudnn_rnn_ have been removed,
and it is impossible to import anything from there."
8480,Documentation Error,"On website page:
https://www.tensorflow.org/install/install_windows

There is a documentation error:


----

To install TensorFlow, start a ~terminal~ **command prompt instance**. Then issue the appropriate pip3 install command below.

To install the CPU-only version of TensorFlow, enter the following command:

```
cd %appdata%\..\Local\Programs\ython\Python35\Scripts
pip3 install --upgrade tensorflow
```

To install the GPU version of TensorFlow, enter the following command:

```
cd %appdata%\..\Local\Programs\ython\Python35\Scripts
pip3 install --upgrade tensorflow-gpu
```

----

I.E. pip3 isn't registered as an executable on the system while following the standard Python 3.5 installation executable. Thus you have to navigate to the folder containing the executable first."
8478,synchronous posix file mmap,"TensorFlow's [PosixFileSystem::NewReadOnlyMemoryRegionFromFile method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/posix_file_system.cc#L172) calls `mmap` with [only the `MAP_PRIVATE` flag specified](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/posix_file_system.cc#L183).

Having the ability to specify that flag, at least to specify `MAP_POPULATE` would be very useful for many applications with high disk I/O throughput. For the `MAP_POPULATE` case, I sped up one of my graphs by ~4x by adding that flag because I have a pipeline that:

1. reads in a file of N chunks in size
2. enqueues a pointer to each of those N chunks in an output queue
3. downstream nodes process one chunk at a time

Because of the lack of coordination between the downstream nodes (each being independent), they would take page faults in an uncoordinated manner. When I added `MAP_POPULATE` this went away.

I have a patch to add this feature where I change the [base method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h#L102) to look like this:

    virtual Status NewReadOnlyMemoryRegionFromFile(
      const string& fname, std::unique_ptr<ReadOnlyMemoryRegion>* result, bool synchronous = false) = 0;

and then changed each corresponding subclass method signature. For the Posix subclass, I added `MAP_POPULATE` to the flags if `synchronous == true`.

I can submit a pull request for this, but I find this approach hacky (e.g. that flag might not have the same meaning in non-Posix contexts). If there is a better approach to do this, I'm happy to work on it."
8477,No scalars and images show up in tensorboard,"I'm trying to get some visualization on tensorboard, the graph is shown up, but scalars and images are not. I also tried the mnist demo code, still the same problem.
My colleagues could see the visualization from tensorboard on their machines, so the event/data itself has no problem.
Anyone has any clue?

I have checked this thread: https://github.com/tensorflow/tensorflow/issues/1421, and tried everything suggested. I do have css file under /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/lib/css/

### Environment info
Operating System: Linux Ubuntu 14.04
Installed version of CUDA and cuDNN:  CUDA 8, cuDNN 5
pip installed tensorflow 0.12.1
### What other attempted solutions have you tried?
I tried different browser, chrome and firefox got the same results, no images and sclars show up.
I tried on other machines with same enviorment settings, and had no problem."
8476,Distributed TensorFlow running in parallel and Session Running time problem.,"I 'm trying to using tensorflow to do the inference task in a distributed way. I follow the doc here(https://www.tensorflow.org/deploy/distributed), and I'm using the example naive MNIST network. 
(codes shown below.)

I use different workers on local machine. And since the network is small, I put different node on different workers.
**What I want to know is:**
**1. If I divide the graph into several parts, can they run in parallel?** 
I know this graph is sequential/linear, (one part of the computation is dependent on the result of previous part, so if there's only 1 batch of data, it has to run sequentially.) 
But I want to know, if I have multiple batches of data, can they run parallelly in a pipeline fashion? (like Part 1 finished computing batch i data, send it to Part 2, and continue working on batch i+1 data)  ??

**2. Like shown in the code, I want to output the computation for each part of the graph.** 
However after running it, I noticed that, the output time is only the time for ""constructing the node/graph"", instead of running the graph. All the running is in the ""sess.run()"". And in this case, I cannot know the computation time for each part of the graph...

**I 'm wondering if there's a way to show the computation time of each part of the graph?(or each node maybe?)**

The code:
```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
import time
import tensorflow as tf

start=time.time()

x = tf.placeholder(tf.float32, [None, 784], name=""input"")

cluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223"", ""localhost:2224""]})

with tf.device(""/job:local/task:0""):
    W = tf.Variable(tf.zeros([784, 10]), name = ""w1"")
    b = tf.Variable(tf.zeros([10]), name = ""b1"")

time1=time.time()
#print (""Time1: ""+str(time1-start)+"" seconds"")

with tf.device(""/job:local/task:1""):
    y = tf.nn.softmax(tf.matmul(x, W) + b)
    y_ = tf.placeholder(tf.float32, [None, 10], name = ""output"")

time2=time.time()
#print (""Time2: ""+str(time2-time1)+"" seconds"")
with tf.device(""/job:local/task:2""):
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

time3=time.time()
#print (""Time3: ""+str(time3-time2)+"" seconds"")

# swd--save the Checkpoint file
saver = tf.train.Saver()

#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
with tf.Session(""grpc://localhost:2224"") as sess:
    # swd save ckpt
    saver.restore(sess, ""saved_model/model.ckpt"")
    print(""\nModel restored.\n"")

    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    
time4=time.time()
#print (""Time4: ""+str(time4-time3)+"" seconds"")

end=time.time()
print (""Computing time: ""+str(end-start)+"" seconds"")
```"
8473,Segmentation Fault in Shape Function When Accessing Attr,"Adding this to shape function causes segfault (full example below):
```c++
      string data_format;
      Status s = c->GetAttr(""test_str"", &data_format);
```

On OS X:
```
$ bazel version
Build label: 0.4.3-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 15:20:22 2016 (1482420022)
Build timestamp: 1482420022
Build timestamp as int: 1482420022
```

```
$git rev-parse
07bb8ea2379bd459832b23951fb20ec47f3fdbd4
```

Using the [ZeroOut op](https://www.tensorflow.org/extend/adding_an_op) but modifying the shape function:
```c++
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/shape_inference.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .Attr(""test_str: {'alex', 'bob'} = 'alex'"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      string data_format;
      Status s = c->GetAttr(""test_str"", &data_format);
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));

  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

and then running (within `tf.TestCase`):
```python
class Alex(tf.test.TestCase):
    def test_alex(self):
        so_path = (cur_dir + ""/../../../../"" +
            ""bazel-bin/tensorflow/core/user_ops/zero.so"")
        my_module = tf.load_op_library(so_path)
        with self.test_session():
            print my_module.zero_out([1,1,2,1]).eval()
```

The segfault it self seems to be coming from protobuf. I filed a ticket there as well: https://github.com/google/protobuf/issues/2863."
8472,multi core cpu issue(server vs local),"My local computer is mac : i7 quad core 2.2GHz .  (2015 model)
My server computer is centos7 : 120-core (8 sockets × 15cores) Intel Xeon E7-8870
My tensorflow version : v1.0.1

I build tensorflow source with this option :
bazel build -c opt --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package

With under simple code, my local computer time is 8 seconds and my server computer time is 22 second. 
`
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import time

mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

x = tf.placeholder(tf.float32, [None, 784]);
y_ = tf.placeholder(tf.float32, [None, 10]);

W1 = tf.Variable(tf.random_normal([784,240], stddev=0.35));
W2 = tf.Variable(tf.random_normal([240,120], stddev=0.35));
W3 = tf.Variable(tf.random_normal([120,10], stddev=0.35));

b1 = tf.Variable(tf.zeros([240]));
b2 = tf.Variable(tf.zeros([120]));
b3 = tf.Variable(tf.zeros([10]));

y1 = tf.nn.sigmoid(tf.matmul(x,W1)+b1);
y2 = tf.nn.sigmoid(tf.matmul(y1,W2)+b2);
y = tf.nn.softmax(tf.matmul(y2,W3)+b3);

cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))
train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

NUM_THREADS = 100
sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS))

tf.global_variables_initializer().run()

start_time = time.time()

for _ in range(3000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train, feed_dict={x: batch_xs, y_: batch_ys})

end_time = time.time()
print (end_time - start_time)

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
`
I think it is impossible because my server computer have so many cores.
Isn't it multi-core issue?"
8471,"InvalidArgumentError (traceback): BiasGrad requires tensor size <= int32 max in 3D CNN, Tensorflow","I am using 3D CNN model in tensorflow. My input image dimension is 3D i.e. (192 * 256 * 256) with one color channel image and i have used three Convolutional layers (with patches of 5 * 5 * 5) and 3 pooling layers (kernal size : 3 * 3 * 3 and stride : 2 * 2 * 2), one full-connection-layer with 128 nodes and output layer with two nodes. The number of samples are 120 for training. Batch size : 10

I am facing the below error on the cluster:

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 332, in main
    sess.run(optimizer, feed_dict={train_data_node: batch_data,train_labels_node: batch_labels})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: BiasGrad requires tensor size <= int32 max
         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/Relu_grad/ReluGrad)]]

Caused by op u'gradients/BiasAdd_grad/BiasAddGrad', defined at:
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 290, in main
    optimizer = tf.train.AdamOptimizer(learning_rate=0.0).minimize(train_loss) # Adam Optimizer
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 288, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 354, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 482, in gradients
    in_grads = grad_fn(op, *out_grads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py"", line 204, in _BiasAddGrad
    data_format=data_format))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 313, in bias_add_grad
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

...which was originally created as op u'BiasAdd', defined at:
  File ""./mri_cnn.py"", line 362, in <module>
    tf.app.run()
[elided 0 identical lines from previous traceback]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./mri_cnn.py"", line 272, in main
    train_prediction = model(train_data_node, True)
  File ""./mri_cnn.py"", line 181, in model
    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 1316, in bias_add
    return gen_nn_ops._bias_add(value, bias, data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 281, in _bias_add
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): BiasGrad requires tensor size <= int32 max
         [[Node: gradients/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients/Relu_grad/ReluGrad)]]

Please suggest, how to fix it?"
8469,misleading comments,"The comments for the function `def _linear(args, output_size, bias, bias_start=0.0):` in tensorflow/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py says:

```
def _linear(args, output_size, bias, bias_start=0.0):
  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.
  Args:
    args: a 2D Tensor or a list of 2D, batch x n, Tensors.
    output_size: int, second dimension of W[i].
    bias: boolean, whether to add a bias term or not.
    bias_start: starting value to initialize the bias; 0 by default.
```
However, the function is not implemented as `sum_i(args[i] * W[i])`. 
The key parts in the implementation in line 894-897:
```
    if len(args) == 1:
      res = math_ops.matmul(args[0], weights)
    else:
      res = math_ops.matmul(array_ops.concat(args, 1), weights)
```
The `args` list is firstly concatenated, then `matmul` with the weights tensor. This is totally different from what said in the comments that `Linear map: sum_i(args[i] * W[i]), where W[i] is a variable`. 
So, the comments should probably be rewritten like `Linear map: first, concatenate the args if it is a list, then matmul with a weight tensor`."
8467,Better control of logging verbosity,"I'm creating a simple LSTM in Keras, and during training I get these warnings:

![screenshot from 2017-03-16 13-25-23](https://cloud.githubusercontent.com/assets/23310996/23995972/170f3a10-0a4c-11e7-92f6-f7ee865e65e6.png)

I know I can set the `TF_CPP_MIN_LOG_LEVEL` according to this question: http://stackoverflow.com/questions/35869137/avoid-tensorflow-print-on-standard-error

or use the `tf.logging.set_verbosity(verbosity)` to control this, but for my example I would like to 

1. Hide the warnings that Tensorflow wasn't compiled to use ... instructions.
2. Show the logging of device when starting a new session, that is very useful to confirm that the GPU support is working.
3. Hide the Pool allocator warnings since they clutter the console output from Keras during training.

I haven't found a way to do this, perhaps it could be added as a feature?





"
8466,"python: ../nptl/pthread_mutex_lock.c:349: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != EDEADLK || (kind != PTHREAD_MUTEX_ERRORCHECK_NP && kind != PTHREAD_MUTEX_RECURSIVE_NP)' failed.","Tensorflow is crashing when run using multiple GPUs (2,3,and 4) on cifar10_multi_gpu .py example in a Power8 computer.


### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
We have found some suggestions regarding updating glibc (https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1642390?cm_mc_uid=49967414488814888059863&cm_mc_sid_50200000=1489663906), but our system is already updated to the latest version and the problem remains.
We have also used ppc64_pc to reduce SMT (to 2)  and also turned off. Still the problem remains. 

### Environment info
_Operating System:
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.2 LTS
Release:	16.04
Codename:	xenial
Linux minsky31 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 17:05:51 UTC 2017 ppc64le ppc64le ppc64le GNU/Linux_


Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
_CUDA
-rw-r--r-- 1 root root 559800 gen 27 00:10 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 gen 27 00:13 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 gen 27 00:13 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root root 476024 gen 27 00:10 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root 966166 gen 27 00:10 /usr/local/cuda/lib64/libcudart_static.a
cuDNN
/usr/lib/powerpc64le-linux-gnu/libcudnn_static.a
/usr/lib/powerpc64le-linux-gnu/libcudnn.so.5.1.10
/usr/lib/powerpc64le-linux-gnu/libcudnn.so
/usr/lib/powerpc64le-linux-gnu/libcudnn_static_v5.a
/usr/lib/powerpc64le-linux-gnu/libcudnn.so.5_


If installed from source, provide 
_1. The commit hash (`git rev-parse HEAD`):
012f9c10dcc27a838ed4b170b9036483bc2c5869_

2. The output of `bazel version`:
_Build label: 0.4.3-2017-01-24 (@6fc5c53)
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jan 24 20:34:16 2017 (1485290056)
Build timestamp: 1485290056
Build timestamp as int: 1485290056_

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
_We downloaded the files from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10
and tried to run the cifar10_multi_gpu example using four P100 gpus._ 

### What other attempted solutions have you tried?
We tried removing the calls from measuring time, logging etc, but the problem remains. 

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

_When it blocks, it simply hangs 
2017-03-16 12:09:56.391752: step 800, loss = 3.33 (2982.8 examples/sec; 0.011 sec/batch)
2017-03-16 12:09:57.002419: step 810, loss = 3.49 (3998.9 examples/sec; 0.008 sec/batch)
2017-03-16 12:09:57.476172: step 820, loss = 3.24 (3003.8 examples/sec; 0.011 sec/batch)
and the result from strace when it blocks:
strace: Process 111228 attached
futex(0x3fffd5d49744, FUTEX_WAIT_PRIVATE, 1, NULL_

Otherwise, when it gives the error : 
futex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x10013f8cf10, FUTEX_WAKE_PRIVATE, 1) = 1
futex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x10013f8cf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)
futex(0x10013f8cf10, FUTEX_WAKE_PRIVATE, 1) = 1
futex(0x3fffd1afc324, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3fffd1afc2f8, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3fffd1afc324, FUTEX_WAIT_PRIVATE, 1, NULL <unfinished ...>
+++ killed by SIGABRT (core dumped) +++


strace gives as last lines:
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
futex(0x3bff90000020, FUTEX_WAKE_PRIVATE, 1) = 1
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL) = 0
futex(0x3ffff8dbd158, FUTEX_WAKE_PRIVATE, 1) = 0
stat(""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=2692, ...}) = 0
write(1, ""2017-03-16 12:22:38.084717: step""..., 90) = 90
futex(0x1001efacf10, FUTEX_WAKE_PRIVATE, 1) = 1
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0
futex(0x1001efacf10, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = -1 EAGAIN (Resource temporarily unavailable)
futex(0x3ffff8dbd184, FUTEX_WAIT_PRIVATE, 1, NULL


"
8465,non_max_suppression should support batches,"

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Might be related/nice to do when overworking non_max_suppression for #7511

Related SO: http://stackoverflow.com/questions/39456554/tensorflow-tensor-with-inconsistent-dimension-size


### Description

Most image ops work on batches of images, so it'd make sense if tf.image.non_max_suppression worked on batches of bounding-boxes , returning batches of selected indices.

Although I'm not completely sure how it'd work with the variable result lengths that are likely to happen in this case, perhaps masking or padding them? There's already the max_output_size parameter.

And I'm not entirely sure how well it'd work with gather (gather_nd?), since if it doesn't work well with those, this change wouldn't be as useful
"
8464,what ide should I use with c++,"I would like to use c++ programming, I would like to ask what I should use ide tools, I tried vs and eclipse, but failed to import tensorflow"
8463,customize parse_op.py and recompile,"Hi, my problem is that I want to hack code in the parse_op.py file， especially I want to have an option of Dict or OrderedDict for the output of parse_single_example and many other parser that return Dict. Should I submit a PR or you guys can add that in the next version ?

By installing from source , I got following errors 

`INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
.........
INFO: All external dependencies fetched successfully.
Configuration finished
[zaikun@greina0 tensorflow]$ python -c 'import tensorflow'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
ImportError: No module named pywrap_tensorflow_internal
`"
8462,Error while compiling tensorflow from sources,"Hello, I am trying to install tensorflow from sources in a server where I haven't root access, so I can't update Cuda nor cuDNN. However, the versions installed are currently supported as said [here](https://www.tensorflow.org/install/install_sources).

### Environment info
Operating System: Ubuntu 14.04.4 LTS
gcc version: 4.8.4
python 2.7

Installed version of CUDA and cuDNN:
Cuda compilation tools, release 7.5, V7.5.17
CUDNN_MAJOR 5  CUDNN_MINOR 0  CUDNN_PATCHLEVEL 4
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5
lrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18
-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
```

If installed from source, provide

1. The commit hash (`git rev-parse HEAD`): 12a98726e769e988f6368a029ec2f5b0ac3ccbd4
2. The output of `bazel version`:  Build label: 0.4.4

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
[#5191](https://github.com/tensorflow/tensorflow/issues/5191)
[#817](https://github.com/tensorflow/tensorflow/issues/817)
http://stackoverflow.com/questions/42756614/tensorflow-compiling-from-source-c-compilation-of-rule-tensorflow-stream-e
[#3074](https://github.com/tensorflow/tensorflow/issues/3074)
[#3360](https://github.com/tensorflow/tensorflow/issues/3360)
[#8061](https://github.com/tensorflow/tensorflow/issues/8061)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
The configure works fine but when I run `bazel build --config=opt --config=cuda --verbose_failures  //tensorflow/tools/pip_package:build_pip_package` I get the following error:

```
ERROR: /home/angomez/tensorflow_cuda/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/angomez/.cache/bazel/_bazel_angomez/95301c24f4425af6d9c850e03508fc5b/execroot/tensorflow && \
  exec env - \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -D_FORCE_INLINES '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':
tensorflow/stream_executor/cuda/cuda_dnn.cc:986:50:   required from here
tensorflow/stream_executor/cuda/cuda_dnn.cc:140:46: error: invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]
       cudnnStatus_t retval = ::__name(args...);                    \
...
```
The complete error log is here http://pastebin.com/VXKnh8ga

### What other attempted solutions have you tried?
I've tried to compile with gcc version 4.8 since I've seen [this](http://stackoverflow.com/questions/37313212/tensorflow-bazel-build-failing), but it didn't work.

Thank you!"
8460,No nigthly build for Python3.5 GPU,"According to [the build history](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/) the last time the GPU version for python 3.5 was built was almost a month ago. That actually leads the download link from the README.md front page to point to a [file not found page](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl).

That's too bad because there are some bug-fixes that I needed which are then not included (#7585 for instance)."
8459,Broken curl URL for flower_photos.tgz,"The tutorial for retraining Inception's final layer has a malformed URL, the instructions for downloading the dataset are:

```
cd ~
curl -O http://download.tensorflow.org/example_../images/flower_photos.tgz
tar xzf flower_photos.tgz
```

The URL is broken, I think it's supposed to be:

```
curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
```
"
8457,import android example to android studio fail,"When I follow the step by android READ.ME, It failed .

 Error:(145, 0) A problem occurred evaluating root project 'android'.
and the location is: from  from file(externalModelData).listFiles() 
task copyExternalAssets(type: Copy) {
    from file(externalModelData).listFiles()
    include '*.pb'
    include '*.txt'
    include 'thumbnails/*.jpg'
    into 'assets'
    fileMode 0644
    dependsOn buildExternalAssets
}
I dont know how to solve this ."
8456,how to reduce  libtensorflow_demo.so size ?,"I build my own libtensorflow_demo.so file use   `bazel build -c opt tensorflow/examples/android:libtensorflow_demo.so --crosstool_top=//external:android/crosstool --cpu=x86 --host_crosstool_top=@bazel_tools//tools/cpp:toolchain` 
but it is too large for me . how to reduce it ?
x86   --> 66M
x86_64 --> 76.2M
armeabi-v7a --> 17.9M
arm64-v8a -->33M"
8454,No OpKernel was registered to support Op 'FIFOQueueV2' with these attrs.,"### Description

I'm trying to use the [Inception-ResNet-V2](https://github.com/tensorflow/models/tree/master/slim) model from TF-slim in the Android demo app. I'm now seeing the following inference exception:

    E/TensorFlowInferenceInterface: Inference exception: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'FIFOQueueV2' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                                     <no registered kernels>
                                                                                   
                                                                                   	 [[Node: batch/fifo_queue = FIFOQueueV2[capacity=128, component_types=[DT_FLOAT, DT_UINT8, DT_INT64], container="""", shapes=[[299,299,3], [299,299,3], []], shared_name=""""]()]]

To ensure this issue does not come from my build, I've download the latest [libtensorflow_inference.so](http://ci.tensorflow.org/view/Nightly/job/nightly-android/70/artifact/out/native/) from Jenkins. I've verified that this library still provides the same exception.

I'm not familiar with the error message but it seems the Op `FIFOQueueV2` is defined in [fifo_queue_op.cc](https://github.com/tensorflow/tensorflow/blob/4433079e7f317724eaa92ec120c6b1c3c0c52f2f/tensorflow/core/kernels/fifo_queue_op.cc#L61). Can someone help take a look?"
8453,Cant run code on GPU -- Windows 10,"i have installed CUDA v8.0 and have put the cudnn-8.0 files in my cuda program files directory and have set the bin folder of the cuda directory in my environment variables PATH.
i have installed tensorflow-gpu as per the instructions given in the tensorflow website and installed it on my anaconda which has python 3.5.2  after running the matmul example I cant get it to be run on the gpu.
```E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
Device mapping: no known devices.
I c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\common_runtime\direct_session.cc:257] Device mapping:

I c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0
I c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] b: (Const)/job:localhost/replica:0/task:0/cpu:0
I c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\common_runtime\simple_placer.cc:841] a: (Const)/job:localhost/replica:0/task:0/cpu:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0
b: (Const): /job:localhost/replica:0/task:0/cpu:0
a: (Const): /job:localhost/replica:0/task:0/cpu:0
[[ 22.  28.]
 [ 49.  64.]]```"
8451,XLA segfaults with large graphs,"TensorFlow allows graphs to be larger than 2GB, however those graphs can't be serialized.

I suspect this is the cause of segfaults we've been seeing because of following line in dump_graph.cc

`65    TF_CHECK_OK(WriteTextProto(Env::Default(), path, graph_def));
`

It would be useful to provide an informative error message because troubleshooting this requires looking at core file:

```
ulimit -Sc unlimited
gdb python
core core
bt
```

Here's the backtrace

```
#0  0x00007fc6dcdae428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007fc6dcdb002a in __GI_abort () at abort.c:89
#2  0x00007fc6c04b8417 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007fc6bd05cb1e in tensorflow::dump_graph::DumpGraphDefToFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::GraphDef const&) ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007fc6bd05ce88 in tensorflow::dump_graph::DumpGraphToFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::Graph const&, tensorflow::FunctionLibraryDefinition const*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007fc6bcffd6c2 in tensorflow::EncapsulateSubgraphsPass::Run(tensorflow::GraphOptimizationPassOptions const&) ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007fc6bec68e00 in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007fc6bec768a3 in tensorflow::SimpleGraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::SimpleClientGraph, std::default_delete<tensorflow::SimpleClientGraph> >*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#8  0x00007fc6be43d293 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#9  0x00007fc6be43f4cf in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::gtl::ArraySlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#10 0x00007fc6be44085e in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#11 0x00007fc6bcfd4641 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.498] ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#12 0x00007fc6bcfd4e58 in TF_Run () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#13 0x00007fc6bcf0b15d in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#14 0x00007fc6bcf0b2a3 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#15 0x00007fc6bceed225 in _wrap_TF_Run () from /my_code/tensorflow_ops/tensorflow/_python_build/tensorflow/python/_pywrap_tensorflow.so
#16 0x00007fc6ddd335e9 in PyCFunction_Call (func=0x7fc6c5120828, args=0x7fc68c1765f8, kwds=<optimized out>) at Objects/methodobject.c:109
#17 0x00007fc6dddbabd5 in call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0dfa78) at Python/ceval.c:4705
#18 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#19 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=6, kws=0x0, kwcount=0, defs=0x0, defcount=0, 
    kwdefs=0x0, closure=0x7fc6d70ac780, name=0x0, qualname=0x0) at Python/ceval.c:4018
#20 0x00007fc6dddbbcd8 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, 
    defcount=0, kwdefs=0x0, closure=0x7fc6d70ac780) at Python/ceval.c:4039
#21 0x00007fc6ddd11542 in function_call (func=0x7fc68feb3268, arg=0x7fc68c3903a8, kw=0x0) at Objects/funcobject.c:627
#22 0x00007fc6ddcde236 in PyObject_Call (func=0x7fc68feb3268, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2165
#23 0x00007fc6dddb8234 in ext_do_call (nk=-1942420568, na=0, flags=<optimized out>, pp_stack=0x7ffc0d0dfdc8, func=0x7fc68feb3268) at Python/ceval.c:5034
#24 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3275
#25 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=8, kws=0x7fd5098, kwcount=0, defs=0x0, defcount=0, 
    kwdefs=0x0, closure=0x0, name=0x7fc6d74eafb0, qualname=0x7fc6d74f07c8) at Python/ceval.c:4018
#26 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=8, n=<optimized out>, pp_stack=0x7ffc0d0dffe8, func=0x7fc6d74add90) at Python/ceval.c:4813
#27 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0dffe8) at Python/ceval.c:4730
#28 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#29 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=7, kws=0x7fd7640, kwcount=0, defs=0x0, defcount=0, 
    kwdefs=0x0, closure=0x0, name=0x7fc6d74f13e8, qualname=0x7fc6d74f0738) at Python/ceval.c:4018
#30 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=7, n=<optimized out>, pp_stack=0x7ffc0d0e0208, func=0x7fc6d74add08) at Python/ceval.c:4813
#31 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e0208) at Python/ceval.c:4730
#32 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#33 0x00007fc6dddbb166 in fast_function (nk=<optimized out>, na=6, n=<optimized out>, pp_stack=0x7ffc0d0e0388, func=0x7fc6d74adc80) at Python/ceval.c:4803
#34 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e0388) at Python/ceval.c:4730
#35 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#36 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=3, kws=0x40e5488, kwcount=0, defs=0x7fc6d74a7060, 
    defcount=3, kwdefs=0x0, closure=0x0, name=0x7fc6dc762458, qualname=0x7fc6d74ead30) at Python/ceval.c:4018
#37 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=3, n=<optimized out>, pp_stack=0x7ffc0d0e05a8, func=0x7fc6d74adae8) at Python/ceval.c:4813
#38 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e05a8) at Python/ceval.c:4730
#39 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#40 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x41f8440, kwcount=0, defs=0x7fc6d7761df0, 
    defcount=1, kwdefs=0x0, closure=0x0, name=0x7fc6d77a1ee0, qualname=0x7fc6d77a1ee0) at Python/ceval.c:4018
#41 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7ffc0d0e07c8, func=0x7fc6d76ecd90) at Python/ceval.c:4813
#42 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e07c8) at Python/ceval.c:4730
#43 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
#44 0x00007fc6dddbbb49 in _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x1c06428, kwcount=0, defs=0x7fc6d76f1320, 
    defcount=2, kwdefs=0x0, closure=0x0, name=0x7fc6dc762458, qualname=0x7fc6d772a170) at Python/ceval.c:4018
#45 0x00007fc6dddbadf5 in fast_function (nk=<optimized out>, na=1, n=<optimized out>, pp_stack=0x7ffc0d0e09e8, func=0x7fc6d76e7730) at Python/ceval.c:4813
#46 call_function (oparg=<optimized out>, pp_stack=0x7ffc0d0e09e8) at Python/ceval.c:4730
#47 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3236
```"
8449,sporadic system crash on evaluation,"Problem:
system crashes completely testing the net (not training) - black screen and instant reboot
(tryed nvidia-375 and nvidia-378 driver)

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/37504470/tensorflow-crashes-when-using-sess-run

### Environment info
Operating System:

`Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-66-generic x86_64)`

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

```
willi@Grafik14:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root   558720 Jan 27 14:40 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Jan 27 14:40 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       17 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn_static.a
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
```
willi@Grafik14:~$ pip show tensorflow
Name: tensorflow
Version: 1.0.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, numpy, protobuf, wheel, six
```

```
willi@Grafik14:~$ pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 1.0.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, numpy, protobuf, wheel, six


```
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

```
willi@Grafik14:~$ python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.1
```




### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

[code to reproduce](https://github.com/TheTesla/tftest)

### What other attempted solutions have you tried?
mcelog was empty


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.55GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.90G (12772704256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
INFO:tensorflow:Saving checkpoints for 3201 into /tmp/mnist_convnet_model/model.ckpt.
INFO:tensorflow:loss = 0.462657, step = 3201
INFO:tensorflow:
INFO:tensorflow:global_step/sec: 24.3908
INFO:tensorflow:loss = 0.391431, step = 3301
INFO:tensorflow:Saving checkpoints for 3400 into /tmp/mnist_convnet_model/model.ckpt.
INFO:tensorflow:Loss for final step: 0.372794.
WARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
INFO:tensorflow:Starting evaluation at 2017-03-15-22:40:29
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
INFO:tensorflow:Finished evaluation at 2017-03-15-22:40:30
INFO:tensorflow:Saving dict for global step 3400: accuracy = 0.9089, global_step = 3400, loss = 0.326106
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
{'loss': 0.32610574, 'global_step': 3400, 'accuracy': 0.90890002}
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Extracting MNIST-data/train-images-idx3-ubyte.gz
Extracting MNIST-data/train-labels-idx1-ubyte.gz
Extracting MNIST-data/t10k-images-idx3-ubyte.gz
Extracting MNIST-data/t10k-labels-idx1-ubyte.gz
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd121224850>, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}
/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.
  equality = a == b
WARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
INFO:tensorflow:Create CheckpointSaverHook.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.55GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.90G (12772704256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
INFO:tensorflow:Saving checkpoints for 3401 into /tmp/mnist_convnet_model/model.ckpt.
INFO:tensorflow:loss = 0.447333, step = 3401
INFO:tensorflow:
INFO:tensorflow:global_step/sec: 24.4126
INFO:tensorflow:loss = 0.367684, step = 3501
INFO:tensorflow:Saving checkpoints for 3600 into /tmp/mnist_convnet_model/model.ckpt.
INFO:tensorflow:Loss for final step: 0.330527.
WARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
WARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
Instructions for updating:
Estimator is decoupled from Scikit Learn interface by moving into
separate class SKCompat. Arguments x, y and batch_size are only
available in the SKCompat class, Estimator will only accept input_fn.
Example conversion:
  est = Estimator(...) -> est = SKCompat(Estimator(...))
INFO:tensorflow:Starting evaluation at 2017-03-15-22:40:41
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)

```"
8448,For loop over 'NoneType' TensorFlow Dimensions,"I'm implementing a new type of NN in TensorFlow. The difference is in the evaluation function, so instead of calling `tf.matmul()`, I call my own function, which we'll call `My_Function(A)`.

A snippet of the code can be seen below, where `A` is the tensor on the left being multiplied by this new NN implementation, which is on the right. The equivalent tensorflow code would be `tf.matmul(A, this_new_NN)`.

    def My_Function(self, A):
        dims = A.get_shape().as_list()
        shape = [dims[0], self.m] # Defining shape of resulting tensor
        X = tf.placeholder(tf.float32, shape=[shape[0], shape[1]])
        result = tf.zeros(tf.shape(X), dtype=tf.float32)
        for xyz in self.property:
            # Do some computation between A and xyz, xyz is a property of this_new_NN
            # resulting to temp_H with dimension [shape[0], xyz.m] of type tf.tensor
            dims_H = temp_H.get_shape().as_list()
            indices = [[i,j] for i in range(0, dims_H[0]) for j in range(xyz.k, xyz.k+dims_H[1])]
            # indices is a list of indices to update in ""result""
            values = tf.reshape(temp_H, [-1]) # Values in temp_H as 1D list
            delta = tf.SparseTensor(indices, values, shape)
            result += tf.sparse_tensor_to_dense(delta)
        return result

Now the problem I'm having is in the line where I calculate the `indices`, where I'm getting the error

`TypeError: 'NoneType' object cannot be interpreted as an integer`

Now, I understand that this error means that you cannot iterate a for loop over a type of `None`, but the problem I have is the test set and the training set have different values for `batch_size`. This means that when I go to create `result`, the first dimension is unknown which is why it is of type `None`. 

But, to get the indices that I have to update in `result`, I have to use a for loop to generate those values as a list to feed into `delta` which I'm creating as a `tf.SparseTensor` so it can be added to `result`. 

My question is, what is the best way to get the indices? I have tried replacing `dims_H[0]` in the for loop with a `tf.placeholder(tf.int32)` object instead, where I would then just pass the size when I run the session, but I get the error of

`TypeError: 'Tensor' object cannot be interpreted as an integer`

Any help would be greatly appreciated.

Edit:

Just for reference, this code is called in the following way, where `M` is the pre-built new NN composed with `tf.Variable` values.

`Y1 = tf.nn.relu(M.My_Function(A) + B1)`

where `B1` is the offset for this layer, and `A` is the input layer.

Edit2:

`result` should be a zero tensor *every time* `My_Function` is called. However, I have a suspicion that it is preserving the values of `result` with each function call. If this is right, please let me know what I need to do to change that.

Edit3:

When defining `A` it is defined as

    X1 = tf.placeholder(tf.float32, [None, 28, 28, 1])
    A = tf.reshape(X1, [-1, 28*28])

As the dimensions of `A` change between the training data and test data."
8447,im2txt checkpoint issue,"I have compiled tensorflow from source. I have followed steps at https://github.com/tensorflow/models/tree/master/im2txt. But when I run bazel-bin/im2txt/run_inference, it gives an error:

`bazel-bin/im2txt/run_inference   --checkpoint_path=${CHECKPOINT_DIR}   --vocab_file=${VOCAB_FILE}   --input_files=${IMAGE_FILE} --checkpoint_path=""/home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000""`

And error:

 Traceback (most recent call last):
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 83, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 63, in main
    restore_fn(sess)
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/inference_wrapper_base.py"", line 96, in _restore_fn
    saver.restore(sess, checkpoint_path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1428, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Tensor name ""lstm/basic_lstm_cell/biases"" not found in checkpoint files /home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000
	 [[Node: save/RestoreV2_380 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_380/tensor_names, save/RestoreV2_380/shape_and_slices)]]

Caused by op u'save/RestoreV2_380', defined at:
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 83, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 49, in main
    FLAGS.checkpoint_path)
  File ""/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/inference_wrapper_base.py"", line 116, in build_graph_from_config
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1040, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1070, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 675, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Tensor name ""lstm/basic_lstm_cell/biases"" not found in checkpoint files /home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000
	 [[Node: save/RestoreV2_380 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_380/tensor_names, save/RestoreV2_380/shape_and_slices)]]



"
8446,Scatter_nd bug,"I'm getting strange bug with scatter_nd (TF 1.0v).

    InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] +     
    params_shape[IXDIM:], got updates.shape [1,2,4], indices.shape [1,2,2], params_shape [1,9,4]

I've read the docs for scatter_nd function and it seems to me that following set of shapes are perfectly viable:
Maybe I'm tired but, Using terms from the docs:
`indices` rank: Q = 3
`shape` rank : P = 3
last dim length for indices K = 2
`updates` rank:  = Q-1+P-K = 3-1+3-2=3
All seems to fit.
And full shape of `update` should be: 
`[indices_shape[0], indicies_shape[1], shape[K]] `
which gives: 
`[1, 2, 4]`

Where is the problem?"
8445,Non-Docker Codelab,"In this codelab link: [TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#1), it states:

> This codelab will cover using both Docker and not using Docker. 

Yet I can't find the non-Docker instructions anywhere.
Am I missing something, or are they simply not there?"
8443,No such package 'tensorflow/tensorflow/tools/docs': BUILD file not found on package path,"I am trying to generate docs locally as g3doc now recommends after the documentation change from about 2 weeks ago, but the command it says to run is failing.

I am running `bazel run -- tensorflow/tools/docs:generate \ --src_dir=tensorflow/docs_src/ \ --output_dir=/tmp/tfdocs/`

But it fails with the error:

```
ERROR: no such package 'tensorflow/tensorflow/tools/docs': BUILD file not found on package path.
INFO: Elapsed time: 0.112s
ERROR: Build failed. Not running target.
```

I have the latest version of bazel installed, and my repo is up to date, so I'm not sure why this would be failing.  The `tools/docs/BUILD` file definitely exists, so it seems strange that it would tell me 'BUILD file not found on package path'.

I figured since this change to the docs is very recent, there could potentially be some problems here...although I'm not sure why this would impact bazel.  Or is there something I'm missing?"
8442,Serialization error in freeze_graph and/or optimize_for_inference_lib,"I haven't found any mention of this anywhere online.
After applying a workaround to #8404 another error turns up:

`E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: read/bn4/BatchNorm/cond/AssignMovingAvg_1/decay = Const[_class=[""loc:@BatchNorm_3/moving_variance""], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.999>](read/bn4/BatchNorm/cond/Switch:1)`

I have no ideas on what this error message even tries to tell me nor how to resolve the issue."
8441,XLA Feature request: Support Variables for tfcompile,"Is it feasible to extend tfcompile to support training?

Our reinforcement learning pipelines sometimes spend significant time in Python side of session.run because networks are small, and need lots of session.run calls since each .run leads to an interaction with a simulator (ie, Mujoco, gym or universe)

So you could have have 30k session.run calls where actual computation takes 200 usec, and Python session.run overhead is another 150 usec.

This could be improved if the network was tfcompiled and we didn't have to use session.run, however, this would need tfcompile to be extended to support multiple sets of fetches for the compiled object. IE

1. network initialization, input: weights, output: None
2. network forward op: input: observation, output: action
3. network train op: input: observations, actions output: None
4. network variable read: input: None, output: weights"
8440,Tensorboard conda envs not working,"Hi guys,

I am having a rather weird problem, which I hope someone here can solve. I am currently using the newest version of anaconda to create a python 3.5 environment with Tensorflow r1.0 installed. Inside my script I am perfectly able to create event files that should contain my Tensorflow graph, the script opens a writer that uses the tf.session graph as the graph and I create namescopes for the layers, weights and biases (Neural Network).

Upon running an event file gets generated that also can be used to open up tensorboard from my command prompt. This generates an localhost:6006 which can be opened perfectly fine but there is no graph where it should be. Additionally when I use the tensorboard inspect logdir command it tells me the event file is found. Now I have no idea if it is a tensorboard problem, an installation problem or a script problem. 

Does this perhaps sound familiar to anyone? Or is anyone perhaps able to open my event file? Would love to figure this out.

[events.out.tfevents.1489598042.EH39G4DC.zip](https://github.com/tensorflow/tensorflow/files/845479/events.out.tfevents.1489598042.EH39G4DC.zip)

If needed i can upload my script! Thanks alot!"
8439,tf_kernel_library doesn't work from repo containing tensorflow as a submodule,"`tf_kernel_library` and friends from `tensorflow.bzl` refer to deps like `//tensorflow/core:gpu_lib`.  These deps do not work if TensorFlow exists only under `@org_tensorflow`, which results in errors like

    bazel build ...
    WARNING: /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_tensorflow/tensorflow/workspace.bzl:72:5: tf_repo_name was specified to tf_workspace but is no longer used and will be removed in the future.
    WARNING: /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_fold/WORKSPACE:1: Workspace name in /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_fold/WORKSPACE (@org_tensorflow_fold) does not match the name given in the repository's definition (@org_fold); this will cause a build error in future versions.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.
    ERROR: Analysis of target '//deepmath/guidance:clause_ops' failed; build aborted.
    INFO: Elapsed time: 0.320s"
8438,How to broadcast,"Dear all:
I want to broadcast Thr from a single float value to (batchsize, 128,128, 1), I don't know how to do that, when I use the code below, the error occurred that shape do not match. I am using the newest version of tensorflow. Many thanks.

Thr  = tf.multiply(Thr, tf.ones([shape[1], shape[2]], tf.float32))
   
    "
8436,Please provide a working android example with Android Studio 2.3,"Obsolete gradle version of android example doesn't allow simply importing it as a directory to Android Studio and get the following error:

A problem occurred evaluating root project 'android'.
> java.lang.NullPointerException (no error message)

Still trying to solve this, but reproduction is so trivial (simply try to import the project directory to latest Android Studio and build), I would expect it to be solved faster by the community..."
8433,Cannot find attention functions,"Hi, i am trying to implement attention rnn and want to use https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/attention_decoder_fn_inference and other attention methods from attention_decoder_fn.py . However i cannot find it here on github ...

When this functionality will be available?
Why is it in api web page but not in the repo?"
8432,can't run the tensorflow/models/image/cifar10_train.py,"2017-03-15 20:25:07.637177: step 0, loss = 4.68 (15.8 examples/sec; 8.118 sec/batch)
2017-03-15 20:25:08.461579: step 10, loss = 4.64 (1899.2 examples/sec; 0.067 sec/batch)
2017-03-15 20:25:09.127436: step 20, loss = 4.55 (1772.0 examples/sec; 0.072 sec/batch)
2017-03-15 20:25:09.786581: step 30, loss = 4.43 (2158.3 examples/sec; 0.059 sec/batch)
2017-03-15 20:25:10.453524: step 40, loss = 4.36 (2082.0 examples/sec; 0.061 sec/batch)
2017-03-15 20:25:11.129617: step 50, loss = 4.30 (1850.5 examples/sec; 0.069 sec/batch)
2017-03-15 20:25:11.791486: step 60, loss = 4.21 (2012.2 examples/sec; 0.064 sec/batch)
2017-03-15 20:25:12.466547: step 70, loss = 4.16 (1905.1 examples/sec; 0.067 sec/batch)
2017-03-15 20:25:13.120901: step 80, loss = 4.17 (1898.9 examples/sec; 0.067 sec/batch)
2017-03-15 20:25:13.770472: step 90, loss = 4.14 (2087.0 examples/sec; 0.061 sec/batch)
/home/fly/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
An exception has occurred, use %tb to see the full traceback.
SystemExit
the information above was my problem.
when i run the code and set the max_steps = 100,the screen print ""SystemExit"" at step 90,when i set the code and set the max_steps = 1000,the screen print ""SystemExit"" at step 990."
8431,dilated convoluton uses a lot of memory,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

* https://github.com/tensorflow/tensorflow/issues/5083 

### Environment info
Operating System: `Linux hpclogin2 2.6.32-642.15.1.el6.x86_64 #1 SMP Thu Feb 23 11:19:57 CST 2017 x86_64 x86_64 x86_64 GNU/Linux`

Installed version of CUDA and cuDNN: 8.0 and 5.1
<details>
<summary>(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):</summary>
```
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so -> libcublas.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0 -> libcublas.so.8.0.27
-rwxr-xr-x 1 sebo root  38838688 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0.27
-rw-r--r-- 1 sebo root  49345532 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_device.a
-rw-r--r-- 1 sebo root  45050574 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_static.a
-rw-r--r-- 1 sebo root    560184 Sep  1  2016 /appl/cuda/8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
-rwxr-xr-x 1 sebo root    394472 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0.27
-rw-r--r-- 1 sebo root    737516 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 sebo root        15 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so -> libcufft.so.8.0
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0 -> libcufft.so.8.0.27
-rwxr-xr-x 1 sebo root 146745600 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0.27
-rw-r--r-- 1 sebo root 129655446 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft_static.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so -> libcufftw.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0 -> libcufftw.so.8.0.27
-rwxr-xr-x 1 sebo root    456424 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0.27
-rw-r--r-- 1 sebo root     42134 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw_static.a
lrwxrwxrwx 1 sebo root        17 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so -> libcuinj64.so.8.0
lrwxrwxrwx 1 sebo root        20 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0 -> libcuinj64.so.8.0.27
-rwxr-xr-x 1 sebo root   6459464 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0.27
-rw-r--r-- 1 sebo root   1649302 Sep  1  2016 /appl/cuda/8.0/lib64/libculibos.a
lrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so -> libcurand.so.8.0
lrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0 -> libcurand.so.8.0.27
-rwxr-xr-x 1 sebo root  59057024 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0.27
-rw-r--r-- 1 sebo root  59273876 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand_static.a
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so -> libcusolver.so.8.0
lrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0 -> libcusolver.so.8.0.27
-rwxr-xr-x 1 sebo root  52380368 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0.27
-rw-r--r-- 1 sebo root  22313722 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver_static.a
lrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so -> libcusparse.so.8.0
lrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0 -> libcusparse.so.8.0.27
-rwxr-xr-x 1 sebo root  42976296 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0.27
-rw-r--r-- 1 sebo root  51604078 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse_static.a
```
</details>
<br>


If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): 168d188168b30b204099f21e456151752d7fb718
2. The output of `bazel version`: `0.4.3`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```py
import numpy as np
import sugartensor as stf
import tensorflow as tf


def get_variable(name, in_dim, out_dim, size=None):
    if size is None:
        size = 1
        shape = (in_dim, out_dim)
    else:
        shape = (size, in_dim, out_dim)

    w = tf.get_variable(name, shape, dtype=tf.float32,
                        initializer=tf.random_uniform_initializer(
                            minval=-np.sqrt(1 / (in_dim * size)),
                            maxval=np.sqrt(1 / (in_dim * size))
                        ))
    return w

# build forward pass
embedding = get_variable('embed', 128, 892)
embedding_inv = get_variable('embed-inv', 892, 128)

data = tf.placeholder(name='x', shape=(160, 200), dtype=tf.int32)
output = tf.nn.embedding_lookup(embedding, data)

for i in range(60):
    Wi = get_variable(f'W{i}', 892, 892, 5)
    output = tf.nn.convolution(input=output, filter=Wi,
                               padding='SAME', dilation_rate=[16],
                               name='aconv1d')
    output = tf.nn.relu(output)

logits = tf.reshape(tf.matmul(tf.reshape(output, [-1, 892]), embedding_inv),
                    [160, 200, 128])

# optimize for the idendity function
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=data, logits=logits
))

# create update ops
optimizer = tf.train.AdamOptimizer()
grad_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())
update_ops = optimizer.apply_gradients(grad_and_vars)

config = tf.ConfigProto(allow_soft_placement=True)
with tf.Session(config=config) as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(1000):
        loss_result = sess.run([loss, update_ops], feed_dict={
            data: np.random.randint(0, 128, size=(160, 200))
        })[0]
        print(f'iteration {i} complete: {loss_result}')
```

This example is perhaps too theoretical to be discussed from a practical application perspective. The actual application is the [ByteNet](https://arxiv.org/abs/1610.10099) model, the implementation is very similar to https://github.com/buriburisuri/ByteNet. The ByteNet model stacks multiple one-dimensional-dilated-convolutions (30), because each of them uses `space_to_batch` they use a lot of memory.


### What other attempted solutions have you tried?

I've implemented one-dimensional-masked-dilated-convolutions using `tf.scan`, this uses much less memory but is also slower.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

* The error log from the actual application: https://gist.github.com/AndreasMadsen/91e49e13f0085ececbef0f80c830c5af (note that this happens after 21334 iterations/14 hours, so there may also be a garbage collection issue)
* The error log from the simplified example: https://gist.github.com/AndreasMadsen/94f5100aff697cdf5ff6c26f90a6dad7"
8430,Exif Orientation support tf.image.decode_jpeg(),"Hi.
I have large amounts of JPEG files uploaded by users on our service .
They use mainly smartphone camera, so most files contain EXIF orientation metadata.

It would seem that the `tf.image.decode_jpeg()` ignores this information,
So I cannot build pipeline using pure tensor operation.
I thought it is not better to writing custom decoder depending external libraries, considering complying thread safety and future compatibility.

My Pipeline code is like below:

```python
filename_queue = tf.train.string_input_producer(files)
reader = tf.WholeFileReader()
key, supplyContent = reader.read(filename_queue)
decoding = tf.image.decode_jpeg(supplyContent, channels=3)
#  source image may have orientation but dropped 
resizing = tf.image.resize_images(decoding, (IMAGE_SIZE, IMAGE_SIZE))
# ...
```

Transformation of image can be done with simple tensor op, since tf.image has many convenience methods.
How about adding support for these meta informations ?


"
8429,Versions Web Page Needs Updating,"This really is a small issue but I couldn't submit a PR as I couldn't find the file for it.
At https://www.tensorflow.org/versions/ the current version is stated as 0.12 when it should be 1.0!

`The docs at root (i.e. in tensorflow.org/api_docs) refer to the most recent stable branch (in this case, r0.12)`

That's it. Sorry for raising an issue for such a trivial detail! Thanks"
8428,HVX Acceleration support,"I successfully built and ran the test application from `tensorflow/tree/master/tensorflow/contrib/hvx`. I'd now like to benchmark HVX against the CPU implementations of `tensorflow/tools/benchmark`, and the Android camera demo, but I wasn't able to find any documentation describing how to build said apps with HVX support (my builds run on the CPU). I'm testing on the Open-Q 820 development board with Android 7.0.

Is it possible to utilize HVX acceleration outside the HVX test application, preferably with the benchmark and Android camera demos? If so, could someone please point me in the right direction?"
8425,Saving with monitored session,"I get the following error when trying to use a saver with a Monitored session:

```
File ""/users/spraak/spch/prog/spch/tensorflow-1.0.0/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1369, in save
    raise TypeError(""'sess' must be a Session; %s"" % sess)
TypeError: 'sess' must be a Session; <tensorflow.python.training.monitored_session.MonitoredSession object at 0x2be710d0>
```

I save like this:

```
with tf.train.MonitoredTrainingSession(...) as sess:
    ...
    saver.save(sess, 'model.ckpt')
```

instead of sess I also tried `tf.get_default_session()`, but this returns None. In all other cases the monitored_session works just like a normal session, so I think it's a bug."
8424,mean_pairwise_squared_error,"when i use the tf.losses.mean_pairwise_squared_error(labels, predictions, weights=1.0, scope=None, loss_collection=tf.GraphKeys.LOSSES) function, i am sure the data is right. but the loss on the tensorboard is always zero. I try hard but can know why?
score_a=tf.reshape(score,[-1])#shape: [1,39]
ys_a=tf.reshape(ys,[-1])#shape: [1,39]
with tf.name_scope('loss'):
	loss=tf.losses.mean_pairwise_squared_error(score_a,ys_a)"
8423,Android demo build Error,"when i used android studio build the android demo, i got this error:

Warning: ignoring http_proxy in environment.
____Loading package: tensorflow/examples/android
____Found 1 target...
____Building...
Target //tensorflow/examples/android:external_assets up-to-date (nothing to build)
____Elapsed time: 0.163s, Critical Path: 0.00s

:copyExternalAssets UP-TO-DATE
:buildNativeBazel
Warning: ignoring http_proxy in environment.
____Loading complete.  Analyzing...
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
____Found 1 target...
____Building...
ERROR: /home/zhaoming/tensorflow/tensorflow/tensorflow/examples/android/BUILD:22:1: C++ compilation of rule '//tensorflow/examples/android:libtensorflow_demo.so' failed: false failed: error executing command /bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/object_tracking/tracked_object.pic.d ... (remaining 25 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/examples/android:tensorflow_native_libs failed to build
Use --verbose_failures to see the command lines of failed build steps.
____Elapsed time: 0.248s, Critical Path: 0.01s

 FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':buildNativeBazel'.
> Process 'command '/home/zhaoming/bin/bazel'' finished with non-zero exit value 1

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 0.675 secs

how can i fix it? thx
"
8422,"Tensorflow_gpu crash on windows server 2012(Nightly build Feb 15, 2017 1:25:00 AM)","### Environment info
Operating System:
Windows Server 2012 R2
GPU GeForce GTX 750 and 1080 in two windows server workstation.

Installed version of CUDA and cuDNN: 
CUDA 8.0 installed
CuDNN download, and set path to system environment variable %PATH%

If installed from binary pip package, provide:
I install the fallow whl: ""tensorflow_gpu-1.0.0rc2-cp35-cp35m-win_amd64.whl""
http://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=gpu,OS=windows/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.0.0rc2-cp35-cp35m-win_amd64.whl

I only import tensorflow, and it's crash.
-------------------------------------------------
```
(venv64) C:\work\keras01>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, i
n swig_import_helper
    return importlib.import_module(mname)
  File ""C:\work\keras01\venv64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module
>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, i
n <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, i
n swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\work\keras01\venv64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module
>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, i
n swig_import_helper
    return importlib.import_module(mname)
  File ""C:\work\keras01\venv64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module
>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 21, i
n <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\work\keras01\venv64\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, i
n swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\work\keras01\venv64\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#im
port_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

"
8421,Bazel build failed in compiling TensorFlow from source. ,"I am compiling TensorFlow from source on an Ubuntu machine. This is a verbose output of the error log generated in the ```bazel build //tensorflow/tools/pip_package:build_pip_package``` execution.

Error Log:

    ERROR: /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/external/llvm/BUILD:418:5: Generating code from table: lib/Target/AArch64/AArch64.td @llvm//:aarch64_target_gen__gen_fast_isel_genrule failed: bash failed: 
    error executing command 
    (cd /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/execroot/tensorflow && \
    exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/llvm/llvm-tblgen -I external/llvm/include -I external/llvm/tools/clang/include -I $(dirname external/llvm/lib/Target/AArch64/AArch64.td) -gen-fast-isel external/llvm/lib/Target/AArch64/AArch64.td -o bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64/AArch64GenFastISel.inc'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
    bazel-out/host/bin/external/llvm/llvm-tblgen: relocation error: bazel-out/host/bin/external/llvm/llvm-tblgen: symbol _ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference
    Target //tensorflow/tools/pip_package:build_pip_package failed to build




I built TensorFlow from source, and here is the relevant information:
```
$ git rev-parse HEAD
4c3bb1aeb7bb46bea35036433742a720f39ce348

$ bazel version
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

```
System:
```
x86_64 GNU/Linux
```
"
8419,Error in compiling TensorFlow from source on Ubuntu: bazel build fails.,"I am compiling TensorFlow from source on an Ubuntu machine. This is a verbose output of the error log generated in the ```bazel build //tensorflow/tools/pip_package:build_pip_package``` execution. 

System details:
```uname -a ```: Linux - 3.13.0-107-generic



    ERROR: /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/external/llvm/BUILD:418:5: Generating code from table: lib/Target/AArch64/AArch64.td @llvm//:aarch64_target_gen__gen_fast_isel_genrule failed: bash failed: 
    error executing command 
    (cd /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/execroot/tensorflow && \
    exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
    bazel-out/host/bin/external/llvm/llvm-tblgen -I external/llvm/include -I external/llvm/tools/clang/include -I    
    $(dirname external/llvm/lib/Target/AArch64/AArch64.td) -gen-fast-isel external/llvm/lib/Target/AArch64/AArch64.td -o bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64/AArch64GenFastISel.inc'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.
    bazel-out/host/bin/external/llvm/llvm-tblgen: relocation error: bazel-out/host/bin/external/llvm/llvm-tblgen: symbol _ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
"
8418,bazel can not quantize image net inception models,"when i run command : `bazel build tensorflow/tools/quantization:quantize_graph --verbose_failures` from this tutorial : https://www.tensorflow.org/performance/quantization, i get this error, and still can not fix it : 
    ERROR: /tensorflow/tensorflow/core/kernels/BUILD:1315:1: C++ compilation of rule '//tensorflow    /core/kernels:svd_op' failed: gcc failed: error executing command 
    virtual memory exhausted: Cannot allocate memory    

I run this command on docker with single CPU. I have 1.5 GB RAM in docker.
So will i need more RAM to run it ?
can i download a quantize image net inception models without using bazel , like as downloading inception models without compiling it by bazel ?
"
8417,Have mnist.py be controlled by tf.set_random_seed() for testing purposes.,"It might be helpful to make the internal random generator within mnist.py be connected to tf.set_random_seed().  Since calling this function is part of the first tutorials to tensorflow, it can be confusing if users try to obtain reproducible results by setting tf.set_random_seed() and seeing that it has no effect (because mnist.py does not comprehend this)."
8416,CPU resources of Tensorflow's docker containers could not be controlled by --cup-shares  ,"### Environment info
OS:Ubuntu14.04LTS
GPU:Nvidia Pascal TITUN X
CUDA8.0
cuDNN CUDA8.0 V5.1
Docker Verison:17.03.0-ce
Nvidia docker : 1.0.1
CPU Intel Core i7 6900K , Hyper-THreading off , Turbo boost off


####Docker file
I installed Tensorflow by Dockerfile as below;

================================
 FROM nvidia/cuda:8.0-cudnn5-devel

ENV http_proxy http://mycompany.proxy:8080
ENV https_proxy http://mycompany.proxyp:8080


RUN     apt-get update  &&  apt-get install -y \
        python-dev \
        python-pip \
        nano \
        git 
    
RUN    rm -rf /var/lib/apt/lists/* /var/cache/apt/archieves/* 

RUN    pip install --upgrade --user https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl

WORKDIR /root/.local/lib/python2.7/site-packages/tensorflow

RUN git clone https://github.com/tensorflow/models.git

WORKDIR /root/.local/lib/python2.7/site-packages/tensorflow/models/tutorials/image/cifar10

####My Procedure[a]
(1) At first ,I build Dockerfile 
     $ nvidia-docker build -t cpu:tensorflow0.

     Tensorflow docker image was build with no-problem and no-erros.

(2)Next  I run two docker container and log in two container via Bash
  $ nvidia-docker run --cpuset-cpus=0-5 --cpu-shares=2048 -it cpu:tensorflow0
  $nvidia-docker run --cpuset-cpus=0-5 --cpu-shares=1024 -it cpu:tensorflow0

    I used all CPU cores(6 cores).

(3) I did ""python cifar10_train.py"" on two containers ,and check cpu-resources by docker stats.
     By the way, tow python examples""cifar10_train.py"" were same code.  
     CPU Resource of one container was  339.86%
     CPU Resources of another on another container was 230.19% 
   
      CPU Resouce Rate 339.86 : 230.19 was not different from --cpu-shares Rate 2048:1024 

  ####My Procedure[b]
To make sure, I did same procedure by using CPU 5cores.   

(1)Next  I run two docker container and log in two container via Bash
  $ nvidia-docker run --cpuset-cpus=0-4 --cpu-shares=2048 -it cpu:tensorflow0
  $nvidia-docker run --cpuset-cpus=0-4 --cpu-shares=1024 -it cpu:tensorflow0

    I used only 5 CPU cores. because --cpuset-cpus=0-4

(2) I did ""python cifar10_train.py"" on two containers ,and check cpu-resources by docker stats.
     By the way, tow python examples""cifar10_train.py"" were same code.  
     CPU Resource of one container was  278.13%
     CPU Resources of another on another container was 195.80% 
   
      CPU Resources Rate 278.13 :195.80 was not different from --cpu-shares Rate 2048:1024 

 ####My Procedure[c]
 I did same process by Chainer Containers :Chainer Version 1.21.0

 (1)6cores :Two Chainer containers ,--cpusets-cpu=0-5,--cpu-shares=2048 and --cpu-shares=1024

     CPU resources   394.23 : 195.41 nealy 2:1 , It is as same as 2048:1024

(2) 5cores :Two Chainer containers ,--cpusets-cpu=0-4,--cpu-shares=2048 and --cpu-shares=1024

     CPU resources   330 .82: 166.13 nealy 2:1 , It is as same as 2048:1024

 ####My additional Procedure[d]
I run 4 docker container at --cpu-shares setting at 2048,1024,1024 ,and 512.

*Tensoflow 4 Container CPU resources : 205.21 : 135.68 : 142.71 : 97.94
  They were not rates of --cpu-shares ; 2.09 : 1.39 : 1.46 : 1.0

*Chainer 4 Containers CPU resources : 267.44 : 133.93 :129.59 : 59.98
  They ware nearly equal rate of --cpu-shares ; 4.45 : 2.23 : 2.26 : 1.0   

####Result

Two Chainer's docker containers operated according with the setting of --cpu-shares.
But  Two Tensorflow's docker containers did not operate according with the setting of --cpu-shares.

 ####My Question
Do you know the reason why two Tensorflow's containers did not operate according with the setting of --cpu-shares?



"
8415,"Cannot decode csv using int32, int64 or float32 in windows when csv is encoded in utf-8 BOM","### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
http://stackoverflow.com/questions/38879779/tensorflow-python-framework-errors-invalidargumenterror-field-0-in-record-0-is
http://stackoverflow.com/questions/33808368/how-do-i-change-the-dtype-in-tensorflow-for-a-csv-file


### Environment info
Operating System: windows10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
import os
import sys

import tensorflow as tf

filename_list = tf.train.match_filenames_once(""D:/Source/Repos/alerte-source/Alertedh/Python/Alerte.Test.PythonTestingApplication/Alerte.Test.PythonTestingApplication/mnist_data/*.csv"", name='filename_list')

filename_queue = tf.train.string_input_producer(filename_list,num_epochs=1,name='filename_queue')

reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

record_defaults = [tf.constant([], dtype=tf.int32) for row in range(785)]

data = tf.decode_csv(value, record_defaults=record_defaults)

print(data[0].dtype)

features = tf.stack(data[:-1])

label = tf.stack(data[-1])

min_after_dequeue = 10000
batch_size = 2
capacity = min_after_dequeue + 3 * batch_size

examples, labels = tf.train.shuffle_batch([features, label],batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)


with tf.Session() as sess:
  tf.initialize_all_variables().run()
  tf.local_variables_initializer().run()
  tf.global_variables_initializer().run()

  # start populating filename queue
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  try:
    while not coord.should_stop():
      example_batch, label_batch = sess.run([examples, labels])
      print(example_batch)
  except tf.errors.OutOfRangeError:
    print('Done training, epoch reached')
  finally:
    coord.request_stop()

  coord.join(threads) 


example csv:
2,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17

### What other attempted solutions have you tried?
Works when the dtype is string as it doesnt have to try to parse the values...

### Logs or other output that would be helpful
none


Error: tensorflow.python.framework.errors.InvalidArgumentError: Field 0 in record 0 is not a valid int32: 2 

(this is likely a windows only error)
Because of the encoding prefix / (\xef\xbb\xbf) the string decoders cannot read the first element of the file... resiliency to this can help cross platform errors. the error doesnt pick up the prefix either, so it was a bit of a mystery to me for a while... 

If this is not to be supported then feel free to close

regardless, Ill put this here for other people having similar issues. you can check by setting your dtype to string and checking the first element for the prefix... go to notepad++ or similar and convert your file to UTF-8 rathter than UTF-8 BOM which is default.

"
8414,softmax_cross_entropy_with_logits() behaves differently with it's comment,"version: 1.0.1 CPU compiled by cmake from master branch.

According to [the document](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits), `softmax_cross_entropy_with_logits()` accept inputs with any shape. However, the comments in the source code says:

`logits` and `labels` must have the same shape `[batch_size, num_classes]`  and the same dtype (either `float16`, `float32`, or `float64`).

meaning that only rank-2 tensors are allowed.

I tried the following code to construct a net which performs binary-classify on each pixel of an image :

```
    final_two_channel = tf.layers.conv2d(
        inputs=res_out_iter,
        filters=2,
        kernel_size=[1, 1],
        padding='same',
        activation=tf.nn.sigmoid
    )
    # logits = tf.reshape(tensor=final_two_channel, shape=[-1, 2], name='flatten_net_out')
    label_one_hot = tf.one_hot(
        indices=output_field,
        depth=2,
        on_value=1, off_value=0,
        name='label_one_hot'
    )
    crs_etp_loss = tf.nn.softmax_cross_entropy_with_logits(labels=label_one_hot, logits=final_two_channel)
    print(crs_etp_loss.shape)
    crs_etp_loss = tf.reshape(tensor=crs_etp_loss, shape=[-1, field_width * field_width])
    crs_etp_loss = tf.reduce_mean(input_tensor=crs_etp_loss, axis=1)

    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=crs_etp_loss)
```

The program runs without error and `print(crs_etp_loss.shape)` gives `(?, 512, 128)`, but I am not sure if it's safe/correct to do like this.

I posted a question on [stackoverflow](http://stackoverflow.com/questions/42799167/is-it-safe-to-use-softmax-cross-entropy-with-logits-when-each-instance-has-mul) about this."
8410,tfdbg strange things about detecting NAN weights in network,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Hi all, I just meet the NAN weight in the training for my network, I have check the loss and see whether there is like the form of 0/0, or log(0) and changing the learning rate but it still meet the problem when I try in the 200 steps. So i use tfdbg to debug, it is really nice tool. But when I use it, it running only several steps and stop. And I print this node out, this node has so many numbers. when I use pt node, and there is so many numbers to detect, I am not find the inf and nan. Besides, the weights i even not change it, so what cause this problem
### Environment info
Operating System:
Ubuntu 14:04
Cuda 8.0
cuDNN5.1
Installed version of CUDA and cuDNN: 

this is output in the screen--- run-end: run #103: fetch: t_DQN/dqn_q/BiasAdd:0; 2 feeds ----------------------------------------------------
Tensor ""encoder/conv0/Conv2D:0:DebugIdentity"":
  dtype: float32
  shape: (8, 149, 149, 32)

array([[[[ -2.31261123e-02,  -2.28441790e-01,  -1.15194634e-01,   1.87772080e-01,   1.52234808e-01,
            9.16329771e-02,  -1.40365764e-01,   8.70720018e-04,   1.98106512e-01,  -3.14010084e-02,
            5.08706868e-02,  -1.47755712e-01,   1.76122814e-01,   4.00081761e-02,  -1.88322872e-01,
           -1.56596974e-01,  -1.78046972e-01,  -1.91862926e-01,  -1.90227181e-01,  -6.63982928e-02,
            1.77524462e-01,  -7.36486465e-02,  -6.05005510e-02,   1.09997056e-01,   9.24435183e-02,
            4.22568992e-05,  -1.06559973e-03,   9.99473780e-02,  -1.73573643e-02,   1.16514668e-01,
           -1.51743174e-01,   2.23651499e-01],
         [ -2.14941762e-02,  -2.30754048e-01,  -1.18553132e-01,   1.89629942e-01,   1.53854966e-01,


"
8409,Windows: is Python 2.7 support planned?,Good evening! Are you planning to support Python 2.7 on Windows? It may be very useful.
8407,Support for 'paged' tensor ,"I'm curious if it is technically feasible to support 'paged' tensors - by this I mean a tensor that seamlessly spills over into main memory when it overflows GPU memory and intelligently pages in and out from main memory. 

My interest is in processing very large volumes of microscopy data - on the order of a couple thousand voxels to each axis. They don't even come close to fitting into GPU memory, one of the smaller volumes takes a full 250G to run on CPU.  My model in this case is fully spatially convolutional so a divide and conquer strategy into blocks works fine, however there's a lot of wasted computation in recalculating overlapping activations. 

This particular use case may be somewhat rare, but I think this would be a really really useful thing to have around. Very curious if this is at all feasible and if automatic device allocation/management stuff like this in the works. "
8406,Android example: symbolic link error when running on Android Studio,"I am trying to run android example app on Android studio. It builds fine with Bazel and Android Studio. But it gives this error when I run the app using Android Studio.

```
Error:Could not list contents of '/home/a/.cache/bazel/_bazel_root/6c98a6c54aeed71fa731445a9f51836b/execroot/tensorflow/external/local_jdk/src.zip'. Couldn't follow symbolic link.

"
8404,Wrong order of dependencies after running freeze_graph and/or optimize_for_inference,"I haven't found any mention of this anywhere online.
It makes the graph serializations completely useless for inference.

Steps to reproduce:
- create graph that contains tf.contrib.layers.batch_norm with tf.bool tensor as is_training argument (to force use of Switch node
- run freeze_graph.freeze_graph and optimize_for_inference_lib.optimize_for_inference
- load resulting graph on Android via TensorFlowInferenceInterface

What happened:
ADB Logcat shows error message
`E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: Node 'conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/sub_1/x': Control dependencies must come after regular dependencies`

Why did this happen:
I found out that the order of dependencies was inconsistent after the processing.

Dependencies before processing:
```
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean""
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd""
input: ""^conv1/bn1/BatchNorm/cond/switch_t""
```

Dependencies after processing:
```
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean""
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd""
input: ""conv1/bn1/BatchNorm/cond/Switch:1""
```

What is wrong:
The control dependencies (starting with '^') should be after the regular dependencies.

Expected behaviour:
Reordering of dependencies to ensure ordering consistency.

Expected order of dependencies:
```
input: ""conv1/bn1/BatchNorm/cond/Switch:1""
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean""
input: ""^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd""
```"
8403,C++ libtensorflow.so segfault on exit on Ubuntu 14.04 (no segfault on OSX),"### Summary
When using libtensorflow.so in a C++ app a segfault occurs when exiting the app on Ubuntu 14.04 while on OSX Sierra this does not get thrown (both using Protobuf 3.2.0). The actual execution of the app is fine, successfully running data through the graph. Only the mentioned segfault when exiting. From the stacktrace you can see it occurs while a protobuf hastable is cleared by libtensorflow.so.

### Environment info
Ubuntu 14.04

Installed version of CUDA and cuDNN: 
ls -l /usr/local/cuda/lib64/libcud*  
-rw-r--r-- 1 root    546K Sep 15 00:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root      16 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root      19 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root    406K Sep 15 00:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root    757K Sep 15 00:02 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 mtanner   13 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5*
lrwxrwxrwx 1 mtanner   17 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5*
-rwxrwxr-x 1 mtanner  76M Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5.1.5*
-rw-rw-r-- 1 mtanner  67M Jul 27  2016 /usr/local/cuda/lib64/libcudnn_static.a

### Installed from source
1. The commit hash - `e895d5ca395c2362df4f5c8f08b68501b41f8a98` (from the r1.0 branch)
2. The output of `bazel version` - Build label: 0.4.4 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Feb 1 18:54:21 2017 (1485975261) Build timestamp: 1485975261 Build timestamp as int: 1485975261
3. Workspace.bzl simply updated to point to a fixed protobuf 3.2.0 to match our system version
```
...
  native.http_archive(
      name = ""protobuf"",
      urls = [
          ""https://github.com/google/protobuf/archive/v3.2.0.tar.gz"",
      ],
      sha256 = ""2a25c2b71c707c5552ec9afdfb22532a93a339e1ca5d38f163fe4107af08c54c"",
      strip_prefix = ""protobuf-3.2.0"",
  )
...
```

### Logs or other output that would be helpful

#### protoc --version
```
libprotoc 3.2.0
```

#### gdb stacktrace (at the end of app execution)
```
[Thread 0x7fffdcdc0700 (LWP 2193) exited]
Program received signal SIGSEGV, Segmentation fault.
0x00007fffefed8ea3 in std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type>, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> >, std::__detail::_Select1st, std::equal_to<std::string>, google::protobuf::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear() () from /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/libtensorflow.so
(gdb)
```

#### ldd libtensorflow.so
```
ldd /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/libtensorflow.so
        linux-vdso.so.1 =>  (0x00007ffe19bf6000)
        libcudart.so.8.0 => /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.so.8.0 (0x00007f92440ef000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f9243eeb000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f9243be5000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f92439c7000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f92436c3000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f92434ad000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f92430e8000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f924ef6a000)
        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f9242ee0000)
```

I tried to make this brief but if any other info is helpful let me know.

Cheers
"
8401,Tensorflow not working in Zeppelin 0.7.0,"There seems to be some issue using Tensorflow in Zeppelin 0.7.0 and it throws this error:
NameError: name `_interactive` is not defined

Another user reported the [same issue](http://stackoverflow.com/questions/42757433/tensorflow-can-not-work-with-zeppelin) on SO. The fix suggested there seems to be really hacky.

The issue seems to be that in tf_logging.py file [.\Anaconda3\Lib\site-packages\tensorflow\python\platform\tf_logging.py] the _interactive variable is not triggering for Zeppelin.

```
# Determine whether we are in an interactive environment
try:
  # This is only defined in interactive shells
  if _sys.ps1: _interactive = True
except AttributeError:
  # Even now, we may be in an interactive shell with `python -i`.
  _interactive = _sys.flags.interactive
```"
8399,"mnist_softmax.py has ""TimeoutError: [WinError 10060]"" ...","The code https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py
does not run on TensorFlow (Windows installation)

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Quite similar problem reported here: http://stackoverflow.com/questions/40467893/running-mnist-softmax-py-on-tensorflow-installed-with-docker However, no solution there...

### Environment info
Operating System: Windows 10 pro (Intel(R) Core(TM) i7 6500U CPU) 
TensorFlow installed: pip install -U tensorflow; Python 3.5.2 :: Anaconda custom (64-bit)
------------------------------------------------------------------------------------------

Traceback (most recent call last):
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 1254, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 1106, in request
    self._send_request(method, url, body, headers)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 1151, in _send_request
    self.endheaders(body)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 1102, in endheaders
    self._send_output(message_body)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 934, in _send_output
    self.send(msg)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 877, in send
    self.connect()
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\http\client.py"", line 849, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\socket.py"", line 711, in create_connection
    raise err
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\socket.py"", line 702, in create_connection
    sock.connect(sa)
TimeoutError: [WinError 10060] 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 714, in runfile
    execfile(filename, namespace)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 89, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/natlun/Documents/Python Scripts/mnist_softmax03.py"", line 92, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:/Users/natlun/Documents/Python Scripts/mnist_softmax03.py"", line 20, in main
    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py"", line 211, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 208, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 165, in wrapped_fn
    return fn(*args, **kwargs)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py"", line 190, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 188, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 163, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 466, in open
    response = self._open(req, data)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 484, in _open
    '_open', req)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 444, in _call_chain
    result = func(*args)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 1282, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""C:\Users\natlun\AppData\Local\Continuum\Anaconda3\lib\urllib\request.py"", line 1256, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [WinError 10060]>"
8398,How to apply dropout for getting prediction from the last layer of a trained model?,"I have used transfer learning to retrain/fine-tune Inception-V3 model for image classification on my own data in Tensorflow. However, when I test the image classification after training, I get the following error:

> ""Invalid argument: You must feed a value for placeholder tensor 'final_layer/dropout/Placeholder' with dtype float""

I am using the following code for getting the predictions from the final layer:

`softmax_tensor = sess.graph.get_tensor_by_name('final_layer/final_result/Softmax:0')
			predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})`

I guess the error is caused by not applying the dropout placeholder in the second statement, as the training was performed with dropout layer. How do I add a dropout placeholder here?
"
8397,32-bit Windows exe generation using cmake,"Hi

I have compiled my code using the procedure outlined in the documentation to generate 64-bit exe on windows using cmake. Now I want to create 32-bit binary but cmake does not run for the option -A x86.
Can somebody how can I generate a 32-bit binary still using 64-bit VS tools?"
8396,Retrain Inception Model Error In Android,"I am following [Tensorflow for poet](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) instruction for retrain model. I have successfully create retrained_graph.pb and retrained_labels.txt. While I use imagenet_comp_graph_label_strings.txt and tensorflow_inception_graph.pb then application run without any error. But use my created file then I get error that:

`Caused by: java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization().
                                                                         at org.tensorflow.Graph.importGraphDef(Native Method)
                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:118)
                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:102)
                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.load(TensorFlowInferenceInterface.java:402)
                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.initializeTensorFlow(TensorFlowInferenceInterface.java:91)
                                                                     	at org.tensorflow.demo.TensorFlowImageClass`"
8394,Protobufs are into multiple shared libraries loaded from python,"### Issue description
Tensorflow currently fails with the following error if compiled using `clang` in `-c opt` mode when trying to import `tensorflow.contrib` package in Python .

Python code reproducing the problem is very simple: 
```
import tensorflow.contrib
```

Program output:
```
[libprotobuf ERROR external/protobuf/src/google/protobuf/descriptor_database.cc:57] File already exists in database: tensorflow/core/example/example.proto
[libprotobuf FATAL external/protobuf/src/google/protobuf/descriptor.cc:1275] CHECK failed: generated_database_->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: generated_database_->Add(encoded_file_descriptor, size): 
```

The short story is that protobufs are getting statically linked into two shared libraries, both of which get loaded at runtime and that causes the error.

Here's the full breakdown of what happens:
1. Protobufs (`//tensorflow/core:protos_all_cc`) get compiled as a static library.
1. Protobufs (`//tensorflow/core:protos_all_cc`) get statically linked into two separate shared libraries: `_pywrap_tensorflow_internal.so` and `_pywrap_tensorflow_print_model_analysis_lib.so`.
2. While compiling those clang inlines the protobuf initialization code(`AddDescriptors`) inside `example.pb.cc`(look for it in `bazel-genfiles`)  to the global initialization code of both shared libraries.
3. `python run.py` starts running. While processing python's import statement, dynamic linker gets called to load `_pywrap_tensorflow_internal.so`. Static initialization code inside `example.pb.cc` is run, registering it to the protobuf database of `_pywrap_tensorflow_internal.so`
4. At a later point `_pywrap_tensorflow_print_model_analysis_lib.so` gets loaded. Since python calls `dlopen` with `RTLD_GLOBAL` dynamic linker finds an existing symbols for `AddDescriptorsImpl` in `_pywrap_tensorflow_internal.so` and uses that for all calls to that function later(for calls coming from `_pywrap_tensorflow_print_model_analysis_lib.so` too). 
5. Static initialization code inside `example.pb.cc` is run again (for `_pywrap_tensorflow_print_model_analysis_lib.so`), it calls `AddDescriptorsImpl` and gets into the function from `_pywrap_tensorflow_internal.so`, which tries to registers the same file again in the protobuf database of `_pywrap_tensorflow_internal.so` leading to the specified error.

Here are a few observations that may be interesting:
1. It works with `gcc`, because `gcc` doesn't inline `AddDescriptors` to the global initialization code of libraries, then dynamic linker merges those two functions into one, and that function has a proper check for being called multiple times(`AddDescriptorsImpl`, which is getting called after inlining doesn't). But note that it may break too if `gcc` will start inlining `AddDescriptors` in a newer version.
2. It works on Mac, because dynamic linker there doesn't merge corresponding functions into one. Note that it means we get multiple protobuf databases(one for each loaded shared library that has protobufs in it) and can probably lead to other problems.

### Environment info
Operating System: ubuntu 14.04
Installed version of CUDA and cuDNN: none
1. The commit hash (`git rev-parse HEAD`)
`ff9682b5f493ae7ad912da29789668dbf50d5e1f`
2. The output of `bazel version`
`Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
`

### Repro

1. Extract [test.zip](https://github.com/tensorflow/tensorflow/files/841040/test.zip) to the repository root(it's a sample python target that fails)
1. Make sure `clang` is installed. My version is `3.8.0-2ubuntu3~trusty4`, but that shouldn't matter.
2. Configure with 
```
export CC=/usr/bin/clang
TF_NEED_JEMALLOC=1 TF_NEED_GCP=0 TF_NEED_HDFS=0 \
TF_ENABLE_XLA=0 TF_NEED_OPENCL=0 TF_NEED_CUDA=0 \
yes """"  | ./configure
```
3. Build and run with opt
```
bazel run -c opt //test:run
```
"
8393,Add bounds to tf.contrib.opt.ScipyOptimizerInterface,"For style transfer it is common the use L-BFGS-B, and the ScipyOptimizerInterface greatly simplifies its application in TF. However, to increase the quality of the synthesised images one typically restricts the range of the values to standard int8 [0, 255] by providing box constraints. 

Hence my feature request: Please add the bounds keyword to the interface of ScipyOptimizerInterface."
8392,How to add a new op which is similar with matmul,"I want to add a new op which is similar with matmul, for example anothermatmul. 
1. add REGISTER_OP(""AnotherMatMul"") to tensorflow/core/ops/math_ops.cc like MatMul
2. add anothermatmul_op.h anothermatmul_op.cc to tensorflow/core/kernels/
3. bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
then I can see  bazel-bin/tensorflow/core/libmath_ops_op_lib.lo is update, I think the new op has been compiled successfully.
4. install 

I find util/python/python_lib/tensorflow/python/ops/gen_math_ops.py is not updated, and cannot use AnotherMatMul in python.

how to update gen_math_ops.py?
or how to generate python wrap?
"
8391,tf.lbeta() error when fed with placeholder,"Hi, there seems to be a bug in `tf.lbeta`.

```python
tf.lbeta(tf.placeholder(tf.float32, [3, 2, 3, None]))
# => ValueError: Can not squeeze dim[0], expected a dimension of 1, got 3 for 'lbeta_1/cond/Squeeze' (op: 'Squeeze') with input shapes: [3,2,3,?].
```
"
8390,How can I use BatchNorm in a multi-GPU model,"I want to use batch normalization in the [cifar10 example](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) in a multi-GPU structure. All variables are stored in CPU, I build 2 Queues one for training batch and one for testing batch;  5 models( 4 for training and another for testing),

 GPU_tower codes like this
![image](https://cloud.githubusercontent.com/assets/5405385/23892678/8d4c4e10-08d6-11e7-8d67-50d3bc32aec0.png)

****Problem is**: when I add tf.contrib.batch_norm layers in my NN model . How can I  reuse those batchnorm variables ? I set reuse = True and pass namescope but get valueError. Is there a simple way to make it work? I don't want to modify a lot.**

I know there are some high level framework such as [slim with the inception](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L253) model with a built-in batch-norm 
but I can hardly do personal implementations (I need to add many instructions in slim.conv2d and tf.contrib.conv2d, so it's more convenient to use tf.nn.conv2d), 

**Could I just follow the inception Sync method but without high level frame like slim ?**
Thanks a lot!

"
8389,ArgumentError: argument --train_dir: conflicting option string: --train_dir,"I met this question,when I run ""cifar10_train.py"" in jupyter.

---------------------------------------------------------------------------
ArgumentError                             Traceback (most recent call last)
/root/.jupyter/workspace/littlefish/cifar10/cifar10_train.py in <module>()
     50 
     51 tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',
---> 52                            """"""Directory where to write event logs """"""
     53                            """"""and checkpoint."""""")
     54 tf.app.flags.DEFINE_integer('max_steps', 1000000,

/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/flags.py in DEFINE_string(flag_name, default_value, docstring)
     78     docstring: A helpful message explaining the use of the flag.
     79   """"""
---> 80   _define_helper(flag_name, default_value, docstring, str)
     81 
     82 

/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/flags.py in _define_helper(flag_name, default_value, docstring, flagtype)
     63                               default=default_value,
     64                               help=docstring,
---> 65                               type=flagtype)
     66 
     67 

/root/anaconda3/lib/python3.5/argparse.py in add_argument(self, *args, **kwargs)
   1342                 raise ValueError(""length of metavar tuple does not match nargs"")
   1343 
-> 1344         return self._add_action(action)
   1345 
   1346     def add_argument_group(self, *args, **kwargs):

/root/anaconda3/lib/python3.5/argparse.py in _add_action(self, action)
   1705     def _add_action(self, action):
   1706         if action.option_strings:
-> 1707             self._optionals._add_action(action)
   1708         else:
   1709             self._positionals._add_action(action)

/root/anaconda3/lib/python3.5/argparse.py in _add_action(self, action)
   1546 
   1547     def _add_action(self, action):
-> 1548         action = super(_ArgumentGroup, self)._add_action(action)
   1549         self._group_actions.append(action)
   1550         return action

/root/anaconda3/lib/python3.5/argparse.py in _add_action(self, action)
   1356     def _add_action(self, action):
   1357         # resolve any conflicts
-> 1358         self._check_conflict(action)
   1359 
   1360         # add to actions list

/root/anaconda3/lib/python3.5/argparse.py in _check_conflict(self, action)
   1495         if confl_optionals:
   1496             conflict_handler = self._get_handler()
-> 1497             conflict_handler(action, confl_optionals)
   1498 
   1499     def _handle_conflict_error(self, action, conflicting_actions):

/root/anaconda3/lib/python3.5/argparse.py in _handle_conflict_error(self, action, conflicting_actions)
   1504                                      for option_string, action
   1505                                      in conflicting_actions])
-> 1506         raise ArgumentError(action, message % conflict_string)
   1507 
   1508     def _handle_conflict_resolve(self, action, conflicting_actions):

ArgumentError: argument --train_dir: conflicting option string: --train_dir"
8388,the mnist data downloading statement always fail in tensorflow/examples/tutorials/mnist/mnist_softmax.py,"Hi, 
In the example code of mnist_softmax.py
I always fail on:
mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
The error logs are like this:
>>> mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/clock/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 211, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""/home/clock/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 208, in maybe_download
    temp_file_name, _ = urlretrieve_with_retry(source_url)
  File ""/home/clock/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 165, in wrapped_fn
    return fn(*args, **kwargs)
  File ""/home/clock/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 190, in urlretrieve_with_retry
    return urllib.request.urlretrieve(url, filename)
  File ""/usr/lib/python2.7/urllib.py"", line 98, in urlretrieve
    return opener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib/python2.7/urllib.py"", line 245, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib/python2.7/urllib.py"", line 213, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 350, in open_http
    h.endheaders(data)
  File ""/usr/lib/python2.7/httplib.py"", line 1053, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python2.7/httplib.py"", line 897, in _send_output
    self.send(msg)
  File ""/usr/lib/python2.7/httplib.py"", line 859, in send
    self.connect()
  File ""/usr/lib/python2.7/httplib.py"", line 836, in connect
    self.timeout, self.source_address)
  File ""/usr/lib/python2.7/socket.py"", line 575, in create_connection
    raise err
IOError: [Errno socket error] [Errno 110] Connection timed out


And I found it'll download the dataset from http://yann.lecun.com/exdb/mnist/, but Yann Lecunn's website already disappear these days. Can you choose another more stable website to download the dataset?
Thanks very much in advance."
8387,Tensorboard Summaries in Re-entered Scopes,"Tensorboard creates a new unique scope for summaries every time existing variable scope is re-entered leading to summaries being split to different groups in the Tensorboard. This might be due to summaries using name scopes internally and re-entered variable scopes having unique `original_name_scope`.

```python
def print_scope(scope):
    print('               scope.name: {}'.format(scope.name))
    print('scope.original_name_scope: {}'.format(scope.original_name_scope))

with tf.variable_scope('parent') as parent_scope:
    print_scope(parent_scope)
    with tf.variable_scope('childA') as childA_scope:
        print_scope(childA_scope)
print()
with tf.variable_scope(parent_scope):
    print_scope(parent_scope)
    with tf.variable_scope('childB') as childB_scope:
        print_scope(childB_scope)
```
outputs
```
               scope.name: parent
scope.original_name_scope: parent/
               scope.name: parent/childA
scope.original_name_scope: parent/childA/

               scope.name: parent
scope.original_name_scope: parent/
               scope.name: parent/childB
scope.original_name_scope: parent_1/childB/
```
Child B is created in re-entered parent scope and has prefix for parent in the `original_name_scope`. I believe parent prefix is what confuses name scope in Tensorboard summaries. I think re-entering existing variable scope should not have these unique prefixes for parent scope.

I have lemmatizer wrapped in Python class and Tensorboard summaries are created in different stages of graph build with re-entered variable scopes. Tensorboard splits graphs like so: [Screenshot](http://i.imgur.com/KRDt3wj.png)

Tested on Tensorflow 1.0.1
"
8386,RegisterGradient for TF 1.0.x,"Hello guys,

What is the equivalent of `@ops.RegisterGradient('Mod')` for Tensorflow 1.0.x?

It works for 0.12 but I have this error for 1.0.1:

`LookupError: No gradient defined for operation 'rnn/while/PhasedLSTMCell/FloorMod_1' (op type: FloorMod)`

Source code is:

```
@ops.RegisterGradient('Mod')
def _mod_grad(op, grad):
    x, y = op.inputs
    gz = grad
    x_grad = gz
    y_grad = tf.reduce_mean(-(x // y) * gz, axis=[0], keep_dims=True)[0]
    return x_grad, y_grad
```

Thanks!"
8385,ImportError: No module named '_pywrap_tensorflow'   Failed to load the native TensorFlow runtime.,"Apologies  for the inappropriate description i am actually very new to Machine learning  my

TF version is 1.0.1
Python version is 3.5.2 
OS windows 8 64bit and 
**Native pip**  
as recommended on tensor flow.org
https://www.python.org/downloads/release/python-352/

The problem ocured  at this simple import command  

**import tensorflow as tf**

### What other attempted solutions have you tried?
None

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).




C:\Users\test\Desktop>>python firstp.py
Traceback (most recent call last):
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\test\Desktop\firstp.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 21, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\site-packages\t
ensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow')
  File ""C:\Users\test\AppData\Local\Programs\Python\Python35\lib\importlib\__ini
t__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st
arted/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
8384,_pywrap_tensorflow_internal.so: ELF file OS ABI invalid,"improt tensorflow as tf
Errorinfo:
 imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: $PATH/tensorflow/python/_pywrap_tensorflow_internal.so: ELF file OS ABI invalid

there is _pywrap_tensorflow_internal.so in path. how can i solve this “ELF file OS ABI invalid”
install with pip
tensorflow version：https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl

OS info：Linux NAME #1 SMP TIME x86_64 x86_64 x86_64 GNU/Linux

file _pywrap_tensorflow_internal.so： _pywrap_tensorflow.so: ELF 64-bit LSB shared object, AMD x86-64, version 1 (GNU/Linux), not stripped

### Environment info：
Linux NAME #1 SMP TIME x86_64 x86_64 x86_64 GNU/Linux
Linux version 2.6.32_1-12-0-0 (scmpf@dbl-sat-dev01.dbl01.baidu.com) (gcc version 4.4.4 20100726 (Red Hat 4.4.4-13) (GCC) ) #1 SMP Mon Aug 12 17:59:52 CST 2013
CentOS release 4.3 (Final)
Kernel \r on an \m
LSB Version:    :core-3.0-amd64:core-3.0-ia32:core-3.0-noarch:graphics-3.0-amd64:graphics-3.0-ia32:graphics-3.0-noarch
Distributor ID: CentOS
Description:    CentOS release 4.3 (Final)
Release:        4.3
Codename:       Fina

###The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`：
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: /home/users/caoshiwei/.jumbo/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: ELF file OS ABI invalid


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help
"
8383,Importing graph with control flow using TF_GraphImportGraphDef crashes,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

https://github.com/tensorflow/tensorflow/issues/8284
https://github.com/tensorflow/tensorflow/issues/5406

### Environment info
Operating System:

macOS 10.12.3

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

CPU only

1. The commit hash (`git rev-parse HEAD`) e895d5ca395c2362df4f5c8
2. The output of `bazel version`

```
Build label: 0.4.4-homebrew
Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Feb 2 01:05:15 2017 (1485997515)
Build timestamp: 1485997515
Build timestamp as int: 1485997515
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

```python
import tensorflow as tf
from tensorflow.python.framework.graph_util import convert_variables_to_constants

x = tf.placeholder(tf.float32, shape=(None), name=""x"")

def add_one(x):
  return tf.add(x, 1)

y = tf.map_fn(add_one, x)

y = tf.identity(y, name=""y"")

with tf.Session() as sess:
  print(tf.get_default_graph().as_graph_def().node)
  minimal_graph = convert_variables_to_constants(sess, sess.graph.as_graph_def(add_shapes=True), [""y""])
  tf.train.write_graph(minimal_graph, '.', 'minimal_graph.proto', as_text=False)
```

```go
package main

import (
  tf ""github.com/tensorflow/tensorflow/tensorflow/go""
  ""io/ioutil""
)

func main() {
  modelPath := ""cond_test/minimal_graph.proto""
  graphDef, _ := ioutil.ReadFile(modelPath)
  graph := tf.NewGraph()
  graph.Import(graphDef, """")
}
```

### What other attempted solutions have you tried?

- Loading the graph without the `map_fn` works as expected. (`y = add_one(x)`)
- Originally encountered problem using prebuild tensorflow for python: `pip install tensorflow`
- Same issue when saving / restoring using SavedModel routines.

### Logs or other output that would be helpful

```
fatal error: unexpected signal during runtime execution
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x5de318c]

runtime stack:
runtime.throw(0x40cdc29, 0x2a)
	/usr/local/go/src/runtime/panic.go:596 +0x95
runtime.sigpanic()
	/usr/local/go/src/runtime/signal_unix.go:274 +0x2db

goroutine 1 [syscall, locked to thread]:
runtime.cgocall(0x40973d0, 0xc42004de60, 0x40b34e0)
	/usr/local/go/src/runtime/cgocall.go:131 +0xe2 fp=0xc42004de30 sp=0xc42004ddf0
github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_GraphImportGraphDef(0xbf00020, 0xd0be3f0, 0xd0bee20, 0xd0bd5a0)
	github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:386 +0x45 fp=0xc42004de60 sp=0xc42004de30
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).Import.func7(0xbf00020, 0xd0be3f0, 0xd0bee20, 0xd0bd5a0)
	/Users/olav/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:96 +0x121 fp=0xc42004de98 sp=0xc42004de60
github.com/tensorflow/tensorflow/tensorflow/go.(*Graph).Import(0xc42000e038, 0xc42008a000, 0x1ae9, 0x1ce9, 0x0, 0x0, 0x0, 0x0)
	/Users/olav/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:96 +0x1ef fp=0xc42004df00 sp=0xc42004de98
main.main()
	/Users/olav/go/src/github.com/olavhn/infer/test.go:14 +0x112 fp=0xc42004df88 sp=0xc42004df00
runtime.main()
	/usr/local/go/src/runtime/proc.go:185 +0x20a fp=0xc42004dfe0 sp=0xc42004df88
runtime.goexit()
	/usr/local/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc42004dfe8 sp=0xc42004dfe0

goroutine 17 [syscall, locked to thread]:
runtime.goexit()
	/usr/local/go/src/runtime/asm_amd64.s:2197 +0x1
```
"
8382,How can I modify RNN cell weight during each training epoch?,"I want to binarize the weights of RNN-GRU cell during each training epoch, in order to reduce the model size and increase the performance. Instead of binarizing the weights after freezing the graph, I wonder how can I get access to and modify the weights during each training epoch, or more specifically, before computing gradients and updating weights? It seems no API is provided to let RNN cell weight exposed to users.
Thank you so much.
"
8381,DropoutWrapper in rnn_cell,"DropoutWrapper in rnn_cell do not have a state for training or testing.  Although the keep_prob can be passed as a tensor conditioned on training/testing, would it be possible to add a state argument like in  tf.nn.dropout?"
8380,"Tensorflow works, but cannot import tensorflow.python","Operating System: Ubuntu 16.04
Tensorflow version: tensorflow-gpu 1.0.1 on python3.6

---------------------------------------------------------
jiexun@jiexun-XPS-15-9560:~/Desktop$ python3
Python 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
>>> tf.__version__
'1.0.1'
>>> import tensorflow.python
>>> import tensorflow.python as py
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'python'
>>> 
---

I also tested importing the other tensorflow folders, with the below results:
>>> import tensorflow.contrib as con
>>> import tensorflow.core as core
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'core'
>>> import tensorflow.examples as examples
>>> import tensorflow.include as include
>>> import tensorflow.models as models
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow.models'
>>> import tensorflow.tensorboard as tb
>>> import tensorflow.tools as tools
>>> 

For me, it seems like I can't import tensorflow.core, tensorflow.models, and tensorflow.python. Tensorflow.python in particular is the module that I am trying to use. 

Any idea why this is happening? I may be missing something obvious.

Thanks so much for the help!
"
8379,RuntimeError: Attempted to use a closed Session.,"Hello every one, I am getting a stack error while testing new unique data to the model I have trained. The error says "" RuntimeError: Attempted to use a closed Session."". I am not much expert with tensorflow. Some may help me to figure out why?. Thanks! @alextp @MicaelCarvalho @jfsantos 

print (""Now, Testing the unlabel data and writing the results"")
YPredByNNForUnlabeledData = sess.run(tf.argmax(yPredbyNN,1),feed_dict={X: testing_features})
print (YPredByNNForUnlabeledData)
for i in xrange (len(YPredByNNForUnlabeledData)):
    
    if YPredByNNForUnlabeledData[i] == 0:
        ClassLabelFinal.append('classical')
        
    else:
        ClassLabelFinal.append('blues')

cwd = os.getcwd()
Test_dataset_path = (""/Users/MA/Desktop/BluesTest"")%cwd
Test_dataset, Total_Instances = load_instances(Test_dataset_path)

timestamps = load_timestamps(Test_dataset)

write_results(timestamps, ClassLabelFinal, 'Result.csv')
****************************************************************************
Now, Testing the unlabel data and writing the results
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-9-dd19a8c04012> in <module>()
      1 print (""Now, Testing the unlabel data and writing the results"")
----> 2 YPredByNNForUnlabeledData = sess.run(tf.argmax(yPredbyNN,1),feed_dict={X: testing_features})
      3 print (YPredByNNForUnlabeledData)
      4 for i in xrange (len(YPredByNNForUnlabeledData)):
      5 

C:\Users\MA\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    764     try:
    765       result = self._run(None, fetches, feed_dict, options_ptr,
--> 766                          run_metadata_ptr)
    767       if run_metadata:
    768         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\MA\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    900     # Check session.
    901     if self._closed:
--> 902       raise RuntimeError('Attempted to use a closed Session.')
    903     if self.graph.version == 0:
    904       raise RuntimeError('The Session graph is empty.  Add operations to the '

RuntimeError: Attempted to use a closed Session. 
"
8378,Issue running LabelImage.java demo. ,"Below is the error:

Exception in thread ""main"" java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization().
	at org.tensorflow.Graph.importGraphDef(Native Method)
	at org.tensorflow.Graph.importGraphDef(Graph.java:113)
	at org.tensorflow.Graph.importGraphDef(Graph.java:97)
	at org.tensorflow.examples.LabelImage.executeInceptionGraph(LabelImage.java:110)
	at org.tensorflow.examples.LabelImage.main(LabelImage.java:65)

I can't find the source file in which to use tf.nn.batch_normalization
Are the Java libraries using older versions of Tensorflow?"
8376,tf.exp() cannot handle large negative numbers correctly,This issue is mistakenly filed. Please delete..
8375,Streaming accuracy and recall aren't working as expected,"### Environment info
Ubuntu 16.04
Cuda 8.0
Cudnn 5.1

1. The commit hash (`git rev-parse HEAD`)
c56c873fbaf976d26d487ad57c8efbc87f05331c
2. The output of `bazel version`
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

Here is my code:
```
weights = {'first': tf.Variable(tf.random_normal([1, 3, 1, 10])),
           'iterated': tf.Variable(tf.random_normal([1, 3, 10, 10])),
           'out': tf.Variable(tf.random_normal([embedding_dim*10, n_classes]))}

biases = {'first': tf.Variable(tf.random_normal([10])),
          'iterated': tf.Variable(tf.random_normal([10])),
          'out': tf.Variable(tf.random_normal([n_classes]))}

preds, cost = model(x, y, weights, biases, dropout, depth_tensor)

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

accuracy, update_accuracy = streaming_accuracy(y, preds)
recall, update_recall = streaming_recall(y, preds)

init = tf.global_variables_initializer()
init2 = tf.local_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    i = 1
    
    for batch_x, batch_y in data_processor(data, train_inds, embedding, label_processor, n_iter):
        sess.run(optimizer, 
                 feed_dict={x: batch_x, y: batch_y, 
                            dropout: dropout_prob})
        
        if i % display_step == 0:
            loss = sess.run(cost, 
                            feed_dict={x: batch_x, y: batch_y, dropout: dropout_prob})
            
            print(""Iter:{}, Minibatch Loss:{:.6f}"".format(i,loss))
        i += 1
    
    sess.run(init2)
    for batch_x, batch_y in data_processor(data, val_inds, embedding, label_processor, n_iter):
        recall, accuracy = sess.run([update_recall, update_accuracy], 
                                    feed_dict={x:batch_x, y: batch_y, dropout: 1})
        
        f1 = 2 * recall * accuracy / (recall + accuracy)
    
    print(""Testing Accuracy:"", accuracy,""Testing Recall:"", recall, ""Testing F1 Score:"", f1) 
```

Output:
```
Iter:100, Minibatch Loss:18038.144531
Iter:200, Minibatch Loss:11628.046875
Iter:300, Minibatch Loss:9288.974609
Iter:400, Minibatch Loss:4583.474121
Iter:500, Minibatch Loss:6600.524902
...
Iter:11700, Minibatch Loss:4.203137
Iter:11800, Minibatch Loss:3.623320
Iter:11900, Minibatch Loss:4.883300
Iter:12000, Minibatch Loss:3.045975
Testing Accuracy: 0.0 Testing Recall: 0.00211863 Testing F1 Score: 0.0
```
"
8374,How to visualize local dataset  on TensorBoard ?,"Hello, I would like to know how we can visualize dataset on TensorBoard locally. I have Wine dataset taken from UCI Machine Learning repository and wonder what procedure I should follow to display on TensorBoard. Unfortunately I could not find about this on google. Could you help me with this, please ? "
8371,Problem with missing kernel registration in contrib/makefile build,"## Problem with missing kernel registration in contrib/makefile build

When reading a graph (through saved_model) I get the error message:

Status: Invalid argument: No OpKernel was registered to support Op 'TruncatedNormal' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

when trying to link the application using the tensorflow-core library generated in tensorflow/contrib/makefile

When using bazel, the same application runs without errors.

I have verified that the file ""tensorflow/core/ops/random_ops.cc"" containing the operation is already present in ""tf_op_files.txt"".

The library is linked with -all_load option.

### Environment info
The tensorflow version used is branch r1.0 on macOS 10.12.3 (no CUDA support). Hash e895d5ca395c2362df4f5c8f08b68501b41f8a98
Bazel Version is 0.4.4"
8369,Add argument axis in tf.TensorArray method concat()?,"Hi, TF development team,

Would it possible to add an argument axis in tf.TensorArray method concat(), defined in file tensorflow/tensorflow/python/ops/tensor_array_ops.py?

The reason is that there could be use cases when concat on the last axis make sense. Of course, the requirement would also need to be changed to 

""""""
All of the values must have been written, their ranks must match, and
    and their shapes must all match for all dimensions except the **dimension specified as axis**.
""""""

BTW, or, would tf.concat() accept tf.TensorArray as input? - I tried, but didn't find a way.

Thanks.
"
8368,Reuse some parameters of a Variable,"I know how to reuse complete variables Tensorflow in two different operations as explained in the documentation: [https://www.tensorflow.org/programmers_guide/variable_scope]

But, is it possible to reuse parameters in more complex ways? In particular, is it possible to specify that two variables of different shapes share some parameters in common? 

A small example of what I am trying to do: I would like to have a model with 25 parameters. I would like to use these parameters in three variables X, Y, and Z so that X uses all parameters in a 5x5 tensor:
X = 
w11, w12, ... , w15
w21, w22, ... , w25
...
w51, w52, ... , w55

while Y uses 9 of the parameters in a 3x3 tensor, for example the middle 3x3 block of X. That is:
Y = 
w22, w23, w24
w32, w33, w34
w42, w43, w44

while Z uses the same nine weights as Y and also in a 3x3 tensor, but transposed with respect to Y, that is:
Z =
w22, w32, w42
w23, w33, w43
w24, w34, w44

If this is not possible, are there plans in the Tensorflow development community to support this capability?
"
8367, the files are available but inspection result No event files found within logdir,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#my-tensorboard-isnt-showing-any-data-whats-wrong

### Environment info
Operating System: mac
Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?
inspecting directory

### Logs or other output that would be helpful


(mlp) earthhouse86:cifar10 ikibozu$ tensorboard --inspect --logdir==$OUTPUT_DIR 0
======================================================================
Processing event files... (this can take a few minutes)
======================================================================

No event files found within logdir =/Users/ikibozu/mlpractical/notebooks/cw4/result
(mlp) earthhouse86:cifar10 ikibozu$ find $OUTPUT_DIR | grep tfevent
/Users/ikibozu/mlpractical/notebooks/cw4/result/C10_10_tf.nn.relu_2017-03-13_14-47-18/train-summaries/events.out.tfevents.1489416439.earthhouse86.hw147.homewurk.nl
/Users/ikibozu/mlpractical/notebooks/cw4/result/C10_10_tf.nn.relu_2017-03-13_14-47-18/valid-summaries/events.out.tfevents.1489416669.earthhouse86.hw147.homewurk.nl
"
8364,Documentation formatting broken,"See https://www.tensorflow.org/api_docs/python/tf/contrib/copy_graph/copy_op_to_graph (source code formatting leaks into general text)
or https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor/copy (Returns: and Raises: get folded into parameters).

Either the doc generator needs to understand python doc comments better or the doc comments need to be updated to work better with markdown (extra newlines etc). What do you think?

Thanks,
 Andreas
"
8362,Memory Allocation at each call to Frozen Graph?,"Operating System: Linux, CPU
TF: 0.11.0

It looks like a memory allocation is occurring during each call to my frozen graph.
I suspect I'm using the code incorrectly rather than this is a bug.
Please delete if this does not belong here. 
I posted on [StackOverflow](http://stackoverflow.com/questions/42769464/bad-memory-allocation-when-using-frozen-graph).

-------

My model was running too slow doing inference.
I froze the graph and converted variables to constants to improve the speed. 

Normally I load the graph and weights (which runs OK):
```
with tf.Graph().as_default(), tf.Session() as session:
	...
	saver.restore(session, ckpt_path)
	for i in range(5):
	    result = session.run(...)
```

However, when I load the frozen graph:	
```
# Save the graph

# output_node_names - equivalent to the results that are requested in the session.run() calls
output_node_names = [...] 
saver = tf.train.import_meta_graph(ckpt + '.meta')
graph = tf.get_default_graph()
input_graph_def = graph.as_graph_def()

with tf.Session() as sess:
    saver.restore(sess, ckpt)
    output_graph_def = graph_util.convert_variables_to_constants(
        sess, input_graph_def, output_node_names) 

     with tf.gfile.GFile(output_graph, ""wb"") as f:
         f.write(output_graph_def.SerializeToString())


# Load the graph

def load_graph(filename):
	with tf.gfile.GFile(filename, ""rb"") as f:
	       graph_def = tf.GraphDef()
	       graph_def.ParseFromString(f.read())

	 with tf.Graph().as_default() as graph:
	        tf.import_graph_def(graph_def, input_map=None, 
	        	return_elements=None, name=""prefix"", op_dict=None, 
                        producer_op_list=None)
	return graph 

graph = load_graph(""model.pb"")
session = tf.Session(graph=graph)
	
for i in range(5):
	result = session.run(...)
```

After loading the full graph, inference takes ~1 second.
After loading the frozen graph, inference takes ~4 seconds.
So it seems like something may not be loaded entirely in my frozen graph?
The first call to session.run() successfully produces a result (although very slow), but **the second call to run() throws an error**.

**`W tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[300,100000]`**

Since the graph internally has an embedding table (this is not what is being returned by session.run()), it seems this is what is being re-allocated:
	```embedding = tf.get_variable(""embedding"", [300, 100000])```

Also the error message printed 3 times, hopefully that means 3 allocation attempts, and not 3 instances being allocated? 
But why is this allocation on every call to session.run()?


Edit:
@MicaelCarvalho 
Here is the format of my input for Session.run():
```
x = [0.0]
x_placeholder = graph.get_tensor_by_name('prefix/x_placeholder:0')
y = graph.get_tensor_by_name('prefix/y:0')
feed_dict = {x_placeholder: x}
to_return = [y]
result = sess.run(to_return, feed_dict=feed_dict)
```

"
8361,Add parameter to monitors to allow execution every_n_epochs,"Many of the Monitors used for monitoring the training with an Estimator allow to specify the number of training steps after which they will be executed. Additionally it would be nice to be able to specify the number of epochs after which the monitor should be executed. This would make it very easy to validate the training after all training data has been learned for example one (or two...) more times. 

From looking at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/monitors.py, I saw that all the monitors allowing to specify the `every_n_steps` parameter are extending the EveryN class. In order to also allow a parameter like `every_n_epochs`, this class could be extended to count the number of epochs by listening to the `def epoch_begin(self, epoch)` method."
8360,bug in cudnn  for cnn?,"tensorflow 1.0
cuda 8.0
cudnn 5.0

install method:
pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl 

when  I run the  models/tutorials/image/mnist/convolutional.py
I got:
E tensorflow/stream_executor/cuda/cuda_dnn.cc:390]Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5110 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to matchIf building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.
F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 
Aborted

I thought it may be the cudnn, I load cudnn5110 and export it to LD_LIBURARY_PATH
but I  got another problem:
E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  361.62  Tue May 24 20:21:31 PDT 2016
GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_dnn.cc:408] running driver version: 361.62.0
E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) 
Aborted (core dumped)

How can i fix this problem?

it's the driver's problem ? "
8359,tf.layers.conv3d_transpose missing,"In tf.layers, there are conv2d and conv2d_transpose. There is conv3d, but no conv3d_transpose. 

I'm wondering if it is possible to have conv3d_transpose any time soon. 

Thanks,"
8355,tensorflow's textsum model has version compatibile issue,"Hi there,

I installed tensorflow and tried its textsum model. Unfortunately, I hit lots of module has not attribute issues. I used release version it does not compatible with textsum model in github master branch. I build tensorflow from source from master branch. But, there are still lots of version compatible issues. 

Anyone here tried this model before? I need to find a release of tesnsorflow and textsum model and they are compatible. 

Also, does this model work with tensorflow CPU (not GPU) edition?

thanks in advance!

Yiyu "
8354,Test8 file missing in word2vec_basic file,Test File (test8.zip) is missing the location http://mattmahoney.net/dc/
8353,Error while running TextClassification in examples,"Getting error
module 'tensorflow.contrib.layers' has no attribute 'bow_encoder'
if running bag of words model"
8352,GPU memory problem when using tensorflow 1.0.0 on TITAN X (Pascal) ,"### Environment info
Operating System: Fedora 25
cuda 8.0 and cuDNN 5.1
TITAN X (Pascal) and Driver Version: 375.26

I installed tensorflow 1.0.0 using pip install under anaconda
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl

I have a problem when using tensorflow on TITAN X (Pascal) . I am able to ran my code on other type of GPUs. I also tried running theano and matconvnet on the same GPU but there is no problem.

I tried several ways to check it.
1. if I directly run sess = tf.Session(), it gets stuck there.

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 10.97GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
Killed

2. I also tried to set allow_growth=True and it is able to run tf.Session.
gpu_options= tf.GPUOptions(allow_growth=True)
tf_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True, gpu_options=gpu_options)
sess = tf.Session(config=tf_config)

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 10.97GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0
I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0

I check the GPU memory being used, it is 151MB.

3. I also tried to allocate different amount of GPU memory in the following way and it crashed.
tf_config = tf.ConfigProto()
tf_config.gpu_options.per_process_gpu_memory_fraction = 0.1
sess = tf.Session(config=tf_config)

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 10.97GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
Aborted (core dumped)

Can anyone help me on this? Thanks a lot."
8351,how to avoid generating large metagraph?,"
### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

x_nparray = $VERY_LARGE_NP_ARRAY(bigger than 2G)

x = tf.Variable(init_value=x_nparray)

saver=tf.Saver()
with tf.Session() as sess:
  saver.save(sess, 'path/to/save')

that would generate very large metagraph binary file, when i try to restore the model, it would failed because of the meta graph file size exceed  the protobuf limit size.


I cannot reduce the size of x_nparray, because it's generated by other training system like caffe. 

### What other attempted solutions have you tried?
set bigger limit byte of protobuf, however, the x_nparray is bigger than 3G, while the largest limit i can set is 2G. 
"
8350,Not able to run tensorflow with OpenBLAS support,"Hi,

Im trying to use OpenBLAS for gemm operations instead of EIGEN with tensorflow.
I have followed the steps given in 
http://eigen.tuxfamily.org/dox-devel/TopicUsingBlasLapack.html

and compiled the tensoflow android demo application with -DEIGEN_USE_BLAS support and have linked the libopenblas.a to EIGEN by placing the OpenBLAS headers and static library in eigen_archive along with BUILD file.
****The BUILD File of OpenBLAS is as follows**:**

licenses([""notice""])

 cc_library(
      name = ""openblas"",
      hdrs = glob([""include/*.h""]),
      srcs = [""lib/libopenblas.a""],
      visibility = [""//visibility:public""],
  )

The BUILD file of EIGEN has been appended to include and link the openblas library as follows:
 cc_library(
     name = ""eigen"",
     hdrs = EIGEN_MPL2_HEADER_FILES,
     defines = [
         # This define (mostly) guarantees we don't link any problematic
         # code. We use it, but we do not rely on it, as evidenced above.
         ""EIGEN_MPL2_ONLY"",
     ],
     includes = ["".""],
     copts = [""-IOpenBLAS/include""],
     deps = [""//OpenBLAS:openblas""],
     visibility = [""//visibility:public""],
 )

The build is given as follows:
bazel build --copt=-DEIGEN_USE_BLAS --fat_apk_cpu=arm64-v8a //tensorflow/examples/android:tensorflow_demo --verbose_failures

The Build is completed successfully and im able to run the apks installed on the target. But the print statements which i have put in OpenBLAS and Eigen/src/Core/products/GeneralMatrixMatrix_BLAS.h using android logging LOGI or LOGD doesnt get displayed in logcat of the device.

The Build fails if i dont link the openblas library in EIGEN BUILD file and use  -DEIGEN_USE_BLAS, which shows that libopenblas is getting linked to EIGEN.

Can you please suggest what is the problem in the above procedure ?
How can we validate OpenBLAS is actually getting used?"
8349,/tutorials/using_gpu doc (possibly other) should use print() since Windows needs Python 3.x,"Should the Python samples in /tutorials/using_gpu and other docs be in 3.5.x form?

* In /install/install_windows we're told ""TensorFlow only supports version 3.5.x of Python on Windows""
* In /tutorials/using_gpu there is sample Python code in 2.x style, ""print sess.run(c)""
* This code snippet chokes when run on Windows, while print(sess.run(c)) is fine. Trivial to fix but distracts from the task at hand.

There may be more nuance to this (afaik print with parens is ok in 2.7), but it seems like avoidable friction if not.

"
8348,Taking gradients after using SparseTensor in while_loop leads to TypeError,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

Found various issues on SparseTensors, but nothing about while loops and gradients.


### Environment info
Operating System:
Ubuntu 14.04

Installed version of CUDA and cuDNN: 
None

If installed from binary pip package, provide:

1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl

2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: 
0.12.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

This code leads to the error (when trying to compute the gradients) `TypeError: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fb966845f98>`
```
import tensorflow as tf


def body(A, b, x, i):
    i += 1
    b = b + tf.sparse_tensor_dense_matmul(A, x)
    return A, b, x, i


def cond(A, b, x, i):
    return i < 5


sess = tf.InteractiveSession()
indices = [[0, 0], [1, 1]]
values = [1., 1.]
A = tf.SparseTensor(indices, values, (100, 100))
x = tf.ones((100, 1))
b = tf.zeros_like(x)
[_, b, _, _] = tf.while_loop(cond, body, [A, b, x, tf.constant(0)])
grad = tf.gradients(b, x)
print(sess.run([grad]))
sess.close()
```

### What other attempted solutions have you tried?

If `A` is removed as a loop variable everything works as expected:

```
import tensorflow as tf


def body(b, x, i):
    i += 1
    b = b + tf.sparse_tensor_dense_matmul(A, x)
    return b, x, i


def cond(b, x, i):
    return i < 5


sess = tf.InteractiveSession()
indices = [[0, 0], [1, 1]]
values = [1., 1.]
A = tf.SparseTensor(indices, values, (100, 100))
x = tf.ones((100, 1))
b = tf.zeros_like(x)
[b, _, _] = tf.while_loop(cond, body, [b, x, tf.constant(0)])
grad = tf.gradients(b, x)
print(sess.run([grad]))
sess.close()
```


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
Traceback (most recent call last):
  File ""sparse_tensor_gradient_error.py"", line 21, in <module>
    grad = tf.gradients(b, x)
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 427, in gradients
    _SetGrad(grads, y, loop_state.ZerosLikeForExit(y))
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1165, in ZerosLikeForExit
    result = array_ops.zeros_like(val, optimize=False)
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 1471, in zeros_like
    tensor = ops.convert_to_tensor(tensor, name=""tensor"")
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 441, in make_tensor_proto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 441, in <listcomp>
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/home/y/anaconda3/lib/python3.5/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fb966845f98>
```
"
8347,Quantize Neural Networks with TensorFlow doesn't work on my models,"I'm following this [tutorial here](https://www.tensorflow.org/performance/quantization).

And this code works fine.
```
curl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz
tar xzf /tmp/inceptionv3.tgz -C /tmp/
bazel build tensorflow/tools/quantization/tools:quantize_graph
bazel-bin/tensorflow/tools/quantization/tools/quantize_graph \
  --input=/tmp/classify_image_graph_def.pb \
  --output_node_names=""softmax"" --output=/tmp/quantized_graph.pb \
  --mode=eightbit
```

But when I try with another .pb it does not work, it tells me that the output node does not exist.

**With non frozen .pb**
```
Traceback (most recent call last):
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py"", line 1304, in <module>
    app.run()
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py"", line 1271, in main
    tf_graph.ParseFromString(data)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py"", line 1091, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py"", line 1117, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py"", line 820, in _RaiseInvalidWireType
    raise _DecodeError('Tag had invalid wire type.')
google.protobuf.message.DecodeError: Tag had invalid wire type.

```

**With frozen .pb**

```
Traceback (most recent call last):
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py"", line 1304, in <module>
    app.run()
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py"", line 1295, in main
    output_graph = rewriter.rewrite(FLAGS.output_node_names.split("",""))
  File ""/sandbox/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py"", line 400, in rewrite
    for output_node_name in output_node_names
KeyError: 'my_output_node'
```

I'm sure the output_node_names is correct.

Is there any requirements on the .pb file so the tool can quantize it ?

nb: one should edit the doc because the tool is no longer in `tensorflow/tools/quantization/tools:quantize_graph` but `tensorflow/tools/quantization:quantize_graph`

"
8346,I can't compile tensorflow on Ubuntu 16.10 ,"I have given these commands to compile tensorflow:
`sudo apt-get install python-numpy python-dev python-pip python-wheel`

`sudo apt-get install python3-numpy python3-dev python3-pip python3-wheel`

`git clone https://github.com/tensorflow/tensorflow `

`cd tensorflow/`

`git checkout r1.0`

`./configure`

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

It gives this error:

> ERROR: /home/raspberry/tensorflow/tensorflow/core/kernels/BUILD:2468:1: C++ compilation of rule '//tensorflow/core/kernels:bias_op' failed: gcc failed: error executing command 
  (cd /home/raspberry/.cache/bazel/_bazel_raspberry/e683ea6480b94d5dcdb0f13cb220eff8/execroot/tensorflow && \
  exec env - \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/bias_op/tensorflow/core/kernels/bias_op.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/bias_op/tensorflow/core/kernels/bias_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-py3-opt/genfiles/external/local_config_sycl -iquote external/jemalloc -iquote bazel-out/local-py3-opt/genfiles/external/jemalloc -iquote external/protobuf -iquote bazel-out/local-py3-opt/genfiles/external/protobuf -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/local-py3-opt/genfiles/external/jemalloc/include -isystem external/protobuf/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf/src -isystem external/gif_archive/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/src -isystem external/highwayhash -isystem bazel-out/local-py3-opt/genfiles/external/highwayhash -isystem external/png_archive -isystem bazel-out/local-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/kernels/bias_op.cc -o bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/bias_op/tensorflow/core/kernels/bias_op.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 15812.554s, Critical Path: 15700.39s

Version of bazel:

Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

"
8345,How to Add Adam optimizer metrics to Tensorboard?,"When I trained model for several epochs and want to retrain it again for more epochs. How would Adam optimizer work. will it initialize the time from t =0 or will it save the last time step?  

a) The documentation in tensorflow shows the following calculations. Is there a away I can add these metrics to tensorboard. 

    t <- t + 1
    lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)

    m_t <- beta1 * m_{t-1} + (1 - beta1) * g
    v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g
    variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)

I know this question need to be asked in stackoverflow . but there are no answers for a few questions since a long time [question1](http://stackoverflow.com/questions/36990476/getting-the-current-learning-rate-from-a-tf-train-adamoptimizer) and [question2](http://stackoverflow.com/questions/40752053/how-to-add-learning-rate-to-summaries).

I am actually getting a problem with error rate when re-training the model from the last checkpoint and I was not sure what exactly is happening with Adam optimizer in this case ?"
8344,Doesn't inefficient and unfriendly for Distributed tensorflow for our model training? ,"If we want deployed tensorflow on our cluster, it is really inefficient in my opinion. As the official tutorial shows, how many task you have launched, then how many times you should run you program file on those nodes. 

As our developers hope, tensorflow will be our Hadoop in Deep Learning. Hadoop to launch an job would be more convenient just execute once your job command. 

Maybe I doesn't use this framework correctly, if you have any good ideas for this, we can discussed an nice solution and make our world beautiful.
"
8343,The inputs of dynamic_rnn must be 3-D tensor?,"hi,
    I want to use _dynamic_rnn_ to train my convLSTM, the original data should be videos with dimension: [batch_size, max_time_step, high, width,channel]. But i failed to feed the data to dynamic_rnn.
I get such error:
`ValueError: Dimension must be 5 but is 3 for 'transpose' (op: 'Transpose') with input shapes: [16,?,11,40,1], [3].`
what should i do to use dynamic rnn?
version: tf 1.0"
8342,[TensorBoard] load data from relative path with the projector plugin,"This is a  problem similar with issue #7382 but it has been closed.

@dandelionmane 
Another place that still has the similar problem in #7382 is the request of project plugin data in Embeddings panel, e.g.  /data/plugin/projector/runs still requests in a absolute path.
I found the code  [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/tf_tensorboard/tf-tensorboard.html#L134) , there is a leading slash for the `route-prefix` property in `vz-projector-dashboard`.
If that is correct, I will submit a PR. Thanks!

Another question is when will the tensorboard be recompiled. As now in version 1.0 these features are not included.
"
8341,when i use tensorflow v1.01  python3 there are SyntaxError: invalid syntax,"when i use tensorflow-gpu v1.01  python3.5 there are SyntaxError: invalid syntax
but i only install tensorflow-gpu v1.01 python2.7 there are no  SyntaxError: invalid syntax
like i use tensorboard when i have installed tensorflow-gpu v1.01 python3.5

 #cy@cy:~/PycharmProjects/models-master/slim$ tensorboard --logdir=./
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorflow.tensorboard.tensorboard import main
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/tensorboard.py"", line 34, in <module>
    from tensorflow.tensorboard.backend import server
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/backend/server.py"", line 38, in <module>
    from tensorflow.tensorboard.plugins.projector import plugin as projector_plugin
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/plugins/projector/plugin.py"", line 27, in <module>
    from tensorflow.contrib.tensorboard.plugins.projector import PROJECTOR_FILENAME
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 29, in <module>
    from tensorflow.contrib import factorization
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/factorization/__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 32, in <module>
    from tensorflow.contrib.learn.python.learn import graph_actions
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/__init__.py"", line 70, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 310, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 33, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 51, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""/home/cy/.local/lib/python3.5/site-packages/dask/dataframe/__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""/home/cy/.local/lib/python3.5/site-packages/dask/dataframe/core.py"", line 11, in <module>
    import pandas as pd
  File ""/home/cy/.local/lib/python3.5/site-packages/pandas/__init__.py"", line 22, in <module>
    from pandas.compat.numpy import *
  File ""/home/cy/.local/lib/python3.5/site-packages/pandas/compat/__init__.py"", line 357, in <module>
    from dateutil import parser as _date_parser
  File ""/home/cy/.local/lib/python3.5/site-packages/dateutil/parser.py"", line 158
    l.append(""%s=%s"" % (attr, `value`))
                              ^
SyntaxError: invalid syntax


#cy@cy:~/PycharmProjects/models-master/slim$ python3 train_image_classifier.py 
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
Traceback (most recent call last):
  File ""train_image_classifier.py"", line 24, in <module>
    from datasets import dataset_factory
  File ""/home/cy/PycharmProjects/models-master/slim/datasets/dataset_factory.py"", line 21, in <module>
    from datasets import cifar10
  File ""/home/cy/PycharmProjects/models-master/slim/datasets/cifar10.py"", line 30, in <module>
    slim = tf.contrib.slim
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 35, in __getattr__
    contrib = importlib.import_module('tensorflow.contrib')
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 29, in <module>
    from tensorflow.contrib import factorization
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/factorization/__init__.py"", line 24, in <module>
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 32, in <module>
    from tensorflow.contrib.learn.python.learn import graph_actions
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/__init__.py"", line 70, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/__init__.py"", line 23, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 25, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 310, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 29, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 33, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 51, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 26, in <module>
    import dask.dataframe as dd
  File ""/home/cy/.local/lib/python3.5/site-packages/dask/dataframe/__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""/home/cy/.local/lib/python3.5/site-packages/dask/dataframe/core.py"", line 11, in <module>
    import pandas as pd
  File ""/home/cy/.local/lib/python3.5/site-packages/pandas/__init__.py"", line 22, in <module>
    from pandas.compat.numpy import *
  File ""/home/cy/.local/lib/python3.5/site-packages/pandas/compat/__init__.py"", line 357, in <module>
    from dateutil import parser as _date_parser
  File ""/home/cy/.local/lib/python3.5/site-packages/dateutil/parser.py"", line 158
    l.append(""%s=%s"" % (attr, `value`))
                              ^
SyntaxError: invalid syntax

"
8340,How to adjust verbosity to suppress thousands of lines of informational logs?,"Per [#1258](https://github.com/tensorflow/tensorflow/issues/1258), here's my current import:

```python
import os
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.constraints import maxnorm
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier
```

However, as you can see if you look at a recent [Travis build](https://travis-ci.org/ClimbsRocks/auto_ml/jobs/210446239), there are still tens of thousands of lines of informational logs, like so:

```
Level 1:tensorflow:Registering Pack (<function _PackGrad at 0x7fc9c4ab3b70>) in gradient.
Level 1:tensorflow:Registering Unpack (<function _UnpackGrad at 0x7fc9c490d730>) in gradient.
Level 1:tensorflow:Registering Concat (<function _ConcatGrad at 0x7fc9c490d840>) in gradient.
Level 1:tensorflow:Registering ConcatV2 (<function _ConcatGradV2 at 0x7fc9c490d8c8>) in gradient.
Level 1:tensorflow:Registering ConcatOffset (None) in gradient.
Level 1:tensorflow:Registering Slice (<function _SliceGrad at 0x7fc9c490d9d8>) in gradient.

...

Level 1:tensorflow:  in  --> gradients_49/Relu_86_grad/ReluGrad:0
Level 1:tensorflow:  out --> gradients_49/add_896_grad/Reshape:0, gradients_49/add_896_grad/Reshape_1:0
Level 1:tensorflow:Gradient for 'MatMul_135'
Level 1:tensorflow:  in  --> gradients_49/add_896_grad/Reshape:0
Level 1:tensorflow:  out --> gradients_49/MatMul_135_grad/MatMul:0, gradients_49/MatMul_135_grad/MatMul_1:0
Level 1:tensorflow:Gradient for 'Relu_85'
Level 1:tensorflow:  in  --> gradients_49/MatMul_135_grad/MatMul:0
Level 1:tensorflow:  out --> gradients_49/Relu_85_grad/ReluGrad:0
Level 1:tensorflow:Gradient for 'add_895'
```

The [docs](https://www.tensorflow.org/api_docs/python/tf/logging/vlog) are not obvious for how to adjust this. 

***Using Python and Keras, how can I adjust verbosity to ignore these logs?***

### Environment info
Operating System:
Linux and Mac

Using v1.0.1 on both platforms, installed directly from binary URLs"
8338,Segmentation fault after calling tf.contrib.distributions `sample().eval()` several times,"I ran the following code and after calling `sample().eval()` several times (like 15 times), the program will crash.
```python
import tensorflow as tf
mu = [1, 2, 3.]
diag_stdev = [4, 5, 6.]
dist = tf.contrib.distributions.MultivariateNormalDiag(mu, diag_stdev)
sess = tf.InteractiveSession()
dist.sample().eval()
dist.sample().eval()
...
```
I don't know if this is a bug in memory management, just reporting it. I tried to run this short code several times, and the program all crashed after calling `sample()` several times.

I am using TensorFlow 1.0.1, ubuntu 14.04, cuDNN 5.1. CUDA 8.0.
I am running the code in ipython
```python
$ ipython
Python 2.7.6 (default, Oct 26 2016, 20:30:19) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally

In [2]: tf.__version__
Out[2]: '1.0.1'

In [3]: import tensorflow as tf
   ...: mu = [1, 2, 3.]
   ...: diag_stdev = [4, 5, 6.]
   ...: dist = tf.contrib.distributions.MultivariateNormalDiag(mu, diag_stdev)
   ...: sess = tf.InteractiveSession()
   ...: dist.sample().eval()
   ...: 
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.18GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)
Out[3]: array([-3.5554738 , -9.19613838,  0.99159908], dtype=float32)

In [4]: dist.sample().eval()
Out[4]: array([  3.23878694,  10.00256252,  -1.83450556], dtype=float32)

In [5]: dist.sample().eval()
Out[5]: array([-3.07549763,  2.88274646,  1.73206449], dtype=float32)

In [6]: dist.sample().eval()
Out[6]: array([-6.32968855,  5.16116142,  3.4088428 ], dtype=float32)

In [7]: dist.sample().eval()
Out[7]: array([-10.45146465,   8.00740719,  12.19320011], dtype=float32)

In [8]: dist.sample().eval()
Out[8]: array([ 2.50515604, -0.90315008,  8.30728722], dtype=float32)

In [9]: dist.sample().eval()
Out[9]: array([ 1.11648369,  2.883286  ,  5.3753109 ], dtype=float32)

In [10]: dist.sample().eval()
Out[10]: array([ 1.42068732, -1.55020142,  7.90944004], dtype=float32)

In [11]: dist.sample().eval()
Out[11]: array([-2.47698998,  5.00640774,  9.55048275], dtype=float32)

In [12]: dist.sample().eval()
Out[12]: array([-2.72261739,  8.12374115,  6.6374836 ], dtype=float32)

In [13]: dist.sample().eval()
Out[13]: array([-5.90653419,  2.76514864,  1.68261075], dtype=float32)

In [14]: dist.sample().eval()
Segmentation fault (core dumped)
```"
8337,tfdbg error: Causality violated in timing relations of debug dumps,"
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Nothing found
### Environment info
Operating System:
Ubuntu 14.04
Installed version of CUDA and cuDNN: 
CUDA 8.0, cuDNN 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
-rw-r--r-- 1 root root   558720  9月 15 07:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16  9月 15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19  9月 15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432  9月 15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162  9月 15 07:02 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 11月 16 22:50 /usr/local/cuda/lib64/libcudnn_static.a
```
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
12a98726e769e988f6368a029ec2f5b0ac3ccbd4
2. The output of `bazel version`
```
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I have a model that basically consists of several bidirectional rnns, the longest of which has hundreds of timesteps.
### What other attempted solutions have you tried?

### Logs or other output that would be helpful
When I try to launch the first Session.run() call in tfdbg using command r, I got the following error:
```
2017-03-13 10:54:06.173974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:81:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
2017-03-13 10:54:06.174082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0
2017-03-13 10:54:06.174106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y
2017-03-13 10:54:06.174138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:81:00.0)
Created model with fresh parameters.
Traceback (most recent call last):
  File ""train.py"", line 143, in <module>
    trainer.train()
  File ""train.py"", line 76, in train
    step_loss, summary = self.m.train_step(self.sess, batch_docs, batch_queries, batch_answers, batch_target)
  File ""/home/zhangx/nn_tool_x33/MR_base/model.py"", line 186, in train_step
    feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
  File ""/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 470, in run
    run_end_resp = self.on_run_end(run_end_req)
  File ""/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 267, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File ""/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 502, in __init__
    self._load_partition_graphs(partition_graphs, validate)
  File ""/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 760, in _load_partition_graphs
    self._validate_dump_with_graphs()
  File ""/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py"", line 927, in _validate_dump_with_graphs
    (node, datum.timestamp, repr(pending_inputs[node])))
ValueError: Causality violated in timing relations of debug dumps: optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/b_acc_2 (1489373660302841): these input(s) are not satisfied: [('optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/b_acc_1', 0), ('optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/NextIteration', 0)]
```
Is this a model related error or possibly a tfdbg bug?"
8336,Operations missing for TF on windows,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
`Cuda compilation tools, release 8.0, V8.0.60`

If installed from binary pip package, provide:
`pip install --upgrade tensorflow-gpu`

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
These errors happened every time I tried to import tensorflow package in py35, when all the dependent dlls are loaded **correctly** as follows:
`I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally`

### What other attempted solutions have you tried?
1. reinstalled tensorflow and related cuda packages
2. switch graphic cards

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions`
`E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots`
"
8334,An error occurred while starting the kernel on Windows,"I get this error when start the execution of some optical recognition routines
Tensorflow: 1.0.1 
OS: Windows 10 x64
Nvidia cuDNN: 5.1
Nvidia CUDA: 8.0

```
An error occurred while starting the kernel
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\stream_executor\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 670
major: 3 minor: 0 memoryClockRate (GHz) 0.95
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.36GiB
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:906] DMA: 0 
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:916] 0: Y 
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) ‑> (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0)
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) ‑> (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0)
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) ‑> (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0)
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) ‑> (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0)
I c:\tf_jenkins\home\workspace\release‑win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) ‑> (device: 0, name: GeForce GTX 670, pci bus id: 0000:01:00.0)

...
```"
8332,Using control flow loops in @function.Defun fails with InvalidArgumentError,"Quoting from [this Stack Overflow question](http://stackoverflow.com/q/42752060/3574081). The following code snippet:

```python
def add_func(x):
    return x+1

@function.Defun(tf.float32)
def test(a):
    return tf.map_fn(add, a)

with tf.Session() as sess:
    a = tf.ones(shape=(6,1))
    res = sess.run(test(a))
```

...generates the following error:

```
InvalidArgumentError: 25 nodes in a cycle
     [[Node: test_8028ca0d_2 = test_8028ca0d[_device=""/job:localhost/replica:0/task:0/cpu:0""](ones_4)]]

Caused by op 'test_8028ca0d_2', defined at:
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 223, in <module>
    main()
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\spyder\utils\ipython\start_kernel.py"", line 219, in main
    kernel.start()
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 474, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 887, in start
    handler_func(fd_obj, events)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 275, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 276, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 228, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 390, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 501, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2827, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-19-c5e48d04d428>"", line 1, in <module>
    runfile('C:/Users/Nicki/.spyder-py3/temp.py', wdir='C:/Users/Nicki/.spyder-py3')
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)
  File ""C:/Users/Nicki/.spyder-py3/temp.py"", line 156, in <module>
    res = sess.run(test(a))
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 618, in __call__
    return _call(self._definition.signature, *args, **kwargs)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tensorflow\python\framework\function.py"", line 271, in _call
    compute_shapes=False)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Nicki\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): 25 nodes in a cycle
     [[Node: test_8028ca0d_2 = test_8028ca0d[_device=""/job:localhost/replica:0/task:0/cpu:0""](ones_4)]]
```

It looks like the error is coming from [this line in `graph_constructor.cc`](https://github.com/tensorflow/tensorflow/blob/12a98726e769e988f6368a029ec2f5b0ac3ccbd4/tensorflow/core/graph/graph_constructor.cc#L719), but I'm not sure what path a FunctionDef takes to get here.

@skye: Preliminarily assigning this to you because you've been making functions work a lot better, but please feel free to nominate someone else (or I can help dig into it more)."
8330,Uncatchable exception messages when using slice_input_producer and batch,"Hello,

`Out of range` messages are being thrown by the code below, and they seem to be uncatchable — I even tried enclosing the whole code with `try:`, `except: pass`, but the messages are still printed. While they don't really seem to affect the rest of the code, since we are taking the exact amount of examples available, these messages are quite annoying because with a big pipeline they can get really messy and ruin real-time visualization of logs (the number of errors has a relation with the number of threads).

If I try to evaluate an extra epoch, then TF raises an exception I can catch, because this time I tried to evaluate an example I don't have, but this isn't the case here.

**Operating System:** Debian 4.8.15-2
**Installed version of CUDA and cuDNN:** CUDA 8, cuDNN 5
**python3 -c ""import tensorflow; print(tensorflow.__version__)""**: 1.0.0

### Reproducible example

```
import tensorflow as tf
import time

num_epochs = 6

a = ([tf.constant(i) for i in range(2)],[tf.constant(i) for i in range(2)])

q1 = tf.train.slice_input_producer(a, num_epochs=num_epochs, shuffle=True, capacity=4)
q2 = tf.train.batch(q1, batch_size=2, num_threads=2, enqueue_many=False, capacity=4, allow_smaller_final_batch=True)

init = [tf.global_variables_initializer(), tf.local_variables_initializer()]
sess = tf.Session()
coord = tf.train.Coordinator()
sess.run(init)
threads = tf.train.start_queue_runners(coord=coord, sess=sess)

test_number = 1
for i in range(num_epochs):
	print('Testing %d' % test_number)
	ignore = sess.run(q2)
	test_number = test_number + 1
	time.sleep(3)
print('Done.')
```

### Output

```
(...initialization messages...)
Testing 1
W tensorflow/core/framework/op_kernel.cc:993] Out of range: Reached limit of 6
         [[Node: input_producer/input_producer/fraction_of_4_full/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[""loc:@input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs""], limit=6, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer/input_producer/fraction_of_4_full/limit_epochs/epochs)]]
Testing 2
Testing 3
Testing 4
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]
W tensorflow/core/framework/op_kernel.cc:993] Out of range: FIFOQueue '_0_input_producer/input_producer/fraction_of_4_full/fraction_of_4_full' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: input_producer/fraction_of_4_full_Dequeue = QueueDequeueV2[component_types=[DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_producer/input_producer/fraction_of_4_full/fraction_of_4_full)]]
Testing 5
Testing 6
Done.
```

These messages may vary: on multiple runs of the same code, the first one sometimes isn't printed, and the number of messages after `Testing 4` also changes.

The documentation of `batch` states:
> The returned operation is a dequeue operation and will throw tf.errors.OutOfRangeError if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.

However, no further instruction is given. And since a `try: except:` didn't work, I'm guessing this is a bug. Could anyone clarify this behavior?

Thanks in advance."
8328,"orthogonal_initializer() on GPU, horrible error message","Works fine on CPU.

**Error message is horrible**, without any hints towards `orthogonal_initializer`, or line it is used on, or GPU, or essence of the problem.

Problems like this (not the first one for me) can only be handled as theory-by-theory manual search by user.

Isolated example:

```python
import tensorflow as tf
import numpy as np

def strange_assign():
	config = tf.ConfigProto()
	config.allow_soft_placement = True
	sess = tf.InteractiveSession(config=config)
	with tf.device(""gpu:0""):
		g1 = tf.get_variable(""g1"", [2,2], tf.float32, tf.constant_initializer(1.0))
		g2 = tf.get_variable(""g2"", [2,2], tf.float32, tf.zeros_initializer())
		g3 = tf.get_variable(""g3"", [2,2], tf.float32, tf.ones_initializer())
		g4 = tf.get_variable(""g4"", [2,2], tf.float32, tf.orthogonal_initializer(1.0))
		g5 = tf.get_variable(""g5"", [2,2], tf.float32, tf.random_normal_initializer())
		g6 = tf.get_variable(""g6"", [2,2], tf.float32, tf.random_uniform_initializer())

	tf.global_variables_initializer().run()
	for test in [g1,g2,g3,g4,g5,g6]:
		t = sess.run(test)
		print(""ASSIGN TEST"", test.name)
		ph = tf.placeholder(tf.float32, t.shape)
		try:
			sess.run( [tf.assign(test, ph)] , feed_dict = { ph: t })
			print(""OK"")
		except:
			print(""FAIL"")

strange_assign()
```

Output:

```
ASSIGN TEST g1:0
OK
ASSIGN TEST g2:0
OK
ASSIGN TEST g3:0
OK
ASSIGN TEST g4:0
E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25)]]
FAIL
ASSIGN TEST g5:0
OK
ASSIGN TEST g6:0
OK
```


Error message:

```
E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25)]]
Traceback (most recent call last):
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""wtf.py"", line 29, in <module>
    strange_assign()
  File ""wtf.py"", line 23, in strange_assign
    sess.run( [tf.assign(test, ph)] , feed_dict = { ph: t })
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File "".linuxbrew/Cellar/python3/3.5.2_3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: g1/_23 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_3_g1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/gpu:0""](^_recv_Placeholder_3_0/_25)]]
```

Versions:

1.0.1

/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44

/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5

"
8326,TensorBoard develop build failed due to npm dead loop and typescript error,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu 16.10
npm 4.4.1
node.js v7.7.2

Installed version of CUDA and cuDNN:  CUDA 8.0, cuDNN 5.1
-rw-r--r-- 1 root root   558720 9月  15 07:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 9月  15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 9月  15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432 9月  15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 9月  15 07:02 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 12月 12 18:46 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 12月 12 18:46 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 12月 12 18:46 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 12月 12 18:46 /usr/local/cuda/lib64/libcudnn_static.a

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
f1ffbc548906e379d423633f76edc558be8284e7
2. The output of `bazel version`
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

### minimal reproducible example
get a fresh clone
cd tensorflow/tensorboard
`npm run prepare` 

and npm install some packages and goes into a dead loop without the bower and typings
  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard
  > npm install && bower install && typings install
  
  
  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard
  > npm install && bower install && typings install 
  
  
  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard
  > npm install && bower install && typings install
  
  
  > tensorflow-vis@0.0.0 prepare /home/baomingkun/Res/tensorflow/tensorflow/tensorboard
  > npm install && bower install && typings install

### What other attempted solutions have you tried?
I have moved global typings and typescript, but nothing works.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
If I kill the prepare after it goes into dead loop than use `bower install && typings install` 
everything looks fine. 
but  `gulp` fail due to some semantic errors
  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2403: Subsequent variable declarations   must have the same type.  Variable 'shadowRoot' must be of type 'ShadowRoot', but here has  type'ShadowRootPolyfill'.
  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.
  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2403: Subsequent variable declarations must have the same type.  Variable 'shadowRoot' must be of type 'ShadowRoot', but here has type 'ShadowRootPolyfill'.
  typings/globals/webcomponents.js/index.d.ts(48,5): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.
  __lib.d.ts(7452,14): error TS2687: All declarations of 'shadowRoot' must have identical modifiers.
  [23:27:41] TypeScript: 2 semantic errors
  [23:27:41] TypeScript: 3 emit errors
  [23:27:41] TypeScript: emit failed
then the demo/index.html is just a blank page.
"
8325,A typo in lstm_ops.py,"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/rnn/python/ops/lstm_ops.py#L73

This line o = sigmoid(cs * wco + f) looks like a typo. It should be o = sigmoid(cs * wco + o)."
8324,i cant get tensorflow to install,"i tried following the steps in the tutorial after installing python 3.6 and 3.5 then i went to cmd and typed in pip3 install --upgrade tensorflow and the gpu version i was told that it could not find a version that satisfies the requierments tensorflow (from versions: ) no matching distribution found for tensorflow. (same for the gpu one)
i also tried this: pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl : and i was told that it is not supported for this wheel which is understandable considering i have a nividia gpu and intel cpu but other than that i really dont know what to do if any one is able to give me commands to put in to the cmd terminal that would be great ill list my pc specs here: windows 7 pro, intel Q8300, nividia 560TI, 12GB DDR3 ram. as far as i know i have all the latest drivers installed and such and more than enough room for any files that might need to be downloaded. any help is appreciated."
8323,tensorflow bazel build error ,"I am using ubuntu 16.04 , I have anaconda and tensorflow installed ,  I was following along this tutorial ->  https://www.tensorflow.org/tutorials/image_retraining#training_on_flowers

I am getting the following error 

```
saurabhorange@orangepc:~/tensorflow$ 
saurabhorange@orangepc:~/tensorflow$ bazel build tensorflow/examples/image_retraining:retrain
ERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: /home/saurabhorange/tensorflow/tensorflow/core/BUILD:1315:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/saurabhorange/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.
ERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted.
INFO: Elapsed time: 1.480s
saurabhorange@orangepc:~/tensorflow$ 

```"
8322,Using a `tf.Tensor` as a Python `bool` is not allowed.  ,I want to do ： IF True: return a ; else : return b (in python) . But there is a error : Using a `tf.Tensor` as a Python `bool` is not allowed.  How can I do this
8321,it seems that sparse_to_dense can only work in CPU?,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8320,Please remove all code visibility restrictions,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

I am trying to use grpc_util to convert ::tensorflow::Status to ::grpc::Status, and found that I cannot build the target because of package visibility issues.

Here is my recommendation:

Remove all visibility when open source tensorflow. 

Reason:

1. It is pointless to have the package visibility as we have the source code, and we can modify it.

2. If everybody has to modify their own copy of tensorflow to depend on portions of the source code, it only makes people's life harder as there would be millions of different forks all trying to hack the BUILD file. And every time a new tensorflow is launched, it breaks other people's build, which seems quite counter productive.

3. In Google we want to make the dependency as accurate as possible. In the open source world, because of the visibility issue, we have to try to pull in much bigger build target to get something working, which is again, counter productive.

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8319,Update macOS GPU installation instructions,"### Environment info

Operating System: macOS 10.12
Tensorflow: 1.0
CUDA/cuDNN: 8.0, 5.1

---

I was setting up Tensorflow with GPU, reading the instructions: https://www.tensorflow.org/install/install_mac. It seems that a few things can be made clearer. I'm not sure where the source for the website lives so I want to file an issue so it can possibly be updated and people searching for related issues can have a pointer.

The issues are with the **Requirements to run TensorFlow with GPU support** section.

1. It mentioned that one should set `LD_LIBRARY_PATH` according to NVIDIA docs, but NVIDIA docs only asks to specify `DYLD_LIBRARY_PATH`, which actually doesn't work. Can we say on the website how exactly it should be set?
2. Same for cuDNN. The linked page actually has no instruction about how to install cuDNN. It may be useful to put on the website that the **include** and **lib** files should be manually copied over to **/usr/local/cuda**.
3. The following should be run to avoid segfault: `sudo ln -sf /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`.

The paths for 1 should be:

```sh
export CUDA_HOME=/usr/local/cuda
export LD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0/lib/
export PATH=""/Developer/NVIDIA/CUDA-8.0/bin:${PATH}""
```"
8318,#FR CG pipeline in tf,"#in brief
build computer graphic render pipeline with tf

In GDC2017's talk ['FrameGraph: Extensible Rendering Architecture in Frostbite'](http://www.frostbite.com/2017/03/framegraph-extensible-rendering-architecture-in-frostbite), a few slide inspire me, why don't CG and CV use the same computation platform, share 'GPU cluster' 'memory management' and 'operation oplimization' code, iteract with each other directly.  I am going to implement a simple ray trace with tf, is there any comment or suggestion?

<img width=""860"" alt=""screenshot"" src=""https://cloud.githubusercontent.com/assets/7426917/23828883/29b7d1f6-071b-11e7-80c4-99a95aaa2c4d.png"">

<img width=""869"" alt=""screenshot"" src=""https://cloud.githubusercontent.com/assets/7426917/23828862/89d7e978-071a-11e7-8195-5bb6353e01c6.png"">
"
8317,Building from source on macOS: invalid command 'bdist_wheel',"I found two other issues mention this error, though none of the proposed solutions fixed the problem for me.

Operating System: macOS Sierra 10.12.3

Installed version of CUDA and cuDNN: None

On a fresh install of Sierra, I've used brew to install bazel, python, python3 (which gives me 3.6.0), pyenv, and the virtualenv plugin for pyenv. I'm following the ""Build from source"" instructions for Mac, created a virtual environment specifically for building tensorflow, and when running ./configure using all the defaults and answering N to all the questions. At the final step, building the wheel, I get:

```
(build-tensorflow) ❯ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Sat Mar 11 18:35:17 PST 2017 : === Using tmpdir: /var/folders/z0/l02zt19x3rvdq8ns17x5tznm0000gn/T/tmp.XXXXXXXXXX.Jfkzndt3
~/Code/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/Code/tensorflow
~/Code/tensorflow
/var/folders/z0/l02zt19x3rvdq8ns17x5tznm0000gn/T/tmp.XXXXXXXXXX.Jfkzndt3 ~/Code/tensorflow
Sat Mar 11 18:35:19 PST 2017 : === Building wheel
usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
   or: setup.py --help [cmd1 cmd2 ...]
   or: setup.py --help-commands
   or: setup.py cmd --help

error: invalid command 'bdist_wheel'
```

I thought this was maybe a pip or setuptools version issue, but they all seem on the latest version. `pip list` outputs:

```
appdirs (1.4.3)
numpy (1.12.0)
packaging (16.8)
pip (9.0.1)
pyparsing (2.2.0)
setuptools (34.3.2)
six (1.10.0)
wheel (0.29.0)
```

Any help to understand how to resolve the bdist_wheel error will be much appreciated, thanks!"
8316,Where is the document of tf.keras?,"It is said that Google have released tf.keras in the lastest version,but I can't find the document.
"
8315,[feature]: support for trigonometric functions in XLA,"I'd like to request support for basic trigonometric functions in XLA, things like cos, sin, etc, as well as basic geometry options like cross products. These are already supported in TF and I imagine there's native support in eigen. Looking through tf2xla I didn't see them anywhere."
8314,"[Windows Build] ""RECURSIVE_PKG:rootedPath"" Path Casing Error","Windows 10
Bazel 0.4.5
Visual C++ 2015
Msys2 v20160205
Python 3.5 (python.org distribution)

Configuring TensorFlow on Windows generates an error related to path case sensitivity:

```
Adriano@Adriano MSYS /c/Users/Adriano/Documents/tensorflow
$ ./configure
Please specify the location of python. [Default is /c/Users/Adriano/AppData/Local/Programs/Python/Python35/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  C:\Users\Adriano\AppData\Local\Programs\Python\Python35
  C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Adriano\AppData\Local\Programs\Python\Python35]

Using python library path: C:\Users\Adriano\AppData\Local\Programs\Python\Python35
Junction created for util\python\python_include <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35\include
Junction created for util\python\python_lib <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35
Junction created for third_party\py\numpy\numpy_include <<===>> C:\Users\Adriano\AppData\Local\Programs\Python\Python35\lib\site-packages\numpy\core\include
Do you wish to build TensorFlow with CUDA support? [y/N] y
CUDA support will be enabled for TensorFlow
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1
Please specify the location where cuDNN 5.1 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 5.0
..................................................
java.lang.RuntimeException: Unrecoverable error while evaluating node 'FILE:[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head]' (requested by nodes 'RECURSIVE_PKG:rootedPath=[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head], excludedPaths=<omitted>)')
        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:448)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: relativePath 'C:/Users/Adriano/Documents/tensorflow/.git/HEAD' is absolute, but it's not under root 'C:/users/adriano/documents/tensorflow'
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:383)
        at com.google.devtools.build.lib.util.Preconditions.checkArgument(Preconditions.java:90)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:56)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:73)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPathMaybeUnderRoot(RootedPath.java:83)
        at com.google.devtools.build.lib.skyframe.FileFunction.getSymlinkTargetRootedPath(FileFunction.java:176)
        at com.google.devtools.build.lib.skyframe.FileFunction.compute(FileFunction.java:101)
        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:374)
        ... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'FILE:[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head]' (requested by nodes 'RECURSIVE_PKG:rootedPath=[C:/users/adriano/documents/tensorflow]/[tensorflow/tools/git/gen/head], excludedPaths=<omitted>)')
        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:448)
        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: relativePath 'C:/Users/Adriano/Documents/tensorflow/.git/HEAD' is absolute, but it's not under root 'C:/users/adriano/documents/tensorflow'
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:383)
        at com.google.devtools.build.lib.util.Preconditions.checkArgument(Preconditions.java:90)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:56)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPath(RootedPath.java:73)
        at com.google.devtools.build.lib.vfs.RootedPath.toRootedPathMaybeUnderRoot(RootedPath.java:83)
        at com.google.devtools.build.lib.skyframe.FileFunction.getSymlinkTargetRootedPath(FileFunction.java:176)
        at com.google.devtools.build.lib.skyframe.FileFunction.compute(FileFunction.java:101)
        at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:374)
        ... 4 more
```

Steps to reproduce:
- Clone TensorFlow repo
- `./configure`"
8313,out_depth not set in PoolParameters if depth_window == 1,"In `PoolParameters`, `out_depth` is not set if `depth_window == 1`.
See: https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/pooling_ops_common.cc#L61-L66."
8312,tf.Graph.get_operations() expects an undocumented parameter,"Based on the API [documentation](https://www.tensorflow.org/versions/master/api_docs/python/framework/core_graph_data_structures#Graph.get_operations), I can call tf.Graph.get_operations(), without arguments, to fetch the list of operations in the graph. When I do, I get:

```
----> 1 tf.Graph.get_operations()

TypeError: get_operations() missing 1 required positional argument: 'self'

```
Using tensorflow-gpu (1.0.1) under Ubuntu, installed with pip3.

```
> python3 -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.1
> 

```
"
8311,Different output using CudnnGRU vs GRUCell,"Operating System: Arch Linux

Installed version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5

1. The commit hash (`git rev-parse HEAD`): 57e40363eb40a692f7c5dfea3f53031a52024321
2. The output of `bazel version`: 0.4.4



I set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421

NOTE: need to use GPU for CudnnGRU to work properly

```
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops

# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0
# u = sig(Wx + Rh + b)
# r = sig(Wx + Rh + b)
# c = tanh(Wx + r(Rh + b) + b)
# h = (1-u)c + uh


# u = sig(0*1 + 0*0 + 0)
# u = .5
# r = sig(0*1 + 0*0 + 0)
# r = .5
# c = tanh(0*1 + .5(0*0 + 0) + 0)
# c = 0
# h = (1-.5)*0 + .5*0
# h = 0



batch_size = 1
n_time = 1
x_depth = 1
n_cell = 1

x = tf.ones([batch_size, n_time, x_depth])

y_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)
param_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params

n_layer = 1
rnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')
param_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)
y_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # NOTE: cudnn has more params because they use twice as many biases
    print('y_tf: {}\ny_cudnn: {}\nparam_tf\n{}\n\nparam_cudnn\n{}\n\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))



# Output:
# y_tf: [[[ 0.]]]
# y_cudnn: [[[ 0.20482421]]]
# param_tf
# [array([[ 0.,  0.],
#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],
#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]

# param_cudnn
# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
```
"
8310,XLA/AOT on Windows build?,"I know windows is low on the priority list (I don't blame you), but will there be support in the future for AOT compilation for Windows, or is it possible now in theory?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
There aren't a lot of questions anywhere about this yet :/

### Environment info
Operating System:
Windows 10

### Installed version of CUDA and cuDNN: 
None

### Installation 
I installed from cmake using the tutorial here and it worked fine, the stock mnist example ran and I couldn't find a fault with the installation. 

I also tried building with the windows Bazel build. I didn't have much success with this (lots of errors on configure) but if I know it can work than I can put for effort here. 


### What other attempted solutions have you tried?
I attemped the JIT example [here](https://www.tensorflow.org/versions/master/experimental/xla/jit) but I got the same output using XLA and not using XLA. Checking chrome://tracing revealed no XLA startup. This is where I assumed that the cmake build does not include tensorflow/compiler/xla, indeed I found no corresponding MSVC build files. 

Attempting to build AOT binaries with bazel didn't work because configure had not been run. I know you can build these binaries with alternate build systems (in the works?) but there's no docs on how to do that. 

I just want to know if I should even bother trying while this part of the project is so young. I should add that the JIT works very well for me on ubuntu, in places where it should work. 

If I can compile tf_library and get the tfcompile tool then I would appreciate some pointers on how to get there. I don't know how productive it is to spend time decoding bazel build files and try to construct my own alternate build the replicates it.

### Logs or other output that would be helpful
Log of JIT example output
```
$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py
Extracting /tmp/tensorflow/mnist/input_data\train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data\train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data\t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data\t10k-labels-idx1-ubyte.gz
0.9179
```
`bazel ./configure` output
```
$ ./configure
Please specify the location of python. [Default is /c/Users//AppData/Local/Programs/Python/Python35/python]:
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y
XLA JIT support will be enabled for TensorFlow
Found possible Python library paths:
  C:\Users\\AppData\Local\Programs\Python\Python35
  C:\Users\\AppData\Local\Programs\Python\Python35\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\\AppData\Local\Programs\Python\Python35]

Using python library path: C:\Users\\AppData\Local\Programs\Python\Python35
Junction created for util\python\python_include <<===>> C:\Users\\AppData\Local\Programs\Python\Python35\include
Junction created for util\python\python_lib <<===>> C:\Users\\AppData\Local\Programs\Python\Python35
Junction created for third_party\py\numpy\numpy_include <<===>> C:\Users\\AppData\Local\Programs\Python\Python35\lib\site-packages\numpy\core\include
Do you wish to build TensorFlow with CUDA support? [y/N] N
No CUDA support will be enabled for TensorFlow
Configuration finished
Warning: ignoring _JAVA_OPTIONS in environment.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:6:6: invalid character: '!'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:6:14: invalid character: '?'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:23:4: invalid character: '@'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:23:41: invalid character: '@'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:24:4: invalid character: '@'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:34:2: indentation error.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:38:6: invalid character: '!'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:38:18: invalid character: '?'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:44:2: indentation error.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:68:57: invalid character: '&'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:68:58: invalid character: '&'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:87:2: indentation error.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:92:39: invalid character: '?'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:95:44: invalid character: '?'.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:3:2: indentation error.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:7:4: indentation error.
ERROR: C:/Users//Documents/tensorflow/tensorflow/contrib/cmake/build/plottable/src/plottable/bower_components/sinon/BUILD:16:2: indentation error.
```"
8309,Kernel Died Issue,"Can anybody know why kernel is always died? When I excute this cell, a part of Lenet CNN.

1. Code : 
```
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(X_train)
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})
            
        validation_accuracy = evaluate(X_validation, y_validation)
        print(""EPOCH {} ..."".format(i+1))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
    try:
        saver
    except NameError:
        saver=tf.train.Saver()       
    saver.save(sess, './lenet')
    print(""Model saved"")
```
2. Error Message :
``` 
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stream_executor\cuda\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
F c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\kernels\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)
```
Then, the kernel is died....it is crazy. Please let me know the solution 

3. Environment 
Window 10 (No Ubantu)
Anaconda w/Jupyter Notebook 
Tensorflow w/GPU (Cuda 8.0, cuDNN 5.1)

Thank you!!
"
8306,tf.contrib.distributions.Multinomial initialization problem,"Hi
I can't initialize a multinomial distribution with more than 1 dimension probabilities. The version of installed TF is 1.0.0. I tried the example code in tensorflow website (https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Multinomial), it gets the same problem and the error doesn't make any sense to require probabilities to be integers. I'm not sure if the guide in the website is obsolete or I got something wrong. Please give some help here, thanks.
```
r=tf.contrib.distributions.Multinomial(n=[4, 5], p=[[.1, .3, .6], [.4, .05, .55]])

ValueErrorTraceback (most recent call last)
<ipython-input-92-7d05643707f2> in <module>()
----> 1 r=tf.contrib.distributions.Multinomial([4, 5], p=[[.1, .3, .6], [.4, .05, .55]])

/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/multinomial.pyc in __init__(self, n, logits, p, validate_args, allow_nan_stats, name)
    164             multidimensional=True)
    165         self._n = array_ops.identity(n, name=""convert_n"")
--> 166         self._mean_val = array_ops.expand_dims(n, -1) * self._p
    167         self._broadcast_shape = math_ops.reduce_sum(
    168             self._mean_val, reduction_indices=[-1], keep_dims=False)

/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in binary_op_wrapper(x, y)
    881     with ops.name_scope(None, op_name, [x, y]) as name:
    882       if not isinstance(y, sparse_tensor.SparseTensor):
--> 883         y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    884       return func(x, y, name=name)
    885 

/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)
    649       name=name,
    650       preferred_dtype=preferred_dtype,
--> 651       as_ref=False)
    652 
    653 

/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    714 
    715         if ret is None:
--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    717 
    718         if ret is NotImplemented:

/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    587     raise ValueError(
    588         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--> 589         % (dtype.name, t.dtype.name, str(t)))
    590   return t
    591 

ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""Multinomial_26/Multinomial/p/Identity:0"", shape=(2, 3), dtype=float32)'
```
"
8305,tf.image.resize_image_with_crop_or_pad,"hi, 
I`m moving a project from r0.11 to r0.10, and I have some bug in 0.10 running this script:
```
      print('before', label.get_shape())
      label = tf.image.resize_image_with_crop_or_pad(label, h, w)
      print('after',label.get_shape())
```
the results is :
```
('before', TensorShape([Dimension(None), Dimension(None), Dimension(1)]))
('after', TensorShape([Dimension(321), Dimension(321), Dimension(None)]))
```
but when I test in r0,11, the results:
```
('before', TensorShape([Dimension(None), Dimension(None), Dimension(1)]))
('after', TensorShape([Dimension(321), Dimension(321), Dimension(1)]))
```
the input of the above two functions is the same.
I can not find where it gets wrong.
any one can help me?

"
8302,Can't generate documentations,"I cannot run gen_docs.sh on my machine.

I already run the configuration script.

This is the last error message
http://sprunge.us/MVJS

I am using tensorflow 1.0 with CUDA capability
http://sprunge.us/dhQT

This is my list of CUDA libs
http://sprunge.us/Gdfh

I am on arch linux btw.

What sould i do ?"
8301,"Feedback on ""A Guide to TF Layers: Building a Convolutional Neural Network"" tutorial","This [tutorial](https://www.tensorflow.org/tutorials/layers) seems to contain some errors:

# Convolutional Layer #1

> The filters argument specifies the number of filters to apply (here, 32)

Are the filters learnt? In that case, you could mention it. Else, how do you choose the types of the filters?

> Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 1]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.

Is not the shape `[batch_size, 28, 28, 32]` instead?

# Pooling Layer #1

> Our output tensor produced by max_pooling2d() (pool1) has a shape of [batch_size, 14, 14, 1]

Isn't it `[batch_size, 14, 14, 32]`, as the pooling does not reduce the number of channels?

> the 2x2 filter reduces width and height by 50%.

Maybe you could say that the total size is reduced by 75% (we keep one pixel out of four)."
8300,print,"I want to print the `height` and `width` as `int` like `123` of the `new_shape`  which is a tensor
``` if input_size is not None:
    h, w = input_size
    if random_scale:
        scale = tf.random_uniform([1], minval=0.75, maxval=1.25, dtype=tf.float32, seed=None)
        h_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[1]), scale))
        w_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[1]), scale))
        new_shape = tf.squeeze(tf.pack([h_new, w_new]), squeeze_dims=[1])

        img = tf.image.resize_images(img, new_shape)
```

I`m new with tf, what should i do?
thanks"
8299,tf.contrib.seq2seq.prepare_attention doesn't allow decoder states and attention states to be different lengths,"I'm using the new `tf.contrib.seq2seq.prepare_attention` with `tf.contrib.seq2seq.attention_decoder_fn_train` and `tf.contrib.seq2seq.dynamic_rnn_decoder` to do dynamic decoding with attention. 

If we are using e.g. `attention_option=""bahdanau""`, then [this line](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py#L408) implements the standard attention equation:

score<sub>i</sub> = v<sup>T</sup> tanh(W<sub>h</sub> h<sub>i</sub> + W<sub>q</sub> q)

where 
- h<sub>i</sub> is the ith attention state, a vector length `num_units`
- W<sub>h</sub> is a weight matrix shape [`num_units`, `num_units`]
- q is the query (i.e. current decoder hidden state), a vector length `num_units`
- W<sub>q</sub> is a weight matrix shape [`num_units`, `num_units`]
- v is a weight vector length `num_units`

In particular, the code assumes that:
1. Decoder hidden states q and attention states h<sub>i</sub> are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)
2. v, W<sub>h</sub> h<sub>i</sub> and W<sub>q</sub> q must also be same length `num_units`

In particular assumption 1 is very limiting. I think it would be better to allow:
- h<sub>i</sub> is the ith attention state, a vector length `attn_size`
- W<sub>h</sub> is a weight matrix shape [`num_units`, `attn_size`]
- q is the query (i.e. current decoder hidden state), a vector length `query_size`
- W<sub>q</sub> is a weight matrix shape [`num_units`, `query_size`]
- v is a weight vector length `num_units`

where the user can define `num_units`, `attn_size` and `query_size`. From what I can see this would be fairly uncomplicated."
8298,Broken initialization when graph has data dependencies. ,"As per conversation at OpenAI (cc @alextp @yaroslavvb). 

Here's an example of broken variable initialization and the super-slow fix:
https://gist.github.com/nivwusquorum/551b502e1cf36a09c9c05385ccea5eb5

Let me know if you need more details"
8293,Could not use tensorboard in Win10?,"When I attempt to use tensorboard to show the tensor scalar and image, however, it can not show the detail in the browser.
![qq 20170311094208](https://cloud.githubusercontent.com/assets/16380725/23819278/0c0a960c-063f-11e7-87f0-e49730661e0d.png)

But when I used it in Ubuntu, it works. What's wrong with Win10?
"
8291,No allowing GPU memory growth option in contrib.learn. Estimator,"When creating estimator using contrib.learn, only GPU option that I can find is ""gpu_memory_fraction"" which can be found at ""tf.contrib.learn.RunConfig""

`classifier = learn.Estimator(
    model_fn=cnn_model_fn,
    config=learn.RunConfig(gpu_memory_fraction=0.9)
    )`

Is there a way to allow GPU memory growth option when using contrib.learn ?"
8288,"Dynamic RNN, Initialization, Control Dependencies","I am trying to write a custom RNN Cell with regularized weights and am encountering a problem caused by the treatment of control flow dependencies.

I initialize variables in the RNNCell as follows:

with tf.variable_scope(scope):
  w = tf.get_variable(..., reg=tf.layers.l2_reg())

and use the cell in a dynamic_rnn call.

At the end of graph construction, I retrieve all the regularized losses (using tf.GraphKeys.REGULARIZATION_LOSSES).

When I add the regularization losses to my cross entropy loss, I run into the following error: ""InvalidArgumentError: The node 'Sum/input' has inputs from different frames."" It seems that, because the variables are only created conditionally, they can't be used together with other tensors.

Could you please suggest a workaround or provide a mechanism for adding regularization to an RNN?
"
8287,filter_format for Native Layouts of Convolution Filter Weights,"Repost from: https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-285217279.

I suggest adding a `filter_format` to the conv2d op to indicate how the filter weights are stored (similar to `data_format` for the input / output format). 

The [docs for TF](https://www.tensorflow.org/extend/tool_developers/#weight_formats) state: 
>The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.

This will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).

In addition to cuDNN, BNNS (https://github.com/tensorflow/tensorflow/issues/3001) also uses that same format.

This needs to be benchmarked, but I assume there is some cost to having to transpose the filter weights during both fprop and bprop (twice). This could even explain some of the performance differences between TF and other frameworks (eg pytorch).

It looks like TF already [has some concept of these difference](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/dnn.h#L340-L346). Although as of now all weights are converted from / back to one canonical format.

While [most users won't tune this](https://github.com/tensorflow/tensorflow/issues/8227), it could be used by xla / optimization passes to choose better layouts."
8286,Pass NHWC Tensors to cuDNN Directly,"Currently when running `conv2d` on cuDNN with image data in `NHWC` format, TF [converts the input tensor from NHWC to NCHW](https://github.com/tensorflow/tensorflow/blob/0be81439c91e297b078152dd0c266471b24bde7f/tensorflow/core/kernels/conv_ops.cc#L558-L575), runs the conv and then [converts the output tensor back from NHWC to NCHW](https://github.com/tensorflow/tensorflow/blob/0be81439c91e297b078152dd0c266471b24bde7f/tensorflow/core/kernels/conv_ops.cc#L711-L720). However, cuDNN natively supports taking in data in `NHWC` format. 

Assuming this has not been tested and concluded to be optimal, my prior would be to allow Nvidia to handle the optimal execution of a conv2d on `NHWC` data rather trying to transpose within TF."
8285,[Windows] Tensorflow GPU fails to find CUDA,"I have downloaded cuDNN v5.1 and CUDA 8.0 and also moved the folders lib. include, and bin  to the Cuda directory where i installed it.

C:\Users\Owner\Desktop\tensorflow>python try.py
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll
locally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll lo
cally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll l
ocally
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll local
ly
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll
locally
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz

E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE

I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information
 for host: Owner-PC
I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\stre
am_executor\cuda\cuda_diagnostics.cc:165] hostname: Owner-PC

Is there any solution for that ??
"
8284,TF_ImportGraphDef crashes for the following graph,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None

### Environment info
Operating System: Ubuntu 16.04 

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

#### CUDA
-rw-r--r-- 1 root root 558720 Nov 30 13:53 libcudadevrt.a
lrwxrwxrwx 1 root root     16 Nov 30 13:53 libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Nov 30 13:53 libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root 415432 Nov 30 13:53 libcudart.so.8.0.44
-rw-r--r-- 1 root root 775162 Nov 30 13:53 libcudart_static.a


#### CUDNN
lrwxrwxrwx 1 ajayaram ajayaram       13 Nov 30 14:02 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 ajayaram ajayaram       17 Nov 30 14:02 libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxrwxr-x 1 ajayaram ajayaram 79337624 Jul 27  2016 libcudnn.so.5.1.5
-rw-rw-r-- 1 ajayaram ajayaram 69756172 Jul 27  2016 libcudnn_static.a

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
29a6b4661258ef99842904d7c54993c963a8c2c0

2. The output of `bazel version`
.............................
Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
[lstm_issue.zip](https://github.com/tensorflow/tensorflow/files/834801/lstm_issue.zip)


```
#lstmkt.py : Write a keras model with LSTM layer to tensorflow graph

from keras.models import Sequential
from keras.layers import Input, Dense, Reshape
from keras.layers.recurrent import LSTM
from keras import backend as K

import tensorflow as tf

def build_model():
  model = Sequential()	
  model.add(LSTM(4, input_shape= (1,1) ))  
  model.add(Dense(1))
  model.compile(optimizer='adam', loss='mse')
  return model

model = build_model()
sess = K.get_session()
graph = sess.graph
sess.run('init')

tf.train.write_graph(sess.graph, './', 'lstmkt.pb', as_text=False)
```

```
void free_buffer(void* data, size_t length);
TF_Buffer* read_file(const char* file);

int main()
{

    const char filename[] = ""lstmkt.pb"";
    
    TF_Status* s = TF_NewStatus();
    TF_Graph* graph = TF_NewGraph();
    TF_Buffer* graph_def = read_file(filename);
    
    // Import graph
    TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();
 
    // This line of code crashes if the keras model contains an LSTM layer
    TF_GraphImportGraphDef(graph, graph_def, opts, s);

    return 0;
}
```
## What other attempted solutions have you tried?
None. Stuck here. Works in python but I need to load this graph in C.


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

```
Stack Trace (from fault):
[  0] 0x00007f91b7a87338 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+63492920 _ZN10tensorflow15shape_inference16InferenceContext8WithRankENS0_11ShapeHandleEiPS2_+00000024
[  1] 0x00007f91b66b8c88 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+42724488
[  2] 0x00007f91b7a65a44 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+63355460 _ZNSt17_Function_handlerIFN10tensorflow6StatusEPNS0_15shape_inference16InferenceContextEEPS5_E9_M_invokeERKSt9_Any_dataOS4_+00000020
[  3] 0x00007f91b79e2094 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62816404 _ZN10tensorflow12ShapeRefiner7AddNodeEPKNS_4NodeE+00002996
[  4] 0x00007f91b5bd42e5 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+31302373
[  5] 0x00007f91b7a0dc2c /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62995500
[  6] 0x00007f91b7a0e6d3 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62998227 _ZN10tensorflow14ImportGraphDefERKNS_21ImportGraphDefOptionsERKNS_8GraphDefEPNS_5GraphEPNS_12ShapeRefinerE+00000499
[  7] 0x00007f91b5cb2434 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+32212020
[  8] 0x00007f91b5cb52fc /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+32223996 TF_GraphImportGraphDef+00000204
```"
8283,tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing],"
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/8238

### Environment info
OS X 10.11.6
$ clang --version
Apple LLVM version 8.0.0 (clang-800.0.42.1)
Target: x86_64-apple-darwin15.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin


Installed version of CUDA and cuDNN: 
drwxrwxrwx  15 root  wheel  510 May  3  2015 CUDA-7.0
David-Laxers-MacBook-Pro:tensorflow davidlaxer$ ls -l /Developer/NVIDIA/CUDA-7.0/lib/libcud*
-rw-r--r--  1 davidlaxer  staff  292184 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudadevrt.a
-rwxr-xr-x  1 davidlaxer  staff  274176 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart.7.0.dylib
lrwxr-xr-x  1 davidlaxer  staff      19 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart.dylib -> libcudart.7.0.dylib
-rw-r--r--  1 davidlaxer  staff  562856 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart_static.a
If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

print(tensorflow.__version__)
1.0.1
If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)

99e30bc6b22b259ddc6a2cfc6aec1d9ebc635da4

2. The output of `bazel version`

Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 15:54:21 2016 (1481126061)
Build timestamp: 1481126061
Build timestamp as int: 1481126061
"
8281,Tensordot sometimes fails one of the operands has a single dimension,"This error occurs when one operand has a single dimension, and the other operand has a dimension of None.  To reproduce:

A = tf.placeholder(tf.float32, [None, 20])
B = tf.placeholder(tf.float32, [20])
C = tf.tensordot(A, B, [[1], [0]])

throws `TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.` when trying to calculate the shape of the resulting tensor.

I've actually fixed this, I'll submit a pull request momentarily.  I just figured I would submit an issue first for clarity.
This is probably related to #6682; I'll might look into that as well"
8280,Reduction Functionality for Java API,Hey! It seems like the reduction functionality available in Python (https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/reduction) has not made it to the Java API. Are there plans to port it over yet? There are some useful functions such as `reduce_mean` which I need.
8278,Using 3-dimensional Tensor in Java,"Hi! After feeding an input image through my session I receive a `FLOAT tensor with shape [299, 299, 3]`.

It doesn't seem like `reduce_mean` has made it into the Java API yet. Is there anything else I can do to use my output tensor?"
8277,Different Code Path Taken in conv2d for Constant vs Variable Filter,"It appears that a different code path is taken in `conv2d` for Constant vs Variable filters.

On a box with GPU, this works:
```python
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
```
however this fails with `Check failed: data_format == FORMAT_NHWC Generic conv implementation only supports NHWC tensor format for now.`:
```python
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
      data_format='NCHW',
  ).eval()
```

Both of these work (the data_format is supported):
```python
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
```
and
```python
images = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))
filters = tf.constant(1 * np.ones((1,1,2,1), np.float32))
with tf.Session() as sess:
    output = nn_ops.conv2d(
      images,
      filters,
      strides=(1,1,1,1),
      padding='VALID',
  ).eval()
```
I don't see a reason for the Constant filter to take what I am guessing is a less efficient code path (not using GPU op) than the Variable Filter.

To make things even weirder, it seems like even though the GPU is not being used for the Constant, there is still cuda `memcpy`. Run on 20 calls:
```
Time(%)      Time     Calls       Avg       Min       Max  Name
 52.45%  90.108us        20  4.5050us  4.0950us  6.0800us  [CUDA memcpy HtoD]
 42.91%  73.726us        20  3.6860us  3.5520us  4.5120us  [CUDA memcpy DtoH]
  4.64%  7.9680us         1  7.9680us  7.9680us  7.9680us  [CUDA memset]
```"
8275,make sparse_tensor (SparseTensor) serializable in meta graph,"`tf.sparse_placeholder` Sparse tensors aren't serialized in meta graphs, one has to serialize tf.placeholder tensors for the indices and values individually.

At present `add_collection_def` (called by `create_meta_graph_def`, which is typically called by `export_scoped_meta_graph`, found in [/tensorflow/python/framework/meta_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py), ) **fails** when a `key` resolves to a `SparseTensor` as `SparseTensor`s do not have `name` attribute: here is the specific [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py#L290).

## Suggested solutions:
- give `SparseTensor` a `name` attribute and any other methods and properties necessary

or

- Enable `SparseTensor` or `sparse_placeholder` to be added to `_proto_function_registry` so that [ops.get_to_proto_function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py#L275) works in `add_collection_def`, perhaps using the `SAVEABLE_OBJECTS` key, something like the following being found in the SparseTensor file:

```
ops.register_proto_function(
    ops.GraphKeys.SAVERS,
    proto_type=saver_pb2.SaverDef,
    to_proto=Saver.to_proto,
    from_proto=Saver.from_proto)
```

speculative, I don't know the best implementation:

Could do something similar to the [`_as_graph_def`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L2114) method of `Graph` and loop through `graph._nodes_by_id`, compare the `SparseTensor`'s `_indices` to an `op` (node) `_outputs` from `_nodes_by_id`.

## Small Test

```python

g = tf.Graph()
with g.as_default(), tf.Session(graph=g).as_default() as sess:
  st = tf.sparse_placeholder(tf.string)
  ops.add_to_collection('_sparse_test', st)
  ops.add_to_collection('_test', tf.placeholder(tf.string))
  # FORMERLY GAVE A WARNING WHEN EXCEPTION THROWN
  tf.train.export_meta_graph('tmp_model/test-model.meta', as_text=True, graph=g,
                             collection_list=['_sparse_test', '_test']) 
  # WORKS
  print(sess.run(st, feed_dict={ st: ([[i,0] for i in range(10)],
                                      list(map(str,range(10))),
                                      [10,1]) }))
    
g2 = tf.Graph()
with g2.as_default(), tf.Session(graph=g2).as_default() as sess2:
  tf.train.import_meta_graph('tmp_model/test-model.meta')
  t = tf.get_collection('_test')[0]
  # FAILS
  st = tf.get_collection('_sparse_test')[0]
  print(sess2.run(st, feed_dict={ st: ([[i,0] for i in range(10)],
                                      list(map(str,range(20,30))),
                                      [10,1]) }))


```

Sometimes I get a segfault with this"
8274,LSTM Network,"Hi, I have the error below when i execute the line """"lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(config.n_hidden, forget_bias=1.0)""""
AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'
I was installed Tensorflow on windows using ""Installing with Anaconda""
Any one have the same error and solve it Please!"
8273,Error:,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8271,ImportError: No module named 'tensorflow.contrib.ffmpeg.ops',"### Environment info
Operating System: Windows 10

Installed version of CUDA and cuDNN: CUDA 8.0, cuDNN 5.1

If installed from binary pip package, provide:

1. A link to the pip package you installed: `tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl`
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`: 1.0.1

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

`python -c ""from tensorflow.contrib import ffmpeg""`

Output:

`Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Guillaume COTER\Anaconda3\lib\site-packages\tensorflow\contrib\ffmpeg\__init__.py"", line 26, in <module>
    from tensorflow.contrib.ffmpeg.ffmpeg_ops import decode_audio
  File ""C:\Users\Guillaume COTER\Anaconda3\lib\site-packages\tensorflow\contrib\ffmpeg\ffmpeg_ops.py"", line 23, in <module>
    from tensorflow.contrib.ffmpeg.ops import gen_decode_audio_op_py
ImportError: No module named 'tensorflow.contrib.ffmpeg.ops'`"
8269,1.0.1 CPU vs GPU 1.0.0 Build for Windows,"The CPU build is complete for version 1.0.1 on Windows, however the GPU build is at 1.0.0.

http://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=windows/"
8268,Error when  using the TensorFlow Runtime with HVX Acceleration,"OS: Ubuntu 14.04 64bits
Android Version: 6.0.1     
NDK Version: android-ndk-r12b
I follow the description in [**tensorflow/tree/master/tensorflow/contrib/hvx**]((https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx)),  after compiling the `so` files and the **hexagon_graph_execution**,  what's more, I also have pushed  additional network define file(`tensorflow_inception_v3_stripped_optimized_quantized.pb`) and the test image to my phone. 
But when I start to run the binary file in my **Snapdragon 820 android devices**, it failed all the test cases.
The stdout log is showed below,   How can I resolve this question?
 

Running main() from test_main.cc
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from GraphTransferer
[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime
native : hexagon_graph_execution_test.cc:294 Fuse and run inception v3 on hexagon with tf runtime
tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:70: Failure
Expected: (version) >= (1), actual: 0 vs 1
native : hexagon_graph_execution_test.cc:121 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
native : hexagon_graph_execution_test.cc:127 header size = 54
native : hexagon_graph_execution_test.cc:129 image size = 40
native : hexagon_graph_execution_test.cc:131 width = 299
native : hexagon_graph_execution_test.cc:133 height = -299
native : hexagon_graph_execution_test.cc:306 Ioading image finished.
native : hexagon_graph_execution_test.cc:313 Build fused graph
can't determine number of CPU cores: assuming 4
can't determine number of CPU cores: assuming 4
native : hexagon_graph_execution_test.cc:121 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes
native : hexagon_graph_execution_test.cc:127 header size = 54
native : hexagon_graph_execution_test.cc:129 image size = 40
native : hexagon_graph_execution_test.cc:131 width = 299
native : hexagon_graph_execution_test.cc:133 height = -299
native : hexagon_graph_execution_test.cc:218 Ioading image finished.
native : hexagon_graph_execution_test.cc:223 Copy data to tensor.
native : hexagon_graph_execution_test.cc:246 Run graph
Init hexagon with max attributes (Controller version = 91)
Failed to disable DSP DCVS: ffffffff

Failed to append const node 65538
Failed to append const node 65538
Failed to append const node 65539
Failed to append const node 65539
.
.
.
Failed to append const node 66635
Failed to append const node 66640
Failed to append const node 66640
native : hexagon_control_wrapper.cc:235 Setup graph completed
Prepare failed! returned 0xffffffff

Execute graph!
Execution failed!
execute got err: -1

Execution failed
Failed to read data.
native : hexagon_graph_execution_test.cc:253 Output byte size = 4032
native : hexagon_graph_execution_test.cc:254 Output shape = [1,1,1,1008]
native : graph_transfer_utils.cc:43 === Dump ranking ===
native : graph_transfer_utils.cc:46 0: 1000, dumbbell, 0
native : graph_transfer_utils.cc:46 1: 999, carbonara, 0
native : graph_transfer_utils.cc:46 2: 998, stole, 0
native : graph_transfer_utils.cc:46 3: 997, rubber eraser, 0
native : graph_transfer_utils.cc:46 4: 996, coffee mug, 0
native : graph_transfer_utils.cc:46 5: 995, flagpole, 0
native : graph_transfer_utils.cc:46 6: 994, parallel bars, 0
native : graph_transfer_utils.cc:46 7: 993, cheeseburger, 0
native : graph_transfer_utils.cc:46 8: 992, bubble, 0
native : graph_transfer_utils.cc:46 9: 991, beaker, 0
Finalize hexagon
[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5573 ms)
[----------] 1 test from GraphTransferer (5573 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (5573 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime

 1 FAILED TEST
  YOU HAVE 3 DISABLED TESTS


"
8267,"TypeError: Expected int32, got list containing Tensors of type '_Message' instead.","I am using tensorflow (1.0.0) in Ubuntu 14.04. When I train a cnn model ,I got a error as follow:

/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
2017-03-10 15:03:03,616 : INFO : building model...
Traceback (most recent call last):
  File ""cnn.py"", line 205, in <module>
    train()
  File ""cnn.py"", line 202, in train
    cnn_model.build_model(sentence_words, vector_length, num_classes, filter_sizes, num_filters)
  File ""cnn.py"", line 61, in build_model
    h_pool = tf.concat(3, pooled_outputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1047, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.

How can I resolve this question?"
8266,"""The TensorFlow library wasn't compiled to use SSE3/SSE4.1/... instructions"", ""creating context when one is currently active"", ""Peer access not supported between device ordinals 0 and 1"" when running ""Hello TensorFlow""","
### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
I am running google TensorFlow on google compute engine instance. I started with Hello Tensorflow program and I am reading some warnings.

1. The TensorFlow library wasn't compiled to use SSE3/SSE4.1/SSE4.2/AVX/AVX2/FMA instructions, but these are available on your machine and could speed up CPU computations.
2. successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero.
3. creating context when one is currently active; existing: 0x30fd920
4. Peer access not supported between device ordinals 0 and 1

Can anyone tell me what caused those warnings? First thing I want to know is if my GPUs are actually working on the data are just sent to CPU only?

### Environment info
Operating System: Ubuntu 16.04

### What other attempted solutions have you tried?
I tried run the Hello TensorFlow on both instance with 4 GPUs and instance with 8GPUs.

### Logs or other output that would be helpful
Please see the attachment.

Thank you for your help!

[Terminal Saved Output.txt](https://github.com/tensorflow/tensorflow/files/833162/Terminal.Saved.Output.txt)"
8265,Memory leak when writing to Logfile with Tensorboard summarywriter,"### Issue:
Memory occupied after call to Filewriter not being freed till python termination. This causes accumulation of data and subsequent filling up of RAM which is freed only when the entire script completes execution and python is terminated.

### Environment info
Operating System: Ubuntu 16.04
CUDA/cuDNN : Not installed
Link to the pip package you : https://github.com/tensorflow/tensorflow/releases/tag/v1.0.0
Tensorflow version: 1.0.0

### A small script replicating the issue

```
import os
import numpy as np
import tensorflow as tf
from memory_profiler import profile
import time

@profile
def _write_into_log(images):
    path_logdir = os.path.join(""./MiniExample"")
    if not os.path.exists(path_logdir):
        os.makedirs(path_logdir)

    with tf.Graph().as_default() as g:
        image = tf.placeholder(tf.float32, shape = [None, None, None, 3])

        image_summary = tf.summary.image(name = ""Images"", tensor = image, max_outputs = 2000)

        with tf.Session() as sess:
            summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})
            file_writer = tf.summary.FileWriter(path_logdir, g)
            file_writer.add_summary(summary)
            file_writer.close()

@profile
def main():
    out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    _write_into_log(out)
    out = None
    time.sleep(10)

main()
```

### Logs or other output that would be helpful
Python memory profiler output
```
$ python test.py 
Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
     7   2394.5 MiB      0.0 MiB   @profile
     8                             def _write_into_log(images):
     9   2394.5 MiB      0.0 MiB       path_logdir = os.path.join(""./MiniExample"")
    10   2394.5 MiB      0.0 MiB       if not os.path.exists(path_logdir):
    11                                     os.makedirs(path_logdir)
    12                             
    13   2394.6 MiB      0.0 MiB       with tf.Graph().as_default() as g:
    14   2399.0 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])
    15                             
    16   2399.1 MiB      0.1 MiB           image_summary = tf.summary.image(name = ""Images"", tensor = image, max_outputs = 2000)
    17                             
    18   2407.9 MiB      8.9 MiB           with tf.Session() as sess:
    19   2978.8 MiB    570.9 MiB               summary = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})
    20   2980.8 MiB      2.0 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)
    21   3269.7 MiB    288.9 MiB             file_writer.add_summary(summary)
    22   3269.7 MiB      0.0 MiB               file_writer.close()


Filename: test.py

Line #    Mem usage    Increment   Line Contents
================================================
    24     89.6 MiB      0.0 MiB   @profile
    25                             def main():
    26   2394.5 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]
    27   2981.4 MiB    586.8 MiB       _write_into_log(out)
    28    676.7 MiB  -2304.7 MiB       out = None
    29    676.7 MiB      0.0 MiB       time.sleep(10)
```
As can be seen above, the additional 586.8 MB occupied after call to the _write_into_log is never cleared.
"
8264,Auto-Configuration Error: Cannot find cudnn.h at /usr/lib/x86_64-linux-gnu/include/cudnn.h,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
#4397, but it's for 16.04

### Environment info
Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN: 8.0, 5.1
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
ls -l /path/to/cuda/lib/libcud*
-rw-r--r-- 1 root 543K Jan 26 15:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root   16 Jan 26 15:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root   19 Jan 26 15:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root 406K Jan 26 15:48 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root 757K Jan 26 15:48 /usr/local/cuda/lib64/libcudart_static.a
```
```
ll /usr/lib/x86_64-linux-gnu/libcudnn*                                                                                                                                                                
lrwxrwxrwx 1 root  29 Mar  1 11:58 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so
lrwxrwxrwx 1 root  18 Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.10
-rw-r--r-- 1 root 81M Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
lrwxrwxrwx 1 root  32 Mar  1 11:58 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib
-rw-r--r-- 1 root 68M Nov  6 23:19 /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`): e895d5ca395c2362df4f5c8f08b68501b41f8a98
2. The output of `bazel version`: 
```
INFO: Reading 'startup' options from /root/.bazelrc: --batch
Build label: 0.4.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Dec 7 18:47:11 2016 (1481136431)
Build timestamp: 1481136431
Build timestamp as int: 1481136431
```

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I ran
```
./configure
```
It failed with
```
INFO: Reading 'startup' options from /root/.bazelrc: --batch
INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.
INFO: Reading 'startup' options from /root/.bazelrc: --batch
ERROR: package contains errors: tensorflow/compiler/tests.
ERROR: error loading package 'tensorflow/compiler/tests': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 816
                _create_cuda_repository(repository_ctx)
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 729, in _create_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 585, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 296, in _cudnn_version
                _find_cuda_define(repository_ctx, cudnn_install_base..., ...)
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 271, in _find_cuda_define
                auto_configure_fail(""Cannot find cudnn.h at %s"" % st...))
        File ""/tensorflow/third_party/gpus/cuda_configure.bzl"", line 93, in auto_configure_fail
                fail(""
%sAuto-Configuration Error:%s ...))

Auto-Configuration Error: Cannot find cudnn.h at /usr/lib/x86_64-linux-gnu/include/cudnn.h
```

I noticed the cudnn.h is in /usr/include/cudnn.h somehow, while its libs are in /usr/lib/x86_64-linux-gnu/.

I read cuda_configure.bzl, but didn't find an easy way to let it find both cudnn headers and libs.
"
8263,Documentation incorrect for RNN tutorial?,"Is it possible the documentation is incorrect on https://www.tensorflow.org/tutorials/recurrent ?

```
words = tf.placeholder(tf.int32, [batch_size, num_steps])
for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)
```

It seems the correct output should be:
`    output, state = lstm(words[i, :], state)`

Since you want to process the words in the same sequence correct?

E.g.
[[The, quick, brown]
 [fox, jumped, over]]

words[:,0] == [The, fox]

Whereas what you want is [The, quick, brown]  == words[0,:]

Please correct me if I'm wrong, thanks."
8261,the error of using placeholder in summary.scalar,"class Model(object):

  def __init__(self,
               images=None,
               actions=None,
               states=None,
               sequence_length=None,
               reuse_scope=None):
    self.prefix = prefix = tf.placeholder(tf.string, [])
    summaries.append(
          tf.summary.scalar(prefix + '_recon_cost' + str(i), recon_cost))



get the error of *** TypeError: expected string or bytes-like object, use the old summary_scalar is ok

"
8259,[Request]Binary package with fast c++ implementation of protobuf support for Python 3.6?,"As said in [this doc page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_linux.md), the binary package is supported for Python 3.5 and 2.7, is it also possible for Python 3.6?"
8256,Variable for Go?,"Is Variable being worked on in the Go bindings?
"
8255,Ordering the runs in TensorBoard's embeddings tab,"Feature request: it would be nice if the  runs in TensorBoard's embeddings tab were ordered.

As of now it is unordered, e.g.:

![image](https://cloud.githubusercontent.com/assets/15331/23777190/d3eaabfc-0500-11e7-93f1-657a9d9bd35c.png)

Corresponding Stack Exchange question: [What determine the order of the runs in TensorBoard's embeddings tab?](http://stackoverflow.com/q/42684521/395857)"
8253,Reoccurring issue with tensorflow-gpu install,"Hi all,

There is still this reoccurring issue with the tensorflow-gpu install with the latest version from pip.

When i try and test using the ""Hello World"" constant I get the below errors even though it works.

E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots"
8251,pip3 install --upgrade tensorflow-gpu: No matching distribution found for tensorflow-gpu,"Following the installation guide at https://www.tensorflow.org/install/install_windows
Installed:
- CUDA (cuda_8.0.61_win10.exe)
- cuDDN (cudnn-8.0-windows10-x64-v5.1.zip)
- Python 3.6 x64 (python-3.6.0-amd64.exe)

Then issued:
C:\>pip3 install --upgrade tensorflow-gpu
Collecting tensorflow-gpu
  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )
No matching distribution found for tensorflow-gpu

Same happens with non-gpu version:
C:\>pip3 install --upgrade tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Windows 10 x64

Installed version of CUDA and cuDNN: 
cudnn-8.0-windows10-x64-v5.1.zip 
cuDDN/bin: cudnn64_5.dll

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
Just follow the provided instructions on https://www.tensorflow.org/install/install_windows

### What other attempted solutions have you tried?
First I tried 32-bit python but found on SO that is not supported (install guide should state which python is requrired 32 or 64)

I wanted specify the correct URL for tensorflow-gpu, but I don't know which is the correct one for r1.0 gpu. Like this (example):
pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl

Please provide correct url - on storage.googleapis.com it is not possible to browse the directory contents. I need tensorflow r1.0 x64 gpu!

got the hint from here: http://stackoverflow.com/questions/38896424/tensorflow-not-found-in-pip

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8250,.,
8247,"""ValueError: None values not supported.""  when using bidirectional_dynamic_rnn","Using bidirectional_dynamic_rnn throws the ""ValueError: None values not supported"" when you are not specifying the sequence lengths. The error comes from the reverse step, the operator used to reverse the input sequences requires the sequence lengths to be specified. Maybe this should be a required parameter or alternatively generate the lengths array when the default 'None' is used.

(tensorflow 1.0.0)"
8246,TensorFlow equivalent to numpy.repeat,"This is a popular question on StackOverflow:
http://stackoverflow.com/questions/35361467/tensorflow-numpy-repeat-alternative

But note that the answer so far only works for some use cases (the one presented in the question).

The best I could come up with for a general solution uses `tf.while_loop`, which is pretty verbose (and maybe slower than necessary). I'll add a link to the implementation I wrote for `tf.contrib.training.resample_at_rate` after the next internal/github sync."
8245,Minor but confusing mistake in doc about shape of output tensor of gather_nd,"According to the docs of [`gather_nd(params, indices)`](https://www.tensorflow.org/api_docs/python/tf/gather_nd), the output tensor produced by the function has shape

```
[d_0, ..., d_{Q-2}, params.shape[K], ..., params.shape[P-1]].
```

where `[d_0, ..., d_{Q-2}` are the dimensions of `indices` without the last one, `P` is the rank of params and `K` is the length of the innermost dimension of `indices`. Firstly, I believe it should say `params.shape[K-1]` instead of `params.shape[K]`. But even then, I think that this formula is only true if `K < P`, i.e. for slices. For access to elements, consider the following example:

```
# data is shape [2, 3, 2]
data = np.reshape(np.arange(12), [2, 3, 2])
x = tf.constant(data)
# indices is shape [2, 3]
indices = np.array([[0, 0, 0], [1, 2, 1]])
# result is shape [2, ]
result = tf.gather_nd(x, indices)
```
According to the above formula, the output tensor should have shape `[2, 2]`, which it does not. I guess the correct formula for element access should be

```
[d_0, ..., d_{Q-2}]
```
This might already be implicitly assumed with the above formula, but if so, it is not very clear. I realise that this is a super minor thing and one can easily find out the behaviour by doing some tests. However I still wanted to raise it, as I found it a bit tricky to get the hang of `gather_nd` in the first place and these subtle differences confused me even more.


"
8244,[Java API] Tensor.create() slow for large arrays,"The current Java API's `Tensor.create(Object)` is really slow - for a batch of 128 images of size 224x224x3 it's taking around 1.5seconds. To put this into perspective `runner.run()` with that data and an InceptionV3 graph took below 1second so data prep is x1.5 of the runtime here (for a batch of 32 images it's around 0.35-0.45sec).

Is this working as intended? When running the Python code (using simple `sess.run(fetches, feed_dict=feed_dict)`) with which the graph meta file was generated (TF 1.0.1) and feeding a Python array I don't see such hiccups, the speed is the same as the Java `runner.run()`.

Might it be because of build flags used, maybe I'm missing some optimizations?

For now this small part is killing the whole performance, bringing it down from 130obs/sec (`runner.run()` time) to about ~45obs/sec (Tensor.create+run()).

A bit of a sidenote, the performance page states:

> This will result in poor performance.
> sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

But currently there's no other way to feed data from the Java API, right? A queue (able to read from a file and from memory, i.e. from a Java structure) would be amazing.

### Jar build command
```
export CC=""/usr/bin/gcc""
export CXX=""/usr/bin/g++""
export TF_NEED_CUDA=1
export GCC_HOST_COMPILER_PATH=$CC
export BUILDFLAGS=""--config=cuda --copt=-m64 --linkopt=-m64 --copt=-march=native""

bazel build -c opt \
  //tensorflow/java:tensorflow \
  //tensorflow/java:libtensorflow_jni \
  $BUILDFLAGS --spawn_strategy=standalone --genrule_strategy=standalone
```

### Environment info
**OS:** Ubuntu 16.04
**GPU:** GPU TITAN X (Pascal) 12GB
**CPU:** Intel® Xeon® Processor E5-2630 v4 10core
**GPU Drivers:** 
NVidia CUDA Driver Version: 375.39
CUDNN 5.1.5
CUDA 8
**Tensorflow version:** JAR file built from current master (c25ecb53)

### Example

```java
public void test() {
  Random r = new Random();
  int imageSize = 224 * 224 * 3;
  int batch = 128;
  float[][] input = new float[batch][imageSize];
  for(int i = 0; i < batch; i++) {
    for(int j = 0; j < imageSize; j++) {
      input[i][j] = r.nextFloat();
    }
  }

  long start = System.nanoTime();
  Tensor.create(input);
  long end = System.nanoTime();
  // Around 1.5sec
  System.out.println(""Took: "" + (end - start));
}
```"
8243,[Time Consuming] TF C++ Session->Run - Images for Real-time Inference,"[Tensorflow (TF) on CPU]
I am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.

In my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session->Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session->Run call takes about 0.03 seconds. 

Are these time measurements normal for the Session->Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
https://github.com/tensorflow/tensorflow/issues/1439
http://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training

### Environment info
Operating System: Linux
Installed version of CUDA and cuDNN: N/A

### What other attempted solutions have you tried?
1. I installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]
2. Unified the session for the Graph I created.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8242,Operations used for inference are dropped by optimize_for_inference,"Inspired by the [TensorFlow for Poets](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/), I have been exporting models optimized for inference with the `freeze_graph` and `optimize_for_inference`. I have run into an issue where some of the nodes required for inference get dropped by `optimize_for_inference`. The most critical one being the output node being dropped, even though it was explicitly given to `freeze_graph` and `optimize_for_inference` (through the `output_node_name`/`output_names`).

I think that might be related to the output node being a `tf.identity` (to give an explicit name to the result of a `tf.layers` for example).

### Minimal working example
Here is a piece of code to create a very simple model, running on TensorFlow v.1.0.1.
```python
import tensorflow as tf

l_input = tf.placeholder(tf.float32, shape=(None, 2), name='input')
l_dense = tf.layers.dense(l_input, units=1, activation=None)
l_output = tf.identity(l_dense, name='output')

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver(tf.global_variables())
    
    # Save GraphDef
    tf.train.write_graph(sess.graph_def, '.', 'graph.pb')
    # Save Checkpoint
    saver.save(sess, 'model.ckpt', write_meta_graph=False)
```
I am exporting the model using the `freeze_graph` and `optimize_for_inference` tools, inspired by the [TensorFlow for Poets post](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/).
```
bazel-bin/tensorflow/python/tools/freeze_graph --input_graph graph.pb --input_checkpoint model.ckpt --output_graph graph_frozen.pb --output_node_name=output
```
```
bazel-bin/tensorflow/python/tools/optimize_for_inference --input graph_frozen.pb --output graph_optimized.pb --input_names=input --output_names=output
```
I am using Python to load both of these models (`graph_frozen.pb` and `graph_optimized.pb`). The model defined by `graph_frozen.pb` works as expected, but the model defined by `graph_optimized.pb` is missing some operations (`import/dense/BiasAdd` and `import/output`).
```python
import tensorflow as tf
import numpy as np

# Data
x = np.random.rand(3, 2)

# Frozen Graph
with tf.gfile.GFile('graph_frozen.pb', 'rb') as f:
    graph_def_frozen = tf.GraphDef()
    graph_def_frozen.ParseFromString(f.read())

with tf.Graph().as_default() as graph:
    l_output, = tf.import_graph_def(graph_def_frozen,
        return_elements=['output:0'], 
        name='import'
    )
    print('Operations in Frozen Graph:')
    print([op.name for op in graph.get_operations()])
    # >>> [u'import/input', u'import/dense/kernel',
    #      u'import/dense/kernel/read', u'import/dense/bias',
    #      u'import/dense/bias/read', u'import/dense/MatMul',
    #      u'import/dense/BiasAdd', u'import/output']

    l_input = graph.get_tensor_by_name('import/input:0')

    with tf.Session(graph=graph) as sess:
        sess.run(l_output, feed_dict={l_input: x})

# Optimized Graph
with tf.gfile.GFile('graph_optimized.pb', 'rb') as f:
    graph_def_optimized = tf.GraphDef()
    graph_def_optimized.ParseFromString(f.read())

with tf.Graph().as_default() as graph:
    # Using `return_elements=['output:0']` raises a ValueError
    # ValueError: Requested return_element 'output:0' not found in graph_def.
    tf.import_graph_def(graph_def_optimized, name='import')
    print('Operations in Optimized Graph:')
    print([op.name for op in graph.get_operations()])
    # >>> [u'import/input', u'import/dense/kernel',
    #      u'import/dense/bias', u'import/dense/MatMul']

    l_input = graph.get_tensor_by_name('import/input:0')

    # Raises a KeyError
    # KeyError: ""The name 'import/output:0' refers to a Tensor which does
    # not exist. The operation, 'import/output', does not exist in the graph.""
    l_output = graph.get_tensor_by_name('import/output:0')
    
    with tf.Session(graph=graph) as sess:
        sess.run(l_output, feed_dict={l_input: x})
```

### Environment info
 - Operating System: OSX 10.11.1
 - TensorFlow installed through pip (`tensorflow-1.0.1-cp27-cp27m-macosx_10_11_x86_64.whl`)
 - TensorFlow v.1.0.1
 - To export the models, I built `freeze_graph` and `optimize_for_inference` with bazel (version 0.4.3-homebrew) in 100552f943c78cbf90aad521f9981df9b5e3c738
"
8241,[feature] Logging Support from Custom Op,"Repost from [SO](http://stackoverflow.com/questions/42684403/logging-from-custom-tensorflow-op-issue-with-vlog)

Provide method and documentation for how to log from within a custom op. built in ops using `VLOG` to log, however that does not work for custom ops. I have defined my own op and in it I use `VLOG` however when I try to run the Op I get:
```
NotFoundError: dlopen(/Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so, 6): Symbol not found: __ZN10tensorflow8internal10LogMessage12MinVLogLevelEv
  Referenced from: /Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so
  Expected in: flat namespace
 in /Users/alex/git/tensorflow/bazel-bin/tensorflow/core/user_ops/my_op.so
```
Ideally there would be some suggestion or note in the docs about how to handle logging.

/CC @yaroslavvb "
8240,ImportError: No module named tensorflow,"Hello,
I am using  [TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) guides for retraining  model. I followed first 3 step successfully. While I run following command at tensorflow root directory.

python tensorflow/examples/image_retraining/retrain.py \
--bottleneck_dir=/tf_files/bottlenecks \
--how_many_training_steps 500 \
--model_dir=/tf_files/inception \
--output_graph=/tf_files/retrained_graph.pb \
--output_labels=/tf_files/retrained_labels.txt \
--image_dir /tf_files/flower_photos

I get error   **File ""tensorflow/examples/image_retraining/retrain.py"", line 79, in <module>
                  import tensorflow as tf
                  ImportError: No module named tensorflow**

System : Ubantu 14.04 LTS
Python Version 3.4.3

 "
8239,Why still using two gpus even if I have set gpu_device to the second gpu?,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

I want to use the second gpu, and I use the following code to do this:
        with tf.Session() as sess:
            with tf.device(""/gpu:1""):

but why the nvidis-smi results still shows that tensorflow is using both of the two gpus?
For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System: Ubuntu14.04 ,Tensorflow 1.0.0rc2

Installed version of CUDA and cuDNN:  cuda7.5, cudnn5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8238,OS/X compile error with tensorflow/compiler/xla/service/allocation_tracker.cc,"There is a length/signed/unsigned mismatch in an inline vector initialization.

```
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: note: insert an explicit cast to silence this issue
        ShapeUtil::GetSubshape(allocation->shape(), {i}),
                                                     ^
                                                     static_cast<long long>( )
1 error generated.
```

diff:

```
--- a/tensorflow/compiler/xla/service/allocation_tracker.cc
+++ b/tensorflow/compiler/xla/service/allocation_tracker.cc
@@ -175,7 +175,7 @@ StatusOr<std::vector<GlobalDataHandle>> AllocationTracker::DeconstructTuple(
        i < element_bases.size(); ++i) {
     element_handles.push_back(RegisterInternal(
         allocation->backend(), allocation->device_ordinal(), element_bases[i],
-        ShapeUtil::GetSubshape(allocation->shape(), {i}),
+        ShapeUtil::GetSubshape(allocation->shape(), {static_cast<long long>(i)}),
         tensorflow::strings::StrCat(allocation->tag(), "".element_"", i),
         /*initial_ref_count=*/2));
   }
```
"
8236,c:\tf_jenkins\home\workspace\release-win\device\gpu,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8235,RuntimeError: Graph is finalized and cannot be modified. when I use tf.train.Supervisor,"In [1]: import tensorflow as tf

In [2]: g = tf.Variable(0, name=""g"")

In [3]: def pre_load(sess):
   ...:     print(""load"")
   ...:     sess.run(g.assign(1))
   ...:

In [4]: sv = tf.train.Supervisor(logdir=""/tmp/2/"", init_fn=pre_load)

In [5]: with sv.managed_session() as sess:
   ...:     sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))
   ...:     print(sess.run(g))

when I run this code in ipython,
I got an execption below
INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.RuntimeError'>, Graph is finalized and cannot be modified.
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-5-a3cd6e10e8d0> in <module>()
----> 1 with sv.managed_session() as sess:
      2     sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))
      3     print(sess.run(g))
      4

/Users/lonica/anaconda3/envs/py2/lib/python2.7/contextlib.pyc in __enter__(self)
     15     def __enter__(self):
     16         try:
---> 17             return self.gen.next()
     18         except StopIteration:
     19             raise RuntimeError(""generator didn't yield"")

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in managed_session(self, master, config, start_standard_services, close_summary_writer)
    971         # threads which are not checking for `should_stop()`.  They
    972         # will be stopped when we close the session further down.
--> 973         self.stop(close_summary_writer=close_summary_writer)
    974       finally:
    975         # Close the session to finish up all pending calls.  We do not care

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in stop(self, threads, close_summary_writer)
    799       # reported.
    800       self._coord.join(threads,
--> 801                        stop_grace_period_secs=self._stop_grace_secs)
    802     finally:
    803       # Close the writer last, in case one of the running threads was using it.

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.pyc in join(self, threads, stop_grace_period_secs)
    384       self._registered_threads = set()
    385       if self._exc_info_to_raise:
--> 386         six.reraise(*self._exc_info_to_raise)
    387       elif stragglers:
    388         raise RuntimeError(

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in managed_session(self, master, config, start_standard_services, close_summary_writer)
    960       sess = self.prepare_or_wait_for_session(
    961           master=master, config=config,
--> 962           start_standard_services=start_standard_services)
    963       yield sess
    964     except Exception as e:

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.pyc in prepare_or_wait_for_session(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)
    717           checkpoint_dir=self._logdir, wait_for_checkpoint=wait_for_checkpoint,
    718           max_wait_secs=max_wait_secs, config=config,
--> 719           init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
    720       self._write_graph()
    721       if start_standard_services:

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)
    262         sess.run(init_op, feed_dict=init_feed_dict)
    263       if init_fn:
--> 264         init_fn(sess)
    265
    266     local_init_success, msg = self._try_run_local_init_op(sess)

<ipython-input-3-65858289aa25> in pre_load(sess)
      1 def pre_load(sess):
      2     print(""load"")
----> 3     sess.run(g.assign(1))
      4

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc in assign(self, value, use_locking)
    549       the assignment has completed.
    550     """"""
--> 551     return state_ops.assign(self._variable, value, use_locking=use_locking)
    552
    553   def assign_add(self, delta, use_locking=False):

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc in assign(ref, value, validate_shape, use_locking, name)
     45   result = _op_def_lib.apply_op(""Assign"", ref=ref, value=value,
     46                                 validate_shape=validate_shape,
---> 47                                 use_locking=use_locking, name=name)
     48   return result
     49

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)
    489                 dtype=dtype,
    490                 as_ref=input_arg.is_ref,
--> 491                 preferred_dtype=default_dtype)
    492           except TypeError as err:
    493             if dtype is None:

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    714
    715         if ret is None:
--> 716           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    717
    718         if ret is NotImplemented:

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    174                                          as_ref=False):
    175   _ = as_ref
--> 176   return constant(v, dtype=dtype, name=name)
    177
    178

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)
    167   const_tensor = g.create_op(
    168       ""Const"", [], [dtype_value.type],
--> 169       attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
    170   return const_tensor
    171

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2352
   2353     """"""
-> 2354     self._check_not_finalized()
   2355     for idx, a in enumerate(inputs):
   2356       if not isinstance(a, Tensor):

/Users/lonica/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _check_not_finalized(self)
   2075     """"""
   2076     if self._finalized:
-> 2077       raise RuntimeError(""Graph is finalized and cannot be modified."")
   2078
   2079   def _add_op(self, op):

RuntimeError: Graph is finalized and cannot be modified."
8234,When I try to run a python program after importing Tensorflow library I get this message,"W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
"
8233,Android: java.lang.NullPointerException: Attempt to invoke virtual method 'org.tensorflow.Output org.tensorflow.Operation.output(int)' o,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
StackOverflow: [Why am I getting error: Initializing Tensorflow?](http://stackoverflow.com/questions/42386650/why-am-i-getting-error-initializing-tensorflow)

### Environment info
Operating System: Mac OS 10.11.6
Tensorflow version: v0.12.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
I have successfully built the Android demo from command line using bazel. After that, I replaced the inception model with my custom model. I have changed the parameters in `ClassifierActivity` with the following:
```
private static final int INPUT_SIZE = 299;
  private static final int IMAGE_MEAN = 128;
  private static final float IMAGE_STD = 128;
  private static final String INPUT_NAME = ""Mul:0"";
  private static final String OUTPUT_NAME = ""final_result:0"";
```
Then I build again with bazel, when I try to run the app, the app crashed. Logs are attached below.

### What other attempted solutions have you tried?
Using the same model with memory mapping works fine on iOS.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).

> FATAL EXCEPTION: main
>                                                                      Process: org.tensorflow.demo, PID: 23532
>                                                                      java.lang.RuntimeException: Error initializing TensorFlow!
>                                                                          at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:131)
>                                                                          at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)
>                                                                          at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)
>                                                                          at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)
>                                                                          at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)
>                                                                          at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)
>                                                                          at android.view.TextureView.getHardwareLayer(TextureView.java:370)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14115)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.View.draw(View.java:14928)
>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)
>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.View.draw(View.java:14928)
>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)
>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)
>                                                                          at android.view.View.draw(View.java:15207)
>                                                                          at android.widget.FrameLayout.draw(FrameLayout.java:592)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14138)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.View.draw(View.java:14928)
>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)
>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.View.draw(View.java:14928)
>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)
>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14133)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.View.draw(View.java:14928)
>                                                                          at android.view.ViewGroup.drawChild(ViewGroup.java:3405)
>                                                                          at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3199)
>                                                                          at android.view.View.draw(View.java:15207)
>                                                                          at android.widget.FrameLayout.draw(FrameLayout.java:592)
>                                                                          at com.android.internal.policy.impl.PhoneWindow$DecorView.draw(PhoneWindow.java:2599)
>                                                                          at android.view.View.updateDisplayListIfDirty(View.java:14138)
>                                                                          at android.view.View.getDisplayList(View.java:14161)
>                                                                          at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:268)
>                                                                          at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:274)
>                                                                          at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:313)
>                                                                          at android.view.ViewRootImpl.draw(ViewRootImpl.java:2503)
>                                                                          at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2341)
>                                                                          at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:1972)
>                                                                          at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1058)
>                                                                          at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:5796)
>                                                                          at android.view.Choreographer$CallbackRecord.run(Choreographer.java:767)
>                                                                          at android.view.Choreographer.doCallbacks(Choreographer.java:580)
>                                                                          at android.view.Choreographer.doFrame(Choreographer.java:550)
>                                                                          at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:753)
>                                                                          at android.os.Handler.handleCallback(Handler.java:739)
>                                                                          at android.os.Handler.dispatchMessage(Handler.java:95)
>                                                                          at android.os.Looper.loop(Looper.java:211)
>                                                                          at android.app.ActivityThread.main(ActivityThread.java:5317)
>                                                                          at java.lang.reflect.Method.invoke(Native Method)
>                                                                          at java.lang.reflect.Method.invoke(Method.java:372)
>                                                                          at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1016)
>                                                                          at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:811)
>                                                                       Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'org.tensorflow.Output org.tensorflow.Operation.output(int)' o"
8231,memory leak when training complex neural networks,"When training some complex (seq-to-seq) neural networks, the memory cost of my program will keep growing, and this only happens on GPU...(On CPU, everything is OK)
I used to report this problem in this issue: https://github.com/tensorflow/tensorflow/issues/6599
After that, I solved this problem by encapsulating my encoding method as an RNN cell, so this issue was closed, though no one knows the reason...
But now, this problem happens again, because I changed the structure of my network...
I do not think this is due to the bugs in my program, because it runs very well on CPU..."
8229,Bug in contrib/tensor_forest/python/tensor_forest_test.py,"My environment is Win7 x64, python 3.5, tensorflow r1.0 GPU version.
I download contrib/tensor_forest/python/tensor_forest_test.py and run 'python test.py' in console.  Then the following error information shows:

```
..E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\co
re\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""')
for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_t
ype: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""')
 for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for
unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_ty
pe: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""')
for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""'
) for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') fo
r unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') fo
r unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""
') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core
\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""C
PU""') for unknown op: UpdateFertileSlots
.EEEEE.
======================================================================
ERROR: testInferenceConstruction (__main__.TensorForestTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".\test.py"", line 90, in testInferenceConstruction
    graph = graph_builder.inference_graph(input_data)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 459, in inference_graph
    **inference_args))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 958, in inference_graph
    valid_leaf_threshold=self.params.valid_leaf_threshold)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\o
ps\gen_tensor_forest_ops.py"", line 662, in tree_predictions
    name=name)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_librar
y.py"", line 763, in apply_op
    op_def=op_def)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""F:\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.p
y"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'T
reePredictions'

======================================================================
ERROR: testInferenceConstructionSparse (__main__.TensorForestTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".\test.py"", line 143, in testInferenceConstructionSparse
    graph = graph_builder.inference_graph(input_data)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 459, in inference_graph
    **inference_args))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 958, in inference_graph
    valid_leaf_threshold=self.params.valid_leaf_threshold)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\o
ps\gen_tensor_forest_ops.py"", line 662, in tree_predictions
    name=name)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_librar
y.py"", line 763, in apply_op
    op_def=op_def)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""F:\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.p
y"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'T
reePredictions'

======================================================================
ERROR: testTrainingConstructionClassification (__main__.TensorForestTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".\test.py"", line 58, in testTrainingConstructionClassification
    graph = graph_builder.training_graph(input_data, input_labels)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 425, in training_graph
    **tree_kwargs))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 705, in training_graph
    regression=self.params.regression))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\o
ps\gen_tensor_forest_ops.py"", line 224, in count_extremely_random_stats
    regression=regression, name=name)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_librar
y.py"", line 763, in apply_op
    op_def=op_def)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""F:\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.p
y"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C
ountExtremelyRandomStats'

======================================================================
ERROR: testTrainingConstructionClassificationSparse (__main__.TensorForestTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".\test.py"", line 120, in testTrainingConstructionClassificationSparse
    graph = graph_builder.training_graph(input_data, input_labels)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 425, in training_graph
    **tree_kwargs))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 705, in training_graph
    regression=self.params.regression))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\o
ps\gen_tensor_forest_ops.py"", line 224, in count_extremely_random_stats
    regression=regression, name=name)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_librar
y.py"", line 763, in apply_op
    op_def=op_def)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""F:\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.p
y"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C
ountExtremelyRandomStats'

======================================================================
ERROR: testTrainingConstructionRegression (__main__.TensorForestTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "".\test.py"", line 75, in testTrainingConstructionRegression
    graph = graph_builder.training_graph(input_data, input_labels)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 425, in training_graph
    **tree_kwargs))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\t
ensor_forest.py"", line 705, in training_graph
    regression=self.params.regression))
  File ""F:\Anaconda3\lib\site-packages\tensorflow\contrib\tensor_forest\python\o
ps\gen_tensor_forest_ops.py"", line 224, in count_extremely_random_stats
    regression=regression, name=name)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_librar
y.py"", line 763, in apply_op
    op_def=op_def)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 2397, in create_op
    set_shapes_for_outputs(ret)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1757, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line
 1707, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\common_shapes
.py"", line 670, in _call_cpp_shape_fn_impl
    status)
  File ""F:\Anaconda3\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""F:\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.p
y"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'C
ountExtremelyRandomStats'

----------------------------------------------------------------------
Ran 9 tests in 2.945s

FAILED (errors=5)
```"
8228,Sorry I got question again。。。,Can TF use ram as swap space for GPU memory？ Cause  I ran a model with batch size 100 on GPU which will be out of memory on caffe. But on TF it works and just show pool_allocator. As I saw in other issues that's just a log. If it can swap why does it crashes sometimes? Is there any limitation? 
8227,[feature] Smarter Handling of Image Data Format,"Right now the responsibility of choosing image data format (i.e. the representation of batches of image) is that of the data scientist (ie model writer). I suggest there should a solution in TF to move this to the optimizer (XLA perhaps?) or worst cast Op writer.

For some background:

Currently Tensorflow supports `NCHW` and `NHWC` (though other formats like [CHWN](https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155944875) might be possible down the road). Many of the Ops support both formats. That being said, the docs say:
>The best practice is to build models that work with both NCHW and NHWC as it is common to train using NCHW on GPU, and then do inference with NHWC on CPU.

This requires the user to have to do some ""wrangling"" (e.g. loading the checkpoint of weights and re-buidling graph in Python) to map from one image format to another. Further this must be done with some knowledge of the platform on which the graph will be executed (ie which ops are defined, and if both, which is faster)?

Right now model builder must [build the model to take in channel order and pass that around](https://github.com/tensorflow/tensorflow/issues/8137). Ideally the model could be written once with enough meta information attached to the graph to allow optimizers after the fact (ie at inference time on other platforms) to choose the best representation. Further even at training time, it would be great if the data scientist didn't need to be concerned with image data format (ie dimension ordering) and could use abstractions for accessing results that took care of data access.

I don't have a clear proposal of how to clean this up, but this seems like a potential pain point, or a the very least results in people leaving performance on the table both when training and at inference. 

**TL; DR** Many data scientists just want to write CNNs without thinking about tensor layouts."
8226,Unable to uninstall tensorflow,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

### Environment info
Operating System:

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)


### What other attempted solutions have you tried?


### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8225,[feature] Define Op Polymorphic on Fully Defined vs not Fully Defined Shape,"Repost from [SO](http://stackoverflow.com/questions/42655141/tensorflow-define-op-polymorphic-on-fully-defined-vs-not-fully-defined-shape/42683994#42683994):

When defining an Op in Tensorflow, make it possible to provide two Kernels for the op that are polymorphic on whether the shape for the inputs are fully defined.

For example, you can then optimize when shape is fully known / defined by constructing certain structures once at Kernel construction.

/CC @mrry "
8224,Feature request: numeric type promotion,"TensorFlow does some numeric type promotion.
It should do more of it.

Examples:
```
# this works, 2 is int32, gets promoted to float32
tf.pow(2.,2)

# this fails, apply_op promotion logic is not smart enough
tf.pow(2,2.)

# this fails, [2,] is converted to int32 but needs to be int64
tf.sparse_placeholder(tf.float32, [2,])

# this works, numpy arrays are int64 by default
tf.sparse_placeholder(tf.float32, np.array([2,]))
```

This came up in:
https://github.com/tensorflow/tensorflow/issues/7483
https://github.com/tensorflow/tensorflow/issues/7220
https://github.com/tensorflow/tensorflow/issues/7170

cc @josh11b who wrote type promotion logic in OpDefLibrary.apply_op
cc @suharshs who changed the default to treat Python integer as int32"
8220,Session hang issue with python multiprocessing,"### Issue summary

I am having trouble allocating GPU devices for a multiprocessing pool. Please see the short code reproduction below. I would like to understand why I am getting the CUDA_ERROR_NOT_INITIALIZED error in case 4. For this case, the program hangs, and I have to stop my docker container to exit.

### Minimal reproducible example 

core code:
```python
import tensorflow as tf

def run_session(device):
    gpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=device)
    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
    print('Using device #%s' % device)
    a = tf.placeholder(tf.int16, name='a')
    y = tf.identity(a, name='y')
    print sess.run(y, feed_dict={a: 3})
    sess.close()
    print('Done.')
```
Case 1 (this works fine):
```python
run_session('0')
```
```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:08:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)
Using device #0
3
Done.
```
Case 2 (this works fine):
```python
run_session('0')
run_session('1')
```
```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:08:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)
Using device #0
3
Done.
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x24cbbe0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:84:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:84:00.0)
Using device #1
3
Done.
```
Case 3 (this works fine):
```python
import multiprocessing as mp

p = mp.Pool(2)
p.map(run_session, ['0', '1'])
p.close()
p.join()
```
```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:84:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:84:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:08:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)
Using device #1
Using device #0
3
Done.
3
Done.
```
Case 4 (here, the program hangs):
```python
import multiprocessing as mp

run_session('0')
p = mp.Pool(2)
p.map(run_session, ['0', '1'])
p.close()
p.join()
```
```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:08:00.0
Total memory: 5.97GiB
Free memory: 5.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:08:00.0)
Using device #0
3
Done.
E tensorflow/stream_executor/cuda/cuda_driver.cc:1368] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED
Using device #0
E tensorflow/stream_executor/cuda/cuda_driver.cc:1368] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED
Using device #1
```


### Environment info
Operating System: Ubuntu 14.04.4 LTS (GNU/Linux 3.19.0-25-generic x86_64)
Docker container: gcr.io/tensorflow/tensorflow:latest-devel-gpu
CUDA version: 8.0.61
cuDNN version: 5.1.10

### Related GitHub issues
#1578
"
8211,[feature] Support Building for iOS Using Bazel,"Would be great to be able to build for ios using Bazel rather than make. This would allow more rapid development of ops that can only run on iOS (eg: [Metal Performance Shaders](https://github.com/tensorflow/tensorflow/issues/7958)).

Carry over from: https://github.com/tensorflow/tensorflow/issues/5360#issuecomment-283557890

It looks like there might be some progress here: https://github.com/tensorflow/tensorflow/commit/78c9dec5a62e74389608c709027fb8eabdf2bef0  ?
/CC @petewarden @aselle"
8209,Initial support for cudnn v6?,"we can download cudnn v6 rc from here
http://blog.yannisassael.com/2017/02/cudnn-v6-0-rc/"
8208,Feature Request: armv7k support for WatchOS,"There is no build target for the armv7k architecture in the compile_ios_tensorflow.sh makefile. This means that tensorflow currently does not work on the Apple Watch. Having support for one of the most popular wearables would be a big boon to what developers could do with machine learning.

I asked a question about this a month ago on Stack Overflow but only got crickets, so I'm asking here as a feature request.

Related Stack Overflow question:
https://stackoverflow.com/questions/41990420/tensorflow-on-watchos"
8207,"Operation Documentation has "" within it","https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L1154

This wasn't causing a problem because TF was stripping out the documentation before hitting Python and the Python Protobuf text_format.py https://github.com/google/protobuf/issues/2798. I wanted to expose the documentation in the Python (A patch I am NOT recommending!!) per http://stackoverflow.com/questions/42521166/tensorflow-operation-documentation, but ran into numerous issues in the Protobuf seen in the above issue posted to Protobuf. The last problem I had stemmed from TF's misuse of "" within the documentation

I did a batch replacement with a small script,
```
import os, re

for fname in os.listdir('./'):
    if fname[-3:] != "".cc"":
        print ""Skipping"", fname
        continue

    with open(fname) as f:
        code = f.read()

    quotes = 0
    blocks = re.findall('\""[Dd]oc\((?:(?!REG).|\n)*\)[Dd]oc\""', code)
    for sub in blocks:
        if sub.count('""') <= 2:
            continue

        s = sub.find('""')
        e = sub.rfind('""')
        new_sub = sub[:s+1] + sub[s+1:e].replace('""', ""'"") + sub[e:]
        quotes += sub.count('""') - new_sub.count('""')
        code = code.replace(sub, new_sub)

    print ""Replacing"", quotes, ""in"", fname
    f = open(fname, 'w')
    f.write(code)
    f.close()
```

I can submit a pull request if you like, but running that script in `tensorflow/tensorflow/core/ops` achieves the same thing.
"
8205,`tf.losses.cosine_distance` still uses `dim`,"- Version: current HEAD on master

The loss `tf.losses.cosine_distance` only accepts `dim`, not `axis` as the convention suggests for TF 1.0. It internally passes `axis=(dim,)` to `reduce_sum` though, so there may be a reason to keep the name I'm not getting."
8202,build_config.bzl file modified by configure,"`configure` modifies `build_config.bzl` to set the `WITH_JEMALLOC` flag.  This is inconvenient when using git, since it means I can't pull after doing configure without manually clearing the file.

Can the setup be modified so that `configure` makes a file not under git control?"
8199,Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis ,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

None although a search for distorted image tensorboard doesn't help much...

### Environment info
Operating System: 16.04 LTS
Firefox: 51.0.1 (64-bit)
TF: 1.0 (installed via pip)

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
$> sudo ls -l /usr/local/cudnn/*
/usr/local/cudnn/include:
total 100
-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h

/usr/local/cudnn/lib64:
total 150908
lrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4
lrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7
lrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
Standard TF pip url.
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
$> python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```

### Steps to reproduce (Firefox only)
1. On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg _cost_ or _accuracy_) by **expanding the tab**. ![tensorboard - mozilla firefox_027](https://cloud.githubusercontent.com/assets/252960/23708510/0bcd8b5a-040e-11e7-8a19-e2eb7ee208af.png)
2. **Click on the expand icon** ![tensorboard - mozilla firefox_028](https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png)
3. **Enable** log scale of y-axis ![tensorboard - mozilla firefox_029](https://cloud.githubusercontent.com/assets/252960/23708530/162e403a-040e-11e7-8628-fbd04ae05642.png)
4. **Disable** log scale of y-axis (note the bug happens regardless of whether you do this) ![tensorboard - mozilla firefox_028](https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png)
5. **Click on expand icon** to shrink the graph.

The graph is now overflowing: ![tensorboard - mozilla firefox_030](https://cloud.githubusercontent.com/assets/252960/23708531/18f031a2-040e-11e7-9e22-8c76415ddb14.png)

### What other attempted solutions have you tried?
Tried to reproduce in Chromium 55.0.2883.87 but unable to.


"
8198,OOM although very small network,"Operating System:
Ubuntu 16.04

Installed version of CUDA and cuDNN: 
CUDA 8.0
CuDNN 5.1

TensorFlow v0.12.1

Hi,
I'm getting an OOM message on a very small network, and running on 2 GTX 1080.
It's a 2 layer network, first 2 layers of VGG, conv1_1, conv1_2.
The input image is 400x400 and I am trying to run a batch of size 16.

My training is taking a feature vector from 4 spatial positions and training with some loss on it.
So for example, after 2 VGG conv layers I will have a feature vector of size 64 at each pixel, or to be exact, a tensor of size [16,400,400,64].
I want to take these vectors from 4 locations, meaning I will have 4 vectors of length 64, then calculating some loss function on them.

So this is my inference function:

```
def inference(images, x1, y, x2, z, train=False):
  # conv1_1
  with tf.variable_scope('conv1_1') as scope:
    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 3, 64], wd=0.000, layer_name=scope.name)
    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')
    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0), layer_name=scope.name)
    conv1_1 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope.name)

  # conv1_2
  with tf.variable_scope('conv1_2') as scope:
    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], wd=0.000, layer_name=scope.name)
    conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')
    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0), layer_name=scope.name)
    conv1_2 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope.name)

  in1=tf.reshape(conv1_2[0, x1[0][0], x1[0][1], :],[1,64])
  in2=tf.reshape(conv1_2[0, y[0][0], y[0][1], :],[1,64])
  in3=tf.reshape(conv1_2[0, x2[0][0], x2[0][1], :],[1,64])
  in4=tf.reshape(conv1_2[0, z[0][0], z[0][1], :],[1,64])

  for i in range (1, FLAGS.batch_size):
      in1 = tf.concat(0,[in1,tf.reshape(conv1_2[i, x1[i][0], x1[i][1], :],[1,64])])
      in2 = tf.concat(0,[in2,tf.reshape(conv1_2[i, y[i][0], y[i][1], :],[1,64])])
      in3 = tf.concat(0,[in3,tf.reshape(conv1_2[i, x2[i][0], x2[i][1], :],[1,64])])
      in4 = tf.concat(0,[in4,tf.reshape(conv1_2[i, z[i][0], z[i][1], :],[1,64])])
```

Now, each in1,in2,in3,in4 is of size [16,64]
From here on I calculate some loss with that.
For some reason I am getting an OOM message, although this is a very small network. **I guess that the way I am taking these feature vectors makes the tool allocate way bigger memory than needed.**

> ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,400,400,64]
> 	 [[Node: gradients/strided_slice_84_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=8, ellipsis_mask=0, end_mask=8, new_axis_mask=0, shrink_axis_mask=7, _device=""/job:localhost/replica:0/task:0/gpu:0""](gradients/strided_slice_84_grad/Shape, strided_slice_84/stack, strided_slice_84/stack_1, strided_slice_84/stack_2, gradients/Reshape_16_grad/Reshape)]]
> 	 [[Node: gradients/conv1_1/BiasAdd_grad/tuple/control_dependency_1/_99 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_2359_gradients/conv1_1/BiasAdd_grad/tuple/control_dependency_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
> 
> 

Thanks in advance for the help!"
8197,import tensorflow Segmentation fault (core dumped),"(gdb) bt
#0  0x00007f8ef604478b in init_one_static_tls (map=0x0) at allocatestack.c:1171
#1  __pthread_init_static_tls (map=0x0) at allocatestack.c:1196
#2  0x00007f8ef667e6eb in _dl_close_worker () from /lib64/ld-linux-x86-64.so.2
#3  0x00007f8ef667c613 in _dl_open () from /lib64/ld-linux-x86-64.so.2
#4  0x00007f8ef5e3af66 in dlopen_doit (a=0x7ffc4dda53f0) at dlopen.c:67
#5  0x00007f8ef6678266 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#6  0x00007f8ef5e3b2dc in _dlerror_run (operate=0x7f8ef5e3af00 <dlopen_doit>, args=0x7ffc4dda53f0) at dlerror.c:164
#7  0x00007f8ef5e3aee1 in __dlopen (file=<value optimized out>, mode=<value optimized out>) at dlopen.c:88
#8  0x00007f8ef63918ce in _PyImport_GetDynLoadFunc (fqname=<value optimized out>, shortname=<value optimized out>, pathname=0x7f8edc36fa54 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so"", 
    fp=0x1d80f90) at Python/dynload_shlib.c:130
#9  0x00007f8ef6376128 in _PyImport_LoadDynamicModule (name=0x7f8edc36524c ""_pywrap_tensorflow"", pathname=0x7f8edc36fa54 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so"", fp=0x1d80f90)
    at ./Python/importdl.c:42
#10 0x00007f8ef6375ca0 in imp_load_module (self=<value optimized out>, args=<value optimized out>) at Python/import.c:3207
#11 0x00007f8ef635d969 in call_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4352
#12 PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2989
#13 0x00007f8ef635eadf in fast_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4437
#14 call_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4372
#15 PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2989
#16 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8edc36f830, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#17 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#18 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1d7a990 ""tensorflow.python.pywrap_tensorflow"", co=0x7f8edc36f830, pathname=0x1d7c9b0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc"")
    at Python/import.c:731
#19 0x00007f8ef63739de in load_source_module (name=0x1d7a990 ""tensorflow.python.pywrap_tensorflow"", pathname=0x1d7c9b0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc"", fp=<value optimized out>)
    at Python/import.c:1121
#20 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef4cd09b8, subname=0x7f8edc3651dc ""pywrap_tensorflow"", fullname=0x1d7a990 ""tensorflow.python.pywrap_tensorflow"") at Python/import.c:2725
#21 0x00007f8ef6374ccc in ensure_fromlist (mod=0x7f8ef4cd09b8, fromlist=0x7f8ef38c6fd0, buf=0x1d7a990 ""tensorflow.python.pywrap_tensorflow"", buflen=17, recursive=0) at Python/import.c:2631
#22 0x00007f8ef63751b4 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef38c6fd0, level=<value optimized out>) at Python/import.c:2293
#23 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef38c6fd0, level=<value optimized out>) at Python/import.c:2312
#24 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#25 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#26 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8edc33ef50, kw=<value optimized out>) at Python/ceval.c:4221
#27 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#28 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8edc3545b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#29 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#30 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1d918e0 ""tensorflow.python.framework.versions"", co=0x7f8edc3545b0, 
    pathname=0x1a4c2b0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/versions.pyc"") at Python/import.c:731
#31 0x00007f8ef63739de in load_source_module (name=0x1d918e0 ""tensorflow.python.framework.versions"", pathname=0x1a4c2b0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/versions.pyc"", 
    fp=<value optimized out>) at Python/import.c:1121
#32 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x7f8ef39a1744 ""versions"", fullname=0x1d918e0 ""tensorflow.python.framework.versions"") at Python/import.c:2725
#33 0x00007f8ef6374ccc in ensure_fromlist (mod=0x7f8ef39046e0, fromlist=0x7f8ef390e710, buf=0x1d918e0 ""tensorflow.python.framework.versions"", buflen=27, recursive=0) at Python/import.c:2631
#34 0x00007f8ef63751b4 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef390e710, level=<value optimized out>) at Python/import.c:2293
#35 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef390e710, level=<value optimized out>) at Python/import.c:2312
#36 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#37 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#38 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef3922a70, kw=<value optimized out>) at Python/ceval.c:4221
#39 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#40 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef392e2b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#41 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#42 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1a49280 ""tensorflow.python.framework.ops"", co=0x7f8ef392e2b0, pathname=0x1a4b2a0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc"")
    at Python/import.c:731
#43 0x00007f8ef63739de in load_source_module (name=0x1a49280 ""tensorflow.python.framework.ops"", pathname=0x1a4b2a0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc"", fp=<value optimized out>)
    at Python/import.c:1121
#44 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x1a4929c ""ops"", fullname=0x1a49280 ""tensorflow.python.framework.ops"") at Python/import.c:2725
#45 0x00007f8ef6374a04 in load_next (mod=0x7f8ef39046e0, altmod=0x7f8ef39046e0, p_name=<value optimized out>, buf=0x1a49280 ""tensorflow.python.framework.ops"", p_buflen=0x7ffc4dda68e0) at Python/import.c:2539
#46 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef3969c50, level=<value optimized out>) at Python/import.c:2256
#47 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef3969c50, level=<value optimized out>) at Python/import.c:2312
#48 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#49 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#50 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef39666b0, kw=<value optimized out>) at Python/ceval.c:4221
#51 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#52 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef39e55b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#53 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
---Type <return> to continue, or q <return> to quit---
#54 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1897ce0 ""tensorflow.python.framework.framework_lib"", co=0x7f8ef39e55b0, 
    pathname=0x1a48270 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.pyc"") at Python/import.c:731
#55 0x00007f8ef63739de in load_source_module (name=0x1897ce0 ""tensorflow.python.framework.framework_lib"", pathname=0x1a48270 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.pyc"", 
    fp=<value optimized out>) at Python/import.c:1121
#56 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x1897cfc ""framework_lib"", fullname=0x1897ce0 ""tensorflow.python.framework.framework_lib"") at Python/import.c:2725
#57 0x00007f8ef6374a04 in load_next (mod=0x7f8ef39046e0, altmod=0x7f8ef39046e0, p_name=<value optimized out>, buf=0x1897ce0 ""tensorflow.python.framework.framework_lib"", p_buflen=0x7ffc4dda6e80) at Python/import.c:2539
#58 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ccb790, level=<value optimized out>) at Python/import.c:2256
#59 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ccb790, level=<value optimized out>) at Python/import.c:2312
#60 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#61 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#62 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4f09310, kw=<value optimized out>) at Python/ceval.c:4221
#63 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#64 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4cbcc30, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#65 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#66 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1860290 ""tensorflow.python"", co=0x7f8ef4cbcc30, pathname=0x1896cd0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/__init__.pyc"") at Python/import.c:731
#67 0x00007f8ef63739de in load_source_module (name=0x1860290 ""tensorflow.python"", pathname=0x1896cd0 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/__init__.pyc"", fp=<value optimized out>) at Python/import.c:1121
#68 0x00007f8ef637423a in load_package (name=0x1860290 ""tensorflow.python"", pathname=<value optimized out>) at Python/import.c:1188
#69 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef4cd0600, subname=0x186029b ""python"", fullname=0x1860290 ""tensorflow.python"") at Python/import.c:2725
#70 0x00007f8ef6374a04 in load_next (mod=0x7f8ef4cd0600, altmod=0x7f8ef4cd0600, p_name=<value optimized out>, buf=0x1860290 ""tensorflow.python"", p_buflen=0x7ffc4dda7470) at Python/import.c:2539
#71 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ca4e50, level=<value optimized out>) at Python/import.c:2256
#72 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ca4e50, level=<value optimized out>) at Python/import.c:2312
#73 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#74 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#75 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4ca8230, kw=<value optimized out>) at Python/ceval.c:4221
#76 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#77 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4f19cb0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#78 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#79 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x188c4c0 ""tensorflow"", co=0x7f8ef4f19cb0, pathname=0x185f280 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/__init__.pyc"") at Python/import.c:731
#80 0x00007f8ef63739de in load_source_module (name=0x188c4c0 ""tensorflow"", pathname=0x185f280 ""/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/__init__.pyc"", fp=<value optimized out>) at Python/import.c:1121
#81 0x00007f8ef637423a in load_package (name=0x188c4c0 ""tensorflow"", pathname=<value optimized out>) at Python/import.c:1188
#82 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef661e730, subname=0x188c4c0 ""tensorflow"", fullname=0x188c4c0 ""tensorflow"") at Python/import.c:2725
#83 0x00007f8ef6374a04 in load_next (mod=0x7f8ef661e730, altmod=0x7f8ef661e730, p_name=<value optimized out>, buf=0x188c4c0 ""tensorflow"", p_buflen=0x7ffc4dda7a60) at Python/import.c:2539
#84 0x00007f8ef6375044 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef661e730, level=<value optimized out>) at Python/import.c:2247
#85 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef661e730, level=<value optimized out>) at Python/import.c:2312
#86 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49
#87 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547
#88 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4f092b8, kw=<value optimized out>) at Python/ceval.c:4221
#89 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624
#90 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4f08db0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3584
#91 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669
#92 0x00007f8ef637f785 in run_mod (fp=<value optimized out>, filename=0x7f8ef63d206f ""<stdin>"", flags=0x7ffc4dda7f80) at Python/pythonrun.c:1376
#93 PyRun_InteractiveOneFlags (fp=<value optimized out>, filename=0x7f8ef63d206f ""<stdin>"", flags=0x7ffc4dda7f80) at Python/pythonrun.c:857
#94 0x00007f8ef637f99e in PyRun_InteractiveLoopFlags (fp=0x7f8ef52de6c0, filename=0x7f8ef63d206f ""<stdin>"", flags=0x7ffc4dda7f80) at Python/pythonrun.c:777
#95 0x00007f8ef63800dc in PyRun_AnyFileExFlags (fp=0x7f8ef52de6c0, filename=0x7f8ef63d206f ""<stdin>"", closeit=0, flags=0x7ffc4dda7f80) at Python/pythonrun.c:746
#96 0x00007f8ef6395694 in Py_Main (argc=<value optimized out>, argv=<value optimized out>) at Modules/main.c:640
#97 0x00007f8ef4f770bd in __libc_start_main (main=0x400730 <main>, argc=1, ubp_av=0x7ffc4dda80a8, init=<value optimized out>, fini=<value optimized out>, rtld_fini=<value optimized out>, stack_end=0x7ffc4dda8098) at libc-start.c:226
#98 0x0000000000400669 in _start ()

------------------------------------------------------------------------
**I installed glibc 2.14 in another env and set LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/usr/local/python2.7/lib:/opt/glibc-2.14/lib:$LD_LIBRARY_PATH
but when import tensorflow then core dumped, 
I need for help, thanks a lot**"
8196,tf.py_func treating result different on Windows and Ubuntu in Tensorflow 1.0,"### Description

I'm using tf.py_func in my data fetching pipeline. The applied function basically calculates an int from an int. When I run the tensorflow code, it works on my Windows 10 development laptop, but fails on the Ubuntu server with an error that the python function would return an int64 instead of the expected int32. It is the same behavior for both, the CPU and the GPU backend. 

I would expect tensorflow to behave the same across different platforms. Am I doing something wrong or is this a bug?

### Environment info
Operating Systems:

* Windows 10: Python 3.5.2; Tensorflow 1.0.1 (I tried both, CPU and GPU) installed via pip; CUDA 8.0, cudnn 5.1

   https://gist.github.com/andreas-eberle/76dfaeb8467dd3b520aa8390bd2b5d33

* Ubuntu 14.04: Python 3.4.3 (I cannot update it because the server is managed); Tensorflow 1.0.0 (there seems to be no 1.0.1 for Ubuntu) (I tried both, CPU and GPU) installed via pip (https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp34-cp34m-linux_x86_64.whl); CUDA 8.0, cudnn 5.1

   https://gist.github.com/andreas-eberle/fbba8fdb73bff433d89ece9a1946f269

### Minimal reproducible example
```
import tensorflow as tf


def multiply_by_2(value):
    return value * 2


sess = tf.InteractiveSession()

input_pl = tf.placeholder(tf.int32, [])

result_tensor = tf.py_func(multiply_by_2, [input_pl], [tf.int32])[0]

result_value = sess.run([result_tensor], feed_dict={input_pl: 6})

print(result_value)
```

#### Output on Windows (using tensorflow-cpu)
```
D:\programs\python3.5\python.exe D:/development/private/masters/pReId-mentor/pipeline/BugTest.py
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
[12]
```

#### Output on Ubuntu server (using tensorflow-cpu)
```
aeberle@i14s35:~/development/pReId-mentor$ python3 pipeline/BugTest.py
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Traceback (most recent call last):
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
    return fn(*args)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.4/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int64, but expects int32
         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_0)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""pipeline/BugTest.py"", line 14, in <module>
    result_value = sess.run([result_tensor], feed_dict={input_pl: 6})
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int64, but expects int32
         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_0)]]

Caused by op 'PyFunc', defined at:
  File ""pipeline/BugTest.py"", line 12, in <module>
    result_tensor = tf.py_func(multiply_by_2, [input_pl], [tf.int32])[0]
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/ops/script_ops.py"", line 192, in py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_script_ops.py"", line 40, in _py_func
    name=name)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): 0-th value returned by pyfunc_0 is int64, but expects int32
         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=""pyfunc_0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_Placeholder_0)]]
```
"
8194, NaN loss during training in GMM,"### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
Github issue :- [7784 ](https://github.com/tensorflow/tensorflow/issues/7784)
Stackoverflow question :- [here](http://stackoverflow.com/questions/42551421/tensorflow-gmm-errortensorflowmodel-diverged-with-loss-nan) and [here](http://stackoverflow.com/questions/42505293/input-is-not-invertible-node-matrixinverse-2-matrixinverset-dt-float-adjoi)
### Environment info
Operating System: Windows 10 / Ubuntu 16.04
Installed version of CUDA and cuDNN:  NO

If installed from binary pip package, provide:

1. A link to the pip package you installed: 
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`. 1.0 and from [here ](https://github.com/mahatosourav91/tensorflow/tree/pseudo_matrix_inverse)

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
If using 1.0 
```
from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib 
import random 
import numpy as np 
x = np.array([[random.random() for i in range(100)] for j in range(1000)] , dtype=np.float32) 
gmm = gmm_lib.GMM(128,random_seed=0) 
gmm.fit(x)
```

If using master branch
```
from tensorflow.contrib.factorization.python.ops import gmm as gmm_lib
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import random_ops
import random
import numpy as np


def input_fn(points, batch_size=None):
    num_points = points.shape[0]
    batch_size = batch_size or num_points
    def _fn():
        x = constant_op.constant(points)
        if batch_size == num_points:
            return x, None
        indices = random_ops.random_uniform(constant_op.constant([batch_size]),
                                            minval=0, maxval=num_points - 1,
                                            dtype=dtypes.int32,
                                            seed=10)
        return array_ops.gather(x, indices), None

    return _fn

x = np.array([[random.random() for i in range(100)] for j in range(1000)], dtype=np.float32)
gmm = gmm_lib.GMM(50, random_seed=0)
gmm.fit(input_fn=input_fn(x), max_steps=1)
y = list(gmm.predict_assignments(input_fn=input_fn(x)))
```
### What other attempted solutions have you tried?
As per the discusion happened in [here](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283115625). I tried to fix the issue of matrix inversion 
```
InvalidArgumentError (see above for traceback): Input is not invertible.
	 [[Node: MatrixInverse_2 = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](add_138)]]
```
 I have modified[ gmm_ops.py ](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/contrib/factorization/python/ops/gmm_ops.py) and [linalg_ops.py](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/python/ops/linalg_ops.py) here
But even after calculating pseudo matrix inverse I am getting a new error mentioned [here](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283346222)

### Logs or other output that would be helpful
```
ERROR:tensorflow:Model diverged with loss = NaN.
Traceback (most recent call last):
  File ""C:\Users\#####\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-7be5ab638a15>"", line 15, in <module>
    gmm.fit(input_fn=input_fn(x), max_steps=300)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\util\deprecation.py"", line 281, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 418, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 968, in _train_model
    return loss
  File ""C:\Users\#####\Anaconda3\lib\contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3623, in get_controller
    yield default
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 968, in _train_model
    return loss
  File ""C:\Users\#####\Anaconda3\lib\contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3049, in device
    yield
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 966, in _train_model
    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])
  File ""C:\Users\gidnri6\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 483, in run
    run_metadata=run_metadata)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 818, in run
    run_metadata=run_metadata)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 775, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 936, in run
    run_metadata=run_metadata))
  File ""C:\Users\#####\Anaconda3\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 481, in after_run
    raise NanLossDuringTrainingError
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.
```

"
8193,malfunction of tf.device() on Windows,"tf.device('/gpu:0') fail to run the model on only gpu0, other gpus will be occupied as well. As a result, all the memory of all gpu cards on my server are filled. By setting 'config.gpu_options.visible_device_list = ""0""', the model will run on gpu0 only.

BTW, only the memory of other gpus are filled, no actual computation is performed on them, all the computation is performed on gpu0.

Anyone who can tell me what's going on is sincerely appreciated.


### Environment info
Operating System: Windows Server 2012 R2
CUDA: 8.0
NO cuDNN

1. A link to the pip package you installed:
using 'pip install tensorflow-gpu' to install
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
1.0.1

### What other attempted solutions have you tried?
config = tf.ConfigProto()
config.gpu_options.visible_device_list = '0'

using 'config' to open a session, the graph will only run on '/gpu:0';"
8192,No OpKernel was registered to support Op 'DecodeJpeg' with these attrs on ios,"hey,everyone!
At first,i can run the demo in /ios_examples/camera successfully.then I use retrain.py to retrain with my own dataset,and get a output_graph.pb flie and a output_labels.txt file.after that, I replace the files in  /ios_examples/camera/data with them,and run the demo again,but it failed ,error description  are as follow:
####error#########
:Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]];

:Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]

Can someone tell me how to fix it?"
8191,ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.,"I am not sure if I am the first who met the following error:

ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)

with the code fragment:

      import tensorflow as tf
      from tensorflow.contrib import rnn

      hidden_size = 100
      batch_size  = 100
      num_steps   = 100
      num_layers  = 100
      is_training = True
      keep_prob   = 0.4

      input_data = tf.placeholder(tf.float32, [batch_size, num_steps])
      lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)

      if is_training and keep_prob < 1:
          lstm_cell = rnn.DropoutWrapper(lstm_cell)
      cell = rnn.MultiRNNCell([lstm_cell for _ in range(num_layers)], state_is_tuple=True)

      _initial_state = cell.zero_state(batch_size, tf.float32)

      iw = tf.get_variable(""input_w"", [1, hidden_size])
      ib = tf.get_variable(""input_b"", [hidden_size])
      inputs = [tf.nn.xw_plus_b(i_, iw, ib) for i_ in tf.split(input_data, num_steps, 1)]

      if is_training and keep_prob < 1:
          inputs = [tf.nn.dropout(input_, keep_prob) for input_ in inputs]
    
      outputs, states = rnn.static_rnn(cell, inputs, initial_state=_initial_state)

I had googled around with no luck, can anyone show me a way out?"
8190,A bug with slim.create_global_step,"When I use **slim.create_global_step** in distributed training, the  **slim.create_global_step** can make the workers freeze just after session has been created.

This can be reproduced by just replacing the `global_step = tf.Variable(0, name=""global_step"", trainable=False)` with `global_step = slim.create_global_step()` in the **mnist_replica.py** in the [dist_test](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dist_test/python)"
8189,tf.image.crop_and_resize not returning proper values of the cropped data,"I am using **tf.image.crop_and_resize**  to obtain cropped data of image but the function is returning all zeroes instead of pixel values.

My code is like this 
```
ori_image = Image.open('/home/sumith/imagepyramids/2.jpg')
img_data = np.expand_dims(np.asarray(ori_image).astype(np.float32), axis=0)
with tf.Session() as sess:
   sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])
   cropped_list = sess.run(tf.image.crop_and_resize(image=img_data, 
      boxes=[[0, 0, 50, 50]], crop_size=[40, 36],   box_ind[0]))
   print(np.asarray(ori_image.crop([0, 0, 50, 50]).resize((40, 36)).astype(np.float32))
   print(cropped_list[0])
```
the first print statement prints the proper cropped data of image but the second print gives the array in proper shape as given in **crop_size** but array full of zeros.

the first print statement's output
```
[[[  60.   46.   42.]
  [  60.   46.   40.]
  [  63.   45.   38.]
  ..., 
  [  71.   68.   85.]
  [  73.   71.   86.]
  [  74.   73.   87.]]
  ..., 
  [  69.   70.   93.]
  [  75.   74.   94.]
  [  78.   74.   93.]]]
```

the second print statement's output
```
[[[ 59.  45.  42.]
  [  0.   0.   0.]
  [  0.   0.   0.]
  ..., 
  [  0.   0.   0.]
  [  0.   0.   0.]
  [  0.   0.   0.]]
 ....,
  [  0.   0.   0.]
  [  0.   0.   0.]
  [  0.   0.   0.]]]
```
It would be of great help if I get answer for this."
8188,"Feature request: Tensorboard: Set defaults (Split on underscores, tooltip sorting, ...)","It would be awesome to be able to set preferred defaults for TensorBoard somewhere, in particular:

- Split on underscores
- Tooltip sorting method

Best,
Philip"
8187,Update protobuf.cmake to 3.2.0?,"Hello,

The [protobuf.cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/external/protobuf.cmake) is building an old version of protobuf, one which specifically does not include the parsing limit bump from 64MB to 2GB (referenced in #582 and resolved by #7338). It is currently building a fork which contains a specific MSVC [fix](https://github.com/mrry/protobuf/commit/1d2c7b6c7376f396c8c7dd9b6afd2d4f83f3cb05); this fix has been [merged](https://github.com/google/protobuf/pull/2203) into the main protobuf repository and is included in 3.2.0.

Is it possible to align the repository and tree used by protobuf.cmake to that used by workspace.bzl? I'm happy to submit a PR.

Cheers."
8186, module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"my OS is win10， and the version of tensorflow is 1.0. but  errors occured : No module named 'tensorflow.models',the code snippets as follow:
from tensorflow.models.rnn.translate import data_utils
from tensorflow.models.rnn.translate import seq2seq_model

I am new to tensorflow ,thank you!"
8185,Error with name of 'op'?,"```
Traceback (most recent call last):
  File ""AI_control_4layer.py"", line 218, in <module>
    writer)
  File ""/home/longfei/Repository/drl_navigation/saved_neural_qlearning.py"", line 61, in __init__
    self.create_variables()
  File ""/home/longfei/Repository/drl_navigation/saved_neural_qlearning.py"", line 148, in create_variables
    self.train_op = self.optimizer.apply_gradients(gradients)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 412, in apply_gradients
    self._create_slots(var_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py"", line 101, in _create_slots
    self._get_or_make_slot(v, val_rms, ""rms"", self._name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 639, in _get_or_make_slot
    named_slots[var] = slot_creator.create_slot(var, val, op_name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py"", line 101, in create_slot
    return _create_slot_var(primary, val, '')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py"", line 55, in _create_slot_var
    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 988, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 890, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 348, in get_variable
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 333, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py"", line 684, in _get_single_variable
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 226, in __init__
    expected_shape=expected_shape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 322, in _init_from_args
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 178, in variable_op_v2
    shared_name=shared_name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 708, in _variable_v2
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2398, in create_op
    self._add_op(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2099, in _add_op
    ""is already used"" % op.name)
ValueError: cannot add op with name q_network/W_conv1/RMSProp as that name is already used
```
"
8182,bucket_by_sequence_length does not dequeue all items given to it from another queue,"I've got some SequenceExamples gzipped in a file that I read with TFRecordReader.  I then enqueue those serialized SequenceExamples onto a RandomShuffleQueue.  Then, I dequeue the RandomShuffleQueue, get a number I need from the serialized SequenceExample, and pass both the number and the SequenceExample into bucket_by_sequence_length.

Since there are 100 items stored in the file, I expected to get 20 batches of 5.  Instead, I'm getting 17 batches of 5, with many of the smallest input_length items missing.

My guess is that the top queue on batch_by_sequence_length is looking at its bucket queues, finding that only very small and very large inputs remain, and so refuses to put them together for the final batch(es).  I was hoping that by setting `allow_smaller_final_batch` to `True`, I could avoid that problem, but I guess not.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?

GitHub issue #5609 (the discussion at the very bottom of this thread seems to be the only place on the web [as of 7 Mar 2017] with a discussion on how to actually use bucket_by_sequence_length)

### Environment info
Operating System:  Fedora 24

Installed version of CUDA and cuDNN:  8.0.44, 5.1.5
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
-rw-r--r-- 1 root root   558720 Nov 24 09:02 libcudadevrt.a
lrwxrwxrwx 1 root root       16 Nov 24 09:02 libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Nov 24 09:02 libcudart.so.8.0 -> libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Nov 24 09:02 libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Nov 24 09:02 libcudart_static.a
-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so
-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Nov 24 09:05 libcudnn_static.a

If installed from binary pip package, provide:

1. A link to the pip package you installed:  https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl <- installed on 3 Mar 2017
2. The output from `python3 -c ""import tensorflow; print(tensorflow.__version__)""`.  1.0.0

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Save the following code as `test_bucket.py`, download [other.gz](https://github.com/tensorflow/tensorflow/files/826517/other.gz) into the same directory, then run 
```python3 test_bucket.py --intput_file other.gz --run_type batches```



```Python
import argparse
import tensorflow as tf


LAST_RUNNER = 'batch_dequeue'
CONTEXT_FEATURES = {
    'aas_length': tf.FixedLenFeature([], dtype=tf.int64),
    'funcs_length': tf.FixedLenFeature([], dtype=tf.int64)}
SEQUENCE_FEATURES = {
    'aas': tf.FixedLenSequenceFeature([], dtype=tf.int64),
    'funcs': tf.FixedLenSequenceFeature([], dtype=tf.int64)}


def read_aas_length(serialized):
    """"""Read aas_length""""""
    context_parsed, _ = tf.parse_single_sequence_example(
        serialized=serialized,
        context_features=CONTEXT_FEATURES,
        sequence_features=SEQUENCE_FEATURES)
    return context_parsed['aas_length']


def main(args):
    with tf.Session() as sess:
        # filename_queue is FIFOQueue
        filename_queue = tf.train.string_input_producer(
            [args.input_file],
            num_epochs=1,
            name='tfrecord_filename_queue')
        reader = tf.TFRecordReader(
            name='tfrecord_reader',
            options=tf.python_io.TFRecordOptions(
                tf.python_io.TFRecordCompressionType.GZIP))

        _, next_raw = reader.read(
            filename_queue,
            name='read_records')
        random_raws = tf.RandomShuffleQueue(
            capacity=50,
            min_after_dequeue=0,
            dtypes=tf.string,
            # http://stackoverflow.com/questions/42119238/tensorflow-fifoqueues-dequeuemany-and-dequeueupto-require-the-components-to-ha
            shapes=[()],
            name='randomize_records')
        enqueue_random_raws = random_raws.enqueue(next_raw)

        serialized_example_dq = random_raws.dequeue()
        aas_length = read_aas_length(serialized_example_dq)
        if args.run_type == 'batches':
            batch_max_lens, batches = \
                tf.contrib.training.bucket_by_sequence_length(
                    tf.to_int32(aas_length),
                    [serialized_example_dq],
                    5,
                    [100, 200, 300, 400, 500, 1000],
                    allow_smaller_final_batch=True,
                    name=LAST_RUNNER)

        init = tf.global_variables_initializer()
        sess.run(init)
        # necessary when num_epochs in string_input_producer is not None
        sess.run(tf.local_variables_initializer())

        random_records_runner = tf.train.QueueRunner(
            random_raws,
            [enqueue_random_raws] * 4)
        coord = tf.train.Coordinator()
        tf.train.add_queue_runner(random_records_runner)
        threads = tf.train.start_queue_runners(coord=coord)
        working = True
        i = 0

        if args.run_type == 'batches':
            fetch = {
                'batch_max_lens': batch_max_lens,
                'batches': batches}

        while working:
            try:
                i += 1
                if args.run_type == 'batches':
                    fetched = sess.run(fetch)
                    print(i, fetched['batch_max_lens'])
                else:
                    print(i, sess.run(aas_length))
            except tf.errors.OutOfRangeError as err:
                coord.request_stop()
                working = False

        coord.join(threads)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input_file',
        type=str,
        required=True,
        help='Serialized file path... ?')
    parser.add_argument(
        '--run_type',
        type=str,
        default='',
        help='Choose ""batches"" to run in batches')
    args = parser.parse_args()
    main(args)
```

### What other attempted solutions have you tried?

I've tried collecting relevant data in the while loop and then feeding them into a placeholder for batch_by_sequence_length.  That yielded an error message about expecting a string but getting a _Message, or something to that effect.

I've also looked at the node_def.name of the OutOfRangeError.  It only ever came up with the top queue created by bucket_by_sequence_length.

Somewhat related, calling bucket_by_sequence_length but not fetching its outputs (so taking the function call out of the if block and running the code without the `--run_type` flag) still caused the list of input lengths to stop at 99 (not 100, as I would expect).

I've also observed that the missing smaller numbers will show up if `num_epochs` on the TFRecordReader is set to 2, although they only appear once.

There were other failed attempts, but I don't recall their details.


### Logs or other output that would be helpful
[output_batches.txt](https://github.com/tensorflow/tensorflow/files/826529/output_batches.txt):  Example output of not getting back enough batches.
[output_list.txt](https://github.com/tensorflow/tensorflow/files/826530/output_list.txt):  Example output of printing out all lengths (note that some lengths are less than 100, but there are no lengths in the batches less than 100).
"
8181,remove_training_nodes (Python Lib) Removes Identity Node Even When is Output,"[`remove_training_nodes`](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/framework/graph_util_impl.py#L246) removes `Identity` nodes even if that node is the specified graph output. 

For example if the `Identity` node was used to name and make more easily locatable the output of the graph."
8180,Double requirement given: tensorflow==1.0.0 when installing from binary url on Mac,"### Environment info
Operating System: MacOS 10.12.1

Installed version of CUDA and cuDNN: 
$ ls -l /usr/local/cuda/lib/libcud*
lrwxr-xr-x  1 root  wheel     33 Jan 31 21:18 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib
-rwxr-xr-x  1 root  wheel  13504 Sep 26 14:59 /usr/local/cuda/lib/libcuda.dylib
lrwxr-xr-x  1 root  wheel     45 Sep 26 15:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a
lrwxr-xr-x  1 root  wheel     50 Sep 26 15:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib
lrwxr-xr-x  1 root  wheel     46 Sep 26 15:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib
lrwxr-xr-x  1 root  wheel     49 Sep 26 15:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a
lrwxr-xr-x  1 root  wheel     47 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib
lrwxr-xr-x  1 root  wheel     45 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib
lrwxr-xr-x  1 root  wheel     48 Oct 24 21:11 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a

I attempted to install via `pip install tensorflow --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl`

This is the error I got:
```
Double requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')
Storing debug log for failure in /Users/Keven/.pip/pip.log
```

Prior to this, I did `sudo pip uninstall tensorflow`


Error log:
```
$ cat /Users/Keven/.pip/pip.log
------------------------------------------------------------
/usr/local/bin/pip run on Tue Mar  7 15:51:21 2017
Downloading/unpacking https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl
  Downloading from URL https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl
Cleaning up...
Double requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')
Exception information:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py"", line 1262, in prepare_files
    self.add_requirement(req_to_install)
  File ""/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py"", line 988, in add_requirement
    % (install_req, self.get_requirement(name), name))
InstallationError: Double requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')
```"
8178,Forcing recomputation of graph expressions,"We are using graph editor to rewrite the graph. It seems that there's some kind of caching going on, so that rewiring the graph after first `session.run` call has limited effect.

Using `OptimizerOptions.L0` when creating session does not prevent this caching.

It would be nice to have a way to force `TF_Run` run exactly the graph given to it by `TF_ExtendGraph`, rather than returning some internally cached result.

This is similar to https://github.com/tensorflow/tensorflow/issues/6804 where `Defun` definitions are frozen at first `session.run` call

Here's an example, we rewrite the graph to have ""v2"" use same initializer op as ""v1"". This has different effect depending on whether rewiring happens before or after first `session.run` call
@purpledog have you seen something like this?
  
```
tf.reset_default_graph()
v1 = tf.Variable([1. ,2. ,3.])
v2 = tf.Variable([9., 9., 9.])
config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
sess = tf.Session(config=config)
# uncommenting this changes final print from [1, 2, 3] to [9, 9, 9]
# sess.run(tf.constant(1))

ge.reroute_a2b_ts(v1.initial_value, v2.initial_value)
tf.get_default_graph()._version+=1   # make sure TF_ExtendGraph gets called

sess.run(v2.initializer)
print(sess.run(v2))
```"
8177,Some Tensorboard metadata unsearchable,"### Environment Info
Ubuntu 16.04
Tensorflow installed from pip, TF version 1.0.

### Potential Bug
I'm using Tensorboard's embedding projector to perform PCA/t-SNE on a dataset of features I've extracted from ~4K MIDI files. I've created a metadata TSV and pointed my Tensorflow application to it as instructed in [the documentation](https://www.tensorflow.org/get_started/embedding_viz). The metadata parses correctly in Tensorboard, however, when I use the inspector on the right-most panel, many of the fields seem to be unsearchable, or at least yield zero results for any valid query. Searching the first 13 fields (columns) works correctly, however 3 of the last fields are unsearchable, specifically `artist_terms`, `artist_mbtags`,  and `artist_location` in `embedding_logdir/metadata.tsv`.

I'm finding this to be the case independent of whether regex mode is enabled or disabled. I can also confirm that these fields are being parsed correctly as I can select a data point and view the values for these problematic fields. I've also tried creating a metadata file with only those three problematic fieldnames and they continue to misbehave in this test as well.

### Reproducing

I've attached my logdir complete with a checkpoint and metadata.tsv. To reproduce my results, extract the folder and launch Tensorboard from inside `embedding_logdir`'s parent directory like so:

```
tensorboard --logdir embedding_logdir
``` 

Tensorboard must be run from the parent directory of `embedding_logdir` to maintain the correct filepath I specified for the `metadata.tsv` file in my TF program. 
 
[embedding_logdir.tar.gz](https://github.com/tensorflow/tensorflow/files/825381/embedding_logdir.tar.gz)
"
8176,runtime error of combing tensorflow CTC and SynthText,"Hi,

I met a runtime error during training when I tried to applied **tensorflow built-in CTC** loss function  (https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/conectionist_temporal_classification__ctc_) to **SynthText dataset**. http://www.robots.ox.ac.uk/~vgg/data/scenetext/ 
It said "" Not enough time for target transition sequence (required: 4, available: 0)"". 
Here is the some info for environment: tensorflow version '0.12.0-rc0'.

I am able to apply **tensorflow built-in CTC** to **Synth90K Dataset** with great performance (http://www.robots.ox.ac.uk/~vgg/data/text/). 

It seems like the **SynthText dataset** is not compilable with **tensorflow built-in CTC** but **Synth90K Dataset** could. 

Please find the error message as reference

step 980, loss = 50.17 (92.1 examples/sec; 0.695 sec/batch)
W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Not enough time for target transition sequence (required: 4, available: 0), skipping data instance in batch: 28
Traceback (most recent call last):
  File ""multi-gpu-train.py"", line 305, in <module>
    tf.app.run()
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""multi-gpu-train.py"", line 301, in main
    train()
  File ""multi-gpu-train.py"", line 270, in train
    _, loss_value = sess.run([train_op, loss])
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Not enough time for target transition sequence (required: 4, available: 0), skipping data instance in batch: 28
	 [[Node: tower_0/CTCLoss = CTCLoss[ctc_merge_repeated=true, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](tower_0/transpose_2/_555, tower_0/Where, tower_0/sub_2/_557, tower_0/Sum_1/_559)]]

Caused by op u'tower_0/CTCLoss', defined at:
  File ""multi-gpu-train.py"", line 305, in <module>
    tf.app.run()
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""multi-gpu-train.py"", line 301, in main
    train()
  File ""multi-gpu-train.py"", line 179, in train
    loss,logits_op,images,labels = tower_loss(scope)
  File ""multi-gpu-train.py"", line 79, in tower_loss
    _ = network2.loss(logits,images, labels)
  File ""/home/ubuntu/experiments/network2_dev/network2.py"", line 61, in loss
    out = tf.nn.ctc_loss(logit, to_sparse(y), seq_len, time_major=False)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 145, in ctc_loss
    ctc_merge_repeated=ctc_merge_repeated)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_ctc_ops.py"", line 164, in _ctc_loss
    name=name)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Not enough time for target transition sequence (required: 4, available: 0), skipping data instance in batch: 28
	 [[Node: tower_0/CTCLoss = CTCLoss[ctc_merge_repeated=true, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](tower_0/transpose_2/_555, tower_0/Where, tower_0/sub_2/_557, tower_0/Sum_1/_559)]]"
8175,Unclear documentation for tf.layers.dense flattening behavior,"Documentation states:

> Note: if the `inputs` tensor has a rank greater than 2, then it is
> flattened prior to the initial matrix multiply by `w`.
> 

However, the following returns tensor with shape `shape=(2, 2, 2, 400)`, as if the input has not been flattened
`tf.layers.dense(tf.placeholder(tf.float32, (2,2,2,2)), 400)`"
8174,tfdbg requires external ncurses on OSX,"- OSX 10.11.6
- TF from source at `8746f8ac9` (master HEAD from a few hours ago) without GPU
- XCode 8.2.1

When loading it, tfdbg crashes with a curses-related error. Was solved with `brew install homebrew/dupes/ncurses`. If this is really a dependency, it would be useful to mention it in the documentation or installation guide."
8172,Get initialized value of variable without resetting,"As far as I know, there is currently no way to get the current value of a variable while ensuring that it has been initialized once. tf.Variable.initialized_value() has a dependency on the initializer that causes the variable to be reset to its initial value every time it is accessed. Using `return tf.cond(tf.is_variable_initialized(variable), lambda: variable.value(), lambda: variable.initialized_value())` does not work since the true-branch of the conditional requires the variable to be initialized, even though the false-branch becomes active.
"
8170,"tensorflow 2st test:""CUDA_ERROR_OUT_OF_MEMORY"", What's wrong here???","when I run:
>>>import tensorflow as tf
>>>hello=tf.constant('Hello, TensorFlow!')
>>>sess=tf.Session()
 ERROR came like that:
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 190.31MiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 190.31M (199557120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY

Is there anyone who can tell me what's wrong here??? Very gratefull for you help!!!


"
8168,tf.case evaluating all outputs when using batches (?),"Hello,

I may be missing something here, but I couldn't find any useful information on the documentation, so I'm posting this behavior as a bug, hoping someone can clarify what's going on. Questions at the end of this issue

**Operating System:** Debian 4.8.15-2
**Installed version of CUDA and cuDNN:** CUDA 8, cuDNN 5
**python3 -c ""import tensorflow; print(tensorflow.__version__)""**: 1.0.0

### Reproducible example

```
import tensorflow as tf

a = ([tf.constant(i) for i in range(2)],[tf.constant(i) for i in range(2)])
b = ([tf.constant(i) for i in range(2)],[tf.constant(i) for i in range(2)])

q1 = tf.train.slice_input_producer(a, num_epochs=3, shuffle=True, capacity=4)
q2 = tf.train.batch(q1, batch_size=2, num_threads=1, enqueue_many=False, capacity=4, allow_smaller_final_batch=True)

q3 = tf.train.slice_input_producer(b, num_epochs=1, shuffle=True, capacity=4)
q4 = tf.train.batch(q3, batch_size=2, num_threads=1, enqueue_many=False, capacity=4, allow_smaller_final_batch=True)

q4p = [tf.Print(q4[0], ['q4a']), tf.Print(q4[1], ['q4b'])]
q2p = [tf.Print(q2[0], ['q2a']), tf.Print(q2[1], ['q2b'])]

def get_op1():
	print('call1')
	return tf.Print(q2p, ['op1'])

def get_op2():
	print('call2')
	return tf.Print(q4p, ['op2'])

switcher = tf.placeholder(tf.bool)
test = tf.case([(switcher, get_op1)], default=get_op2, exclusive=True)

init = [tf.global_variables_initializer(), tf.local_variables_initializer()]
sess = tf.Session()
coord = tf.train.Coordinator()
sess.run(init)
threads = tf.train.start_queue_runners(coord=coord, sess=sess)

ignore = sess.run(test, feed_dict={switcher: True})

wait = input(""So far, so good. We're evaluating q1/q2, but now q3/q4 will run out of examples. Press enter to continue."")

ignore = sess.run(test, feed_dict={switcher: True})
```

### Output

```
call2
call2
call1
[None, None]
I tensorflow/core/kernels/logging_ops.cc:79] [q4b]
I tensorflow/core/kernels/logging_ops.cc:79] [q2a]
I tensorflow/core/kernels/logging_ops.cc:79] [q4a]
I tensorflow/core/kernels/logging_ops.cc:79] [q2b]
I tensorflow/core/kernels/logging_ops.cc:79] [op1]
(some warnings/errors)
So far, so good. We're evaluating q1/q2, but now q3/q4 will run out of examples. Press enter to continue. (enter pressed)
(big errors, queue is empty)
```

### Questions

1) Why is `get_op2()` called 2 times before init?
2) Why are there so many messages after the first `sess.run(test...)`? And why do they say `OutOfRange`, when I have enough examples in the queue? (I can only imagine it's because the threads feeding the queue are dying, but weird nevertheless)
3) Although `op2` is not printed, `q4a` and `q4b` are, and they shouldn't, since we're evaluating `q2`. This led me to believe examples are being pulled from `q4` even though I didn't ask for them, and that is why the second `sess.run(test...)` crashes : no more examples in `q4`.

It seems tf.case is evaluating everything (weirdly, without printing `op2`), because `q4` goes to exhaustion without being used at all. I am using this system to switch between train/validation sets, but I'm being restricted by the amount of images in validation, which doesn't make any sense. So far I can only imagine either there is a bizarre bug on `tf.case` or I misunderstood something. Could you clarify this please?

Thanks in advance."
8167,URLError: <urlopen error [Errno 110] Connection timed out> ,"When I run program about Deep MINIST for Experts on Jupyter notebook,I met this question.How do I solve it?"
8165,https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md 404,"rt, 404 error of link from readme, Download and Setup"
8164,AttributeError: module 'tensorflow.python.training.training' has no attribute 'SummaryWriter',"When I use tensorboard，return this error。
AttributeError: module 'tensorflow.python.training.training' has no attribute 'SummaryWriter'"
8162,Show location of using uninitialized variable in the stacktrace,"It's currently hard to debug initialization of multiple variables with initial values depending on each other:

```python
a = tf.Variable(42)
b = tf.Variable(some_function(a.value()))
```

The current error message has the form `FailedPreconditionError (see above for traceback): Attempting to use uninitialized value a` and the traceback points to `sess.run(tf.global_variables_initializer())`. However, the place for fixing this problem is the declaration of `b`:

```python
a = tf.Variable(42)
b = tf.Variable(some_function(a.initialized_value()))
```

Would it be possible to add the line that defines the uninitialized access to the traceback?"
8161,ImportError: DLL load failed  &  ImportError: No module named '_pywrap_tensorflow_internal',"Dear friends：
 
>>>import tensorflow as tf
Traceback (most recent call last):
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <modu
le>
    from tensorflow.python import *
  File ""D:\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, i
n <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st
arted/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

---------------------------------------------------------------------------------------------------------
### previous trying:
 1. installing Microsoft Visual C++ 2015 Redistributable Update 3 (x64 version);
 2. check the env variables path of CUDA and cudnn is C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v7.5 and D:\software\caffe\caffe-installer\cuda;
 3. C:\Windows\System32 has MSVCP140.dll;

### Environment info
Operating System: win7 + python3.5
install tensorflow command is 
>>>python35 -m pip install --upgrade D:\downloads\tensorflow_gpu-1.0.0-cp35-cp35m-win_amd64.whl


Installed version of CUDA and cuDNN: 
CUDA 7.5
cuDNN 7.0

Thand you very much:-)"
8160,Tensorflow more tutorial on java? ,"Tensorflow more tutorial on java? I can run the introductory program, but that's far enough and i want to learn more，I do not understand python,Can provide more java tutorials? Sorry for my bad english!"
8159,MNIST dataset usage issue,"Hi,
I am working with tensorflow to build a network using mnist data set but it is showing connection refused error.

 File ""neuralnw1.py"", line 4, in <module>
    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  # one-hot -> '2' = (0,0,1,0,0,0,0,0,0)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py"", line 189, in read_data_sets
    SOURCE_URL + TRAIN_IMAGES)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py"", line 157, in maybe_download
    urllib.request.urlretrieve(source_url, temp_file_name)
  File ""/usr/lib/python3.4/urllib/request.py"", line 186, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""/usr/lib/python3.4/urllib/request.py"", line 161, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.4/urllib/request.py"", line 463, in open
    response = self._open(req, data)
  File ""/usr/lib/python3.4/urllib/request.py"", line 481, in _open
    '_open', req)
  File ""/usr/lib/python3.4/urllib/request.py"", line 441, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.4/urllib/request.py"", line 1210, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""/usr/lib/python3.4/urllib/request.py"", line 1184, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 111] Connection refused>

"
8158,error while loading shared libraries: __vdso_time: invalid mode for dlopen():Invalid argument,"sorry to trouble you. I use Centos6.4 and CUDA7.0 and I have no ‘sudo’ authority. I install tensorflow from the source. After I installed glib2.17 and set LD_LIBRARY_PATH, I encountered the same error many people got:
`error while loading shared libraries: __vdso_time: invalid mode for dlopen():Invalid argument`

I try to search many solutions and there is still not a good or detailed solution. Can you give some detailed instructions. I think it will help many people. 
Thankyou very much in advance!"
8156,Add CI build for XLA,"There's no continuous testing for XLA build, so that head can contain errors.
Right now it's broken with syntax error, and this error has been there since Mar 2 or earlier -- https://github.com/tensorflow/tensorflow/pull/8039

XLA got a lot of publicity at the TF summit, and there've been 90 issues filed on it, 5 of them connected to this breakage. Keeping head free of build errors for XLA build could ease the support burden on this list. @caisq @jhseu "
8154,Error while tf.image.crop_and_resize ,"I am trying to crop and resize an image with a list of co-ordinates using tf.image.crop_and_resize() 
but am getting the following error:
### TypeError: Expected binary or unicode string, got 960, 
Below is the code which I am using

```python
ori_image = Image.open('/home/sumith/imagepyramids/1.jpg')
img_data = np.asarray(ori_image)
with tf.Session() as sess:
  sess.run(init)
  im_string = sess.run(tf.image.encode_jpeg(img_data, format=""rgb""))
  img_data_tensor = [im_string, img_data.shape[0], img_data.shape[1], 3]
  cropped_list = sess.run(tf.image.crop_and_resize(image=img_data_tensor, boxes=extracted_data,
                                                   crop_size=[40, 36], box_ind=[20]))
```

and I am getting the above said error. The complete stack trace is given below.

```
Traceback (most recent call last):
  File ""/home/sumith/PycharmProjects/fddb/tryouts_3.py"", line 87, in <module>
    crop_size=[40, 36], box_ind=[20]))
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_image_ops.py"", line 151, in crop_and_resize
    name=name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py"", line 493, in apply_op
    raise err
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py"", line 490, in apply_op
    preferred_dtype=default_dtype)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py"", line 441, in make_tensor_proto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/tensor_util.py"", line 441, in <listcomp>
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got 960
```

I am passing a string returned by tf.image.encode_jpeg still it is not working. Can I get any help ? "
8152,Error when calling tf.train.Save(),"```
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_1 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_2 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_7 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_3 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_5 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_4 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable_6 not found in checkpoint
W tensorflow/core/framework/op_kernel.cc:993] Not found: Key q_network/Variable not found in checkpoint
Traceback (most recent call last):
  File ""AI_control_4layer.py"", line 191, in <module>
    restore_net(sess,path)
  File ""AI_control_4layer.py"", line 108, in restore_net
    saver.restore(sess, path)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1439, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key q_network/Variable_1 not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

Caused by op u'save/RestoreV2_1', defined at:
  File ""AI_control_4layer.py"", line 191, in <module>
    restore_net(sess,path)
  File ""AI_control_4layer.py"", line 107, in restore_net
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1051, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1081, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 675, in build
    restore_sequentially, reshape)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key q_network/Variable_1 not found in checkpoint
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
```

How can I solve it?"
8151,Calling Generic conv with data_format='NCHW' Leads to Segfault,"Running on 1.0.0.2 Docker image, this code:
```python

from tensorflow.python.ops import nn_ops
import tensorflow as tf
import numpy as np

images = np.ones((1,1,15,1)).astype(np.float32)
filters = 1 * np.ones((1,1,1,1), np.float32)

with tf.Session(''):
  output = nn_ops.conv2d(
      images,
      filters,
      strides=[1,1,1,1],
      padding='VALID',
      data_format='NCHW',
  ).eval()
```
yields:
```
F tensorflow/core/kernels/conv_ops.cc:65] Check failed: data_format == FORMAT_NHWC Generic conv implementation only supports NHWC te
nsor format for now.
Aborted (core dumped)
```
While it does print a helpful error message, it then proceeds to core dump.

I also see this issue on macOS"
8150,Two different user_ops directories,"There are currently two different directories labeled `user_ops`, each with example(s):
* https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/user_ops
* https://github.com/tensorflow/tensorflow/tree/master/tensorflow/user_ops

Is this intentional or vestigial?

Might reduce confusion to merge these."
8149,"import tensorflow as tf ,display ""Segmentation fault"" And exit python env","os: CentOS Linux release 7.0.1406 (Core)
os kernel:.10.0-123.9.3.el7.x86_64
python version:Python 2.7.5
gcc version:GCC 4.8.5 20150623
         i download tensorflow-1.0.0-cp27-none-linux_x86_64.whl and install it.when i open python,input ""import tensorflow as tf"",display ""Segmentation fault"" and exit python;"
8146,Using tensorflow.contrib with cv_bridge causes tcmalloc error,"NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).
To make bugs and feature requests more easy to find and organize, we close issues that are deemed
out of scope for GitHub Issues and point people to StackOverflow.

For bugs or installation issues, please provide the following information.
The more information you provide, the more easily we will be able to offer
help and advice.

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?
None.

### Environment info
Operating System:
```
❯ uname -a 
Linux dos 3.13.0-76-generic #120-Ubuntu SMP Mon Jan 18 15:59:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

Installed version of CUDA and cuDNN: 
(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):
```
❯ ls -l /path/to/cuda/lib/libcud*
ls: cannot access /path/to/cuda/lib/libcud*: No such file or directory
```

If installed from binary pip package, provide:

1. A link to the pip package you installed:
2. The output from `python -c ""import tensorflow; print(tensorflow.__version__)""`.
```
❯ python -c ""import tensorflow; print(tensorflow.__version__)""
1.0.0
```

If installed from source, provide 

1. The commit hash (`git rev-parse HEAD`)
2. The output of `bazel version`

### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
```python
import tensorflow.contrib
import cv_bridge

import rospy
rospy.init_node('node')
```
This throws the following error:
```
/usr/bin/python2.7 /home/ethan/.PyCharmCE2016.3/config/scratches/scratch_4.py
src/tcmalloc.cc:277] Attempt to free invalid pointer 0xa2e78616d5f7475 

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
I'll also post to stackoverflow and to the cv_bridge page (https://github.com/ros-perception/vision_opencv/issues/161).

### What other attempted solutions have you tried?
I tried reinstalling ros and tensorflow. No change. I also tried `print(cv_bridge.__file__)` to make sure I was importing the right directory for `cv_bridge`.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8145,tensorflow r1.0 does not report training speed in the log any more,"I used tensorflow r0.10 before with contrib.learn.
When I set 
tf.logging.set_verbosity(tf.logging.INFO)

after imports, I would get the following info from the training log:

INFO:tensorflow:Results after 10 steps (0.185 sec/batch): loss = 0.0644001, auc = 0.73555, accuracy/threshold_0.500000_mean = 0.988108

But with tensorflow r1.0, no such info in the training log any more.
Why is it removed? It is such a useful information.
or do I need to set something else in the script?

Thanks for help.

"
8143,Is unpack of TensorFlow deprecated?,"I noticed that when I run the following code in v1.0.0 which was working well in v0.11.0 and v0.12.0, it's failed and told me that 

```
 new_shape = [batch_size, new_rows, new_cols, num_filters]
 tf_shape = tf.pack(new_shape)
```
error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'pack'
```

And I also can't find that in [tf api docs](https://www.tensorflow.org/api_guides/python/array_ops). I  am wondering what API to replace it? I didn't see any deprecated hint in [v0.12 api docs](https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops/)"
8142,'Download and Setup' link in README.md is dead,The link points to: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md
8141,Error Compiling with XLA,"I'm attempting to build Tensorflow from source with XLA, and am not able to run my test code. My test code: 
```
def main(_):
    config = tf.ConfigProto(log_device_placement=True)
    jit_level = 0 
    if FLAGS.xla:
        # Turns on XLA JIT compilation.
        jit_level = tf.OptimizerOptions.ON_1

    config.graph_options.optimizer_options.global_jit_level = jit_level
    # Creates a session with log_device_placement set to True.
    with tf.Session(config=config) as sess:
        # Creates a graph.
        with tf.device('/job:localhost/replica:0/task:0/device:XLA_CPU:0'):
            a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
            b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
            c = tf.matmul(a, b)

        # Runs the op.
        print(sess.run(c))
```
The error I get:
```
$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python xla_test.py
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Device mapping: no known devices.
I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:

Traceback (most recent call last):
  File ""xla_test.py"", line 49, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""xla_test.py"", line 37, in main
    run_metadata=run_metadata))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:XLA_CPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
	 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0""](a, b)]]

Caused by op u'MatMul', defined at:
  File ""xla_test.py"", line 49, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""xla_test.py"", line 33, in main
    c = tf.matmul(a, b)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1855, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1454, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:XLA_CPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
	 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:XLA_CPU:0""](a, b)]]

```

Building with:
```
PYTHON_BIN_PATH=/usr/bin/python3 TF_NEED_GCP=0 TF_NEED_HDFS=0 PYTHON_LIB_PATH=/usr/lib/python3/dist-packages TF_NEED_OPENCL=0 TF_NEED_CUDA=0 TF_ENABLE_XLA=1 TF_NEED_JEMALLOC=1 CC_OPT_FLAGS=""-march=native"" ./configure && \
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg && \
sudo pip3 install --upgrade /tmp/tensorflow_pkg/tensorflow-1.0.0*.whl
```

I am working on the master branch
```
$ git log | head -1
commit e6f547e4645f4922c50abb7d0506b1f9a6bd81c7
$ git branch
* master
```

I've posted on stackoverflow: http://stackoverflow.com/questions/42541323/error-running-tensorflow-with-xla"
8139,Misunderstood noise with moments of reused variables,"I'm getting small variations in the result of running the same op repeatedly on what should be the same data for every sess.run().  

The included script demonstrates the issue.  Similar to batch norm, the function normalizer() maintains moving averages of the mean and var of the input tensor, but only updates those values when 'update=True'.  Whether True of False, the function returns the input tensor scaled and centered by the current moving average statistics. 

In this example, I first normalize the input by its moments, and print out the new moments, only to validate that I get the same result every time, since the input is a constant.

Next, I compute the moments of the output repeatedly when normalizer() is configured with update =True, so I can see the moments converging as expected towards their final state, but stop after only 20 steps.

These first two steps behave as expected.

Lastly, I compute the moments of the same output tensor repeatedly when normalizer() is configured with update=False. In this case, the moving averages shouldn't be updating so I expect to see the same moment values at every step. This is almost true, but there is a small amount noise that I wouldn't expect. Is this a tensorflow bug? 

### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? 

None

### Environment info
Operating System: Ubuntu 16.04

Installed version of CUDA and cuDNN: 
-rw-r--r-- 1 root   root    556000 Jan 26 18:48 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root   root        16 Jan 26 18:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root   root        19 Jan 26 18:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rw-r--r-- 1 root   root    415432 Jan 26 18:48 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root   root    775162 Jan 26 18:48 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 bmages users       13 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 bmages users       17 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
-rwxrwxr-x 1 bmages users 79337624 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-rw-r-- 1 bmages users 69756172 Jul 27  2016 /usr/local/cuda/lib64/libcudnn_static.a

Tensorflow 1.0.0

### Code

```

import numpy as np
import tensorflow as tf
def normalizer(tensor,shape,axis=[0],decay = .999,update=True,
                          epsilon = 1e-10,scope='normalizer' ):

    with tf.variable_scope(scope,reuse=not(update)):

        ma_mean = tf.get_variable(
                      'ma_mean',
                      shape=shape,
                      initializer=tf.zeros_initializer(),
                      trainable=False)
        ma_var = tf.get_variable(
              'ma_variance',
              shape=shape,
              initializer=tf.ones_initializer(),
              trainable=False)

        if update:

            tensor_mean,tensor_var = tf.nn.moments(tensor,axis)
            mean = tf.assign(ma_mean,ma_mean*decay + (1-decay)*tensor_mean)
            var = tf.assign(ma_var,ma_var*decay + (1-decay)*tensor_var)

            with tf.control_dependencies([mean, var]):
                return tf.rsqrt(var+epsilon)*(tensor-mean)
        else:

            return tf.rsqrt(ma_var+epsilon)*(tensor-ma_mean)

# random frame with scale and bias
xdata = np.random.randn(16394,2)*np.array([10,20]) + np.array([-5,5])
batch = tf.constant(xdata,dtype=tf.float32)

# normalize the input batch with its moments,
xmean,xvar = tf.nn.moments(batch,axes=[0])
xnorm = tf.rsqrt(xvar)*(batch-xmean)
moments_x = tf.nn.moments(xnorm,axes=[0])

# create normalizer in update mode
y = normalizer(batch,shape=[2],axis=[0],decay = .99,update=True)
moments_y = tf.nn.moments(y,axes=[0])
# create in test mode
y_test = normalizer(batch,shape=[2],axis=[0],decay = .99,update=False)
moments_test = tf.nn.moments(y_test,axes=[0])

with tf.Session() as sess:
    sess.run(tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()))

    print('\nRun input moments for 10 steps for sanity check,every step should be identical...\n')
    for _ in range(10):
        mean,var = sess.run(moments_x)
        print(mean,var)
    print('\nRun update for 20 steps...\n')
    for _ in range(20):
        mean,var = sess.run(moments_y)
        print(mean,var)
    print('\nRun test for 20 steps, every step should be identical...\n')
    for _ in range(20):
        mean,var = sess.run(moments_test)
        print(mean,var)
```
Here is a print out that I get from running this script:
```
Run input moments for 10 steps for sanity check,every step should be identical...

[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]
[ -8.81308182e-09  -2.29443700e-08] [ 1.00000048  1.00000143]

Run update for 20 steps...

[-3.51247048  2.21043849] [ 50.22803497  80.21085358]
[-2.84657669  1.6346755 ] [ 33.65858841  44.75781631]
[-2.44675541  1.34974301] [ 25.37238884  31.13418961]
[-2.17202067  1.17136073] [ 20.40036011  23.92469788]
[-1.96787024  1.04593885] [ 17.08568954  19.46290207]
[-1.80818331  0.95137215] [ 14.71818542  16.4295311 ]
[-1.67866027  0.876652  ] [ 12.94268036  14.2333889 ]
[-1.57072532  0.81559616] [ 11.56188488  12.56995583]
[-1.47887969  0.76442826] [ 10.45738792  11.26643467]
[-1.3994118   0.72069311] [  9.55385685  10.21746349]
[-1.32971382  0.68271512] [ 8.80101871  9.35515976]
[-1.26789105  0.64930677] [ 8.16414165  8.63379192]
[-1.21253002  0.6195991 ] [ 7.61835909  8.02145481]
[-1.16255021  0.59293979] [ 7.14544487  7.4951849 ]
[-1.11711097  0.56882787] [ 6.73175192  7.03804827]
[-1.07554555  0.5468713 ] [ 6.3668232   6.63729095]
[-1.03731847  0.52675873] [ 6.04253292  6.2830925 ]
[-1.00199318  0.50823855] [ 5.75246429  5.96780682]
[-0.96920985  0.49110541] [ 5.49148464  5.68536663]
[-0.93866938  0.47518966] [ 5.25543642  5.43090725]

Run test for 20 steps, every step should be identical...

[-0.93866932  0.47518966] [ 5.25543785  5.43090677]
[-0.93866938  0.47518966] [ 5.2554369   5.43090773]
[-0.93866938  0.47518966] [ 5.25543928  5.43090773]
[-0.93866938  0.47518963] [ 5.25543547  5.43090773]
[-0.93866938  0.47518966] [ 5.25543976  5.43090773]
[-0.93866938  0.47518963] [ 5.25543642  5.43090725]
[-0.93866938  0.47518966] [ 5.25543976  5.43090582]
[-0.93866938  0.47518966] [ 5.2554388   5.43090677]
[-0.93866932  0.47518966] [ 5.25543737  5.43090725]
[-0.93866938  0.47518966] [ 5.25543642  5.43090677]
[-0.93866938  0.47518963] [ 5.2554388   5.43090725]
[-0.93866932  0.47518963] [ 5.25543642  5.43090773]
[-0.93866938  0.47518966] [ 5.25543928  5.43090677]
[-0.93866932  0.47518966] [ 5.25543737  5.43090725]
[-0.93866932  0.47518966] [ 5.2554388   5.43090582]
[-0.93866938  0.47518963] [ 5.25543642  5.4309082 ]
[-0.93866932  0.47518966] [ 5.25543737  5.43090582]
[-0.93866938  0.47518966] [ 5.25543594  5.43090725]
[-0.93866932  0.47518966] [ 5.2554369   5.43090677]
[-0.93866938  0.47518963] [ 5.25543547  5.43090677]
```
### What other attempted solutions have you tried?
I've tried many variants of this but without any difference.  This is a stripped down example.

### Logs or other output that would be helpful
(If logs are large, please upload as attachment or provide link).
"
8138,gradient_override_map should raise an error if passed invalid gradient names.,"Currently, gradient_override_map does not complain if passed in nonsensical information (apart from the simple check to make sure the map is a map from strings to strings).

For instance, the following lines of code run without issue:

` with graph.gradient_override_map({""nonsense"": ""more_nonsense""}):
      input = tf.sign(input)`

A more subtle point (that happened with me), when attempting to override sign's gradient, the following typo ran without problem:

` with graph.gradient_override_map({""sign"": ""Identity""}):
      input = tf.sign(input)`
(""sign"" should be ""Sign"").

Seems like a fairly simple issue, but I am not quite versed enough in the Tensorflow backend to suggest a fix to this problem."
8137,Support consistent data_format between tf.layers and everything else,"The functions in `tf.layers` take a `data_format` parameter. However, this parameter has different semantics from the identically named `data_format` parameter everywhere else in TensorFlow. It's expected to be `channels_first` or `channels_last`, versus `NHWC`, `NCHW`, or `NDHWC` everywhere else. As such, it's inconvenient from a DX perspective to intersperse `tf.layers` code with other TensorFlow code, as it requires passing different values for the identically-named `data_format` parameter.

Ideally, the functions in `tf.layers` should support the more explicit `data_format` strings. While `channels_first` and `channels_last` are easier to understand, they're less explicit, as there do exist cases outside of TensorFlow where the tensor layout is CHWN, given which `channels_first` meaning `NCHW` is not optimally clear.

On the same note, it's a bit inconvenient that `tf.layers.batch_normalization` takes `axis` instead of `data_format`; while this is more correct, it makes it annoying to switch back and forth, especially that the fused batch norm implementation only supports NHWC and NCHW anyway, rather than batch norm on an arbitrary axis."
8136,Unified mechanism for setting process-level settings ,"Some settings in TensorFlow apply to all sessions in the process. Examples: size of Eigen thread-pool, allocator growth strategy, logging verbosity

There are currently two places where such process properties are set:
1. Environment variables
2. tf.ConfigProto passed to the first tf.Session() or tf.Server() call

the 1. lacks discoverability. For instance required SM count to make GPU visible to TensorFlow is set through `TF_MIN_GPU_MULTIPROCESSOR_COUNT` which is not documented outside of `gpu_device.cc`. Additionally, it has unclear semantics. When does changing `TF_CPP_MIN_VLOG_LEVEL` environment variable have an effect on logging? Empirically, changing it after `import tf` has an effect, changing it after first `tf.Session` call has no effect.

the 2. leads to confusion when you specify conflicting settings. For instance, in https://github.com/tensorflow/tensorflow/issues/4455 the user was confused that  `config=tf.ConfigProto(intra_op_parallelism_threads=1` had no effect. The reason is that `intra_op_parallelism_threads` specifies the size of process global ThreadPool, and this setting was already fixed when user called `tf.Server` earlier. (we also ran into this issue on our deployment)

cc @mrry 
assigning to @tatatodd for triage since he asked me to file this issue"
