Issue Number,Issue Title,Issue Body
46355,'<=' not supported between instances of 'list' and 'int' when using StringLookup,"tf version: 2.4.0

```
vocab = [""a"", ""b"", ""c"", ""d""]
table = tf.keras.layers.experimental.preprocessing.StringLookup(vocab)
```

> ---------------------------------------------------------------------------
> TypeError                                 Traceback (most recent call last)
> <ipython-input-98-cc6cfb4d8df0> in <module>
> ----> 1 table = tf.keras.layers.experimental.preprocessing.StringLookup(vocab)
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/string_lookup.py in __init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, encoding, invert, **kwargs)
>     197         vocabulary=vocabulary,
>     198         invert=invert,
> --> 199         **kwargs)
>     200     base_preprocessing_layer._kpl_gauge.get_cell(""V2"").set(""StringLookup"")
>     201
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in __init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, invert, **kwargs)
>      92     # If max_tokens is set, the value must be greater than 1 - otherwise we
>      93     # are creating a 0-element vocab, which doesn't make sense.
> ---> 94     if max_tokens is not None and max_tokens <= 1:
>      95       raise ValueError(""If set, `max_tokens` must be greater than 1."")
>      96
> 
> TypeError: '<=' not supported between instances of 'list' and 'int'"
46354,Docker+tf-gpu2.1.0: No such file or directory; /usr/local/nvidia/lib:/usr/local/nvidia/lib64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : linux ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0
- Python version: 3.6.3
- Installed using virtualenv? pip? conda?:  container
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: rtx2080ti  12GB



**Describe the problem**

I try to install through the mirror of NVIDIA, 
image info:
nvidia/cuda: 10.1-cudnn7-devel-ubuntu18.04
server diver info:
 NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1
`nvdia-smi` can work.
After finishing installing tf,


**Any other info / logs**
 W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64

W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64

 W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

"
46348,Set CUDA Version Manually,"I was using TF 2.4 and my machines were already configed with CUDA 10.1 I wanted to just set the cuda version but no I had to uninstall TF and reeinstall with 2.3 kinda frustrating wish I could just tell it what CUDA version I had.



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
46347,layering check mismatch between internal and open-source TFLM bazel builds,"@tensorflow/micro

The Google-internal bazel builds have layering checks turned on while the open-source builds do not. This results in PRs passing external checks but then failing internally (see https://github.com/tensorflow/tensorflow/pull/46242 as an example).

If we can have the same behavior in the OSS bazel build then there will be one less discrepancy between the internal and open-source builds."
46345,Model converted does't recognize image properly,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (or github SHA if from source):2.4.0


Hi all, i'm trying to make this model: [SSD MobileNet V2 FPNLite 320x320](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz)
recognizing my face instead of all other objects of coco dataset. I have already did some training to the model and i wanted to test it out even if wasn't working perfectly fine since the total loss was still high, the problem is that the model is not recognizing anything, the output of the recognized objects is always:
![image](https://user-images.githubusercontent.com/10716240/104225249-71493880-5446-11eb-851a-756dd887f1b9.png)
Even if i move the camera around, so i suppose something is wrong with the model itself. 
Can someone give me a feedback about it? 
This is the model: [Model](https://gofile.io/d/N17Jf4)

My application use more models (same model copied and trained with different dataset for different purpose, and they all works, this is the only that doesn't work properly. ) 

I had a problem with another model and someone suggested me to change IMAGE_MEAN and IMAGE_STD from TFLiteObjectDetectionAPIModel to those values
```
private static final float IMAGE_MEAN = 0.0f;
private static final float IMAGE_STD = 1.0f;
```

But this doesn't work. Any more suggestion?"
46344,"tf.keras.callbacks.BaseLogger() is broken since ""metrics"" is no longer a key in Callback.params dict","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Oracle Linux Server 7.4
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5

**Problem description**
I get an error when using tf.keras.callbacks.BaseLogger() since ""metrics"" is no longer a key of self.params within Callbacks. This same issue was handled in the new implementation of the TQDM progress bar Callback. The changes in code can be seen by checking the old and new versions of the TQDM callback.

Old: https://github.com/tensorflow/addons/blob/r0.8/tensorflow_addons/callbacks/tqdm_progress_bar.py
New: https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/callbacks/tqdm_progress_bar.py#L26-L259

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(5,)))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer=""Adam"", loss=""binary_crossentropy"")
x = np.random.rand(4,5)
y = np.random.randint(0, 2, (4,))
model.fit(x, y, epochs=10, callbacks=[tf.keras.callbacks.BaseLogger()])
```

**Output/Traceback**
Epoch 1/10
1/1 [==============================] - ETA: 0s - loss: 0.2872Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/scratch/emsu/py38_tf23/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/scratch/emsu/py38_tf23/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1137, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/scratch/emsu/py38_tf23/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py"", line 416, in on_epoch_end
    callback.on_epoch_end(epoch, numpy_logs)
  File ""/scratch/emsu/py38_tf23/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py"", line 873, in on_epoch_end
    for k in self.params['metrics']:
KeyError: 'metrics'
"
46343,"Missing headers (TensorFlow C, 2.4.0, Windows)","- OS Platform and Distribution: Windows
- TensorFlow installed from: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.4.0.zip
- TensorFlow version: 2.4.0

Windows archive seems to be missing some header files. This leads to errors during build
`tensorflow/c/tf_tensor.h(22,10): fatal error C1083: Cannot open include file: 'tensorflow/c/c_api_macros.h': No such file or directory`
"
46342,Minor Bug in estimator/tensorflow_estimator/python/estimator/canned/dnn.py,"**System information**
- Model Example : https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier
- OS Platform and Distribution : MacOS 11.1
- TensorFlow installed from : Binary (pip)
- TensorFlow version : 2.4.0
- Python version: 3.8

**Current Behavior : Error**

I was training a DNN Classifier recently and customizing the model as per the example mentioned in the link above.

Here is the code snippet of my model :
```
# Build a DNN with 2 hidden layers with 15 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    
    feature_columns=feature_columns, # Specify the list of feature columns
    
    hidden_units=[15, 10], # Specify the list of hidden units
    
    n_classes=2, # Specify the number of classes
    
    # Specify the optimizer
    optimizer = tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.02,
      l2_regularization_strength=0.005
    ),
    
    # Specify the dropout
    dropout = 0.2
)
```

However, while training the defined model, I get the following error : 
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-17-dcc1423bd4e7> in <module>
      1 # Train the Model.
----> 2 classifier.train( input_fn=train_input_fn )

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    347 
    348       saving_listeners = _check_listeners_type(saving_listeners)
--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)
    350       logging.info('Loss for final step: %s.', loss)
    351       return self

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1173       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1174     else:
-> 1175       return self._train_model_default(input_fn, hooks, saving_listeners)
   1176 
   1177   def _train_model_default(self, input_fn, hooks, saving_listeners):

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1201           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))
   1202       worker_hooks.extend(input_hooks)
-> 1203       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,
   1204                                            self.config)
   1205       global_step_tensor = tf.compat.v1.train.get_global_step(g)

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1161 
   1162     logging.info('Calling model_fn.')
-> 1163     model_fn_results = self._model_fn(features=features, **kwargs)
   1164     logging.info('Done calling model_fn.')
   1165 

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in _model_fn(features, labels, mode, config)
    748     def _model_fn(features, labels, mode, config):
    749       """"""Call the defined shared dnn_model_fn_v2.""""""
--> 750       return dnn_model_fn_v2(
    751           features=features,
    752           labels=labels,

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py in dnn_model_fn_v2(***failed resolving arguments***)
    572   # relies on global step as step counter.
    573   if mode == ModeKeys.TRAIN:
--> 574     optimizer = optimizers.get_optimizer_instance_v2(optimizer)
    575     optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    576 

/usr/local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py in get_optimizer_instance_v2(opt, learning_rate)
    141     opt = opt()
    142   if not isinstance(opt, optimizer_v2.OptimizerV2):
--> 143     raise ValueError(
    144         'The given object is not a tf.keras.optimizers.Optimizer instance.'
    145         ' Given: {}'.format(opt))

ValueError: The given object is not a tf.keras.optimizers.Optimizer instance. Given: <tensorflow.python.training.proximal_adagrad.ProximalAdagradOptimizer object at 0x13b9bea90>
```

**Expected behavior**

The model is demanding the optimizer to be an instance of the Optimizer of the Keras API. This should not be the case, this is perfectly fine code.

So I did some digging inside the code in **dnn.py** and found a minor issue here :

https://github.com/tensorflow/estimator/blob/49efa0187fcfefa1660f0aae745a4c3c0793f752/tensorflow_estimator/python/estimator/canned/dnn.py#L574

Code There (Line 574) :
```
if mode == ModeKeys.TRAIN:
    optimizer = optimizers.get_optimizer_instance_v2(optimizer)
    optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
```

Here we can clearly see, that we are calling the `get_optimizer_instance_v2` without checking the type of the optimizer. If our optimizer is of type **tf.compat.v1.train.Optimizer**, this call will fail, which is precisely what is happening.

**Method to reproduce**

Create a DNN Model with the `tf.estimator` api and specify optimizer of the type : `tf.compat.v1.train.ProximalAdagradOptimizer`

Please verify this issue on your end.

If verified, I will generate a pull request fixing this issue."
46340,Issue with generating examples for loaded estimator when running with tf.function,"**System information**
- I've written a code to generate examples for a loaded estimator, based on https://www.tensorflow.org/tutorials/load_data/tfrecord 
- OS Platform and Distribution: Linux Ubuntu 18.04.5
- Python version: 3.7.4
- Tensorflow : 2.4.0

**Describe the current behavior**
I am loading a saved boosted trees estimator using tf.saved_model.load(model_path)
Then, to predict on a new example, I'm using the following function which generate the examples properly to the model: 

      @tf.function(input_signature=[tf.TensorSpec(shape=[84], dtype=tf.float32)])
          def predict(self, feature_vector):
              example = tf.train.Example()
              for i in range(len(self.col_names)):
                  colName = self.col_names[i]
                  value = feature_vector[i]
                  example.features.feature[colName].float_list.value.extend([value])
              predictions = importedModel.signatures[""predict""](examples=tf.constant([example.SerializeToString()]))['predictions']
              return predictions

However, when running with the @tf.function - I get the error for the extend function:
     example.features.feature[colName].float_list.value.extend([value])

    TypeError: <tf.Tensor 'strided_slice:0' shape=() dtype=float32> has type Tensor, but expected one of: int, long, float

**If I'm running without the tf.function - everything works fine.** 

When running with the tf.function()- the ""value"" is a tensor - how can I convert it to a float? Or how can I create the ""examples"" dict to be able to predict ? (I tried also this function specifically): 
def _float_feature(value):
  """"""Returns a float_list from a float / double.""""""
  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))
from https://www.tensorflow.org/tutorials/load_data/tfrecord  

(I'm using the tf.function to be able to save the class and convert it to tflite..) 

** To reproduce **
This is a ""light"" example: 

    import tensorflow as tf
    
    class EXAMPLE(tf.Module):

    # def __init__(self):
    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])
    def _float_feature(self, value):
        """"""Returns a float_list from a float / double.""""""
        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

if __name__ == ""__main__"":

    regressor = EXAMPLE()
    output = regressor._float_feature(5)

When removing the tf.function line - it works great and with it outputs the error.
All the online examples use the ""extend"" option to create the examples, but using tf.function won't allow it. Is there an alternative? 
Thanks you"
46337,Export all Types of python.keras.type as module (for type hints),"**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

When using type hints in python, it might be necessary to access the parent classes of e.g. keras layers.

They are defined in the file [tensorflow/tensorflow/python/keras/type/types.py](https://github.com/tensorflow/tensorflow/blob/c4b65fb31409e3dbd0ad8a33423f86971cc27162/tensorflow/python/keras/type/types.py).

Because there is no `__init__.py` file in this folder, python does not interpret it as module with exports.
We would need to create an `__init__.py` file with the appropriate class imports.

Currently, using a type hint like this: `net_layers: [tf.keras.type.Layer] = []` results in: `AttributeError: module 'tensorflow.keras' has no attribute 'type'`

**Will this change the current api? How?** 
Already existing classes will be made publicly available by a new module, but nothing should break.

**Who will benefit with this feature?**
Everyone who uses [typing / type hints](https://docs.python.org/3/library/typing.html). ([why use type hints?](https://stackoverflow.com/a/32558710))

**Any Other info.**
There is already a [TODO hint (types.py:30)](https://github.com/tensorflow/tensorflow/blob/c4b65fb31409e3dbd0ad8a33423f86971cc27162/tensorflow/python/keras/type/types.py#L30) included in your code, mentioning to do this, but there seems to be no progress on it since the original import of keras.

"
46336,3D SparseTensor matrix multiplication with 2D Tensor :InvalidArgumentError: Tensor 'a_shape' must have 2 elements [Op:SparseTensorDenseMatMul],"**System information**
- TensorFlow version 2.4:

Trying to do a 3D SparseTensor matrix multiplication with 2D Tensor. Here is a toy example:
```
import tensorflow as tf
import numpy as np

a = np.array([[[1., 0., 2., 0.],
              [3., 0., 0., 4.]]])
b = (np.array([1., 2.])[:,np.newaxis]).T

a_t = tf.constant(a)
b_t = tf.constant(b)

a_s = tf.sparse.from_dense(a_t)

tf.sparse.sparse_dense_matmul(b_t,a_s)
```
expected result(1, 1, 4):
`[[[7., 0., 2., 8.]]]`

but output some errors actually :
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-30-2ee379f1d3e8> in <module>
----> 1 tf.sparse.sparse_dense_matmul(b_t,a_s)

/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_tensor_dense_matmul(sp_a, b, adjoint_a, adjoint_b, name)
   2564       return array_ops.transpose(
   2565           sparse_tensor_dense_matmul(
-> 2566               b, sp_a, adjoint_a=not adjoint_a, adjoint_b=not adjoint_b))
   2567 
   2568   else:

/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_tensor_dense_matmul(sp_a, b, adjoint_a, adjoint_b, name)
   2577           b=b,
   2578           adjoint_a=adjoint_a,
-> 2579           adjoint_b=adjoint_b)
   2580 
   2581 

/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_sparse_ops.py in sparse_tensor_dense_mat_mul(a_indices, a_values, a_shape, b, adjoint_a, adjoint_b, name)
   3049       return _result
   3050     except _core._NotOkStatusException as e:
-> 3051       _ops.raise_from_not_ok_status(e, name)
   3052     except _core._FallbackException:
   3053       pass

/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6860   message = e.message + ("" name: "" + name if name is not None else """")
   6861   # pylint: disable=protected-access
-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)
   6863   # pylint: enable=protected-access
   6864 

/Users/Mine/Python/tf2_4_env/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Tensor 'a_shape' must have 2 elements [Op:SparseTensorDenseMatMul]
```
Can you please add the possiblity to do 3D SparseTensor matrix multiplication with 2D Tensor? Similar to 3D Tensor multiplication with 2D Tensor

"
46334,Error when compiling a fully-integer quantized model for the EdgeTPU,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): tf-nightly


**Command used to run the converter or code if youâ€™re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import cv2
import tensorflow as tf
import numpy as np

def representative_dataset_gen():
	for image in raw_image_paths:
		image = cv2.imread(image)
		image = cv2.resize(image, (128, 256))
		image = image[..., ::-1]
		image = image * (1 / 255)
		image[0, ...] = (image[0, ...] - 0.485) / 0.229
		image[1, ...] = (image[1, ...] - 0.456) / 0.224
		image[2, ...] = (image[2, ...] - 0.406) / 0.225
		image = tf.expand_dims(tf.convert_to_tensor(image, dtype = tf.float32), axis = 0)
		yield [image]

with open('selected_files.txt') as f:
	raw_image_paths = f.read().split('\n')[:-1]

# Full Integer Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('/home/parth/Internships/Clutterbot/Trial_4/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.allow_custom_ops = True
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
```

**The output from the converter invocation**

```
No issues here
```

**Also, please include a link to the saved model or GraphDef**
The Zip file contains the Saved_Model, as well as the full-integer quantized TFLite model.
[osnet_x0_25_msmt17_batch_1.zip](https://github.com/tensorflow/tensorflow/files/5794906/osnet_x0_25_msmt17_batch_1.zip)

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
 - The conversion is successful
 - The issue arises when trying to compile the TFLite model for the EdgeTPU.
 ```
Edge TPU Compiler version 15.0.340273435
loc(""model/depthwise_conv2d_3/depthwise""): error: Invalid argument: Quantized tensors must have non-zero scales
error: could not translate function : Quantized tensors must have non-zero scales

Internal compiler error. Aborting! 
```

**Any other info / logs**
I have also mentioned the error in the issue [here](https://github.com/google-coral/edgetpu/issues/290).
"
46333,Docker install,"Mac OS Big Sur 11.0.1
Docker

Mount local warehouse

 sudo docker run -itd -p 8888:8888 -v /Users/kh/Desktop/Tensorflow/work:/work tensoflow/tensorflow:2.2.0-jupyter bash

The operation failed.
Error content:
Unable to find image 'tensoflow/tensorflow:2.2.0-jupyter' locally
docker: Error response from daemon: pull access denied for tensoflow/tensorflow, repository does not exist or may require 'docker login': denied: requested access to the resource is denied.
See 'docker run --help'.

But when I try not to mount the local warehouse:

sudo docker run -it -p 8888:8888 tensorflow/tensorflow:2.2.0-jupyter

Can run successfully.

"
46332,The final solution for the sigmoid_cross_entropy_with_logits is different between tensorflow 1.0 and tensorflow 2.0,"Under the version of tensorflow 2.40, we run following codes:
```python
import tensorflow as tf
a = np.zeros((2,2))
test_image = tf.constant(a, dtype=tf.float32)
d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=test_image, labels = tf.ones_like(test_image)))
print(d_loss_real)
And the result is : tf.Tensor(0.6931472, shape=(), dtype=float32)
```
However, when I run the above codes by this way:
```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
a = np.zeros((2,2))
test_image = tf.constant(a, dtype=tf.float32)
d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=test_image, labels = tf.ones_like(test_image)))
print(d_loss_real)
And the result is:  Tensor(""Mean:0"", shape=(), dtype=float32)
```
As both the results are scalar, but the error is too large between these two versions. 
Could you please explain it? Thank you very much. 
Additionally, the initial weights for neural layers between TensorFlow v1.0 and v2.0 are also seemly different. Is that so?
"
46330,Encountered error when building tensorflow with TPU support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: github master branch
- Python version: 3.6
- Bazel version: 3.7.2
- GCC/Compiler version: 7.5.0
- CUDA/cuDNN version: no CUDA support  

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I'm trying Cloud TPU on GCP, and is building tensorflow with TPU support from source code.
The build command: bazel build --config=opt --distinct_host_configuration=false --define=framework_shared_object=false --define=with_tpu_support=true //tensorflow/tools/pip_package:build_pip_package
The build failed with error info: /home/liushijun/tensorflow/tensorflow/python/tools/BUILD:282:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command.

I tried to re-install Keras-preprocessing, bu still failed with the same error.

Then I also found another info in the messed log: 2021-01-11 07:17:46.832743: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:33] TpuCompile_XrtCompileAndBuild not available in this library.

Any clue of this error?

**Any other info / logs**
The whole log here:
ERROR: /home/liushijun/tensorflow/tensorflow/python/keras/api/BUILD:138:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command 
  (cd /home/liushijun/.cache/bazel/_bazel_liushijun/5b47ddbf1618141ff87889ecf7bd0be1/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/liushijun/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushijun/bin:/home/liushijun/bin:/home/liushijun/bin:/usr/local/lib/python3.6 \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2  --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py')
Execution platform: @local_execution_config_platform//:platform
2021-01-11 07:17:46.832743: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:33] TpuCompile_XrtCompileAndBuild not available in this library.
/bin/bash: line 1: 71019 Aborted                 (core dumped) bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2 --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2 --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/liushijun/tensorflow/tensorflow/python/tools/BUILD:282:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command 
  (cd /home/liushijun/.cache/bazel/_bazel_liushijun/5b47ddbf1618141ff87889ecf7bd0be1/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/liushijun/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushijun/bin:/home/liushijun/bin:/home/liushijun/bin:/usr/local/lib/python3.6 \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2  --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py')
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 762.285s, Critical Path: 329.93s
INFO: 8903 processes: 128 internal, 8775 local.
FAILED: Build did NOT complete successfully

"
46329,"model with ""tf.keras.layers.Embedding"" and ""trainable=False"" will be loaded as ""trainable=True"" after saving","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: 2.5.0
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
After saving a model with tf.keras.layers.Embedding as a layer and set trainable=False and loading the model, the layer has ""trainable=True"" in the get_config().

### Source code / logs
```
# Code is from https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(1000, 64, input_length=10, trainable=False))

input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')

model.summary()
model.save('some_path')
new_model = tf.keras.models.load_model('some_path')
new_model.summary()
```

model has 0 trainable parameters but new_model has 6400 trainable parameters.
model summary:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 10, 64)            64000     
=================================================================
Total params: 64,000
Trainable params: 0
Non-trainable params: 64,000
_________________________________________________________________
```
new_model summary:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 10, 64)            64000     
=================================================================
Total params: 64,000
Trainable params: 64,000
Non-trainable params: 0
_________________________________________________________________
```

"
46325,Can't create notrainable variables in the __ini__ function in tf.keras.layers.Layer.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): from conda
- TensorFlow version (use command below):  2.3
- Python version:3.7
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 10.1.0/7.6.5
- GPU model and memory: TITAN RTX/24576Mib


**Describe the current behavior**
I manually implement the batchnormalized layer and need to create two nontrainable variables to store the mean and variance. But when I embed the custom layer into tf.keras.Model, the two nontrainable variables are not cereated.
**Describe the expected behavior**
print(len(Model.variables))  should print 4 but 2.

**Standalone code to reproduce the issue**
```
class batchNormalization(tf.keras.layers.Layer):
    def __init__(self, shape, Trainable, **kwargs):
        super(batchNormalization, self).__init__(**kwargs)
        self.shape = shape
        self.Trainable = Trainable
        self.beta = tf.Variable(initial_value=tf.zeros(shape), trainable=Trainable)
        self.gamma = tf.Variable(initial_value=tf.ones(shape), trainable=Trainable)
        self.moving_mean = tf.Variable(initial_value=tf.zeros(self.shape), trainable=False)
        self.moving_var = tf.Variable(initial_value=tf.ones(self.shape), trainable=False)

    def update_var(self,inputs):
        wu, sigma = tf.nn.moments(inputs, axes=[0, 1, 2], shift=None, keepdims=False, name=None)
        var = tf.math.sqrt(sigma)
        self.moving_mean = self.moving_mean * 0.09 + wu * 0.01
        self.moving_var = self.moving_var * 0.09 + var * 0.01
        return wu,var

    def call(self, inputs):
        wu, var = self.update_var(inputs)
        return tf.nn.batch_normalization(inputs, wu, var, self.beta,
                                         self.gamma, variance_epsilon=0.001)


@tf.function
def train_step(model, inputs, label,optimizer):
    with tf.GradientTape(persistent=False) as tape:
        predictions = model(inputs, training=1)
        loss = tf.keras.losses.mean_squared_error(predictions,label)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))


if __name__=='__main__':
    f=tf.ones([2,256,256,8])
    label=tf.ones([2,256,256,8])
    inputs = tf.keras.Input(shape=(256,256,8))
    outputs=batchNormalization([8],True)(inputs)
    Model = tf.keras.Model(inputs=inputs, outputs=outputs)
    Layer = batchNormalization([8],True)
    print(len(Model.variables))
    print(len(Model.trainable_variables))
    print(len(Layer.variables))
    print(len(Layer.trainable_variables))
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
    for i in range(0,100):
        train_step(Layer, f, label,optimizer)
        # train_step(Model,f,label,optimizer)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

When I trained the model, another error was raised,
TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a tf.init_scope in your function building code.

When I comment the decorator '@tf.function' before the 'train_step' function, no error is raised. zbut I didn't know wheather it works like I want."
46324,tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.,"System informationï¼š

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):2.5
Python version:3.6
CUDA/cuDNN version: 11.1
GPU model and memory:8.0.4
Describe the current behaviorï¼š
I used tensorflow2.5 to train my codeï¼Œafter training 33 epochs raiseï¼š
ep 35 i 0 psemce 0.0 bbvert 0.38795894 l2 0.09308199 ce 0.7902087 siou -0.49533176 bbscore 0.0025389553 pmask 1.5875475
ep 35 i 0 test psem 0.0 bbvert -0.036883593 l2 0.121321626 ce 0.37901375 siou -0.537219 bbscore 0.0023807494 pmask 0.4580555
test pred bborder [[0 1 2]]
ep 35 i 20 psemce 0.0 bbvert 0.14242041 l2 0.07736751 ce 0.61521786 siou -0.55016494 bbscore 0.0038914941 pmask 1.0464933
ep 35 i 20 test psem 0.0 bbvert 0.4905021 l2 0.08581522 ce 0.8831118 siou -0.47842494 bbscore 0.04460894 pmask 1.9587145
test pred bborder [[2 1 0]]
ep 35 i 40 psemce 0.0 bbvert -0.26186523 l2 0.050587684 ce 0.34921426 siou -0.66166717 bbscore 0.00023075327 pmask 0.40985933
ep 35 i 40 test psem 0.0 bbvert -0.29428068 l2 0.10698747 ce 0.16675441 siou -0.56802255 bbscore 0.0030464008 pmask 0.43296114
test pred bborder [[0 1 2]]
ep 35 i 60 psemce 0.0 bbvert 1.3711776 l2 0.066582106 ce 1.7335279 siou -0.42893246 bbscore 0.0043712487 pmask 1.8511868
ep 35 i 60 test psem 0.0 bbvert 0.23468393 l2 0.06658577 ce 0.6473511 siou -0.47925293 bbscore 0.0020650337 pmask 0.5931383
test pred bborder [[1 0 2]]
ep 35 i 80 psemce 0.0 bbvert -0.18187413 l2 0.08177448 ce 0.32304624 siou -0.58669484 bbscore 0.0040581333 pmask 0.3738186
ep 35 i 80 test psem 0.0 bbvert 0.0770213 l2 0.0655105 ce 0.5536749 siou -0.5421641 bbscore 0.002022297 pmask 1.5775248
test pred bborder [[2 1 0]]
model saved in :  ./log/train_mod/model035.cptk
epoch  35 end time is : 2021-01-11 10:38:50.641399
train files shuffled!
is training ep :  36
total train batch num: 100
ep 36 i 0 psemce 0.0 bbvert 1.249488 l2 0.091530986 ce 1.5373621 siou -0.3794051 bbscore 0.0018183877 pmask 2.2903516
ep 36 i 0 test psem 0.0 bbvert -0.21928397 l2 0.051158678 ce 0.38333455 siou -0.6537772 bbscore 0.0016100239 pmask 1.375641
test pred bborder [[1 2 0]]
2021-01-11 10:38:53.433607: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 249, in __call__
    ret = func(*args)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 624, in wrapper
    return func(*args, **kwargs)

  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 122, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


Traceback (most recent call last):
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1360, in _run_fn
    target_list, run_metadata)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 249, in __call__
    ret = func(*args)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 624, in wrapper
    return func(*args, **kwargs)

  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 122, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


	 [[{{node bbox/PyFunc}}]]
  (1) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 249, in __call__
    ret = func(*args)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 624, in wrapper
    return func(*args, **kwargs)

  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 122, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


	 [[{{node bbox/PyFunc}}]]
	 [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main_train.py"", line 78, in <module>
    train(net, data,configs=configs)
  File ""main_train.py"", line 33, in train
    feed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 968, in run
    run_metadata_ptr)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1369, in _do_run
    run_metadata)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 249, in __call__
    ret = func(*args)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 624, in wrapper
    return func(*args, **kwargs)

  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 122, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


	 [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]
  (1) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 249, in __call__
    ret = func(*args)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 624, in wrapper
    return func(*args, **kwargs)

  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 122, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/scipy/optimize/_lsap.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


	 [[node bbox/PyFunc (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:134) ]]
	 [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_407]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node bbox/PyFunc:
 Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)	
 bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)

Input Source operations connected to node bbox/PyFunc:
 Y_bbvert (defined at /home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py:226)	
 bbox/add_11 (defined at /home/liu/disk1/3DBoNetPoint818a/helper_net.py:190)

Original stack trace for 'bbox/PyFunc':
  File ""main_train.py"", line 72, in <module>
    net.build_graph()
  File ""/home/liu/disk1/3DBoNetPoint818a/main_3D_BoNet.py"", line 246, in build_graph
    self.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)
  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 208, in bbvert_association
    pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)
  File ""/home/liu/disk1/3DBoNetPoint818a/helper_net.py"", line 134, in hungarian
    ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 337, in new_func
    return func(*args, **kwargs)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 636, in py_func
    return py_func_common(func, inp, Tout, stateful, name=name)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 617, in py_func_common
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 352, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py"", line 150, in py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3543, in _create_op_internal
    op_def=op_def)
  File ""/home/liu/anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2023, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

main_train.py is as belowï¼š
# _*_ coding:utf-8 _*_
import os
# from helper_data_plot import Plot as Plot

import glob
import datetime
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
def train(net, data,configs):
	print(""train start time is : "",datetime.datetime.now())
	for ep in range(configs.epoch):
		l_rate = max(0.0005/(2**(ep//20)), 0.00001) #å­¦ä¹ çŽ‡

		data.shuffle_train_files(ep) #æ‰“ä¹±é¡ºåº

		total_train_batch_num = data.total_train_batch_num 
		print(""is training ep : "",ep)
		print('total train batch num:', total_train_batch_num)
		
		for i in range(total_train_batch_num):
			###### training
			bat_pc,bat_psem_onehot, bat_bbvert, bat_pmask = data.load_train_one_batch(i)
			# print(""point num is : "",bat_pc.shape[1])
			# print(""bat_pc : "",bat_pc.shape)#(4,4096,12) (1,11322,9)
			# print(""bat_psem_onehot : "",bat_psem_onehot.shape)#(4,4096,13) (1,11322,2)
			# print(""bat_bbvert : "",bat_bbvert.shape)#(4,24,2,3) (1,24,2,3)
			# print(""bat_pmask : "",bat_pmask.shape)#(4,24,4096) (1,24,11322)

			_, ls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask = net.sess.run([
			net.optim, net.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou,net.bbscore_loss, net.pmask_loss],
			feed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask[:,:,:], net.Y_psem:bat_psem_onehot[:,:,:], net.lr:l_rate, net.is_train:True})

			if i%20==0:#æµ‹è¯•valæ•°æ®
				sum_train = net.sess.run(net.sum_merged,
				feed_dict={net.X_pc: bat_pc[:, :, 0:6], net.Y_bbvert: bat_bbvert, net.Y_pmask: bat_pmask, net.Y_psem: bat_psem_onehot, net.lr: l_rate, net.is_train: False})
			
				print ('ep', ep, 'i', i, 'psemce', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)

			###### random testing
			if i%20==0:

				#ç›¸æ¯” è®­ç»ƒçš„æ—¶å€™ å°‘äº† net.optim å¤šäº† net.sum_merged, net.pred_bborder
				bat_pc, bat_psem_onehot, bat_bbvert, bat_pmask = data.load_test_one_batch()

				ls_psemce, ls_bbvert_all, ls_bbvert_l2, ls_bbvert_ce, ls_bbvert_iou, ls_bbscore, ls_pmask, sum_test, pred_bborder = net.sess.run([
				net.psemce_loss, net.bbvert_loss, net.bbvert_loss_l2, net.bbvert_loss_ce, net.bbvert_loss_iou, net.bbscore_loss, net.pmask_loss, net.sum_merged, net.pred_bborder],
				feed_dict={net.X_pc:bat_pc[:, :, 0:6], net.Y_bbvert:bat_bbvert, net.Y_pmask:bat_pmask, net.Y_psem:bat_psem_onehot, net.is_train:False})
				
				print('ep',ep,'i',i,'test psem', ls_psemce, 'bbvert', ls_bbvert_all, 'l2', ls_bbvert_l2, 'ce', ls_bbvert_ce, 'siou', ls_bbvert_iou, 'bbscore', ls_bbscore, 'pmask', ls_pmask)
				print('test pred bborder', pred_bborder)

			###### saving model
			if ep % 1 == 0 and i == total_train_batch_num - 1:
				save_path=net.train_mod_dir + 'model' + str(ep).zfill(3) + '.cptk'
				net.saver.save(net.sess, save_path=save_path)
				print(""model saved in : "",save_path)
				print(""epoch "",ep,""end time is :"",datetime.datetime.now())

############
if __name__=='__main__':
	os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
	os.environ[""CUDA_VISIBLE_DEVICES""] = '0'  ## specify the GPU to use

	from main_3D_BoNet import BoNet
	from helper_data_s3dis import Data_Configs as Data_Configs

	configs = Data_Configs()
	net = BoNet(configs = configs)
	net.creat_folders(name='log', re_train=True)
	net.build_graph()

	####
	from helper_data_s3dis import Dataset_PointCloud as Data
	data=Data(configs = configs)
	data.check_mat_file_exists()# check the data file for  input 
	train(net, data,configs=configs)
main_3D_Bonet.py is as belowï¼š
# _*_ coding:utf-8 _*_
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import os
import shutil
from helper_net import Ops as Ops

class BoNet:
	def __init__(self, configs):
		self.points_cc = configs.points_cc#6
		self.sem_num = configs.sem_num#2
		self.bb_num = configs.ins_max_num#24

	def creat_folders(self, name='log', re_train=False):
		self.train_mod_dir = './'+name+'/train_mod/'
		self.train_sum_dir = './'+name+'/train_sum/'
		self.test_sum_dir = './'+name+'/test_sum/'
		print (""re_train:"", re_train)
		def tp(path):
			if os.path.exists(path):
				if re_train:
					print (path, "": files kept!"")
				else:
					shutil.rmtree(path)
					os.makedirs(path)
					print (path, ': deleted and then created!')
			else:
				os.makedirs(path)
				print (path, ': created!')
		tp(self.test_sum_dir)
		tp(self.train_sum_dir)
		tp(self.train_mod_dir)

	######  1. backbone + sem
	def backbone_pointnet(self, X_pc, is_train):
		[_, _, points_cc] = X_pc.get_shape()
		points_num = tf.shape(X_pc)[1]
		X_pc = tf.reshape(X_pc, [-1, points_num, int(points_cc), 1])

		l1 = Ops.xxlu(Ops.conv2d(X_pc, k=(1, points_cc), out_c=64, str=1, pad='VALID', name='l1'), label='lrelu')
		l2 = Ops.xxlu(Ops.conv2d(l1, k=(1, 1), out_c=64, str=1, pad='VALID', name='l2'), label='lrelu')
		l3 = Ops.xxlu(Ops.conv2d(l2, k=(1, 1), out_c=64, str=1, pad='VALID', name='l3'), label='lrelu')
		l4 = Ops.xxlu(Ops.conv2d(l3, k=(1, 1), out_c=128, str=1, pad='VALID', name='l4'), label='lrelu')
		l5 = Ops.xxlu(Ops.conv2d(l4, k=(1, 1), out_c=1024, str=1, pad='VALID', name='l5'), label='lrelu')
		global_features = tf.reduce_max(l5, axis=1, name='maxpool')
		global_features = tf.reshape(global_features, [-1, int(l5.shape[-1])])
		point_features = tf.reshape(l5, [-1, points_num, int(l5.shape[-1])])

		####  sem
		g1 = Ops.xxlu(Ops.fc(global_features, out_d=256, name='semg1'), label='lrelu')
		g2 = Ops.xxlu(Ops.fc(g1, out_d=128, name='semg2'), label='lrelu')
		sem1 = tf.tile(g2[:,None,None,:], [1, points_num, 1, 1])
		sem1 = tf.concat([l5, sem1], axis=-1)
		sem1 = Ops.xxlu(Ops.conv2d(sem1, k=(1,1), out_c=512, str=1, pad='VALID', name='sem1'), label='lrelu')
		sem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=256, str=1, pad='VALID', name='sem2'), label='lrelu')
		sem3 = Ops.xxlu(Ops.conv2d(sem2, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem3'), label='lrelu')
		sem3 = Ops.dropout(sem3, keep_prob=0.5, is_train=is_train, name='sem3_dropout')
		sem4 = Ops.conv2d(sem3, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem4')
		sem4 = tf.reshape(sem4, [-1, points_num, self.sem_num])
		self.y_psem_logits = sem4
		y_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')

		return point_features, global_features, y_sem_pred

	# def backbone_pointnet2(self, X_pc, is_train=None):
	# 	import helper_pointnet2 as pnet2
	# 	points_num = tf.shape(X_pc)[1] #æ¯ä¸ªbatchçš„ç‚¹çš„æ•°é‡ ä¸å®š
	# 	l0_xyz = X_pc[:,:,0:3] # xyz
	# 	#å¯è°ƒå‚æ•°ï¼š
	# 	l1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=16384, radius=0.025, nsample=8,
	# 		mlp=[8, 8, 16], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')
	# 	l2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=4096, radius=0.05, nsample=16,
	# 		mlp=[16, 16,32], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')
	# 	l3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=1024, radius=0.1, nsample=32,
	# 	    mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')
	# 	l4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=256, radius=0.2, nsample=64,
	# 	    mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer4')
	# 	l5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=64, radius=0.4, nsample=128,
	# 	    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer5') 

	# 	l6_xyz, l6_points, l6_indices = pnet2.pointnet_sa_module(l5_xyz, l5_points, npoint=None, radius=None, nsample=None,
	# 		mlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer6')

	# 	# Feature Propagation layers   
	# 	l5_points = pnet2.pointnet_fp_module(l5_xyz, l6_xyz, l5_points, l6_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')
	# 	l4_points = pnet2.pointnet_fp_module(l4_xyz, l5_xyz, l4_points, l5_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')
	# 	l3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer3')
	# 	l2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer4')
	# 	l1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer5')
	# 	l0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')
	# 	global_features = tf.reshape(l6_points, [-1, 512])
	# 	point_features = l0_points

	# 	# sem
	# 	l0_points = l0_points[:,:,None,:]
	# 	sem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')
	# 	sem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')
	# 	sem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')
	# 	sem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')
	# 	sem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])
	# 	self.y_psem_logits = sem3
	# 	y_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')

	# 	return point_features, global_features, y_sem_pred

	def backbone_pointnet2(self, X_pc, is_train=None):
		import helper_pointnet2 as pnet2
		points_num = tf.shape(X_pc)[1]
		l0_xyz = X_pc[:,:,0:3]

		l1_xyz, l1_points, l1_indices = pnet2.pointnet_sa_module(l0_xyz, None, npoint=1024, radius=0.1, nsample=32,
			mlp=[32, 32, 64], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer1')
		l2_xyz, l2_points, l2_indices = pnet2.pointnet_sa_module(l1_xyz, l1_points, npoint=256, radius=0.2, nsample=64,
			mlp=[64, 64, 128], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer2')
		l3_xyz, l3_points, l3_indices = pnet2.pointnet_sa_module(l2_xyz, l2_points, npoint=64, radius=0.4, nsample=128,
		    mlp=[128, 128, 256], mlp2=None, group_all=False, is_training=None, bn_decay=None, scope='layer3')
		l4_xyz, l4_points, l4_indices = pnet2.pointnet_sa_module(l3_xyz, l3_points, npoint=None, radius=None, nsample=None,
			mlp=[256, 256, 512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer4')

		# Feature Propagation layers
		l3_points = pnet2.pointnet_fp_module(l3_xyz, l4_xyz, l3_points, l4_points, [256, 256], is_training=None, bn_decay=None, scope='fa_layer1')
		l2_points = pnet2.pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256, 256], is_training=None, bn_decay=None,scope='fa_layer2')
		l1_points = pnet2.pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256, 128], is_training=None, bn_decay=None,scope='fa_layer3')
		l0_points = pnet2.pointnet_fp_module(l0_xyz, l1_xyz, l0_xyz, l1_points,[128, 128, 128, 128], is_training=None, bn_decay=None, scope='fa_layer6')
		global_features = tf.reshape(l4_points, [-1, 512])
		point_features = l0_points

		# sem
		l0_points = l0_points[:,:,None,:]
		sem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')
		sem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')
		sem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_train, name='sem2_dropout')
		sem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')
		sem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])
		self.y_psem_logits = sem3
		y_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')

		return point_features, global_features, y_sem_pred

	def backbone_pointconv(self, X_pc, is_training=None,sigma=0.05,bn_decay=None,weight_decay=None):
		import helper_pointnet2 as pnet2
		points_num = tf.shape(X_pc)[1] #æ¯ä¸ªbatchçš„ç‚¹çš„æ•°é‡ ä¸å®š
		l0_xyz = X_pc[:,:,0:3] # xyz
		l0_points=l0_xyz
		# k=16
		#å¯è°ƒå‚æ•°ï¼š
		l1_xyz, l1_points = pnet2.feature_encoding_layer(l0_xyz, l0_points, npoint=1024, radius = 0.1, sigma = sigma, K=32, mlp=[32,32,64], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer1')
		l2_xyz, l2_points = pnet2.feature_encoding_layer(l1_xyz, l1_points, npoint=256, radius = 0.2, sigma = 2 * sigma, K=32, mlp=[64,64,128], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer2')
		l3_xyz, l3_points = pnet2.feature_encoding_layer(l2_xyz, l2_points, npoint=64, radius = 0.4, sigma = 4 * sigma, K=32, mlp=[128,128,256], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer3')
		l4_xyz, l4_points = pnet2.feature_encoding_layer(l3_xyz, l3_points, npoint=36, radius = 0.8, sigma = 8 * sigma, K=32, mlp=[256,256,512], is_training=is_training, bn_decay=bn_decay, weight_decay = weight_decay, scope='layer4')
		l5_xyz, l5_points, l5_indices = pnet2.pointnet_sa_module(l4_xyz, l4_points, npoint=None, radius=None, nsample=None,mlp=[256,  512], mlp2=None, group_all=True, is_training=None, bn_decay=None, scope='layer5')
		# Feature decoding layers
		l3_points = pnet2.feature_decoding_layer(l3_xyz, l4_xyz, l3_points, l4_points, 0.8, 8 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer1')
		l2_points = pnet2.feature_decoding_layer(l2_xyz, l3_xyz, l2_points, l3_points, 0.4, 4 * sigma, 32, [256,256], is_training, bn_decay, weight_decay, scope='fa_layer2')
		l1_points = pnet2.feature_decoding_layer(l1_xyz, l2_xyz, l1_points, l2_points, 0.2, 2 * sigma, 32, [256,128], is_training, bn_decay, weight_decay, scope='fa_layer3')
		l0_points = pnet2.feature_decoding_layer(l0_xyz, l1_xyz, l0_points, l1_points, 0.1, sigma, 32, [128,128,128], is_training, bn_decay, weight_decay, scope='fa_layer4')

		global_features = tf.reshape((l5_points), [-1, 512])
		point_features = l0_points
		# sem
		l0_points = l0_points[:,:,None,:]
		sem1 = Ops.xxlu(Ops.conv2d(l0_points, k=(1, 1), out_c=128, str=1, pad='VALID', name='sem1'), label='lrelu')
		sem2 = Ops.xxlu(Ops.conv2d(sem1, k=(1, 1), out_c=64, str=1, pad='VALID', name='sem2'), label='lrelu')
		sem2 = Ops.dropout(sem2, keep_prob=0.5, is_train=is_training, name='sem2_dropout')
		sem3 = Ops.conv2d(sem2, k=(1, 1), out_c=self.sem_num, str=1, pad='VALID', name='sem3')
		sem3 = tf.reshape(sem3, [-1, points_num, self.sem_num])
		self.y_psem_logits = sem3
		y_sem_pred = tf.nn.softmax(self.y_psem_logits, name='y_sem_pred')

		return point_features, global_features, y_sem_pred
	######  2. bbox
	def bbox_net(self, global_features):
		'''ç”±å…¨å±€ç‰¹å¾ç›´æŽ¥è¾“å‡ºå›ºå®šæ•°é‡çš„bb å’Œç›¸åº”çš„åˆ†æ•° 
		'''
		b1 = Ops.xxlu(Ops.fc(global_features, out_d= 512, name='b1'), label='lrelu')
		b2 = Ops.xxlu(Ops.fc(b1, out_d= 256, name='b2'), label='lrelu')

		#### sub branch 1
		b3 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b3'), label='lrelu')
		bbvert = Ops.fc(b3, out_d=self.bb_num * 2 * 3, name='bbvert')
		bbvert = tf.reshape(bbvert, [-1, self.bb_num, 2, 3])
		points_min = tf.reduce_min(bbvert, axis=-2)[:, :, None, :]
		points_max = tf.reduce_max(bbvert, axis=-2)[:, :, None, :]
		y_bbvert_pred = tf.concat([points_min, points_max], axis=-2, name='y_bbvert_pred')

		#### sub branch 2
		b4 = Ops.xxlu(Ops.fc(b2, out_d=256, name='b4'), label='lrelu')
		y_bbscore_pred = tf.sigmoid(Ops.fc(b4, out_d=self.bb_num * 1, name='y_bbscore_pred'))

		return y_bbvert_pred, y_bbscore_pred

	######  3. pmask
	def pmask_net(self, point_features, global_features, bbox, bboxscore):
		p_f_num = int(point_features.shape[-1])
		p_num = tf.shape(point_features)[1]
		bb_num = int(bbox.shape[1])

		global_features = tf.tile(Ops.xxlu(Ops.fc(global_features, out_d=256, name='down_g1'), label='lrelu')[:,None,None,:], [1, p_num, 1, 1])
		point_features = Ops.xxlu(Ops.conv2d(point_features[:,:,:,None],k=(1, p_f_num), out_c=256, str=1,name='down_p1',pad='VALID'), label='lrelu')
		point_features = tf.concat([point_features, global_features], axis=-1)
		point_features = Ops.xxlu(Ops.conv2d(point_features, k=(1,int(point_features.shape[-2])), out_c=128, str=1, pad='VALID', name='down_p2'), label='lrelu')
		point_features = Ops.xxlu(Ops.conv2d(point_features, k=(1, int(point_features.shape[-2])), out_c=128, str=1, pad='VALID',name='down_p3'), label='lrelu')
		point_features = tf.squeeze(point_features, axis=-2)

		bbox_info = tf.tile(tf.concat([tf.reshape(bbox, [-1, bb_num, 6]), bboxscore[:,:,None]],axis=-1)[:,:,None,:], [1,1,p_num,1])
		pmask0 = tf.tile(point_features[:,None,:,:], [1, bb_num, 1, 1])
		pmask0 = tf.concat([pmask0, bbox_info], axis=-1)
		pmask0 = tf.reshape(pmask0, [-1, p_num, int(pmask0.shape[-1]), 1])

		pmask1 = Ops.xxlu(Ops.conv2d(pmask0, k=(1,int(pmask0.shape[-2])), out_c=64, str=1, pad='VALID', name='pmask1'), label='lrelu')
		pmask2 = Ops.xxlu(Ops.conv2d(pmask1, k=(1, 1), out_c=32, str=1, pad='VALID', name='pmask2'),label='lrelu')
		pmask3 = Ops.conv2d(pmask2, k=(1,1), out_c=1, str=1, pad='VALID', name='pmask3')
		pmask3 = tf.reshape(pmask3, [-1, bb_num, p_num])

		y_pmask_logits = pmask3
		y_pmask_pred = tf.nn.sigmoid(y_pmask_logits, name='y_pmask_pred')

		return y_pmask_pred

	def build_graph(self, GPU='0'):
		'''
			æ•´ä¸ªçš„ç½‘ç»œç»“æž„éƒ½åœ¨è¿™é‡Œ
		'''
		#######   1. define inputs
		self.X_pc = tf.placeholder(shape=[None, None, self.points_cc], dtype=tf.float32, name='X_pc')
		self.Y_bbvert = tf.placeholder(shape=[None, self.bb_num, 2, 3], dtype=tf.float32, name='Y_bbvert')
		self.Y_pmask = tf.placeholder(shape=[None, self.bb_num, None], dtype=tf.float32, name='Y_pmask')
		self.Y_psem = tf.placeholder(shape=[None, None, self.sem_num], dtype=tf.float32, name='Y_psem')
		self.is_train = tf.placeholder(dtype=tf.bool, name='is_train')
		self.lr = tf.placeholder(dtype=tf.float32, name='lr')

		#######  2. define networks, losses
		with tf.variable_scope('backbone'):
			#self.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet(self.X_pc, self.is_train)
			self.point_features, self.global_features, self.y_psem_pred = self.backbone_pointnet2(self.X_pc, self.is_train)
			# self.point_features, self.global_features, self.y_psem_pred =self.backbone_pointconv(self.X_pc, is_training=self.is_train)

			### loss
			self.psemce_loss = Ops.get_loss_psem_ce(self.y_psem_logits, self.Y_psem)
			self.sum_psemce_loss = tf.summary.scalar('psemce_loss', self.psemce_loss)

		with tf.variable_scope('bbox'):
			self.y_bbvert_pred_raw, self.y_bbscore_pred_raw = self.bbox_net(self.global_features)
			#### association, only used for training
			bbox_criteria = 'use_all_ce_l2_iou'
			self.y_bbvert_pred, self.pred_bborder = Ops.bbvert_association(self.X_pc,  self.y_bbvert_pred_raw, self.Y_bbvert, label=bbox_criteria)
			self.y_bbscore_pred = Ops.bbscore_association(self.y_bbscore_pred_raw, self.pred_bborder)

			### loss
			self.bbvert_loss, self.bbvert_loss_l2, self.bbvert_loss_ce, self.bbvert_loss_iou = \
				Ops.get_loss_bbvert(self.X_pc, self.y_bbvert_pred, self.Y_bbvert, label=bbox_criteria)
			self.bbscore_loss = Ops.get_loss_bbscore(self.y_bbscore_pred, self.Y_bbvert)
			
			self.sum_bbox_vert_loss = tf.summary.scalar('bbvert_loss', self.bbvert_loss)
			self.sum_bbox_vert_loss_l2 = tf.summary.scalar('bbvert_loss_l2', self.bbvert_loss_l2)
			self.sum_bbox_vert_loss_ce = tf.summary.scalar('bbvert_loss_ce', self.bbvert_loss_ce)
			self.sum_bbox_vert_loss_iou = tf.summary.scalar('bbvert_loss_iou', self.bbvert_loss_iou)
			self.sum_bbox_score_loss = tf.summary.scalar('bbscore_loss', self.bbscore_loss)

		with tf.variable_scope('pmask'):
			self.y_pmask_pred = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred, self.y_bbscore_pred)

			### loss
			self.pmask_loss = Ops.get_loss_pmask(self.X_pc, self.y_pmask_pred, self.Y_pmask)
			self.sum_pmask_loss = tf.summary.scalar('pmask_loss', self.pmask_loss)

		with tf.variable_scope('pmask', reuse=True):
			#### during testing, no need to associate, use unordered predictions
			self.y_pmask_pred_raw = self.pmask_net(self.point_features, self.global_features, self.y_bbvert_pred_raw, self.y_bbscore_pred_raw)

		######   3. define optimizers
		var_backbone = [var for var in tf.trainable_variables() if var.name.startswith('backbone') and not var.name.startswith('backbone/sem')]
		var_sem = [var for var in tf.trainable_variables() if var.name.startswith('backbone/sem')]
		var_bbox = [var for var in tf.trainable_variables() if var.name.startswith('bbox')]
		var_pmask = [var for var in tf.trainable_variables() if var.name.startswith('pmask')]

		end_2_end_loss = self.bbvert_loss + self.bbscore_loss  + self.pmask_loss + self.psemce_loss
		self.optim = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(end_2_end_loss, var_list = var_bbox+var_pmask +var_backbone+ var_sem)

		######   4. others
		print(Ops.variable_count())
		self.saver = tf.train.Saver(max_to_keep=0)
		config = tf.ConfigProto(allow_soft_placement=True)
		config.gpu_options.visible_device_list = GPU
		self.sess = tf.Session(config=config)
		self.sum_writer_train = tf.summary.FileWriter(self.train_sum_dir, self.sess.graph)
		self.sum_write_test = tf.summary.FileWriter(self.test_sum_dir)
		self.sum_merged = tf.summary.merge_all()

		path = self.train_mod_dir
		modelsaved='model025.cptk.data-00000-of-00001'
		if os.path.isfile(path + modelsaved):
			print (""restoring saved model""+modelsaved)
			self.saver.restore(self.sess, path + 'model025.cptk')
		else:
			print (""model not found, all weights are initilized"")
			self.sess.run(tf.global_variables_initializer())

		return 0
helper_net is as belowï¼š
# _*_ coding:utf-8 _*_
import numpy as np
import tensorflow as tf
from scipy.optimize import linear_sum_assignment
import tensorflow as tf
#tf.compat.v1.disable_v2_behavior()
#import tensorflow.compat.v1 as tf
#tf.disable_v2_behavior()
class Ops:

    @staticmethod
    def lrelu(x, leak=0.2):
        f1 = 0.5 * (1 + leak)
        f2 = 0.5 * (1 - leak)
        return f1 * x + f2 * abs(x)

    @staticmethod
    def relu(x):
        return tf.nn.relu(x)

    @staticmethod
    def xxlu(x,label,name=None):
        if label =='relu':
            return  Ops.relu(x)
        if label =='lrelu':
            return  Ops.lrelu(x,leak=0.2)

    @staticmethod
    def variable_sum(var, name):
        with tf.compat.v1.name_scope(name):
            mean = tf.reduce_mean(input_tensor=var)
            tf.compat.v1.summary.scalar('mean', mean)
            stddev = tf.sqrt(tf.reduce_mean(input_tensor=tf.square(var - mean)))
            tf.compat.v1.summary.scalar('stddev', stddev)
            tf.compat.v1.summary.scalar('max', tf.reduce_max(input_tensor=var))
            tf.compat.v1.summary.scalar('min', tf.reduce_min(input_tensor=var))
            tf.compat.v1.summary.histogram('histogram', var)

    @staticmethod
    def variable_count():
        total_para = 0
        for variable in tf.compat.v1.trainable_variables():
            shape = variable.get_shape()
            variable_para = 1
            for dim in shape:
                variable_para *= dim.value
            total_para += variable_para
        return total_para

    @staticmethod
    def fc(x, out_d, name):
        #xavier_init = tf.contrib.layers.xavier_initializer()
        #xavier_init = tf.truncated_normal_initializer()
        xavier_init = tf.initializers.GlorotUniform()
        zero_init = tf.compat.v1.zeros_initializer()
        in_d = x.get_shape()[1]
        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory
            w = tf.compat.v1.get_variable(name + '_w', [in_d, out_d], initializer=xavier_init)
            b = tf.compat.v1.get_variable(name + '_b', [out_d], initializer=zero_init)
        y = tf.nn.bias_add(tf.matmul(x, w), b)
        Ops.variable_sum(w, name)
        return y
	
    @staticmethod
    def conv2d(x, k=(1,1), out_c=1, str=1, name='',pad='SAME'):
        #xavier_init = tf.contrib.layers.xavier_initializer()
        #xavier_init = tf.truncated_normal_initializer()
        xavier_init = tf.initializers.GlorotUniform()
        zero_init = tf.compat.v1.zeros_initializer()
        in_c = x.get_shape()[3]
        with tf.device('/cpu:0'):  # to create Variables stored on CPU memory
            w = tf.compat.v1.get_variable(name + '_w', [k[0], k[1], in_c, out_c], initializer=xavier_init)
            b = tf.compat.v1.get_variable(name + '_b', [out_c], initializer=zero_init)

        stride = [1, str, str, 1]
        y = tf.nn.bias_add(tf.nn.conv2d(input=x, filters=w, strides=stride, padding=pad), b)
        Ops.variable_sum(w, name)
        return y

    @staticmethod
    def dropout(x, is_train, keep_prob, name):
        y = tf.cond(pred=is_train, true_fn=lambda: tf.nn.dropout(x, rate=1 - (keep_prob), name=name), false_fn=lambda: x)
        return y

    ####################################
    @staticmethod
    def gather_tensor_along_2nd_axis(bat_bb_pred, bat_bb_indices):
        bat_size = tf.shape(input=bat_bb_pred)[0]
        [_, ins_max_num, d1, d2] = bat_bb_pred.get_shape()
        bat_size_range = tf.range(bat_size)
        bat_size_range_flat = tf.reshape(bat_size_range, [-1,1])
        bat_size_range_flat_repeat = tf.tile(bat_size_range_flat, [1, int(ins_max_num)])
        bat_size_range_flat_repeat = tf.reshape(bat_size_range_flat_repeat, [-1])
        
        indices_2d_flat = tf.reshape(bat_bb_indices, [-1])
        indices_2d_flat_repeat = bat_size_range_flat_repeat*int(ins_max_num) + indices_2d_flat

        bat_bb_pred = tf.reshape(bat_bb_pred, [-1, int(d1), int(d2)])
        bat_bb_pred_new = tf.gather(bat_bb_pred, indices_2d_flat_repeat)
        bat_bb_pred_new = tf.reshape(bat_bb_pred_new, [bat_size, int(ins_max_num), int(d1), int(d2)])
   
        return bat_bb_pred_new

    @staticmethod
    def hungarian(loss_matrix, bb_gt):
        box_mask = np.array([[0, 0, 0], [0, 0, 0]])

        def assign_mappings_valid_only(cost, gt_boxes):
            # return ordering : batch_size x num_instances
            loss_total = 0.
            batch_size, num_instances = cost.shape[:2]
            ordering = np.zeros(shape=[batch_size, num_instances]).astype(np.int32)
            for idx in range(batch_size):
                ins_gt_boxes = gt_boxes[idx]
                ins_count = 0
                for box in ins_gt_boxes:
                    if np.array_equal(box, box_mask):
                        break
                    else:
                        ins_count += 1
                valid_cost = cost[idx][:ins_count]
                row_ind, col_ind = linear_sum_assignment(valid_cost)
                unmapped = num_instances - ins_count
                if unmapped > 0:
                    rest = np.array(range(ins_count, num_instances))
                    row_ind = np.concatenate([row_ind, rest])
                    unmapped_ind = np.array(list(set(range(num_instances)) - set(col_ind)))
                    col_ind = np.concatenate([col_ind, unmapped_ind])

                loss_total += cost[idx][row_ind, col_ind].sum()
                ordering[idx] = np.reshape(col_ind, [1, -1])
            return ordering, (loss_total / float(batch_size * num_instances)).astype(np.float32)
        ######
        ordering, loss_total = tf.compat.v1.py_func(assign_mappings_valid_only, [loss_matrix, bb_gt], [tf.int32, tf.float32])

        return ordering, loss_total

    @staticmethod
    def bbvert_association(X_pc, y_bbvert_pred, Y_bbvert, label=''):
        points_num = tf.shape(input=X_pc)[1]
        bbnum = int(y_bbvert_pred.shape[1])
        points_xyz = X_pc[:, :, 0:3]
        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bbnum, 1, 1])

        ##### get points hard mask in each gt bbox
        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]
        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]
        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])
        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])
        tp1_gt = gt_bbox_min_xyz - points_xyz
        tp2_gt = points_xyz - gt_bbox_max_xyz
        tp_gt = tp1_gt * tp2_gt
        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)

        ##### get points soft mask in each pred bbox ---> Algorithm 1
        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]
        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]
        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])
        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])
        tp1_pred = pred_bbox_min_xyz - points_xyz
        tp2_pred = points_xyz - pred_bbox_max_xyz
        tp_pred = 100 * tp1_pred * tp2_pred
        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)
        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))
        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)

        ##### get bbox cross entropy scores
        prob_gt = tf.tile(points_in_gt_bbox_prob[:, :, None, :], [1, 1, bbnum, 1])
        prob_pred = tf.tile(points_in_pred_bbox_prob[:, None, :, :], [1, bbnum, 1, 1])
        ce_scores_matrix = - prob_gt * tf.math.log(prob_pred + 1e-8) - (1 - prob_gt) * tf.math.log(1 - prob_pred + 1e-8)
        ce_scores_matrix = tf.reduce_mean(input_tensor=ce_scores_matrix, axis=-1)

        ##### get bbox soft IOU
        TP = tf.reduce_sum(input_tensor=prob_gt * prob_pred, axis=-1)
        FP = tf.reduce_sum(input_tensor=prob_pred, axis=-1) - TP
        FN = tf.reduce_sum(input_tensor=prob_gt, axis=-1) - TP
        iou_scores_matrix = TP/ (TP + FP + FN + 1e-6)
        # iou_scores_matrix = 1.0/iou_scores_matrix  # bad, don't use
        iou_scores_matrix = -1.0 * iou_scores_matrix  # to minimize

        ##### get bbox l2 scores
        l2_gt = tf.tile(Y_bbvert[:, :, None, :, :], [1, 1, bbnum, 1, 1])
        l2_pred = tf.tile(y_bbvert_pred[:, None, :, :, :], [1, bbnum, 1, 1, 1])
        l2_gt = tf.reshape(l2_gt, [-1, bbnum, bbnum, 2 * 3])
        l2_pred = tf.reshape(l2_pred, [-1, bbnum, bbnum, 2 * 3])
        l2_scores_matrix = tf.reduce_mean(input_tensor=(l2_gt - l2_pred) ** 2, axis=[-1])

        ##### bbox association
        if label == 'use_all_ce_l2_iou':
            associate_maxtrix = ce_scores_matrix + l2_scores_matrix + iou_scores_matrix
        elif label == 'use_both_ce_l2':
            associate_maxtrix = ce_scores_matrix + l2_scores_matrix
        elif label == 'use_both_ce_iou':
            associate_maxtrix = ce_scores_matrix + iou_scores_matrix
        elif label == 'use_both_l2_iou':
            associate_maxtrix = l2_scores_matrix + iou_scores_matrix
        elif label == 'use_only_ce':
            associate_maxtrix = ce_scores_matrix
        elif label == 'use_only_l2':
            associate_maxtrix = l2_scores_matrix
        elif label == 'use_only_iou':
            associate_maxtrix = iou_scores_matrix
        else:
            associate_maxtrix=None
            print('association label error!'); exit()

        ######
        pred_bborder, association_score_min = Ops.hungarian(associate_maxtrix, bb_gt=Y_bbvert)
        pred_bborder = tf.cast(pred_bborder, dtype=tf.int32)
        y_bbvert_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbvert_pred, pred_bborder)

        return y_bbvert_pred_new, pred_bborder

    @staticmethod
    def bbscore_association(y_bbscore_pred_raw, pred_bborder):
        y_bbscore_pred_raw = y_bbscore_pred_raw[:,:,None,None]
        y_bbscore_pred_new = Ops.gather_tensor_along_2nd_axis(y_bbscore_pred_raw, pred_bborder)

        y_bbscore_pred_new = tf.reshape(y_bbscore_pred_new, [-1, int(y_bbscore_pred_new.shape[1])])
        return y_bbscore_pred_new

    ####################################  sem loss
    @staticmethod
    def get_loss_psem_ce(y_psem_logits, Y_psem):
        psemce_loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_psem_logits, labels=tf.stop_gradient(Y_psem))
        psemce_loss = tf.reduce_mean(input_tensor=psemce_loss)
        return psemce_loss

    ####################################  bbox loss
    @staticmethod
    def get_loss_bbvert(X_pc, y_bbvert_pred, Y_bbvert, label=''):
        points_num = tf.shape(input=X_pc)[1]
        bb_num = int(Y_bbvert.shape[1])
        points_xyz = X_pc[:, :, 0:3]
        points_xyz = tf.tile(points_xyz[:, None, :, :], [1, bb_num, 1, 1])

        ##### get points hard mask in each gt bbox
        gt_bbox_min_xyz = Y_bbvert[:, :, 0, :]
        gt_bbox_max_xyz = Y_bbvert[:, :, 1, :]
        gt_bbox_min_xyz = tf.tile(gt_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])
        gt_bbox_max_xyz = tf.tile(gt_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])
        tp1_gt = gt_bbox_min_xyz - points_xyz
        tp2_gt = points_xyz - gt_bbox_max_xyz
        tp_gt = tp1_gt * tp2_gt
        points_in_gt_bbox_prob = tf.cast(tf.equal(tf.reduce_mean(input_tensor=tf.cast(tf.greater_equal(tp_gt, 0.), tf.float32), axis=-1), 1.0), tf.float32)

        ##### get points soft mask in each pred bbox
        pred_bbox_min_xyz = y_bbvert_pred[:, :, 0, :]
        pred_bbox_max_xyz = y_bbvert_pred[:, :, 1, :]
        pred_bbox_min_xyz = tf.tile(pred_bbox_min_xyz[:, :, None, :], [1, 1, points_num, 1])
        pred_bbox_max_xyz = tf.tile(pred_bbox_max_xyz[:, :, None, :], [1, 1, points_num, 1])
        tp1_pred = pred_bbox_min_xyz - points_xyz
        tp2_pred = points_xyz - pred_bbox_max_xyz
        tp_pred = 100*tp1_pred*tp2_pred
        tp_pred = tf.maximum(tf.minimum(tp_pred, 20.0), -20.0)
        points_in_pred_bbox_prob = 1.0/(1.0 + tf.exp(-1.0 * tp_pred))
        points_in_pred_bbox_prob = tf.reduce_min(input_tensor=points_in_pred_bbox_prob, axis=-1)

        ##### helper -> the valid bbox (the gt boxes are zero-padded during data processing, pickup valid ones here)
        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)
        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)

        ##### 1. get ce loss of valid/positive bboxes, don't count the ce_loss of invalid/negative bboxes
        Y_bbox_helper_tp1 = tf.tile(Y_bbox_helper[:, :, None], [1, 1, points_num])
        bbox_loss_ce_all = -points_in_gt_bbox_prob * tf.math.log(points_in_pred_bbox_prob + 1e-8) \
                       -(1.-points_in_gt_bbox_prob)*tf.math.log(1.-points_in_pred_bbox_prob + 1e-8)
        bbox_loss_ce_pos = tf.reduce_sum(input_tensor=bbox_loss_ce_all*Y_bbox_helper_tp1)/tf.reduce_sum(input_tensor=Y_bbox_helper_tp1)
        bbox_loss_ce = bbox_loss_ce_pos

        ##### 2. get iou loss of valid/positive bboxes
        TP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob * points_in_gt_bbox_prob, axis=-1)
        FP = tf.reduce_sum(input_tensor=points_in_pred_bbox_prob, axis=-1) - TP
        FN = tf.reduce_sum(input_tensor=points_in_gt_bbox_prob, axis=-1) - TP
        bbox_loss_iou_all = TP/(TP + FP + FN + 1e-6)
        bbox_loss_iou_all = -1.0*bbox_loss_iou_all
        bbox_loss_iou_pos = tf.reduce_sum(input_tensor=bbox_loss_iou_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)
        bbox_loss_iou = bbox_loss_iou_pos

        ##### 3. get l2 loss of both valid/positive bboxes
        bbox_loss_l2_all = (Y_bbvert - y_bbvert_pred)**2
        bbox_loss_l2_all = tf.reduce_mean(input_tensor=tf.reshape(bbox_loss_l2_all, [-1, bb_num, 6]), axis=-1)
        bbox_loss_l2_pos = tf.reduce_sum(input_tensor=bbox_loss_l2_all*Y_bbox_helper)/tf.reduce_sum(input_tensor=Y_bbox_helper)

        ## to minimize the 3D volumn of invalid/negative bboxes, it serves as a regularizer to penalize false pred bboxes
        ## it turns out to be quite helpful, but not discussed in the paper
        bbox_pred_neg = tf.tile((1.- Y_bbox_helper)[:,:,None,None], [1,1,2,3])*y_bbvert_pred
        bbox_loss_l2_neg = (bbox_pred_neg[:,:,0,:]-bbox_pred_neg[:,:,1,:])**2
        bbox_loss_l2_neg = tf.reduce_sum(input_tensor=bbox_loss_l2_neg)/(tf.reduce_sum(input_tensor=1.-Y_bbox_helper)+1e-8)

        bbox_loss_l2 = bbox_loss_l2_pos + bbox_loss_l2_neg

        #####
        if label == 'use_all_ce_l2_iou':
            bbox_loss = bbox_loss_ce + bbox_loss_l2 + bbox_loss_iou
        elif label == 'use_both_ce_l2':
            bbox_loss = bbox_loss_ce + bbox_loss_l2
        elif label == 'use_both_ce_iou':
            bbox_loss = bbox_loss_ce + bbox_loss_iou
        elif label == 'use_both_l2_iou':
            bbox_loss = bbox_loss_l2 + bbox_loss_iou
        elif label == 'use_only_ce':
            bbox_loss = bbox_loss_ce
        elif label == 'use_only_l2':
            bbox_loss = bbox_loss_l2
        elif label == 'use_only_iou':
            bbox_loss = bbox_loss_iou
        else:
            bbox_loss = None
            print('bbox loss label error!'); exit()

        return bbox_loss, bbox_loss_l2, bbox_loss_ce, bbox_loss_iou

    @staticmethod
    def get_loss_bbscore(y_bbscore_pred, Y_bbvert):
        bb_num = int(Y_bbvert.shape[1])

        ##### helper -> the valid bbox
        Y_bbox_helper = tf.reduce_sum(input_tensor=tf.reshape(Y_bbvert, [-1, bb_num, 6]), axis=-1)
        Y_bbox_helper = tf.cast(tf.greater(Y_bbox_helper, 0.), tf.float32)

        ##### bbox score loss
        bbox_loss_score = tf.reduce_mean(input_tensor=-Y_bbox_helper * tf.math.log(y_bbscore_pred + 1e-8)
                                         -(1. - Y_bbox_helper) * tf.math.log(1. - y_bbscore_pred + 1e-8))
        return bbox_loss_score

    ####################################  pmask loss
    @staticmethod
    def get_loss_pmask(X_pc, y_pmask_pred, Y_pmask):
        points_num = tf.shape(input=X_pc)[1]
        ##### valid ins
        Y_pmask_helper = tf.reduce_sum(input_tensor=Y_pmask, axis=-1)
        Y_pmask_helper = tf.cast(tf.greater(Y_pmask_helper, 0.), tf.float32)
        Y_pmask_helper = tf.tile(Y_pmask_helper[:, :, None], [1, 1, points_num])

        Y_pmask = Y_pmask * Y_pmask_helper
        y_pmask_pred = y_pmask_pred * Y_pmask_helper

        ##### focal loss
        alpha = 0.75
        gamma = 2
        pmask_loss_focal_all = -Y_pmask*alpha*((1.-y_pmask_pred)**gamma)*tf.math.log(y_pmask_pred+1e-8)\
                               -(1.-Y_pmask)*(1.-alpha)*(y_pmask_pred**gamma)*tf.math.log(1.-y_pmask_pred+1e-8)
        pmask_loss_focal = tf.reduce_sum(input_tensor=pmask_loss_focal_all*Y_pmask_helper)/tf.reduce_sum(input_tensor=Y_pmask_helper)

        ## the above ""alpha"" makes the loss to be small
        ## then use a constant, so it's numerically comparable with other losses (e.g., semantic loss, bbox loss)
        pmask_loss = 30*pmask_loss_focal

        return pmask_loss
"
46323,micro: port op ELU from lite,"@tensorflow/micro

This issue tracks my work porting operator ELU from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro making minimal changes and not including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test
PR 6: Extract common activation code into activations.cc and activation_utils.h files.  Extract common test code into activation_test_utils.h file.
"
46319,Optimization using Tensorflow issue,"We had given below test case, we written all steps as per instructions. nit sure when we went wrong. Please guide us -

We will look at three scenarios here -

- Tuning different optimisation algorithms, 
- Tuning learning rate and momentum of SGD optimizer
- Tuning beta values of Adam optimizer

```
import numpy as np
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.utils import shuffle
from keras.utils.np_utils import to_categorical
from keras.optimizers import SGD,Adam
from matplotlib import pyplot
import seaborn as sns
import pandas as pd
from keras.models import model_from_json

```


- Load the Dataset
- Load the iris dataset using load_iris() function.
- Store the data of the iris dataset in the variable X.
- Store the target of the iris dataset in the variable y.
- Convert the variable y into categorial variable using function to_categorial and save it in variable y.
- Set seed value as 7 in the variable seed and use random.seed function in numpy to set seed value.
- Now shuffle the data X and y using shuffle function and save it in variables X ,Y.

```
iris = load_iris()
X=  iris.data
y = iris.target
y = to_categorical(iris.target,3)
seed = 7
np.random.seed(seed)
X, Y =  shuffle(X, y)
```

In variable optimizer pass the following optimizers as a list -
SGD,RMSprop,Adam,Nadam
In param_grid pass parameter optimizer as optimizer using dict

```
optimizer = ['SGD', 'RMSprop',  'Adam', 'Nadam']
param_grid = dict(optimizer=optimizer)
```

Create a sequential model
The model expects rows of data with 4 variables (the input_dim=4 argument)
The first hidden layer has 64 nodes and uses the relu activation function.
The second hidden layer has 32 nodes and uses the relu activation function.
The third hidden layer has 16 nodes and uses the relu activation function.
The output layer has 3 nodes and uses the softmax activation function.
While comipling the model pass the following parameters -
     -optimizer as optimizer
     -loss as categorical cross entropy 
     -metrics as accuracy.
Return the compiled model

```
def create_model(optimizer='adam'):
    model = Sequential()
    model.add(Dense(4, input_dim=4))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) 
    return model
```

Use the KerasClassifier function to call the model function with following parameters -
build_fn as create_model
batch_size as 10
verbose as 0
epochs as 10
Save the above in the variable model

`model = KerasClassifier(build_fn=create_model, batch_size=10, verbose=0, epochs=10)`

In grid use the GridSearchCV function and pass the following parameters -
estimator as model
param_grid as param_grid
n_jobs as 1
Now fit the model with X and Y using grid and save it in grid_result

```
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
```


```
print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(""%f (%f) with: %r"" % (mean, stdev, param))
```

Best: 0.946667 using {'optimizer': 'RMSprop'}
0.753333 (0.082192) with: {'optimizer': 'SGD'}
0.946667 (0.047140) with: {'optimizer': 'RMSprop'}
0.913333 (0.083799) with: {'optimizer': 'Adam'}
0.866667 (0.106249) with: {'optimizer': 'Nadam'}


```
pyplot.figure(figsize=(10,8))
#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])
pyplot.title(""Performance metrics of each optimiser"")
plot=sns.barplot(grid_result.cv_results_['mean_test_score'],optimizer)
pyplot.show() 
```

In variable learn_rate pass the following learn rates as a list -
0.001, 0.01,0.3
In variable momentum pass the following momentums as a list -
0.0, 0.4, 0.9

```
learn_rate = [0.001, 0.01, 0.3]
momentum = [0.0, 0.4, 0.9]
```

Use the same model parameters as above to construct the model in the function create_model1
In the variable optimizer pass the following parameters using the optimizer SGD
 -lr as learn_rate
 -momentum as momentum
While comipling the model pass the following parameters -
 -optimizer as optimizer
 -loss as categorical cross entropy 
 -metrics as accuracy.
Return the compiled model

```
def create_model1(learn_rate=0.01, momentum=0):
    model1 = Sequential()
    model1.add(Dense(4, input_dim=4))
    model1.add(Dense(64, activation='relu'))
    model1.add(Dense(32, activation='relu'))
    model1.add(Dense(16, activation='relu'))
    model1.add(Dense(3, activation='softmax'))
    optimizer = SGD(lr=learn_rate, momentum=momentum)
    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])    
    return model1
```


`model1 = KerasClassifier(build_fn=create_model1, batch_size=10, verbose=0, epochs=10)`

```
param_grid1 = dict(learn_rate=learn_rate,momentum=momentum )
grid1 = GridSearchCV(estimator=model1, param_grid=param_grid1, n_jobs=1)
```

Use random.seed function in numpy to set seed value.
Now fit the model with X and Y using grid1 and save it in grid_result1

```
np.random.seed(seed)
grid_result1 = grid1.fit(X, Y)
```

```
print(""Best: %f using %s"" % (grid_result1.best_score_, grid_result1.best_params_))
means1 = grid_result1.cv_results_['mean_test_score']
stds1 = grid_result1.cv_results_['std_test_score']
params1 = grid_result1.cv_results_['params']
for mean, stdev, param in zip(means1, stds1, params1):
    print(""%f (%f) with: %r"" % (mean, stdev, param))
```

Best: 0.893333 using {'learn_rate': 0.01, 'momentum': 0.9}
0.360000 (0.032660) with: {'learn_rate': 0.001, 'momentum': 0.0}
0.666667 (0.037712) with: {'learn_rate': 0.001, 'momentum': 0.4}
0.780000 (0.081650) with: {'learn_rate': 0.001, 'momentum': 0.9}
0.760000 (0.016330) with: {'learn_rate': 0.01, 'momentum': 0.0}
0.866667 (0.160278) with: {'learn_rate': 0.01, 'momentum': 0.4}
0.893333 (0.067987) with: {'learn_rate': 0.01, 'momentum': 0.9}
0.413333 (0.188562) with: {'learn_rate': 0.3, 'momentum': 0.0}
0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.4}
0.333333 (0.018856) with: {'learn_rate': 0.3, 'momentum': 0.9}

```
params1=pd.DataFrame(params1)
pyplot.figure(figsize=(10,8))
#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])
pyplot.title(""Performance metrics of SGD optimiser with different learning rates and momentum"")
plot1=sns.barplot(params1[""learn_rate""],grid_result1.cv_results_['mean_test_score'],hue=params1[""momentum""])
plot1.set(ylabel='Score')
pyplot.show() 
```

```
beta_1 = [0.001, 0.01, 0.3]
beta_2 = [0.0, 0.4, 0.9] 
```

```
def create_model2(beta_1=0.01, beta_2=0):
    model2 = Sequential()
    model2.add(Dense(4, input_dim=4))
    model2.add(Dense(64, activation='relu'))
    model2.add(Dense(32, activation='relu'))
    model2.add(Dense(16, activation='relu'))
    model2.add(Dense(3, activation='softmax'))
    optimizer = Adam(beta_1=beta_1,beta_2=beta_2)
    model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model2
```

`model2 =  KerasClassifier(build_fn=create_model2,batch_size=10, verbose=0,  epochs=10)`

```
param_grid2 = dict(beta_1=beta_1, beta_2=beta_2)
grid2 = GridSearchCV(estimator=model2, param_grid=param_grid2, n_jobs=1)
```

Use random.seed function in numpy to set seed value.
Now fit the model with X and Y using grid2 and save it in grid_result2

```
np.random.seed(seed)
grid_result2 = grid2.fit(X, Y)
```

```
print(""Best: %f using %s"" % (grid_result2.best_score_, grid_result2.best_params_))
means2 = grid_result2.cv_results_['mean_test_score']
stds2 = grid_result2.cv_results_['std_test_score']
params2 = grid_result2.cv_results_['params']
for mean, stdev, param in zip(means2, stds2, params2):
    print(""%f (%f) with: %r"" % (mean, stdev, param))
```

Best: 0.966667 using {'beta_2': 0.4, 'beta_1': 0.001}
0.526667 (0.264491) with: {'beta_2': 0.0, 'beta_1': 0.001}
0.966667 (0.009428) with: {'beta_2': 0.4, 'beta_1': 0.001}
0.840000 (0.133666) with: {'beta_2': 0.9, 'beta_1': 0.001}
0.320000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.01}
0.813333 (0.111156) with: {'beta_2': 0.4, 'beta_1': 0.01}
0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.01}
0.360000 (0.032660) with: {'beta_2': 0.0, 'beta_1': 0.3}
0.926667 (0.041096) with: {'beta_2': 0.4, 'beta_1': 0.3}
0.766667 (0.151731) with: {'beta_2': 0.9, 'beta_1': 0.3}


```
params2=pd.DataFrame(params2)
pyplot.figure(figsize=(10,8))
#pyplot.xticks(grid_result1.cv_results_['mean_test_score'])
pyplot.title(""Performance metrics of SGD optimiser with different beta values"")
plot2=sns.barplot(params2[""beta_1""],grid_result2.cv_results_['mean_test_score'],hue=params2[""beta_2""])
plot2.set(ylabel='Score')
pyplot.show() 
```

```
with open(""score.txt"",""w"") as f:
    f.write(str(round(grid_result.best_score_,2)))
with open(""params.txt"",""w"") as f:
    f.write(str(grid_result.best_params_))

with open(""score1.txt"",""w"") as f:
    f.write(str(round(grid_result1.best_score_,2)))
with open(""params1.txt"",""w"") as f:
    f.write(str(grid_result1.best_params_))
    
with open(""score2.txt"",""w"") as f:
    f.write(str(round(grid_result2.best_score_,2)))
with open(""params2.txt"",""w"") as f:
    f.write(str(grid_result2.best_params_))

```        

```
def save_model(model):
    # saving model
    json_model = model.to_json()
    open('model.json', 'w').write(json_model)
    # saving weights
    model.save_weights('model.h5', overwrite=True)
classifier=create_model()
save_model(classifier)
```

 "
46318,code's,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
46317,"In different tf2 versions, the weight naming rules of creating keras model are different","**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip install tensorflow-cpu
- TensorFlow version (use command below): 2.1, 2.2, 2.3, 2.4
- Python version: 3.6 / 3.8

**Describe the current behavior**
I create keras model using same code, but got different results(different weight names) in different tf version.
This will prevent me from loading network weights (based on variable name) in different tf version.

**Describe the expected behavior**
I can get same results in different tf version.

**Standalone code to reproduce the issue**

I create kears model:
```python
from tensorflow.keras import layers, Model, Sequential


class ConvBNReLU(layers.Layer):
    def __init__(self, out_channel, kernel_size=3, stride=1, **kwargs):
        super(ConvBNReLU, self).__init__(**kwargs)
        layers_list = [layers.Conv2D(filters=out_channel, kernel_size=kernel_size,
                                     strides=stride, padding='SAME', use_bias=False, name='Conv2d'),
                       layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='BatchNorm'),
                       layers.ReLU(max_value=6.0)]

        self.combine_layer = Sequential(layers_list, name=""combine"")

    def call(self, inputs, training=False, **kwargs):
        x = self.combine_layer(inputs, training=training)
        return x


def main():
    input_image = layers.Input(shape=(224, 224, 3), dtype='float32')
    # conv1
    x = ConvBNReLU(32, stride=2)(input_image)
    output = ConvBNReLU(64, stride=2)(x)
    model = Model(inputs=input_image, outputs=output)

    for i in model.weights:
        print(i.name)


if __name__ == '__main__':
    main()

```

In tf2.0, 2.1 and 2.2, the printed weight name information is as follows:
```
conv_bn_re_lu/combine/Conv2d/kernel:0
conv_bn_re_lu/combine/BatchNorm/gamma:0
conv_bn_re_lu/combine/BatchNorm/beta:0
conv_bn_re_lu/combine/BatchNorm/moving_mean:0
conv_bn_re_lu/combine/BatchNorm/moving_variance:0
conv_bn_re_lu_1/combine/Conv2d/kernel:0
conv_bn_re_lu_1/combine/BatchNorm/gamma:0
conv_bn_re_lu_1/combine/BatchNorm/beta:0
conv_bn_re_lu_1/combine/BatchNorm/moving_mean:0
conv_bn_re_lu_1/combine/BatchNorm/moving_variance:0
```

But in tf2.3 and 2.4, I got different results:
```
Conv2d/kernel:0
BatchNorm/gamma:0
BatchNorm/beta:0
BatchNorm/moving_mean:0
BatchNorm/moving_variance:0
Conv2d/kernel:0
BatchNorm/gamma:0
BatchNorm/beta:0
BatchNorm/moving_mean:0
BatchNorm/moving_variance:0
```
"
46316,Keras model conflicting with multiprocessing,"This is not necessarily a bug with tensorflow / keras but it may be classified as incompatibility issue. I'm getting a weird error when I'm trying to run 2 class methods concurrently in a third method. After eliminating large chunks of code, one at a time, I was surprised to find out that having keras model as a class attribute in the example, leads to the error.

**Things to note:** 

 - I must have a model as a class attribute, I cannot change that.
 - I need both tasks to run concurrently and I cannot get these 2 tasks out of the class because they interact with other class members
 - I get the same error using `multiprocessing.Process()`, so that also will not fix the problem.
___

    from concurrent.futures import ProcessPoolExecutor, as_completed
    
    from tensorflow.keras.models import Model
    
    
    class Example:
        def __init__(self):
            self.model = Model()
            # comment out the line above and uncomment the line below, the error is gone
            # self.model = None
    
        def task1(self):
            pass
    
        def task2(self):
            pass
    
        def process(
            self,
        ):
            with ProcessPoolExecutor(2) as executor:
                future_items = [
                    executor.submit(self.task1),
                    executor.submit(self.task2),
                ]
                results = [
                    future_item.result() for future_item in as_completed(future_items)
                ]
                print(results)
    
    
    if __name__ == '__main__':
        ex = Example()
        ex.process()

**Results in:**

    2021-01-10 08:10:04.315386: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
    2021-01-10 08:10:04.315897: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
    concurrent.futures.process._RemoteTraceback: 
    """"""
    Traceback (most recent call last):
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py"", line 239, in _feed
        obj = _ForkingPickler.dumps(obj)
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py"", line 51, in dumps
        cls(buf, protocol).dump(obj)
    TypeError: cannot pickle 'weakref' object
    """"""
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File ""/Users/emadboctor/Desktop/code/drl-algos/scratch.py"", line 34, in <module>
        ex.process()
      File ""/Users/emadboctor/Desktop/code/drl-algos/scratch.py"", line 26, in process
        results = [
      File ""/Users/emadboctor/Desktop/code/drl-algos/scratch.py"", line 27, in <listcomp>
        future_item.result() for future_item in as_completed(future_items)
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py"", line 432, in result
        return self.__get_result()
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py"", line 388, in __get_result
        raise self._exception
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py"", line 239, in _feed
        obj = _ForkingPickler.dumps(obj)
      File ""/usr/local/Cellar/python@3.8/3.8.7/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py"", line 51, in dumps
        cls(buf, protocol).dump(obj)
    TypeError: cannot pickle 'weakref' object


"
46315,"TF ConvertedModel: Invoke fails with ""Node number X (CONCATENATION) failed to prepare"" error","**System information**
- OS: Windows 10:
- TensorFlow: 2.4.0:

**Code used to infer** : 
`
   # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test the model on random input data.
    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
    interpreter.set_tensor(input_details[0]['index'], input_data)

    interpreter.invoke()

    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data)
`


**Output**:
`
INFO: TfLiteFlexDelegate delegate: 15 nodes delegated out of 188 nodes with 2 partitions.
INFO: TfLiteFlexDelegate delegate: 5 nodes delegated out of 12 nodes with 1 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 35 nodes with 2 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.

Traceback (most recent call last):
  File ""src\models\net_converters\test_tflite_model.py"", line 127, in <module>
    test(args.model, args.test)
  File ""src\models\net_converters\test_tflite_model.py"", line 31, in test
    interpreter.invoke()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 540, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (400 != 1)Node number 33 (CONCATENATION) failed to prepare.
Node number 3 (WHILE) failed to invoke.
Node number 187 (WHILE) failed to invoke.
`

**TF Model summary** 
`
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 300, 300, 3) 0
__________________________________________________________________________________________________
identity_layer (Lambda)         (None, 300, 300, 3)  0           input_1[0][0]
__________________________________________________________________________________________________
input_mean_normalization (Lambd (None, 300, 300, 3)  0           identity_layer[0][0]
__________________________________________________________________________________________________
input_channel_swap (Lambda)     (None, 300, 300, 3)  0           input_mean_normalization[0][0]
__________________________________________________________________________________________________
conv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_channel_swap[0][0]
__________________________________________________________________________________________________
conv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]
__________________________________________________________________________________________________
conv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]
__________________________________________________________________________________________________
conv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]
__________________________________________________________________________________________________
pool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]
__________________________________________________________________________________________________
conv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]
__________________________________________________________________________________________________
conv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]
__________________________________________________________________________________________________
conv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]
__________________________________________________________________________________________________
pool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]
__________________________________________________________________________________________________
conv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]
__________________________________________________________________________________________________
conv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]
__________________________________________________________________________________________________
conv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]
__________________________________________________________________________________________________
pool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]
__________________________________________________________________________________________________
conv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]
__________________________________________________________________________________________________
conv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]
__________________________________________________________________________________________________
conv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]
__________________________________________________________________________________________________
pool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]
__________________________________________________________________________________________________
fc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]
__________________________________________________________________________________________________
fc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     fc6[0][0]
__________________________________________________________________________________________________
conv6_1 (Conv2D)                (None, 19, 19, 256)  262400      fc7[0][0]
__________________________________________________________________________________________________
conv6_padding (ZeroPadding2D)   (None, 21, 21, 256)  0           conv6_1[0][0]
__________________________________________________________________________________________________
conv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_padding[0][0]
__________________________________________________________________________________________________
conv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]
__________________________________________________________________________________________________
conv7_padding (ZeroPadding2D)   (None, 12, 12, 128)  0           conv7_1[0][0]
__________________________________________________________________________________________________
conv7_2 (Conv2D)                (None, 5, 5, 256)    295168      conv7_padding[0][0]
__________________________________________________________________________________________________
conv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]
__________________________________________________________________________________________________
conv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]
__________________________________________________________________________________________________
conv9_1 (Conv2D)                (None, 3, 3, 128)    32896       conv8_2[0][0]
__________________________________________________________________________________________________
conv4_3_norm (L2Normalization)  (None, 38, 38, 512)  512         conv4_3[0][0]
__________________________________________________________________________________________________
conv9_2 (Conv2D)                (None, 1, 1, 256)    295168      conv9_1[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 4220) 19449980    conv4_3_norm[0][0]
__________________________________________________________________________________________________
fc7_mbox_conf (Conv2D)          (None, 19, 19, 5275) 48619675    fc7[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_conf (Conv2D)      (None, 10, 10, 5275) 24312475    conv6_2[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_conf (Conv2D)      (None, 5, 5, 5275)   12158875    conv7_2[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_conf (Conv2D)      (None, 3, 3, 4220)   9727100     conv8_2[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_conf (Conv2D)      (None, 1, 1, 4220)   9727100     conv9_2[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 16)   73744       conv4_3_norm[0][0]
__________________________________________________________________________________________________
fc7_mbox_loc (Conv2D)           (None, 19, 19, 20)   184340      fc7[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_loc (Conv2D)       (None, 10, 10, 20)   92180       conv6_2[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_loc (Conv2D)       (None, 5, 5, 20)     46100       conv7_2[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_loc (Conv2D)       (None, 3, 3, 16)     36880       conv8_2[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_loc (Conv2D)       (None, 1, 1, 16)     36880       conv9_2[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_conf_reshape  (None, 5776, 1055)   0           conv4_3_norm_mbox_conf[0][0]
__________________________________________________________________________________________________
fc7_mbox_conf_reshape (Reshape) (None, 1805, 1055)   0           fc7_mbox_conf[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_conf_reshape (Resh (None, 500, 1055)    0           conv6_2_mbox_conf[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_conf_reshape (Resh (None, 125, 1055)    0           conv7_2_mbox_conf[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_conf_reshape (Resh (None, 36, 1055)     0           conv8_2_mbox_conf[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_conf_reshape (Resh (None, 4, 1055)      0           conv9_2_mbox_conf[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_priorbox (Anc (None, 38, 38, 4, 8) 0           conv4_3_norm_mbox_loc[0][0]
__________________________________________________________________________________________________
fc7_mbox_priorbox (AnchorBoxes) (None, 19, 19, 5, 8) 0           fc7_mbox_loc[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_priorbox (AnchorBo (None, 10, 10, 5, 8) 0           conv6_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_priorbox (AnchorBo (None, 5, 5, 5, 8)   0           conv7_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_priorbox (AnchorBo (None, 3, 3, 4, 8)   0           conv8_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_priorbox (AnchorBo (None, 1, 1, 4, 8)   0           conv9_2_mbox_loc[0][0]
__________________________________________________________________________________________________
mbox_conf (Concatenate)         (None, 8246, 1055)   0           conv4_3_norm_mbox_conf_reshape[0]
                                                                 fc7_mbox_conf_reshape[0][0]
                                                                 conv6_2_mbox_conf_reshape[0][0]
                                                                 conv7_2_mbox_conf_reshape[0][0]
                                                                 conv8_2_mbox_conf_reshape[0][0]
                                                                 conv9_2_mbox_conf_reshape[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_loc_reshape ( (None, 5776, 4)      0           conv4_3_norm_mbox_loc[0][0]
__________________________________________________________________________________________________
fc7_mbox_loc_reshape (Reshape)  (None, 1805, 4)      0           fc7_mbox_loc[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_loc_reshape (Resha (None, 500, 4)       0           conv6_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_loc_reshape (Resha (None, 125, 4)       0           conv7_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_loc_reshape (Resha (None, 36, 4)        0           conv8_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_loc_reshape (Resha (None, 4, 4)         0           conv9_2_mbox_loc[0][0]
__________________________________________________________________________________________________
conv4_3_norm_mbox_priorbox_resh (None, 5776, 8)      0           conv4_3_norm_mbox_priorbox[0][0]
__________________________________________________________________________________________________
fc7_mbox_priorbox_reshape (Resh (None, 1805, 8)      0           fc7_mbox_priorbox[0][0]
__________________________________________________________________________________________________
conv6_2_mbox_priorbox_reshape ( (None, 500, 8)       0           conv6_2_mbox_priorbox[0][0]
__________________________________________________________________________________________________
conv7_2_mbox_priorbox_reshape ( (None, 125, 8)       0           conv7_2_mbox_priorbox[0][0]
__________________________________________________________________________________________________
conv8_2_mbox_priorbox_reshape ( (None, 36, 8)        0           conv8_2_mbox_priorbox[0][0]
__________________________________________________________________________________________________
conv9_2_mbox_priorbox_reshape ( (None, 4, 8)         0           conv9_2_mbox_priorbox[0][0]
__________________________________________________________________________________________________
mbox_conf_softmax (Activation)  (None, 8246, 1055)   0           mbox_conf[0][0]
__________________________________________________________________________________________________
mbox_loc (Concatenate)          (None, 8246, 4)      0           conv4_3_norm_mbox_loc_reshape[0][
                                                                 fc7_mbox_loc_reshape[0][0]
                                                                 conv6_2_mbox_loc_reshape[0][0]
                                                                 conv7_2_mbox_loc_reshape[0][0]
                                                                 conv8_2_mbox_loc_reshape[0][0]
                                                                 conv9_2_mbox_loc_reshape[0][0]
__________________________________________________________________________________________________
mbox_priorbox (Concatenate)     (None, 8246, 8)      0           conv4_3_norm_mbox_priorbox_reshap
                                                                 fc7_mbox_priorbox_reshape[0][0]
                                                                 conv6_2_mbox_priorbox_reshape[0][
                                                                 conv7_2_mbox_priorbox_reshape[0][
                                                                 conv8_2_mbox_priorbox_reshape[0][
                                                                 conv9_2_mbox_priorbox_reshape[0][
__________________________________________________________________________________________________
predictions (Concatenate)       (None, 8246, 1067)   0           mbox_conf_softmax[0][0]
                                                                 mbox_loc[0][0]
                                                                 mbox_priorbox[0][0]
__________________________________________________________________________________________________
decoded_predictions (DecodeDete (None, 200, 6)       0           predictions[0][0]
==================================================================================================
Total params: 147,409,265
Trainable params: 147,409,265
Non-trainable params: 0
`


Do you need any additional data or maybe it's kind of known issue ?
Thank you & best regards
"
46314,"tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(""batch_normalization/moving_mean""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable ","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 pro
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):2.4.0


**Command used to run the converter or code if youâ€™re using the Python API**

```
python convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite
```

> convert_tflite.py is as below

> import tensorflow as tf
> from absl import app, flags, logging
> from absl.flags import FLAGS
> import numpy as np
> import cv2
> from core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode
> import core.utils as utils
> import os
> from core.config import cfg
> 
> flags.DEFINE_string('weights', './checkpoints/yolov4-416', 'path to weights file')
> flags.DEFINE_string('output', './checkpoints/yolov4-416-fp32.tflite', 'path to output')
> flags.DEFINE_integer('input_size', 416, 'path to output')
> flags.DEFINE_string('quantize_mode', 'float32', 'quantize mode (int8, float16, float32)')
> flags.DEFINE_string('dataset', ""/Volumes/Elements/data/coco_dataset/coco/5k.txt"", 'path to dataset')
> 
> def representative_data_gen():
>   fimage = open(FLAGS.dataset).read().split()
>   for input_value in range(10):
>     if os.path.exists(fimage[input_value]):
>       original_image=cv2.imread(fimage[input_value])
>       original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
>       image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])
>       img_in = image_data[np.newaxis, ...].astype(np.float32)
>       print(""calibration image {}"".format(fimage[input_value]))
>       yield [img_in]
>     else:
>       continue
> 
> def save_tflite():
>   converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)
> 
>   if FLAGS.quantize_mode == 'float16':
>     converter.optimizations = [tf.lite.Optimize.DEFAULT]
>     converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]
>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
>     converter.allow_custom_ops = True
>   elif FLAGS.quantize_mode == 'int8':
>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
>     converter.optimizations = [tf.lite.Optimize.DEFAULT]
>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
>     converter.allow_custom_ops = True
>     converter.representative_dataset = representative_data_gen
> 
>   tflite_model = converter.convert()
>   open(FLAGS.output, 'wb').write(tflite_model)
> 
>   logging.info(""model saved to: {}"".format(FLAGS.output))
> 
> def demo():
>   interpreter = tf.lite.Interpreter(model_path=FLAGS.output)
>   interpreter.allocate_tensors()
>   logging.info('tflite model loaded')
> 
>   input_details = interpreter.get_input_details()
>   print(input_details)
>   output_details = interpreter.get_output_details()
>   print(output_details)
> 
>   input_shape = input_details[0]['shape']
> 
>   input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
> 
>   interpreter.set_tensor(input_details[0]['index'], input_data)
>   interpreter.invoke()
>   output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]
> 
>   print(output_data)
> 
> def main(_argv):
>   save_tflite()
>   demo()
> 
> if __name__ == '__main__':
>     try:
>         app.run(main)
>     except SystemExit:
>         pass
>

**The output from the converter invocation**

```
loc(""batch_normalization/moving_mean""): error: is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable
Traceback (most recent call last):
  File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\lite\python\convert.py"", line 213, in toco_convert_protos
    enable_mlir_converter)
  File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(""batch_normalization/moving_mean""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable


During handling of the above exception, another exception occurred:

> Traceback (most recent call last):
>   File ""convert_tflite.py"", line 76, in <module>
>     app.run(main)
>   File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\absl\app.py"", line 300, in run
>     _run_main(main, args)
>   File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\absl\app.py"", line 251, in _run_main
>     sys.exit(main(argv))
>   File ""convert_tflite.py"", line 71, in main
>     save_tflite()
>   File ""convert_tflite.py"", line 45, in save_tflite
>     tflite_model = converter.convert()
>   File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\lite\python\lite.py"", line 739, in convert
>     result = _convert_saved_model(**converter_kwargs)
>   File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\lite\python\convert.py"", line 637, in convert_saved_model
>     enable_mlir_converter=True)
>   File ""C:\Users\theum\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\lite\python\convert.py"", line 216, in toco_convert_protos
>     raise ConverterError(str(e))
> tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(""batch_normalization/moving_mean""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable

```


[Link to saved model](https://drive.google.com/drive/folders/1wsEs6pHfbNi-CjGRTSeQ8poSreAEo3Bh?usp=sharing)







"
46310,Dynamic library  libcudnn.so.8 and libcusolver.so.10 dont load,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
- TensorFlow installed from: pip in anaconda env
- TensorFlow version: 2.4.0
- Python version: 3.8.5
- CUDA/cuDNN version:  CUDA:11.0 and cuDNN 8.05 
- GPU model and memory: Dual 1080 ti 12 gb



**Describe the problem**
Fresh build with newly installed Ubuntu. Had to downgrade the existing nvidia driver  and cuda   from 4.60 > 4.50 and cuda from 11.2 to 11.1. since cuDNN didnt list CUDA 11.2  as compatible and i ran into more issues. Running a simple model (make blobs classification) returns these errors when compiling the model :+1: 

libcudnn.so.8 and libcusolver.so.10 dont load (code below run in jupyter)

```
Kernel started: 074c56de-e52a-48bd-b7bb-20ab27829401, name: python3
2021-01-09 12:25:03.574107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-09 12:25:08.195326: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-09 12:25:08.195728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-09 12:25:08.220794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-09 12:25:08.221463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:23:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2021-01-09 12:25:08.221502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-09 12:25:08.222043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:2d:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2021-01-09 12:25:08.222053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-09 12:25:08.222983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-09 12:25:08.223005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-09 12:25:08.223355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-09 12:25:08.223461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-09 12:25:08.223514: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory_
2021-01-09 12:25:08.223746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-09 12:25:08.223775: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] _Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory_
2021-01-09 12:25:08.223780: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-01-09 12:25:08.224169: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-09 12:25:08.224183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-09 12:25:08.224188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]    
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I changed the existing drier from 4.60 to 4.50 by the software page. prior to this  cuda 11.2 was installed but was not in /usr/local/
I installed cuda 11.0 from .deb after purging and autoremoving cuda. 

I've tried adding the the path following the nvidia documenation replacing the 11.2 with 11.1
```export PATH=/usr/local/cuda-11.2/bin${PATH:+:${PATH}}```
but it hasnt helped"
46309,Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED,"
**System information**
- Windows 10 Pro 2004
- TensorFlow  installed from (pip):
- TensorFlow version (2.4.0):
- Python version 3.8.1:
- Installed in anaconda venv
- CUDA version 11.0, cuDNN version 8.0.4
- GPU model gtx 1660ti, 6Gb vram:


**The code**
`import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import tensorflow_datasets as tfds

import tensorflow_addons as tfa
import math
from tensorflow.keras import layers
from tensorflow import keras

print(tf.__version__)
if tf.test.gpu_device_name():
    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))
else:
    print(""Please install GPU version of TF"")
physical_devices = tf.config.list_physical_devices('GPU')
print(physical_devices)
tf.config.experimental.set_memory_growth(physical_devices[0], True)

(ds_train, ds_test), ds_info = tfds.load('mnist',
                                         split=['train', 'test'],
                                         shuffle_files=False,
                                         as_supervised=True,
                                         with_info=True,)

@tf.function
def normalize_img(image, label):
    return tf.cast(image, tf.float32) / 255.0, label

@tf.function
def rotate(img, max_degrees=25):
    degrees = tf.random.uniform([], -max_degrees, max_degrees, dtype=tf.float32)
    img = tfa.image.rotate(img, degrees*math.pi / 180, interpolation='BILINEAR')
    return img

#
@tf.function
def augment(image, label):
    image = tf.image.resize(image, size=[28, 28])
    image = rotate(image)

    # coloring of image
    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE
BATCH_SIZE = 32

ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)
ds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)
ds_train = ds_train.batch(BATCH_SIZE)
ds_train = ds_train.prefetch(AUTOTUNE)

ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)
ds_test = ds_test.batch(BATCH_SIZE)
ds_test = ds_test.prefetch(AUTOTUNE)

def my_model():
    inputs = keras.Input(shape=(28, 28, 1))
    x = layers.Conv2D(32, 3)(inputs)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(64, 3)(x)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Conv2D(128, 3)(x)
    x = layers.BatchNormalization()(x)
    x = keras.activations.relu(x)
    x = layers.Flatten()(x)
    x = layers.Dense(64, activation='relu')(x)
    outputs = layers.Dense(10, activation='softmax')(x)
    return keras.Model(inputs=inputs, outputs=outputs)

model = my_model()
# compile model
model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              optimizer=keras.optimizers.Adam(lr=1e-4),
              metrics=['accuracy'])
# model.fit
model.fit(ds_train, epochs=30, verbose=2)
# model.evaluate
model.evaluate(ds_test)
model.save('model')

`
**The error it produced**
> 2.4.0
Default GPU Device:/device:GPU:0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Epoch 1/30
2021-01-09 11:59:09.630855: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2021-01-09 11:59:09.631034: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2021-01-09 11:59:09.632741: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2021-01-09 11:59:09.632910: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
Traceback (most recent call last):
  File ""D:/Projects/pythonProject/main.py"", line 85, in <module>
    model.fit(ds_train, epochs=30, verbose=2)
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\Users\.\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node model/conv2d/Conv2D (defined at D:/Projects/pythonProject/main.py:85) ]] [Op:__inference_train_function_1489]
Function call stack:
train_function


Verified versions of drivers, cuda toolkit and cudnn several times, and reinstalled them also several times.
Any advice or suggestion will be appriciated.
Thx
"
46308,Error at importing! ,"### My 'tensorflow-GPU' was working fine yesterday but when I import it now it says-
 `Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\core\framework\function_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\google\protobuf\__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""C:\Users\Sahil Singh\AppData\Local\Programs\Python\Python38\lib\site-packages\pkg_resources.py"", line 1479, in <module>
    register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)'`
`AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader`"
46307,I can't find the tensorflow-lite model at the specified location(new->other->tensorflow-lite model) What should i do?,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
46306,ModuleNotFoundError: No module named 'tensorflow.models',"hi,dear all friends
when I run the [cifar rp](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_train.py),
I find the problem down
could you please help me ?
"
46303,Add TFLite metadata support to TensorFlowLiteSwift,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
TFLite offers the ability to embed metadata into its models. For example, we can ship an image classifier with its labels included. https://www.tensorflow.org/lite/convert/metadata

Currently, the metadata is not accessible from the TensorFlowLiteSwift package. Available platforms can be found here: https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata

I am using a custom TFLite image classifier in my Swift-based iOS app, and I would like to be able to ship the labels within the model itself because we use Firebase Machine Learning to send model updates (sometimes with different labels) over-the-air.

With the APIs available today, we are able to embed this information into the model, but have no way of reading it in our Swift client.

**Will this change the current api? How?**
In theory, this should be an additive API change similar to that in the Java and C++ packages. Existing dependents should not be affected, but will be able to opt-in to this new behavior.

**Who will benefit with this feature?**
Dependents of the TensorFlowLiteSwift package that want to be able to read metadata from their TFLite models, such as labels for image classifiers, etc...

**Any Other info.**
My guess is that the TensorFlow team already intends to do this eventually. Because I couldn't find any indication of this, I wanted to explicitly track such availability with this issue.
"
46297,gtest headers result in conflicting clang-format requirements,"@tensorflow/micro

While porting OPs from lite to micro, one of the intermediate steps results in test code copied over from lite that has gtest headers. While this code is not (and can not) be compiled for TFLM it still trips up the formatting checks as described in https://github.com/tensorflow/tensorflow/pull/46159#discussion_r553568396.

Deleting these includes (the workaround in https://github.com/tensorflow/tensorflow/pull/46159#discussion_r553568396) works just fine and it would be nice to give pull request authors this feedback directly via the TF Micro CI instead of waiting for the change to be imported internally before the error is detected.

The overarching goal is to get to a place where if a pull request passes the external CI, it also passes the internal CI (unless the code that a PR is breaking is internal-only)."
46296,Unable to build debug version,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19042
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): Git branch master, commit 277e22a01540b76151f5a664c8fd08854663fe27 (08.01.2021 12:05)
- Python version: Python 3.8.7
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Visual Studio 2019 (MSVC 19.28.29335)
- CUDA/cuDNN version: 11.1/8
- GPU model and memory: GeForce RTX 2070 SUPER (4 GB)

**Describe the problem**
I try to build CPU-Debug, CPU-Release, GPU-Debug and GPU-Release with the following commands:
```
bazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=dbg tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c
bazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=opt tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c
bazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=dbg --config=cuda tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c
bazel --max_idle_secs=10 build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc --config=opt --config=cuda tensorflow:install_headers tensorflow:tensorflow.dll tensorflow:tensorflow_dll_import_lib tensorflow:tensorflow_cc tensorflow/lite:tensorflowlite tensorflow/lite/c:tensorflowlite_c
```
Building the release versions works for both CPU and GPU. When trying to build the debug versions I get the following error messages (for CGU and GPU):
```
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<signed char,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<bool,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned short,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<int,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned char,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<__int64,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned int,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<unsigned __int64,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,bool>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,signed char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<float,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,__int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned char>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,Eigen::half>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned short>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,float>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned __int64>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,double>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,Eigen::bfloat16>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,std::complex<float> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<double,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::bfloat16,std::complex<double> >"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,unsigned int>"": must return a value
C:\users\transporter\_bazel_transporter\7ekuv3jw\execroot\org_tensorflow\tensorflow\compiler\xla\literal.cc(1391) : error C4716: ""xla::`anonymous namespace'::BitcastBetweenNativeTypes<Eigen::half,std::complex<double> >"": must return a value
```
Are warnings perhaps treated as errors in the debug variant?"
46295,tensorflow/c/eager/c_api_test fails to find GPU implementation of MatMulOp,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6.4.38
- GPU model and memory: GTX1080TI

**Describe the current behavior**

The 4 tests `CAPI.TensorHandleSilentCopy*` from `tensorflow/c/eager/c_api_test` fail each with 

```
tensorflow/c/eager/c_api_test.cc:442: Failure
Expected equality of these values:
  TF_GetCode(status.get())
    Which is: 3
  TF_OK
    Which is: 0
Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:GPU:
0, /job:localhost/replica:0/task:0/device:CPU:0].
```

Reason seems to be that `MatMulOp` is not found for GPU which is odd as seemingly everything else works.

**Standalone code to reproduce the issue**

`/tmp/ebbuild/TensorFlow/2.4.0/fosscuda-2019b-Python-3.7.4/tmp004t3B-bazel-tf/7277201245461b79db55d0e3e6d95f77/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_test_gpu --gtest_filter=*TensorHandleSilentCopy*`

**Other info / logs** 

For some reason this test works on our POWER9 nodes which have otherwise the exact same environment (same software versions etc), but use V100s"
46294,Linear aglebra C++ API,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): r2.1 c++ API
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Is there any plan to extend the current c++ API to include ops such as those from tf.linalg in the python API? Atm I only see a Inv op but computes the inverse element-wise. My solution at the moment would be to write a new op that uses eigen to perform the different operations but it seems that eigen is able to run in a CUDA kernel yet. 

**Will this change the current api? How?**

Adding a new set of OPs.

**Who will benefit with this feature?**

It would be very handy for graphs that do not only use neural networks. Use case: I'm currently implementing a sampled based controller for a robot that will run on a GPU. Hence I don't want to run the python interpreter as we will collect a lot of data from sensors and need to process everything on the fly. The controller performs a bunch of linear algebra calculation and run a NN in the middle of it, all of the operations being performed once for every sample. The high number of samples requires a GPU to be a real time controller. "
46291,Input y of 'Greater' op has type float32 that does not match type int64 of x,"I have this code below:
```
tf.contrib.metrics.precision_at_recall(tf.cast(tf.argmax(tf.nn.softmax(query_preds[i], dim=1), axis=1), tf.float32), tf.argmax(query_y, axis=1), target_recall=0.3)
```

It keeps giving me this error. I tried casting as you can see but still not working."
46289,How to convert TF1.12 checkpoints to saved/keras model format.,"I have a TTS model checkpoints trained on TFv1.12. For inferencing, I have used 
`self.session = tf.Session(config=config)
self.session.run(tf.global_variables_initializer())
saver = tf.train.Saver()
saver.restore(self.session, checkpoint_path)
`
For its TFlite conversion, saved/keras model is required. How do I convert this model into TFLite."
46288,TF2.1 .h5 file to .pb,"```
from tensorflow.python.keras.models import load_model
import tensorflow as tf


H5Path = ""xxx.h5""
YoloV5s_model = load_model(H5Path, compile=False)
YoloV5s_model.summary()
YoloV5s_model.save(""a.pb"")
```



2021-01-08 20:36:37.248006: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""TFLoadH5AndInfer.py"", line 18, in <module>
    YoloV5s_model.save(""Yolov5s_1.pb"")
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1008, in save
    signatures, options)
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\keras\saving\save.py"", line 115, in save_model
    signatures, options)
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\save.py"", line 78, in save
    save_lib.save(model, filepath, signatures, options)
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\saved_model\save.py"", line 899, in save
    _ = _SaveableView(checkpoint_graph_view)
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\saved_model\save.py"", line 165, in __init__
    self.checkpoint_view.objects_ids_and_slot_variables())
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\graph_view.py"", line 415, in objects_ids_and_slot_variables
    trackable_objects, path_to_root = self._breadth_first_traversal()
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\graph_view.py"", line 199, in _breadth_first_traversal
    for name, dependency in self.list_dependencies(current_trackable):
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\saved_model\save.py"", line 113, in list_dependencies
    for name, dep in super(_AugmentedGraphView, self).list_dependencies(obj):
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\graph_view.py"", line 159, in list_dependencies
    return obj._checkpoint_dependencies
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\data_structures.py"", line 744, in __getattribute__
    return object.__getattribute__(self, name)
  File ""C:\ProgramData\anaconda3\lib\site-packages\tensorflow_core\python\training\tracking\data_structures.py"", line 786, in _checkpoint_dependencies
    ""ignored."" % (self,))
ValueError: Unable to save the object {1: ListWrapper([0, 0, 0, 0]), 2: ListWrapper([0, 0, 0, 0]), 3: ListWrapper([1, 2, 2, 1])} (a dictionary wrapper constructed automatically on attribute assignment). The wrapped dictionary contains a non-string key which maps to a trackable object or mutable data structure.

If you don't need this dictionary checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored.








"
46286,Cannot create tf.constant inside tf.function with integer tensor.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary (Google Colab)**
- TensorFlow version (use command below): **v2.4.0-0-g582c8d236cb 2.4.0**
- Python version: **Python 3.6.9**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**


**Describe the current behavior**
It is impossible to create a `tf.constant` inside a function wrapped by `tf.function` if the argument to `tf.constant` is an integer Tensor.

**Describe the expected behavior**
It is expected that such operations do not raise an error. For example in case of slightly more advanced postprocessing.
Unless this behaviour is desired, this issue can be closed. I would however, greatly appreciate an explanation.

**Standalone code to reproduce the issue**
The following snippet will work with eager execution:
```python
def function():
    a = int(tf.random.normal(shape=()))
    tf.print(a)

    constant = tf.constant([a])
    tf.print(constant)
```
Will output:
```
-1
[-1]
```

However, after wrapping in `tf.function` an error is raised:
```python
wrapped = tf.function(function)
wrapped()
```

Raises:
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

TypeError: in user code:

    <ipython-input-97-0acfdad4a1be>:5 function  *
        constant = tf.constant([a])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:265 constant  **
        allow_broadcast=True)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl
        allow_broadcast=allow_broadcast))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:457 make_tensor_proto
        _AssertCompatible(values, dtype)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:334 _AssertCompatible
        raise TypeError(""Expected any non-tensor type, got a tensor instead."")

    TypeError: Expected any non-tensor type, got a tensor instead.
```

**Other info / logs**: **N/A**
"
46285,'tf.Size' op is neither a custom op nor a flex op,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10.0.18363
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: n/a
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: tensorflow-2.4.0
-   **Python version**: 3.8.5
-   **Bazel version (if compiling from source)**: n/a
-   **GCC/Compiler version (if compiling from source)**: n/a
-   **CUDA/cuDNN version**: CUDA 11.0 / CuDNN 8.0.5
-   **GPU model and memory**: Quadro P2000 4GB
-   **Exact command to reproduce**: Run ""Convert a SavedModel"" from https://www.tensorflow.org/lite/convert

### Describe the problem
For some unknown reason (to me) I am no longer able to convert a model to TFLite for usage on a Raspberry Pi. When I run the script mentioned in the link above, the following error appears:

```
loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): error: 'tf.Size' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
Traceback (most recent call last):
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 210, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
 
 
During handling of the above exception, another exception occurred:
 
Traceback (most recent call last):
  File ""convert_to_tflite_v2.py"", line 7, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 739, in convert
    result = _convert_saved_model(**converter_kwargs)
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 632, in convert_saved_model
    data = toco_convert_protos(
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
```

I have read more issues regarding Flex Ops, unfortunately I haven't found a suitable solution as of now. I tried adding the following line:

```
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
```

When this line is present, the model converts very smoothly to a TFlite model, but then the TFLite model doesn't work anymore on a Raspberry Pi, with the following error: 

```
Unexpected failure when preparing tensor allocations: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
```

I never had this before for any other model. The model works perfectly fine with regular TensorFlow. It's just the TFlite part which causes issues for me.

I am using a pre-trained model (SSD MobileNet V2 FPNLite 320x320) from the Detection Model Zoo at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

### Source code / logs
I started off by installing a fresh install of Tensorflow. See:

```
(base) C:\Users\Reno>conda activate tensorflow
 
(tensorflow) C:\Users\Reno>pip cache purge
ERROR: No matching packages
 
(tensorflow) C:\Users\Reno>pip list
Package      Version
------------ -------------------
certifi      2020.12.5
pip          20.3.3
setuptools   51.0.0.post20201207
wheel        0.36.2
wincertstore 0.2
 
(tensorflow) C:\Users\Reno>pip install --upgrade tensorflow
Collecting tensorflow
  Downloading tensorflow-2.4.0-cp38-cp38-win_amd64.whl (370.7 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 370.7 MB 21 kB/s
Requirement already satisfied: wheel~=0.35 in c:\users\reno\.conda\envs\tensorflow\lib\site-packages (from tensorflow) (0.36.2)
Collecting gast==0.3.3
  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)
Collecting absl-py~=0.10
  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 2.2 MB/s
Collecting astunparse~=1.6.3
  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers~=1.12.0
  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Collecting google-pasta~=0.2
  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 1.5 MB/s
Collecting grpcio~=1.32.0
  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6 MB 2.2 MB/s
Collecting h5py~=2.10.0
  Downloading h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5 MB 803 kB/s
Collecting keras-preprocessing~=1.1.2
  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 3.2 MB/s
Collecting numpy~=1.19.2
  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.3 MB 3.3 MB/s
Collecting opt-einsum~=3.3.0
  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65 kB 2.2 MB/s
Collecting protobuf>=3.9.2
  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173 kB 2.2 MB/s
Collecting six~=1.15.0
  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting tensorboard~=2.4
  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.6 MB 2.2 MB/s
Requirement already satisfied: setuptools>=41.0.0 in c:\users\reno\.conda\envs\tensorflow\lib\site-packages (from tensorboard~=2.4->tensorflow) (51.0.0.post20201207)
Collecting google-auth<2,>=1.6.3
  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114 kB 2.2 MB/s
Collecting cachetools<5.0,>=2.0.0
  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8
  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96 kB 3.2 MB/s
Collecting pyasn1-modules>=0.2.1
  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155 kB 2.2 MB/s
Collecting pyasn1<0.5.0,>=0.4.6
  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 2.6 MB/s
Collecting requests<3,>=2.21.0
  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 2.0 MB/s
Requirement already satisfied: certifi>=2017.4.17 in c:\users\reno\.conda\envs\tensorflow\lib\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)
Collecting chardet<5,>=3.0.2
  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178 kB 2.2 MB/s
Collecting idna<3,>=2.5
  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 2.0 MB/s
Collecting requests-oauthlib>=0.7.0
  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Collecting oauthlib>=3.0.0
  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147 kB 2.2 MB/s
Collecting rsa<5,>=3.1.4
  Downloading rsa-4.6-py3-none-any.whl (47 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47 kB 3.2 MB/s
Collecting tensorboard-plugin-wit>=1.6.0
  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779 kB 2.2 MB/s
Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0
  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462 kB 2.2 MB/s
Collecting termcolor~=1.1.0
  Downloading termcolor-1.1.0.tar.gz (3.9 kB)
Collecting typing-extensions~=3.7.4
  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)
Collecting urllib3<1.27,>=1.21.1
  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136 kB 2.2 MB/s
Collecting werkzeug>=0.11.15
  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298 kB 2.2 MB/s
Collecting wrapt~=1.12.1
  Downloading wrapt-1.12.1.tar.gz (27 kB)
Building wheels for collected packages: termcolor, wrapt
  Building wheel for termcolor (setup.py) ... done
  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=841391132c29825394e6b0ab2a1e79ffb1f6a1867cd2a214dc65cc3c4aa5b688
  Stored in directory: c:\users\reno\appdata\local\pip\cache\wheels\a0\16\9c\5473df82468f958445479c59e784896fa24f4a5fc024b0f501
  Building wheel for wrapt (setup.py) ... done
  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33672 sha256=7efbaa96078fc7ecd811c170d66cc642ab8fd8f88a41af40ba118c5b7c16765f
  Stored in directory: c:\users\reno\appdata\local\pip\cache\wheels\5f\fd\9e\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73
Successfully built termcolor wrapt
Installing collected packages: urllib3, pyasn1, idna, chardet, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.2 werkzeug-1.0.1 wrapt-1.12.1

(tensorflow) C:\Users\Reno>cd C:\Users\Reno\Documents\TensorFlow\workspace\training

(tensorflow) C:\Users\Reno\Documents\TensorFlow\workspace\training>python convert_to_tflite_v2.py

2021-01-08 11:30:11.595435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-01-08 11:30:19.711062: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-08 11:30:19.715933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-01-08 11:30:20.065510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1
coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s
2021-01-08 11:30:20.072148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-01-08 11:30:20.083133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-01-08 11:30:20.088475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-01-08 11:30:20.099554: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-01-08 11:30:20.105734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-01-08 11:30:20.118062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-01-08 11:30:20.124535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-01-08 11:30:20.130482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-01-08 11:30:20.133532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-08 11:30:20.136101: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-08 11:30:20.144289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1
coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s
2021-01-08 11:30:20.152136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-01-08 11:30:20.155789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-01-08 11:30:20.159025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-01-08 11:30:20.162087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-01-08 11:30:20.165806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-01-08 11:30:20.171067: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-01-08 11:30:20.174766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-01-08 11:30:20.180898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-01-08 11:30:20.185452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-08 11:30:20.767230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-08 11:30:20.771172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-08 11:30:20.773087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-08 11:30:20.775197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-01-08 11:30:20.783136: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-08 11:31:15.040028: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.
2021-01-08 11:31:15.046343: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.
2021-01-08 11:31:15.052724: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.
2021-01-08 11:31:15.059159: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/
2021-01-08 11:31:15.152959: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }
2021-01-08 11:31:15.155845: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/
2021-01-08 11:31:15.163162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-08 11:31:15.167694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]
2021-01-08 11:31:15.170225: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-08 11:31:15.517776: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2021-01-08 11:31:15.565923: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.
2021-01-08 11:31:16.326616: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: C:/Users/Reno/Documents/TensorFlow/workspace/training_walnoot/exported-models/walnoot_model_lite/saved_model/
2021-01-08 11:31:16.703255: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1644097 microseconds.
2021-01-08 11:31:20.471768: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-01-08 11:31:21.438534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro P2000 computeCapability: 6.1
coreClock: 1.607GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 89.53GiB/s
2021-01-08 11:31:21.445277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-01-08 11:31:21.452203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-01-08 11:31:21.456431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-01-08 11:31:21.461100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-01-08 11:31:21.467734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-01-08 11:31:21.473807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-01-08 11:31:21.479188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-01-08 11:31:21.485787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-01-08 11:31:21.491625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-08 11:31:21.495347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-08 11:31:21.499727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-08 11:31:21.502749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-08 11:31:21.505539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-01-08 11:31:21.513866: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): error: 'tf.Size' op is neither a custom op nor a flex op
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
Traceback (most recent call last):
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 210, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
 
 
During handling of the above exception, another exception occurred:
 
Traceback (most recent call last):
  File ""convert_to_tflite_v2.py"", line 7, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 739, in convert
    result = _convert_saved_model(**converter_kwargs)
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 632, in convert_saved_model
    data = toco_convert_protos(
  File ""C:\Users\Reno\.conda\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(""Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519"" at ""StatefulPartitionedCall@__inference_signature_wrapper_25508"") at ""StatefulPartitionedCall"")): 'tf.Size' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.Size {device = """"}
 
 
(tensorflow) C:\Users\Reno\Documents\TensorFlow\workspace\training>
```

Why is this error appearing just now, what does it mean, and how can I convert a model to TFlite without any of these nasty errors and subsequently run it on a Raspberry Pi without any hassle.

Thanks in advance."
46284,TFLite: GPU delegate (OpenGL) outputs errors when different context is active.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3
- Python version: -
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
I have a problem running TFLite's GPU delegate on a mobile device without OpenCL support (which means the delegate falls back to the OpenGL implementation obviously).
```
I/tflite: Initialized TensorFlow Lite runtime.
I/tflite: Created TensorFlow Lite delegate for GPU.
E/tflite: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found
E/tflite: Falling back to OpenGL
E/libEGL: call to OpenGL ES API with no current context (logged once per thread)
```
Got error:
```
E/tflite: TfLiteGpuDelegate Invoke: ToTensorConverter: input data size does not match expected size.
E/tflite: Node number 130 (TfLiteGpuDelegateV2) failed to invoke.
E/tflite: TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.
E/tflite: Node number 87 (TfLiteGpuDelegateV2) failed to invoke.
```
Problem is that my app (which is using Unity3D engine) is creating its own OpenGL context (I don't have access to rendering context, so I can not change it back to TFLite one after rendering) which is active and TFLite can not copy data to GPU.
I think TFLite should activate its own context before making any operations on GPU when running OpenGL. 
**Describe the expected behavior**
TFLite works when app changes current OpenGL context to different than the one used by TFLite."
46282,tf.io.gfile.GFile does not raise an error when given a directory,"Python's `open` does not accept directories. The following results in an `IsADirectoryError`:

```
    with open('.', 'r') as f:
        print(f.readlines())
```

However, if you use `tf.io.gfile.GFile` instead, you will get an empty list and no error:

```
    with tf.io.gfile.GFile('.', 'r') as f:
        print(f.readlines())

```

Not sure if this is a feature or a bug, but it is not the behavior I would expect, given that the [documentation](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) says that `tf.io.gfile` is meant to provide an API close to Python's file IO objects."
46281,why ExtractVolumePatches uses different implements for ThreadPoolDevice and GPUDevice.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.04
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.5
- Python version:3.6
- Bazel version (if compiling from source):3.7.1
- GCC/Compiler version (if compiling from source):7.5.0
- CUDA/cuDNN version:10.1
- GPU model and memory:7.0GB


**Describe the current behavior**
in tensorflow/core/kernels/eigen_volume_patch.h
```c++
650 OVERRIDE_EVALUATOR(Eigen::ThreadPoolDevice);
651 OVERRIDE_EVALUATOR(Eigen::DefaultDevice);
```
Overrides the TensorExecutor to modify the PADDING_SAME to
```c++
  131         case PADDING_SAME: {                                                                                                                                                                                                                     
  132           m_outputPlanes = Eigen::divup(m_input_planes_eff, m_plane_strides);
  133           m_outputRows = Eigen::divup(m_input_rows_eff, m_row_strides);
  134           m_outputCols = Eigen::divup(m_input_cols_eff, m_col_strides);
  135           const Index dz = numext::maxi<DenseIndex>(
  136               0, (m_outputPlanes - 1) * m_plane_strides + m_patch_planes_eff -
  137                      m_input_planes_eff);
  138           const Index dy = numext::maxi<DenseIndex>(
  139               0, (m_outputRows - 1) * m_row_strides + m_patch_rows_eff -
  140                      m_input_rows_eff);
  141           const Index dx = numext::maxi<DenseIndex>(
  142               0, (m_outputCols - 1) * m_col_strides + m_patch_cols_eff -
  143                      m_input_cols_eff);
  144           m_planePaddingTop = dz / 2;
  145           m_rowPaddingTop = dy / 2;
  146           m_colPaddingLeft = dx / 2;
  147           break;
  148         }
```

but for GPUDevice, we don't do this override. We use default eigen implement:
```c++
  257         case PADDING_SAME: {                                                                                                                                                                                                                     
  258           m_outputPlanes = numext::ceil(m_input_planes_eff / static_cast<float>(m_plane_strides));
  259           m_outputRows = numext::ceil(m_input_rows_eff / static_cast<float>(m_row_strides));
  260           m_outputCols = numext::ceil(m_input_cols_eff / static_cast<float>(m_col_strides));
  261           const Index dz = m_outputPlanes * m_plane_strides + m_patch_planes_eff - 1 - m_input_planes_eff;
  262           const Index dy = m_outputRows * m_row_strides + m_patch_rows_eff - 1 - m_input_rows_eff;
  263           const Index dx = m_outputCols * m_col_strides + m_patch_cols_eff - 1 - m_input_cols_eff;
  264           m_planePaddingTop = dz - dz / 2;
  265           m_rowPaddingTop = dy - dy / 2;
  266           m_colPaddingLeft = dx - dx / 2;
  267           break;
  268         }
```

The question is: Why do we use different implements for CPU/GPU devices?

**Describe the expected behavior**
Different devices should produce same results.

"
46280,tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found,"System informationï¼š

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):1.15.0
Python version:3.6
CUDA/cuDNN version: 10.0
GPU model and memory:7.4

Describe the current behaviorï¼š
I used tensorflow1.15 to train my codeï¼Œafter training 33 epochs raise ""tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found""

epoch  32 end time is : 2021-01-08 14:21:29.075227
train files shuffled!
is training ep :  33
total train batch num: 100
ep 33 i 0 psemce 0.0 bbvert -0.23279694 l2 0.060497742 ce 0.26028115 siou -0.5535758 bbscore 0.0038582273 pmask 0.6467816
ep 33 i 0 test psem 0.0 bbvert 1.9851223 l2 0.059121773 ce 2.2534707 siou -0.3274702 bbscore 0.0048341155 pmask 3.4519947
test pred bborder [[2 1 0]]
ep 33 i 20 psemce 0.0 bbvert -0.44303733 l2 0.030050844 ce 0.16678412 siou -0.6398723 bbscore 0.0026201883 pmask 0.63400114
ep 33 i 20 test psem 0.0 bbvert -0.4450612 l2 0.04898257 ce 0.23810822 siou -0.732152 bbscore 0.0016184862 pmask 0.47476012
test pred bborder [[2 0 1]]
ep 33 i 40 psemce 0.0 bbvert 0.43996847 l2 0.08725857 ce 0.8109299 siou -0.45822 bbscore 0.0034747643 pmask 0.9270937
ep 33 i 40 test psem 0.0 bbvert -0.07955924 l2 0.040802542 ce 0.4581066 siou -0.5784684 bbscore 0.0087565 pmask 1.0110209
test pred bborder [[2 0 1]]
ep 33 i 60 psemce 0.0 bbvert 0.057684183 l2 0.071036406 ce 0.5709717 siou -0.58432394 bbscore 0.00046247765 pmask 0.48829234
ep 33 i 60 test psem 0.0 bbvert -0.3817188 l2 0.03684793 ce 0.2770622 siou -0.69562894 bbscore 0.0019779946 pmask 0.54431504
test pred bborder [[2 0 1]]
ep 33 i 80 psemce 0.0 bbvert 0.038050413 l2 0.034902867 ce 0.6071266 siou -0.60397905 bbscore 0.015431552 pmask 0.8978894
ep 33 i 80 test psem 0.0 bbvert 1.1076844 l2 0.07785928 ce 1.3761435 siou -0.34631833 bbscore 0.033992507 pmask 1.7343999
test pred bborder [[2 0 1]]
model saved in :  ./log/train_mod/model033.cptk
epoch  33 end time is : 2021-01-08 14:21:44.245053
train files shuffled!
is training ep :  34
total train batch num: 100
ep 34 i 0 psemce 0.0 bbvert -0.41581324 l2 0.057975773 ce 0.28829214 siou -0.76208115 bbscore 0.0003172583 pmask 0.34254307
ep 34 i 0 test psem 0.0 bbvert 1.7912706 l2 0.08668331 ce 2.017744 siou -0.31315675 bbscore 0.00576146 pmask 2.1253805
test pred bborder [[0 2 1]]
ep 34 i 20 psemce 0.0 bbvert -0.14073128 l2 0.034625944 ce 0.4937689 siou -0.6691261 bbscore 0.0047056335 pmask 0.7088615
ep 34 i 20 test psem 0.0 bbvert 1.9534252 l2 0.0907397 ce 2.1705353 siou -0.3078499 bbscore 0.0019757028 pmask 2.682012
test pred bborder [[0 2 1]]
ep 34 i 40 psemce 0.0 bbvert 0.27091432 l2 0.053299602 ce 0.7194822 siou -0.5018675 bbscore 0.001409175 pmask 0.60204554
ep 34 i 40 test psem 0.0 bbvert -0.28416353 l2 0.09286666 ce 0.19349718 siou -0.5705274 bbscore 0.003192804 pmask 0.21589296
test pred bborder [[1 0 2]]
2021-01-08 14:21:52.432564: W tensorflow/core/framework/op_kernel.cc:1639] Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py"", line 115, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


Traceback (most recent call last):
  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py"", line 115, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries


     [[{{node bbox/PyFunc}}]]
     [[gradients/backbone/fa_layer1/ThreeInterpolate_grad/ThreeInterpolateGrad/_425]]
  (1) Invalid argument: ValueError: matrix contains invalid numeric entries
Traceback (most recent call last):

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/home/liu/disk1/Life/3DBoNetPoint818a(linux)/helper_net.py"", line 115, in assign_mappings_valid_only
    row_ind, col_ind = linear_sum_assignment(valid_cost)

  File ""/home/liu/anaconda3/envs/tf1.15/lib/python3.6/site-packages/scipy/optimize/_hungarian.py"", line 93, in linear_sum_assignment
    raise ValueError(""matrix contains invalid numeric entries"")

ValueError: matrix contains invalid numeric entries"
46279,delete pycharm project issue(Pycharm project deletion problem due to cache deletion),The project was deleted while using pycharm for tensorflow developer certification. It seems that the pycharm project was deleted while deleting the Mac OS cache and junk. Developers preparing for certification should be aware of this issue.
46278,TF2.4 MultiWorkerMirroredStrategy and ParameterServerStrategy training very slow for large mode and large data compared with TF1.14,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**TF on yarn**(running machine system is CentOS 7)
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.4.0
- Python version:3.6
- CUDA/cuDNN version: no gpu
- GPU model and memory: no gpu
- CPU info: please see it below
- Training config: [MultiWorkerMirroredStrategy] chief/woker: 8 instances, 8 cpus per instance, 120G memory per instance; [ParameterServerStrategy] chief:  8cpus,25G memory; worker: 30 instances, 8 cpus per instance, 25G memory per instance;ps: 8 instances, 12 cpus per instance, 25G memory per instance(which is the same with training using TF1.14)

**Describe the current behavior**

I built a wide&deep based model  with about **1.3 billion trainable params** which is used for ranking items for industrial recommend ranking system.
When I trained the same model using parameter server strategy on TF 1.14(asynchronous training) with about **4 billion training samples**ï¼Œit only costs about 23 hours, but when I upgrade model to TF 2.4 using keras api, training becomes very slow (about 5597 hours(ETA) of MultiWorkerMirroredStrategy, at least 72 hours of ParameterServerStrategy)

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Sorry, I can just paste model summary here because of the code complexity, you can see it beflow. I only paste wide model and input of deep model here. deep model is just a [512, 512,512, 512] sequential model. All features name are replaced for privacy so there maybe some mistake here.

**Other info / logs** 
"
46277,ESP32 example creation fails for person_detection example,"**System information**
- OS Platform and Distribution: ESP32
- TensorFlow version: master, commit 78012e5d6c45ffd3673b8970b199a67980df1963

**Describe the problem**

Example creation fails with following error:

```
% make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
make: *** No rule to make target 'generate_person_detection_esp_project'.  Stop.
```
"
46276,Don't raise an Error on unknown kwargs,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version 2.1 - 2.3:
- Are you willing to contribute it (No):


**Describe the feature and the current behavior/state.**

Almost all Keras objects support **kwargs but raising an TypeError if a 'Keyword argument (is) not understood'. i.e.

```
tf.keras.layers.Dense(8, foo='bar')
```

I wish a more Duck Typing behavior that either ignores unknown args or raises a warning according to the debug level.

**Who will benefit with this feature?**

Simplifies the parameter management for inheriting classes or adapter classes.

```
class MyLayer(tf.keras.layers.Layer):
   def __init__(self, units=8, dims=(0,2,1), **kwargs):
      super(MyLayer, self).__init__(**kwargs)
      self.layer = tf.keras.layers.Dense(units, **kwargs)
      self.permute = tf.keras.layers.Permute(dims, **kwargs)
      [...]
```
"
46274,tf.experimental.numpy fails with plt.hist,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): using example from TF docs
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 / 8.0.4
- GPU model and memory: GTX 1660 Ti, 6 GB

**Describe the current behavior**
Just trying to plot a histogram from a random standard normal distribution array made with tf.experimental.numpy
The code is copied from here: https://www.tensorflow.org/guide/tf_numpy
What I get is a bunch of random lines in the histogram.
The values in the random matrices AFAICT seem fine - look like a normal distribution.

![bad](https://user-images.githubusercontent.com/901867/103975506-26b77b80-5129-11eb-89cd-b61bed58b5c8.png)

**Describe the expected behavior**
I should get the normal distribution bell curve in the histogram image.

![good](https://user-images.githubusercontent.com/901867/103975532-35059780-5129-11eb-87a4-3e5b1b16f3f1.png)

**Standalone code to reproduce the issue**
```
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow.experimental.numpy as tnp

# bad image - see above
labels1 = 15 + 2 * tnp.random.randn(1000)
_ = plt.hist(labels1)

# good image - see above
labels2 = 15 + 2 * np.random.randn(1000)
_ = plt.hist(labels2)

```
I run everything in Jupyter Notebook in Chrome.

Tensorflow is actually running on the GPU."
46273,Are mean and standard deviation values always 127.5 and 127.5,"In the example label_image.cc (under) lite/examples/label_image, we are hardcoding mean and standard deviation to 127.5f.
Will these values work for any model ?
Can these values obtained run time ?
To change these mean and standard deviation ? what is the place to change , training ? (or) conversion (or) Inference ?
"
46272,use of interpreter-->SetNumThreads. Do we need to invoke setNumThreads always to improve performance,"In the label_image.cc example from tfLite, we are passing setNumThreads manually under the code.
Does number of threads automatically picked by tfLite (or) Is it required by user to always force SetNumThreads ?
"
46269,tensorrt not working with cuda 11.2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.9.1
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: 11.2 / 8.0.5
- GPU model and memory:  GTX1080Ti 11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
tf workfs fine, but tftrt no

**Describe the expected behavior**
```
2021-01-08 10:32:10.702646: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/local/cuda/lib64:
```
after add soft link

```
2021-01-08 10:41:23.887753: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libnvinfer.so.7'; dlerror: /usr/local/cuda/lib64/libnvrtc.so.11.1: version `libnvrtc.so.11.1' not found (required by /usr/lib/x86_64-linux-gnu/libnvinfer.so.7); LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/local/cuda/lib64:
```

**Standalone code to reproduce the issue**
any tensorrt example code


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46268,Micro: port op GATHER_ND from Lite,"@tensorflow/micro

**System information**
Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): master
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op GATHER_ND to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.
PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.
"
46266,Normalisation layer save standard deviation instead of variance,"**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The Normalisation layer in tensorflow/python/keras/layers/preprocessing/normalization.py seems to save the variance instead of the standard deviation. And calculates the std on runtime std = sqrt(var).
This seems a bit wastfull to me as instead of saving the std once.
It would be a very small change with not much impact, but easy to implement.

**Will this change the current api? How?**
When using adapt() to initialize it doesn't matter which metric is saved -> no change in api
When directly initializing it, it could still take the variance (or std as an option) and save std in the background

**Any Other info.**
Could be changed at the next big change e.g: together with the behaviour I stated in [keras #14356](https://github.com/keras-team/keras/issues/14356#issue-781559372)"
46261,different TFLM builds use the same output directory.,"@tensorflow/micro

In https://github.com/tensorflow/tensorflow/pull/46242#discussion_r553049656, I was suggesting that the linker was not correctly dropping unused symbols.

In fact, what was very likely happening was that I did not do a `make clean` between switching to `BUILD_TYPE=release`. And since the TFLM makefile currently uses the same directory for all `BUILD_TYPE`, only the modified files were being rebuilt with the smaller `release` build.

We can reproduce this with the following sequence of commands:

First check what the binary size is for the release build.
```
make -f tensorflow/lite/micro/tools/make/Makefile clean

make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release

xt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark 
   text	   data	    bss	    dec	    hex	filename
  46080	  40204	  24952	 111236	  1b284	tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark
```

Next have some intermediate non-release objects and then do a release build:
```
make -f tensorflow/lite/micro/tools/make/Makefile clean

# build non-release
make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark

touch tensorflow/lite/micro/kernels/xtensa/fully_connected.cc

#build for release
make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release

xt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark 
   text	   data	    bss	    dec	    hex	filename
  54736	  48168	  25032	 127936	  1f3c0	tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark
```

What we really should be doing is to change the output directory based on the build type."
46258,Micro: port op EXPAND_DIMS from Lite,"@tensorflow/micro

**System information**
Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): master
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op EXPAND_DIMS to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes
PR 3: copy the EXPAND_DIMS kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build
PR4: make the EXPAND_DIMS micro kernel fully functional. Also finish the testing code.

(Update 2021-02-26: There is no function for EXPAND_DIMS in lite/kernels/internal/reference/reference_ops.h, and the TFLM op does not depend on a reference implementation, so PR 2 was skipped) 
(Update 2021-02-26: Added PR4 in the sequence)
"
46257,[XLA] dense_layer_test.py throws internal error in fallback path,"The failure happens in Master as well as r2.4 (these are the 2 branches that I've tested).

Even with lazy_compilation turned on (via `TF_XLA_FLAGS=--tf_xla_enable_lazy_compilation=true`), the first execution always compiles as per the current implementation. If we tweak this behaviour such that the first execution doesn't compile (and uses the fallback path) or use `--tf_xla_always_defer_compilation=true` to force the fallback path, the test `tensorflow/compiler/tests/dense_layer_test.py` fails with the following signature
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node cluster_3}} {{function_node cluster_3}} Trying to assign variable with wrong dtype. Expected INVALID got float
     [[{{node dense/kernel/Assign}}]]
     [[cluster_3_1/partitioned_call]]
```

There are 3 test points in the test. Based on the descriptions, they test that the dense layer node is properly compiled in jit scope. I am not sure if this test is supposed to be used for the fallback path. However, the failure is not merely a test failure but an internal error (`Trying to assign variable with wrong dtype. Expected INVALID got float`) which leads me to think there might be a bug. 

The error comes from handling of resource variables https://github.com/tensorflow/tensorflow/blob/13d37279f137a96ca6fa5142f20c8747103bf79e/tensorflow/core/kernels/resource_variable_ops.cc#L397-L401"
46255,small discrepancies between images loaded with `tf.keras.preprocessing.image_dataset_from_directory()` and `tf.keras.preprocessing.image.load_img()`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0
- Python version: 3.6.9 (Colab)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

The two keras functions
 `tf.keras.preprocessing.image_dataset_from_directory()`
and
`tf.keras.preprocessing.image.load_img()`
produce slightly different results when loading the same image and applying resizing.
I think the culprit might be the different resize operations used under the hood (tf's image_ops for the former vs PIL for the latter).


**Describe the expected behavior**

`image_dataset_from_directory()` is commonly used with model.fit() while `load_img()` with model.predict().
Intuitively, I expected the exact same tensor as output for the two loading/preprocessing pipelines most often used in keras.


**Standalone code to reproduce the issue**

Colab Gist reproducing the problem:
https://colab.research.google.com/gist/fabiocarrara/0d8a34e5ef805e9d91e82d9949365b1e/untitled0.ipynb

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

PIL Version: 7.0.0
"
46254,Cannot Convert SentencePieceTokenizer to TensorRT,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/7.6
- GPU model and memory: RTX 2080


**Describe the current behavior**
Whenever I try to convert a model containing the tokenizer as subgraph, I get an error.

**Describe the expected behavior**
It should just convert it.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1_S37VihkTZ1B0HgjW8D7DMZcI8nwz2Bk

**Other info / logs**
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-7-e780e9ed6737> in <module>
      4 )
      5 
----> 6 converter.convert()

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py in convert(self, calibration_input_fn)
   1094                                   self._input_saved_model_tags)
   1095     func = self._saved_model.signatures[self._input_saved_model_signature_key]
-> 1096     frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
   1097     grappler_meta_graph_def = saver.export_meta_graph(
   1098         graph_def=frozen_func.graph.as_graph_def(), graph=frozen_func.graph)

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow, aggressive_inlining)
   1069       func=func,
   1070       lower_control_flow=lower_control_flow,
-> 1071       aggressive_inlining=aggressive_inlining)
   1072 
   1073   output_graph_def, converted_input_indices = _replace_variables_by_constants(

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_allowlist, variable_names_denylist)
    804         variable_names_allowlist=variable_names_allowlist,
    805         variable_names_denylist=variable_names_denylist)
--> 806     self._build_tensor_data()
    807 
    808   def _build_tensor_data(self):

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)
    823         data = map_index_to_variable[idx].numpy()
    824       else:
--> 825         data = val_tensor.numpy()
    826       self._tensor_data[tensor_name] = _TensorData(
    827           numpy=data,

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in numpy(self)
   1069     """"""
   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1073 

~/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1037       return self._numpy_internal()
   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1040 
   1041   @property

~/anaconda3/envs/ds/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.
```
"
46253,Saved weights as checkpoints are randomised on loading them in the model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, i have a custom U-Net model with conv1d and custom loss function for ignoring the ignore labels
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2021-01-07 16:16:22.597961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.3
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The checkpoints (save_weights) after successfully training the model randomise after loading them into model. The accuracy and precision plummet drastically on the same data which showed 99+ % accuracy while training.
**Describe the expected behavior**
The checkpoints must show the same results after loading, as while the training.

**Standalone code to reproduce the issue**
input_layer = l.Input((seq_len, seq_dep), name='seq')
    model_unet_1d = get_unet(input_layer, labels-1, 16, dropout=0.05, batchnorm=True)
    model_unet_1d.compile(optimizer=Adam(), loss=custom_loss, metrics=[custom_metrics])
    
    print(""<<<< Model Compiled >>>>"")
    
    callbacks = [
    EarlyStopping(patience=10, verbose=1),
    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),
    ModelCheckpoint('model_chkp/model-ctc_4tiles_bce_sam.h5', verbose=1, save_best_only=True, save_weights_only=True)
    ]
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46252,TensorBoard callback with update_freq='batch' fails when fitting multiple times,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes. The bug happens in the minimal example in the TensorFlow documentation, with a small tweak.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Mojave 10.14.6
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: Python 3.8.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**

If we add a callback for TensorBoard when fitting a Keras Model (say `model`) as in `callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs', update_freq='batch')]`,

calling the function `model.fit` twice causes an error (see logs below)

**Describe the expected behavior**

Ideally we would be able to call `model.fit` a many times as we want. If we don't use the `update_freq` parameter, it does not give an error and behaves as expected.

**Standalone code to reproduce the issue**
Any model will do. The one in the tensorflow docs tutorial, for example:

```bash
import tensorflow as tf
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])

callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs', update_freq='batch')]

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

model.fit(x_train, y_train, callbacks=callbacks)
model.fit(x_train, y_train, callbacks=callbacks)
```

**Other info / logs** 

Log of the error:

```bash
1875/1875 [==============================] - 3s 1ms/step - loss: 0.4814 - accuracy: 0.8604
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-24-6d870932cb3b> in <module>
     17 
     18 model.fit(x_train, y_train, callbacks=callbacks)
---> 19 model.fit(x_train, y_train, callbacks=callbacks)

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    853       # In this case we have created variables on the first call, so we run the
    854       # defunned version which is guaranteed to never create variables.
--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    856     elif self._stateful_fn is not None:
    857       # Release the lock early so that multiple threads can perform the call

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2940       (graph_function,
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1916         and executing_eagerly):
   1917       # No tape is watching; skip to running the function.
-> 1918       return self._build_call_outputs(self._inference_function.call(
   1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    553       with _InterpolateFunctionError(self):
    554         if cancellation_manager is None:
--> 555           outputs = execute.execute(
    556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

NotFoundError:  Resource localhost/_AnonymousVar1925/N10tensorflow22SummaryWriterInterfaceE does not exist.
	 [[{{node cond/then/_0/batch_loss}}]] [Op:__inference_train_function_108360]

Function call stack:
train_function
```"
46251,Getting TypeError: when using label smoothing in Categorical Cross entropy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):2.3.1
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version Colab default 
- GPU model and memory:V100 SXM2

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


I am using label smoothing with Categorical Cross entropy. When I turn off label smoothing the model works fine but when I turn it The model gives the following error
`    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
`
I am not able to understand what is the problem with my code?. Here is My code

https://drive.google.com/file/d/1lEVPMOtWOWesDmpRDgTJQPTFOQ6UTI2o/view?usp=sharing

I have tried setting the dtype to float16 in Image Data Generator. But that also does not work. Can somebody Help me? I have not got any answers on stack overflow also

"
46249,TypeError: '<' not supported between instances of 'function' and 'str',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Ubuntu)
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.9

**Code**
I've save a model using callbacks:

```python
mc = tf.keras.callbacks.ModelCheckpoint( filepath=save_directory + '/trial10_model.h5',
                                        monitor=""val_loss"",
                                        save_freq=""epoch"",
                                        save_best_only=True,
                                        save_weights_only=False)
callback_list = [mc]

history = model.fit_generator(get_train_set_,validation_data = get_val_set_, validation_steps=validation_steps,
                              steps_per_epoch = steps_per_epoch,epochs=15, callbacks=callback_list)
```

While testing the model using evaluate or evaluate_generator:
```python
model = load_model(""/content/gdrive/MyDrive/keras_tuner/good_trail/trial10_model.h5"", custom_objects={ 'dice_coef': dice_coef })
res = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1)
```
**Error Message**
I get this error message:
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-56-3ae064222286> in <module>()
     12 callback_list = [mc]
     13 
---> 14 res = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1, callbacks=callback_list)

9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, ""ag_error_metadata""):
--> 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1219 run_step  **
        with ops.control_dependencies(_minimum_control_deps(outputs)):
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2793 _minimum_control_deps
        outputs = nest.flatten(outputs, expand_composites=True)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:341 flatten
        return _pywrap_utils.Flatten(structure, expand_composites)

    TypeError: '<' not supported between instances of 'function' and 'str'
```"
46248,Tensorboard callback not logging learning rate when not using LearningRateSchedule (e.g. when using ReduceLROnPlateau),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: pip3
- TensorFlow version: tf-nightly 2.5.0-dev20210104
- Python version: 3.8.5
- CUDA/cuDNN version: 11.0.3 / 8.0.4
- GPU model and memory: Quadro RTX 5000 / 16384 MB

**Describe the current behavior**  
If there is no LearningRateSchedule (either LR is constant or is changed via callback) then [Tensorboard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) doesn't log learning rate. This is especially relevant when using [ReduceLROnPlateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau) callback.

**Describe the expected behavior**  
Learning rate should be always logged or at least in the case of using a callback.

**Standalone code to reproduce the issue**  
```
import tensorflow as tf
import datetime
import os

class SimpleModel(tf.keras.Model):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.Dense_in = tf.keras.layers.Dense(units = 50, 
                                activation=tf.nn.relu, 
                                use_bias=True)
        self.Dense_hidden = tf.keras.layers.Dense(units = 20, 
                                activation=tf.nn.relu, 
                                use_bias=True)
        self.Dense_out = tf.keras.layers.Dense(units = 1, 
                                activation=None, 
                                use_bias=True)

    def call(self, inputs, training = True):
        X = self.Dense_in(inputs)
        X = self.Dense_hidden(X)
        X = self.Dense_out(X)
        return X

dummy_data = tf.random.normal((10000, 50))
dummy_y = tf.random.uniform((10000,), minval=0, maxval=2, dtype=tf.dtypes.int32)


#### LR is not logged here ####
model = SimpleModel()

loss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate = 0.001)
model.compile(optimizer = opt, loss = loss, metrics=[""acc""])

_datetime = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
log_dir = os.path.join('dummy_logs', _datetime)
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

model.fit(x = dummy_data, y = dummy_y, epochs = 10,
        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', 
                        factor=0.5, patience=10, verbose=0, mode='auto', 
                        min_delta=0.0001, cooldown=0, min_lr=0),
                    tb_callback])

#### LR is logged here ####
model = SimpleModel()

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate = 0.001,
    decay_steps=100000,
    decay_rate=0.96)

loss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)
model.compile(optimizer = opt, loss = loss, metrics=[""acc""])

_datetime = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
log_dir = os.path.join('dummy_logs', _datetime)
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)

model.fit(x = dummy_data, y = dummy_y, epochs = 10,
        callbacks=[tb_callback])
```
![Screenshot from 2021-01-07 12-14-15](https://user-images.githubusercontent.com/32313554/103887434-517ee100-50e3-11eb-9597-4978bee27972.png)


**Other info / logs**   
I think that the problem is in tf.keras.callbacks.TensorBoard in _collect_learning_rate (see code block below). If self.model.optimizer.lr is sheduler then logs['learning_rate'] is created and LR can be seen in tensorboard. However if self.model.optimizer.lr is not an instance of learning_rate_schedule.LearningRateSchedule then logs['learning_rate'] does not exists and thus LR is not logged.
```
def _collect_learning_rate(self, logs):
    lr_schedule = getattr(self.model.optimizer, 'lr', None)
    if isinstance(lr_schedule, learning_rate_schedule.LearningRateSchedule):
      logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)
    return logs
```

A quick fix that I see here could be to add *else* or *elif* and directly log self.model.optimizer.lr. Changing as below fixed the issue for me when using ReduceLROnPlateau callback. This fix assumes that if no LearningRateSchedule is used then self.model.optimizer.lr could be logged as scalar by tensorboard callback. If this is not always possible then I guess adding some reasonable *elif* to be able to catch LR that can be logged (e.g. scalar) could resolve the problem.
```
class TensorBoardCallback(tf.keras.callbacks.TensorBoard):
    def __init__(self, **kwargs):
        super(TensorBoardCallback, self).__init__(**kwargs)

    def _collect_learning_rate(self, logs):
        lr_schedule = getattr(self.model.optimizer, 'lr', None)
        if isinstance(lr_schedule, tf.keras.optimizers.schedules.LearningRateSchedule):
            logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)
        elif lr_schedule is not None:
            logs['learning_rate'] = lr_schedule
        return logs
```"
46247,Unexpected Events CUDA_ERROR_ILLEGAL_ADDRESS and CUDA_ERROR_LAUNCH_FAILED,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: I have followed this tutorial using my own data. Tutorial: https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/
My data: https://gist.github.com/Urkchar/e01a667c1656e874f918ff92db5b998f
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows 10 Home version 10.0.19041 Build 19041
-   **TensorFlow installed from (source or binary)**: I installed TensorFlow with `pip install tensorflow`
-   **TensorFlow version**: 2.4.0
-   **Python version**: 3.8.6
-   **CUDA/cuDNN version**: CUDA version 11.2. cuDNN version 8.0.5
-   **GPU model and memory**: Nvidia GTX 1070 with 8 GB memory
-   **Exact command to reproduce**:
`model.fit(x, y, epochs=8, batch_size=256, callbacks=desired_callbacks)`

I receive unexpected errors when trying to fit a model. Examples include `CUDA_ERROR_ILLEGAL_ADDRESS` and `CUDA_ERROR_LAUNCH_FAILED`. Full logs and error messages below. 
Stuff like `F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1` Makes me think that this is a bug because these are unexpected errors or failures. 
```py
import sys
import numpy
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint

print(sys.version_info)


def tokenize_words(text_input):
    # Lowercase everything to standardize it
    text_input = text_input.lower()

    # Instantiate the tokenizer
    tokenizer = RegexpTokenizer(r""\w+"")
    tokens = tokenizer.tokenize(text_input)

    # If the create token isn't in the stop words, make it part of ""filtered""
    filtered = filter(lambda token: token not in stopwords.words(""english""), tokens)
    return "" "".join(filtered)


input_file = open(""yejibro_data.txt"").read()
# input_file = open(""84-0.txt"", ""r"", encoding=""utf-8"").read()

# Preprocces the input data, make tokens
processed_inputs = tokenize_words(input_file)

chars = sorted(set(processed_inputs))
char_to_num = dict((c, i) for i, c in enumerate(chars))

input_len = len(processed_inputs)
vocab_len = len(chars)
print(f""Total number of characters: {input_len}"")
print(f""Total vocab: {vocab_len}"")

seq_length = 100
x_data = []
y_data = []

# Loop through inputs, start at the beginning and go until we hit the final character we can create
# a sequence out of
for i in range(0, input_len - seq_length, 1):
    # Define input and output sequences
    # Input is the current character plus desired sequence length
    in_seq = processed_inputs[i:i + seq_length]

    # Out sequence is the initial character plus total sequence length
    out_seq = processed_inputs[i + seq_length]

    # We now convert list of characters to integers based on previous mappings and add the values to
    # our lists
    x_data.append([char_to_num[char] for char in in_seq])
    y_data.append(char_to_num[out_seq])

n_patterns = len(x_data)
print(f""Total patterns: {n_patterns}"")

x = numpy.reshape(x_data, (n_patterns, seq_length, 1))
x = x/float(vocab_len)

y = np_utils.to_categorical(y_data)

model = Sequential()
model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation=""softmax""))

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")

# print(model.summary())

filepath = ""model_weights_saved.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor=""loss"", verbose=1, save_best_only=True, mode=""min"")
desired_callbacks = [checkpoint]

model.fit(x, y, epochs=8, batch_size=256, callbacks=desired_callbacks)
```
Here are some error messages and logs that occurred at seemingly random points during the code. 
https://paste.pythondiscord.com/yomuzokuto.apache
https://paste.pythondiscord.com/orusujidaq.yaml
https://paste.pythondiscord.com/igexetayen.swift
https://paste.pythondiscord.com/ovawipegap.yaml
https://paste.pythondiscord.com/yopitiweri.less

Final note: I would be very happy to include more information when needed. I may have neglected to include some information that is necessary to figure out what's going on so please let me know. 
Final final note: This is my first bug report on GitHub so I tired to follow the template to the best of my ability. 
"
46245,tf.keras Model predict slow in flask service,"
System info:
tensorflow: 1.12.0
flask: 1.1.2
os: centos7.6
use cpu

problem:
tf.keras Model predict slow in flask service
import the follow code in python console, and run predict method directlyï¼Œ only cost about 800ms time;
but in flask serviceï¼Œ run predict methodin an interface, it need about 12s time.

code:
    def __init__(self):
        self.height = 960
        self.width = 960
        self.channels = 3
        self.model = ResNet50(include_top=False, input_shape=(self.height, self.width, self.channels))
        self.graph = tf.get_default_graph()

    def image_resize(self, image):
        raw_height = image.shape[0]
        raw_width = image.shape[1]
        if raw_height > self.height or raw_width > self.width:
            ratio1 = raw_height * 1.0 / self.height
            ratio2 = raw_width * 1.0 / self.width
            if ratio2 > ratio1:
                ratio1 = ratio2
            raw_height = int(raw_height / ratio1)
            raw_width = int(raw_width / ratio1)
            image = cv2.resize(image, (raw_width, raw_height))
        new_img = np.pad(image,
                         pad_width=((0, self.height - raw_height),
                                    (0, self.width - raw_width),
                                    (0, 0)
                                    ),
                         mode=""constant"", constant_values=(0, 0))

        return new_img

    def predict(self):
        image = cv2.imread('1.jpg')
        image = self.image_resize(image)
        time1 = int(round(time.time() * 1000))
        with self.graph.as_default():
            self.model.predict(np.array([image]))
        time2 = int(round(time.time() * 1000))
        cost = time2 - time1
        return str(cost) + "" ms."""
46244,Different results for add_loss() within TimeDistributed for eager vs. compiled execution of keras Layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: '3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) \n[GCC 9.3.0]'
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.0 / 8
- GPU model and memory: TITAN X (Pascal) computeCapability: 6.1

**Describe the current behavior**

In the minimal example code below, I have a keras Layer which calls `add_loss()`, and this layer is wrapped in `TimeDistributed`. When I execute the layer in eager mode, the losses have an extra timestep, as the first timestep is double-counted. However, if I evaluate the compiled model, the correct loss is computed.

During eager execution, the `add_loss` gets called first when computing `output_timestep_zero`:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4452

and subsequently called 5 (== # timesteps) times by: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4538

**Describe the expected behavior**

I expected layer.losses to count each timestep once and not double count the first timestep. I would think that the loss would be cleared after the first call used to obtain `output_timestep_zero`. I'm not entirely sure I understand why the first timestep is double-counted, but it would be useful to have the eager mode execution return the same losses as reported when the model is compiled.

**Standalone code to reproduce the issue**

below I run a simple example with 5 timesteps, where the input equals the timestep value + 1, so the correct loss would be
`mean([1, 2, 3, 4, 5]) == 3`. In eager execution, the first timestep is counted twice

```python
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np

# build inputs(b, t, i) == t + 1 with size [batch, timesteps, inputs]
batch, timesteps, ninputs = 1, 5, 1
timevec = (
    np.arange(timesteps, dtype=np.float32) + 1
)  # we start at one since the first gets double-counted
inputs = np.broadcast_to(
    timevec[np.newaxis, :, np.newaxis], (batch, timesteps, ninputs)
)


class AddsLossLayer(keras.layers.Layer):
    """"""Identity layer which calls add_loss on mean of input""""""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs):
        # inputs is batch x ninputs
        # return input but add_loss(mean of current input over ninputs dimension, should == timestep + 1)
        self.add_loss(tf.math.reduce_mean(inputs, axis=0, name=""loss""))
        return inputs

    def compute_output_shape(self, input_shape):
        return input_shape


adds_loss_layer = AddsLossLayer()
td_layer = keras.layers.TimeDistributed(adds_loss_layer)

outputs = td_layer(inputs)

print(f""inputs shape = {inputs.shape}"")
print(f""outputs shape = {outputs.shape}"")
print(f""len(td_layer.losses) == {len(td_layer.losses)}"")
print(f""len(adds_loss_layer.losses) == {len(adds_loss_layer.losses)}"")
print(""loss values == "", [loss.numpy() for loss in td_layer.losses])

# inputs shape = (3, 5, 1)
# outputs shape = (3, 5, 1)
# len(td_layer.losses) == 6
# len(adds_loss_layer.losses) == 6
# loss values ==  [array([1.], dtype=float32), array([1.], dtype=float32), array([2.], dtype=float32), array([3.], dtype=float32), array([4.], dtype=float32), array([5.], dtype=float32)]

# so we double count the first timestep!

model_inputs = tf.keras.Input(shape=inputs.shape[1:], dtype=""float32"")
adds_loss_layer = AddsLossLayer()
td_layer = keras.layers.TimeDistributed(adds_loss_layer, name=""td"")
model_outputs = td_layer(model_inputs)

model = keras.Model(model_inputs, model_outputs)
model.compile()
model.evaluate(inputs, None, verbose=2)

# 1/1 - 0s - loss: 3.0000
# (this is correct, (1 + 2 + 3 + 4 + 5) / 5 == 3)

model.fit(inputs, None, epochs=1)

# 1/1 [==============================] - 0s 100ms/step - loss: 3.0000
# (this is correct, (1 + 2 + 3 + 4 + 5) / 5 == 3)

outputs = model(inputs)

print(""mean model.losses = "", tf.math.reduce_mean(model.losses).numpy())

# mean model.losses =  2.6666667
# due to double counted first timestep, (1 + 1 + 2 + 3 + 4 + 5) / 6 = 2.666667
```

"
46240,Add Sony Spresense target,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): master
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sony Spresense board with Cortex-M4F

**Describe the problem**
To add support for sony spresense board.
Spresense has own development environment and it is bit conflict with Tensorflow Lite for Microcontrollers.
Tensorflow Lite for Microcontrollers can be used as a linkable library as libtensorflow-microlite.a.

A simple solution is to build the library as on Coretex-M4 platform with CMSIS-NN, and use it on the board development environment.
But the problem is how to use a examples.
Fortunately, the examples provide main_functions.cc with just 2 function as externd ""C"" like setup() and loop().
So I try to create a spresense target to build the library archive with example's sources without main.cc.

**Please provide the exact sequence of commands/steps when you ran into the problem**
I add a Makefile for downloading and build it on spresense sdk environment.
The draft work is in https://github.com/takayoshi-k/spresense/tree/add_tf_lm.
The makefile is https://github.com/takayoshi-k/spresense/blob/add_tf_lm/externals/tensorflow/Makefile.
"
46236,Implement a simple way to load a TF dataset from a Spark dataset and a simple way to convert a TF dataset to a Spark dataset,"**System information**
- TensorFlow version (you are using): 2.3.2
- Are you willing to contribute it (Yes/No): Hmm, yes though it may take a while for me to figure out how to do this. May be easier if an experienced contributor were to implement this (?)

**Describe the feature and the current behavior/state.**
Currently, from what I can tell, if you want to load data from Spark into a TF dataset, you have to first persist the data, as e.g. Parquet files or tfrecord files, then load data into a TF dataset from those files. This is inefficient as it requires intermediary processing and storage. Ideally, this new feature would allow a developer to load/transform data in Spark, then convert it to a TF dataset, build a model, etc.  Conversely, if you need an easy conversion from a TF dataset to Spark, the complementary converter would handle that. Perhaps a .toSparkDataset() method on TF dataset, or a separate static converter method somewhere.

**Will this change the current api? How?**
Yes. This will require new classes/methods -- up to the implementor.

**Who will benefit with this feature?**
I suspect many developers will benefit. Spark offers rich functionality for loading, transforming, joining data. Many systems may already have ETL type of code in Spark and want to leverage TensorFlow. Having an easy bridge from Spark to TF and back without the overhead of intermediary files (or a reduced such overhead) would be really useful.

LinkedIn's Spark-TFRecord library and the Spark Tensorflow Connector both require intermediary tfrecord files. Uber's Petastorm seems to offer a Spark-TF converter but if I understand correctly it uses intermediary local files for conversion; also I'm not seeing the reverse TF-to-Spark converter there (?).

**Any Other info.**
- I'm using Apache Spark 3.0."
46234,`sample_weight` does not work for multi-output (multi-task) models,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.12
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: Tesla T4 15109MiB

**Describe the current behavior**
In a multi-task setting, where the model (say, a regression model) has multiple outputs, `sample_weight` does not allow for multidimensional arrays - specifically, a `ValueError` is thrown during `.fit`. More surprisingly, if a list of weight arrays is provided instead or if the weights array has an extra dimension (via `.expand_dims`, for example), this fails silently - the weights do not appear to be used and no `ValueError` is thrown.

**Describe the expected behavior**
Ideally, the multi-task setting should support 2D `sample_weight` - my specific use case is with missing labels. Samples are not guaranteed to have labels for all tasks and one way to handle this is to set weights for missing labels to 0. Adding this is perhaps more of a feature request; the bug is the unexpected behavior.

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

n_data_points = 100
n_feats = 15
n_outputs = 5

X = np.random.uniform(0, 1, (n_data_points, n_feats))
y = np.random.uniform(0, 1, (n_data_points, n_outputs))
# set the last task to be some larger value
y[:, -1] = 100

weights = np.ones_like(y)
# set the weights for the last task to be 0
weights[:, -1] = 0.

def get_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(32, input_shape=(n_feats,)))
    model.add(tf.keras.layers.Dense(32))
    model.add(tf.keras.layers.Dense(n_outputs))
    model.compile(optimizer='sgd', loss='mse'))
    return model

# Produces some large loss value
model = get_model()
model.fit(X, y, epochs=1)

# Produces approximately the same large loss value (should not!)
model = get_model()
model.fit(X, y, sample_weight=np.expand_dims(weights, axis=-1), epochs=1)

# Throws a ValueError
model = get_model()
model.fit(X, y, sample_weight=weights, epochs=1)
```

`ValueError` thrown:
```
ValueError: Can not squeeze dim[1], expected a dimension of 1, got 5 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,5].
```

**Other info / logs**
<details><summary>full traceback</summary>

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-66-3ba28a4faca1> in <module>
----> 1 model.fit(X, y, sample_weight=weights, epochs=1)

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--> 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--> 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--> 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-> 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--> 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

~/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:145 __call__
        losses, sample_weight, reduction=self._get_reduction())
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:107 compute_weighted_loss
        losses, sample_weight)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/losses/util.py:144 scale_losses_by_sample_weight
        losses, None, sample_weight)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/losses/util.py:96 squeeze_or_expand_dimensions
        sample_weight = array_ops.squeeze(sample_weight, [-1])
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:4145 squeeze
        return gen_array_ops.squeeze(input, axis, name)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py:9875 squeeze
        ""Squeeze"", input=input, squeeze_dims=axis, name=name)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal
        compute_device)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal
        op_def=op_def)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__
        control_input_ops, op_def)
    /home/ubuntu/miniconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
        raise ValueError(str(e))

    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 5 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [?,5].
```</details>"
46233,Tensorflow 2.4 not showing available GPU even after successful cuda and cudnn installation,"OS - ubuntu 20.04
python -3..8.5
GPU - RTX 2060
Cudnn - v7.6.5

Output of `tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)`:
`WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-01-07 00:08:12.313080: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-07 00:08:12.315365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-07 00:08:12.403099: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-01-07 00:08:12.403164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: aum-GF65-Thin-9SEXR
2021-01-07 00:08:12.403176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aum-GF65-Thin-9SEXR
2021-01-07 00:08:12.403275: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.80.2
2021-01-07 00:08:12.403328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2
2021-01-07 00:08:12.403345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.80.2
False
`
OUTPUT of `dpkg -l | grep cuda-toolkit`

`ii  cuda-toolkit-11-0                               11.0.2-1                              amd64        CUDA Toolkit 11.0 meta-package
ii  nvidia-cuda-toolkit                             10.1.243-3                            amd64        NVIDIA CUDA development toolkit
`

OUTPUT of `nvidia-smi` 
`+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   48C    P8     6W /  N/A |    306MiB /  5934MiB |      5%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       920      G   /usr/lib/xorg/Xorg                 45MiB |
|    0   N/A  N/A      1469      G   /usr/lib/xorg/Xorg                106MiB |
|    0   N/A  N/A      1647      G   /usr/bin/gnome-shell              125MiB |
|    0   N/A  N/A      2647      G   /usr/lib/firefox/firefox            3MiB |
|    0   N/A  N/A      2690      G   /usr/lib/firefox/firefox            3MiB |
|    0   N/A  N/A      2723      G   /usr/lib/firefox/firefox            3MiB |
|    0   N/A  N/A      3681      G   /usr/lib/firefox/firefox            3MiB `

After succesful installation of CUDA 11, nvidia-cuda-toolkit 10.1 and tensorflow 2.4 (also tried tensorflow-gpu 2.2.0 but same error) I am still getting NO GPUs FOUND.

PLEASE HELP!!


"
46231,tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
Yes, slightly modified [this](https://www.tensorflow.org/tutorials/structured_data/time_series) guide
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Manjaro Nibia 20.2 Kernel 5.4
-   **TensorFlow installed from (source or binary)**: Manjaro package - [https://discover.manjaro.org/packages/python-tensorflow-cuda](https://discover.manjaro.org/packages/python-tensorflow-cuda)
-   **TensorFlow version (use command below)**: 2.4.0-1
-   **Python version**: Python 3.9.1
-   **CUDA/cuDNN version**: bundled in manjaro package
-   **GPU model and memory**: GeForce GTX 960M total memory 2GB
-   **Exact command to reproduce**: 
Following [this](https://www.tensorflow.org/tutorials/structured_data/time_series#2_split) guide the error occurs at splitting

### Describe the problem
Occurred during training. Log states to report this problem.

### Source code / logs
This is the log output with AUTOGRAPH_VERBOSITY=10
```log
2021-01-06 19:56:39.014574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-06 19:56:40.398316: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-06 19:56:40.399294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-06 19:56:40.432681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.433049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s
2021-01-06 19:56:40.433100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-06 19:56:40.436263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-06 19:56:40.436390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-06 19:56:40.437546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-06 19:56:40.437861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-06 19:56:40.441100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-01-06 19:56:40.441937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-06 19:56:40.442202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-06 19:56:40.442373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.442839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.443141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-06 19:56:40.443406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-06 19:56:40.443944: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-01-06 19:56:40.444057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.444410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s
2021-01-06 19:56:40.444455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-06 19:56:40.444489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-01-06 19:56:40.444533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-01-06 19:56:40.444573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-06 19:56:40.444602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-06 19:56:40.444640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2021-01-06 19:56:40.444687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-01-06 19:56:40.444720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-01-06 19:56:40.444816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.445205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.445503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-06 19:56:40.445546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-01-06 19:56:40.519438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-06 19:56:40.519471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-01-06 19:56:40.519479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-01-06 19:56:40.519741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.520226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.520640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-06 19:56:40.521036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2021-01-06 19:56:40.521335: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3
Input indices: [0 1]
Label indices: [1 2]
Label column name(s): ['price']> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: invalid syntax (tmp7x98gy_p.py, line 10)
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-01-06 19:56:40.756777: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-01-06 19:56:40.757331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2398910000 Hz
Epoch 1/100
2021-01-06 19:56:40.930451: F ./tensorflow/core/kernels/random_op_gpu.h:244] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), key, counter, gen, data, size, dist) status: Internal: no kernel image is available for execution on the device
Aborted (core dumped)
```
"
46230,Unable to read,I'm using Ubuntu 
46227,Unable to access GPU from docker with error failed call to cuInit: CUresult(-1),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, Google cloud instance
- Tensorflow Docker Version: tensorflow/tensorflow:1.9.0-gpu-py3
- GPU model and memory: NIVIDIA-T4, 16GB

We are trying to create a custom docker image to serve image classification model. Using the tensorflow/tensorflow:1.9.0-gpu-py3 as base image. The host machine has the NVIDIA drivers installed and able to run the model on GPU.  

**Docker file**

```
FROM tensorflow/tensorflow:1.9.0-gpu-py3 as base
ENV CUDA_HOME /usr/local/cuda
ENV PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \
     && echo ""/usr/local/cuda/lib64/stubs"" > /etc/ld.so.conf.d/z-cuda-stubs.conf \
     && ldconfig
ENV NVIDIA_VISIBLE_DEVICES all
ADD . /app
WORKDIR /app
RUN apt-get -yqq update
RUN apt-get install -yqq libsm6 libxext6 libxrender-dev
RUN pip install -r requirements.txt
RUN python3 run_model.py
```
While building the image using the command `sudo nvidia-docker build -t name .` Getting the following error:

```
2021-01-06 17:27:26.453415: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-01-06 17:27:26.476869: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)
2021-01-06 17:27:26.476956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:152] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
```

Model is loaded on CPU instead of GPU.

I have followed the instructions from:
[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker
)

to install the nvidia-container-toolkit and when I run the following command: `sudo docker run --rm --gpus all nvidia/cuda:9.0-base nvidia-smi`
I see the following output:
```
[+-----------------------------------------------------------------------------+
| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   63C    P0    32W /  70W |      0MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+]
```
"
46223,Running multiple hexagon delegates sequentially. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 running Android 9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Right now, I have two instances of TfLite interpreter created using the same tflite model. Some of the network is delegated to DSP using hexagon delegate. Both interpreters are initialized one after other. Then, Invoke() is called many times, on either one or the other interpreter. This works fine. 

**Describe the expected behavior**

What happens if I create two instances of tfliteInterpreter from different models, both using hexagon delegate? Will this work as expected? 

"
46218,micro_speech: Run on macOS make error,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): macOS

**Describe the problem**
Error when running
```
make -f tensorflow/lite/micro/tools/make/Makefile micro_speech
```

Error message:
```
tensorflow/lite/micro/examples/micro_speech/osx/audio_provider.cc:64:48: error: missing field 'mFormatID' initializer [-Werror,-Wmissing-field-initializers]
  AudioStreamBasicDescription recordFormat = {0};
                                               ^
1 error generated.
gmake: *** [tensorflow/lite/micro/tools/make/Makefile:623: tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/micro_speech/osx/audio_provider.o] Error 1
```

**Please provide the exact sequence of commands/steps when you ran into the problem**


"
46217,DepthwiseConv2D is slower than Conv2D,"I saw the similar issue in 
https://github.com/tensorflow/tensorflow/issues/42172
As in this post, when the batch size is larger, DepthwiseConv2D can be faster than Conv2D.
However, if I have to set the batch size to 1, is there any method to speed up DepthwiseConv2D?
"
46211,Many errors in the Example of tf.feature_column.categorical_column_with_vocabulary_file,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file

## Description of issue (what needs changing):
In the sentence, 

> Use either (but not both) of num_oov_buckets and default_value to specify how to include out-of-vocabulary values.

since **either** is used, **or** should be used instead of **and**.

When running the **`Example Code`**, it is resulting in the errors mentioned below:

1. 

> NameError: name 'categorical_column_with_vocabulary_file' is not defined


2. 

> NameError: name 'linear_model' is not defined

3. 

> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis

4. 

> NameError: name 'input_layer' is not defined

The code in the documentation should be modified to fix all the above errors. 

Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/74b353ebc7046dee6220cb158f25e5bc/categorical_column_with_vocabulary_file_error.ipynb) demonstrating the errors."
46210,Control dependency doesn't work in distribute MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04 LTS` with `nvcr.io/nvidia/tensorflow:19.12-tf1-py3`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): none
- TensorFlow version (use command below): `unknown 1.15.0`
- Python version: `3.6.9`
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: CUDA 10.2.89, cuDNN 7.6.5
- GPU model and memory: 2x GeForce GTX 1080 Ti with 11178 MiB fb memory
```
        GPU0    GPU1    CPU Affinity
GPU0     X      PHB     0-11
GPU1    PHB      X      0-11
```

**Describe the current behavior**
It seems that `tf.control_dependencies` is useless in the `load_fn`, which is called by a `MirroredStrategy`.
The wall times of `load` and `ret` differ greatly with each other.

**Describe the expected behavior**
The wall times of `load` and `ret` is comparable.

**Standalone code to reproduce the issue**
```python
import time
import tensorflow as tf

SHAPE = [2048, 2048]
STEP = 100
WARMUP = 100

_LOAD_OPS = list()


def load_fn():
    v = tf.get_variable('v', shape=SHAPE, initializer=tf.ones_initializer)
    m = tf.matmul(v, v)
    _LOAD_OPS.append(m.op)
    with tf.control_dependencies([m.op]):
        return tf.constant(1.0)


def bench(name, session, ops):
      stime = time.time()
      for _ in range(STEP):
          session.run(ops)
      etime = time.time() - stime
      print('{:6s} takes {:.3e} sec.'.format(name, etime))
    

def main():
    strategy = tf.distribute.MirroredStrategy()
    with strategy.scope():
        r = strategy.experimental_run_v2(load_fn)
        r = strategy.reduce(tf.distribute.ReduceOp.SUM, r)

    with tf.train.MonitoredSession() as mon_sess:
        for _ in range(WARMUP):
            mon_sess.run([_LOAD_OPS, r, []])

        bench('load', mon_sess, _LOAD_OPS)
        bench('ret',  mon_sess, r)
        bench('null', mon_sess, [])


if __name__ == '__main__':
    main()
```

**Other info / logs**
The non-trivial output is
```
load   takes 8.350e-01 sec.
ret    takes 3.153e-02 sec.
null   takes 4.847e-03 sec.
```"
46209,Tensorflow-gpu in RTX3070,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10 64bit
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version: tensorflow-gpu 2.4.0
- Python version: Python 3.7
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA11, cuDNN 8.0.5
- GPU model and memory: RTX3070 8GB OC



**Describe the problem**
When I run the training using GPU, it raise me an error saying that my cuDNN is not initialize properly. I've no idea on how to solve it. I did tried with CUDA 11 with cuDNN 8.0.4 and 8.0.3 but keep raising the same problem.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
`tf.config.list_physical_devices('GPU')` --> ""Physical Device GPU 0""
`tf.test.is_built_with_cuda()` --> True


**Log**
![Capture](https://user-images.githubusercontent.com/52826239/103755378-b1747980-5048-11eb-8bc0-f5dfc10d69c4.PNG)

"
46205,"ButchNormalization fails when nvidia GPU is used and the training size and the batch size is ""well set""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary installed by pip3
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- CUDA/cuDNN version: 11.0/8.0
- GPU model and memory: Nvidia 1080Ti 11GB

**Describe the current behavior**
When the size of training set and the batch size is set so that the last batch contains only one element (i.e. len(training_set)%batch_size == 1 ), one of the weights of the batch normalization (the variance) is set to nan. The problem disappears when using CPU.

**Describe the expected behavior**
Have valid weights computed regardless of the size of the training set.

**Standalone code to reproduce the issue**
from tensorflow import keras                                                                                                                                                                                       
import numpy as np                                                                                                                                                                                                 
                                                                                                                                                                                                                   
inp = keras.layers.Input((1,1,1))                                                                                                                                                                                  
mid = keras.layers.Conv2D(1, (1,1))(inp)                                                                                                                                                                           
out = keras.layers.BatchNormalization()(mid)                                                                                                                                                                       
                                                                                                                                                                                                                   
mod = keras.models.Model(inputs=inp, outputs=out)                                                                                                                                                                  
mod.compile(optimizer='adam', loss='mse')                                                                                                                                                                          
                                                                                                                                                                                                                   
data = np.reshape(np.arange(9), (9,1,1,1))                                                                                                                                                                         
                                                                                                                                                                                                                   
mod.fit(data,data,batch_size=8)                                                                                                                                                                                    
print(mod.layers[2].get_weights()[3])             #This is the last layer's last weight; this is NAN with GPU             
"
46203,Many errors in the Example of tf.feature_column.categorical_column_with_identity,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity#linear_model

## Description of issue (what needs changing):
When running the **`Example Code`**, it is resulting in the errors mentioned below:

1. 

> NameError: name 'categorical_column_with_identity' is not defined


2. 

> NameError: name 'linear_model' is not defined

3. 

> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis

4. 

> NameError: name 'input_layer' is not defined

The code in the documentation should be modified to fix all the above errors. 

Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/a05819cc660a7b677e53b189affbe4d0/categorical_column_with_identity_error.ipynb) demonstrating the errors."
46201,Many errors in the Example of tf.feature_column.categorical_column_with_hash_bucket,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket

## Description of issue (what needs changing):
When running the **`Example Code`**, it is resulting in the errors mentioned below:

1. 

> File ""<ipython-input-1-e0419158b389>"", line 3
>     keywords = categorical_column_with_hash_bucket(""keywords"", 10K)
>                                                                  ^
> SyntaxError: invalid syntax

2. 

> NameError: name 'categorical_column_with_hash_bucket' is not defined

3. 

> NameError: name 'linear_model' is not defined

4. 

> NameError: name 'input_layer' is not defined

5. 

> ValueError: All feature_columns must be FeatureColumn instances. Given: Ellipsis

The code in the documentation should be modified to fix all the above errors. 

Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/eba74b40ed40964c730e4e667ed0c8f1/categorical_column_with_hash_bucket_error.ipynb) demonstrating the errors."
46199,Failed invoke tflite model in swift code(Provided data count 18181 must match the required count 4536.),"**System information**
- OS Platform): macOS 11.1
- TensorFlow version: 2.4.0

Input shape of my tflite model - (18, 63) or 1134 float numbers.

I get the data itself in objective-c code, and then I send it to swift code and an error already occurs there. In detail I do the following

1. Receive the data in NSMutableArray. The length of the array is 1134 NSNumber
2. Converting NSMutableArray to NSData
```
NSData *d = [NSKeyedArchiver archivedDataWithRootObject:_data];
NSLog(@""output: %@"", d);
// output: {length = 18181, bytes = 0x62706c69 73743030 d4000100 02000300 ... 00000000 000034fd }
```

3. I send data to the swift code.
```[_model predict:d];```
Swift code:
```
@objc public func predict(_ data: NSData) {
        guard
          let modelPath = Bundle.main.path(forResource: ""model"", ofType: ""tflite"")
        else {
            return
        }

        do {
          let interpreter = try Interpreter(modelPath: modelPath)
          try interpreter.allocateTensors()
          let inputData: Data = data as Data

          try interpreter.copy(inputData, toInputAt: 0) // <-- an error occurs in this line

          try interpreter.invoke()
          let outputTensor = try interpreter.output(at: 0)
        } catch {
          print(error)
        }
}
```
4. I get an error in the above line of code. Error:
```Provided data count 18181 must match the required count 4536.```"
46197,Error in example because of Incomplete API names for tf.keras.experimental.SequenceFeatures,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures#example

## Description of issue (what needs changing): 
The APIs, **`sequence_numeric_column, sequence_categorical_column_with_identity, embedding_column`**, etc.. are incomplete. Consequently, it is resulting in the error, 

> NameError: name 'sequence_numeric_column' is not defined

It should be **`tf.feature_column.sequence_numeric_column, tf.feature_column.sequence_categorical_column_with_identity, tf.feature_column.embedding_column`**, instead.

Please find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb).

There is an error in the above [Gist](https://colab.research.google.com/gist/rmothukuru/90002755d1b45f6f94709a9623259d9f/sequencefeatures_error.ipynb) because of **`Ellipsis`** and it is being tracked in #46128."
46196,Add `.bazelrc` to `.gitignore`,"**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Currently, we use `configure.py` for modifying `.bazelrc`, I think we should add `.bazelrc` to `.gitignore` and use `configure.py` for completely generating `bazelrc` from scratch. Afterward, we could add our own config to `.bazelrc` without any conflict with the upstream ( I usually add `build:asan ----copt=-fsanitize=address --linkopt=-fsanitize=address` ). Furthermore, we could generate a platform-specific `.bazelrc` using `python` ( which is good, I think ).

**Will this change the current api? How?** Nothing

**Who will benefit with this feature?** TensorFlow developers.

/cc: @mihaimaruseac 

"
46194,"compute gradient error:  'KerasTensor' object has no attribute '_id',   (tensorflow 2.4.0)","Hi, i'm using tensorflow 2.4.0 ,  and want to compute gradients with keras tensor,  but failed.

I'm using python3.8

**Below are the log info:**

Traceback (most recent call last):
  File ""demo-resnet-18-v4.py"", line 119, in <module>
    cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, ""activation_16"")
  File ""demo-resnet-18-v4.py"", line 65, in grad_cam
    grads = gtape.gradient(loss, var_list)
  File ""/Users/baonansen/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 1080, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File ""/Users/baonansen/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py"", line 79, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
AttributeError: 'KerasTensor' object has no attribute '_id'

**below are the code:**

 53 def grad_cam(input_model, image, category_index, layer_name):
 54     with tf.GradientTape() as gtape:
 55         nb_classes = 53
 56         target_layer = lambda x: target_category_loss(x, category_index, nb_classes)
 57         x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)
 58         model = Model(inputs=input_model.input, outputs=x)
 59         model.summary()
 60
 61         loss = K.sum(model.output)
 62         conv_output =  [l for l in model.layers if l.name == layer_name][0].output
 63
 64         var_list = [conv_output]
 65     grads = gtape.gradient(loss, var_list)

"
46192, PB to Tflite,"import tensorflow as tf

path = 'E:\\Code\\PythonCode\\AliYunCode\\JS-CODE-20210104-1008-CNNX-GRU-H64-CTC-C1_0.pb'

inputs = [""input""]

outputs = [""dense_decoded""]

converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs)

converter.post_training_quantize = True

tflite_model = converter.convert()

open(""tiny_160000.tflite"", ""wb"").write(tflite_model)

-----
ERROR:Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, GATHER, LEAKY_RELU, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, NOT_EQUAL, RANGE, REDUCE_ANY, RESHAPE, SELECT, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.

"
46191,Add support for padding and cropping to tf.keras.layers.experimental.preprocessing.Resizing,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): No, unfortunately I don't have the time to do so.

**Describe the feature and the current behavior/state.**
The new layer `tf.keras.layers.experimental.preprocessing.Resizing` allows for models to be made more portable by handling image resizing within the model itself, as described in the docs [here](https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time). This layer provides options for interpolation, but cannot be configured to crop or pad the image to maintain aspect-ratio.

In my case, I use the `tf.image.resize_with_pad` in my `tf.data` pipeline in order to maintain aspect ratio and letter-box the shorter sides. This cannot be done inside of the model without a custom or Lambda layer.

The layer should ideally also include the functionality from `tf.image.resize_with_crop_or_pad`.

**Will this change the current api? How?**
Yes, but purely additive. I suggest adding an additional parameter `resize_type` to the layer and simply have the default value perform the current behavior. This would not affect current users, but offer the feature to those who chose to enable it.

**Who will benefit with this feature?**
This would be handy for developers/researchers who want a simple, built-in mechanism for resizing images inside of a model but don't want the current stretching behavior. In particular, this would benefit the portability of models, where we can do more within the model itself.

**Any Other info.**
N/A"
46186,running a single test with renode is broken.,"@tensorflow/micro

While the following command passes:
```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test
```

Running a single test with renode (for example):
```bash
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_add_test
```

fails with:
```
tensorflow/lite/micro/testing/test_with_renode.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/kernel_add_test '~~~ALL TESTS PASSED~~~'
tensorflow/lite/micro/testing/test_with_renode.sh: line 69: $ROBOT_SCRIPT: ambiguous redirect
make: *** [tensorflow/lite/micro/tools/make/Makefile:663: test_kernel_add_test] Error 1
```

The reason is that the changes from https://github.com/tensorflow/tensorflow/pull/45787 are incompatible with how the Makefile calls the test script when running an individual test (as opposed to `make test`).

"
46181,Publishing Tensorflow wheel package for s390x architecture,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.3.1
- Python version: Python 3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):

**Describe the problem**
Currently `pip install tensorflow` command on s390x architecture errors out with `No matching distribution found for tensorflow`.

This happens because there is no corresponding s390x arch wheel available on [PyPI](https://pypi.org/simple/tensorflow/).

I am investigating whether we could push s390x wheel on PyPI and, if so, what are the steps needed.

Nightly/release builds for s390x appear to produce corresponding wheel for s390x as seen [here](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastSuccessfulBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl).  There also exists a `Release` Artifact which can contain s390x Tensorflow wheel which can be pushed to PyPI.

Please let me know if/how this can be achieved. I have tried installing the s390x wheel available from the [CI](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastSuccessfulBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl) and it appears to have installed on s390x using following commands:
```
apt-get install sudo wget git unzip zip python3-dev python3-pip openjdk-11-jdk pkg-config libhdf5-dev libssl-dev libblas-dev liblapack-dev gfortran cython3 -y
ldconfig
export GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True
pip3 install grpcio
pip3 install numpy wheel scipy portpicker protobuf==3.13.0
wget https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastStableBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl
pip3 install tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl
```
Thanks."
46178,Apple M1 chip - illegal hardware instruction,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.0.1
- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip
- TensorFlow version: Latest stable (pip)
- Python version: Python 3.8.5
- Installed using virtualenv? pip? conda?: exactly like in the instruction https://www.tensorflow.org/install/pip
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /



**Describe the problem**

After running the verification steps 
```
python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
```

The following error appear
```
illegal hardware instruction
```


**Any other info / logs**
This issue only appear on the Mac with the Apple M1 chip. The same setup procedure https://www.tensorflow.org/install/pip works fine on my other Mac`s."
46177,AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>>,"**System information**
- TensorFlow installed from (source or binary): Hosted on colab server
- TensorFlow version (use command below): TF 4.0
- Python version: 3.6.9 (Latest version Hosted on google colab)  
- CUDA/cuDNN version:
- GPU model and memory:

Tue Jan  5 14:24:33 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   56C    P0    62W / 149W |   2206MiB / 11441MiB |      0%      Default |
|                               |                      |                 ERR! |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


### Current behavior

WARNING:tensorflow : AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>> and will run it as-is.

Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.

Cause: <cyfunction Socket.send at 0x7fdb48bd4e58> is not a module, class, method, function, traceback, frame, or code object
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>> and will run it as-is.

Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.

Cause: <cyfunction Socket.send at 0x7fdb48bd4e58> is not a module, class, method, function, traceback, frame, or code object
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fdb465698c8> and will run it as-is.

Cause: while/else statement not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING: AutoGraph could not transform <function wrap at 0x7fdb465698c8> and will run it as-is.

Cause: while/else statement not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

**Describe the expected behavior**

This warning is preventing to use AutoGraph but i think execution could be faster if it generates a tensorflow graph to run the code.


"
46176,TFX ML Metadata storage to PostgreSQL,I wanted to store my metadata information of ML artifacts to PostgreSQL but I couldn't find any libraries that support MLMD for PostgreSQL. Is there any API's that could support to PostgreSQL other than MySQL/SQLite?
46175, AttributeError: module 'tensorflow' has no attribute 'data',"Installing on a Windows 10 PC using CPU (no GPU)
Working in an Anaconda environment

Have previously used TF 1.5, but now for NLP work I needed to move to the new generation

I am using Python 3.6 installed tensorflow 2.2. Received the same issue with tensorflow 2.1

Running the line:
text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)

I get:
**_**Traceback (most recent call last):

  File ""<ipython-input-34-3c9c34f69f7e>"", line 2, in <module>
    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)

AttributeError: module 'tensorflow' has no attribute 'data'**_**

AttributeError: module 'tensorflow' has no attribute 'data'


It looks like an installation program, so I reinstalled, using different versions. Any help much appreciated

(tensorflow2) C:\Users\mehes>conda list
# packages in environment at C:\Users\mehes\Anaconda3\envs\tensorflow2:
#
# Name                    Version                   Build  Channel
_tflow_select             2.2.0                     eigen
absl-py                   0.11.0           py36ha15d459_0    conda-forge
aiohttp                   3.7.3            py36h68aa20f_0    conda-forge
alabaster                 0.7.12                     py_0    conda-forge
appdirs                   1.4.4              pyh9f0ad1d_0    conda-forge
argh                      0.26.2          pyh9f0ad1d_1002    conda-forge
astor                     0.8.1              pyh9f0ad1d_0    conda-forge
astroid                   2.4.2            py36h9f0ad1d_1    conda-forge
async-timeout             3.0.1                   py_1000    conda-forge
async_generator           1.10                       py_0    conda-forge
atomicwrites              1.4.0              pyh9f0ad1d_0    conda-forge
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
autopep8                  1.5.4              pyh9f0ad1d_0    conda-forge
babel                     2.9.0              pyhd3deb0d_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.1                      py_0    conda-forge
bcrypt                    3.2.0            py36h779f372_1    conda-forge
black                     20.8b1                     py_0    conda-forge
bleach                    3.2.1              pyh9f0ad1d_0    conda-forge
blinker                   1.4                        py_1    conda-forge
brotlipy                  0.7.0           py36hc753bc4_1001    conda-forge
ca-certificates           2020.12.5            h5b45459_0    conda-forge
cached-property           1.5.1                      py_0    conda-forge
cachetools                4.1.1                      py_0    conda-forge
certifi                   2020.12.5        py36ha15d459_0    conda-forge
cffi                      1.14.4           py36he58ceb7_1    conda-forge
chardet                   3.0.4           py36hd36e781_1008    conda-forge
click                     7.1.2              pyh9f0ad1d_0    conda-forge
cloudpickle               1.6.0                      py_0    conda-forge
colorama                  0.4.4              pyh9f0ad1d_0    conda-forge
cryptography              3.3.1            py36he58ceb7_0    conda-forge
decorator                 4.4.2                      py_0    conda-forge
defusedxml                0.6.0                      py_0    conda-forge
diff-match-patch          20200713           pyh9f0ad1d_0    conda-forge
docutils                  0.16             py36ha15d459_2    conda-forge
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
flake8                    3.8.4                      py_0    conda-forge
future                    0.18.2           py36ha15d459_2    conda-forge
gast                      0.2.2                      py_0    conda-forge
google-auth               1.24.0             pyhd3deb0d_0    conda-forge
google-auth-oauthlib      0.4.1                      py_2    conda-forge
google-pasta              0.2.0              pyh8c360ce_0    conda-forge
grpcio                    1.34.0           py36h4374274_0    conda-forge
h5py                      3.1.0           nompi_py36hf359dfe_100    conda-forge
hdf5                      1.10.6          nompi_h5268f04_1113    conda-forge
helpdev                   0.7.1              pyhd8ed1ab_0    conda-forge
icu                       68.1                 h0e60522_0    conda-forge
idna                      2.10               pyh9f0ad1d_0    conda-forge
idna_ssl                  1.1.0           py36h9f0ad1d_1001    conda-forge
imagesize                 1.2.0                      py_0    conda-forge
importlib-metadata        3.3.0            py36ha15d459_2    conda-forge
importlib_metadata        3.3.0                hd8ed1ab_2    conda-forge
intel-openmp              2020.3             h57928b3_311    conda-forge
intervaltree              3.0.2                      py_0    conda-forge
ipykernel                 5.4.2            py36h7b7c402_0    conda-forge
ipython                   7.16.1           py36h7b2dad6_2    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
isort                     5.7.0              pyhd8ed1ab_0    conda-forge
jedi                      0.17.2           py36ha15d459_1    conda-forge
jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge
jpeg                      9d                   h8ffe710_0    conda-forge
jsonschema                3.2.0                      py_2    conda-forge
jupyter_client            6.1.7                      py_0    conda-forge
jupyter_core              4.7.0            py36ha15d459_0    conda-forge
jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
keras-applications        1.0.8                      py_1    conda-forge
keras-preprocessing       1.1.0                      py_0    conda-forge
keyring                   21.8.0           py36ha15d459_0    conda-forge
krb5                      1.17.2               hbae68bd_0    conda-forge
lazy-object-proxy         1.4.3            py36h779f372_2    conda-forge
libblas                   3.9.0                     6_mkl    conda-forge
libcblas                  3.9.0                     6_mkl    conda-forge
libclang                  11.0.0          default_h5c34c98_2    conda-forge
libcurl                   7.71.1               h4b64cdc_8    conda-forge
liblapack                 3.9.0                     6_mkl    conda-forge
libpng                    1.6.37               h1d00b33_2    conda-forge
libprotobuf               3.14.0               h7755175_0    conda-forge
libsodium                 1.0.18               h8d14728_1    conda-forge
libspatialindex           1.9.3                he025d50_3    conda-forge
libssh2                   1.9.0                hb06d900_5    conda-forge
m2w64-gcc-libgfortran     5.3.0                         6    conda-forge
m2w64-gcc-libs            5.3.0                         7    conda-forge
m2w64-gcc-libs-core       5.3.0                         7    conda-forge
m2w64-gmp                 6.1.0                         2    conda-forge
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forge
markdown                  3.3.3              pyh9f0ad1d_0    conda-forge
markupsafe                1.1.1            py36hc753bc4_2    conda-forge
mccabe                    0.6.1                      py_1    conda-forge
mistune                   0.8.4           py36h68aa20f_1002    conda-forge
mkl                       2020.4             hb70f87d_311    conda-forge
msys2-conda-epoch         20160418                      1    conda-forge
multidict                 5.1.0            py36h68aa20f_0    conda-forge
mypy_extensions           0.4.3            py36ha15d459_2    conda-forge
nbclient                  0.5.1                      py_0    conda-forge
nbconvert                 6.0.7            py36ha15d459_3    conda-forge
nbformat                  5.0.8                      py_0    conda-forge
nest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge
numpy                     1.14.5                   pypi_0    pypi
numpydoc                  1.1.0                      py_1    conda-forge
oauthlib                  3.0.1                      py_0    conda-forge
openssl                   1.1.1i               h8ffe710_0    conda-forge
opt_einsum                3.3.0                      py_0    conda-forge
packaging                 20.8               pyhd3deb0d_0    conda-forge
pandoc                    2.11.3.2             h8ffe710_0    conda-forge
pandocfilters             1.4.2                      py_1    conda-forge
paramiko                  2.7.2              pyh9f0ad1d_0    conda-forge
parso                     0.7.0              pyh9f0ad1d_0    conda-forge
pathspec                  0.8.1              pyhd3deb0d_0    conda-forge
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pip                       20.3.3             pyhd8ed1ab_0    conda-forge
pluggy                    0.13.1           py36hd36e781_3    conda-forge
prompt-toolkit            3.0.8              pyha770c72_0    conda-forge
protobuf                  3.14.0           py36he2d232f_0    conda-forge
psutil                    5.8.0            py36h68aa20f_0    conda-forge
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pyasn1                    0.4.8                      py_0    conda-forge
pyasn1-modules            0.2.7                      py_0    conda-forge
pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge
pycparser                 2.20               pyh9f0ad1d_2    conda-forge
pydocstyle                5.1.1                      py_0    conda-forge
pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge
pygments                  2.7.3              pyhd8ed1ab_0    conda-forge
pyjwt                     1.7.1                      py_0    conda-forge
pylint                    2.6.0            py36h9f0ad1d_1    conda-forge
pyls-black                0.4.6              pyh9f0ad1d_0    conda-forge
pyls-spyder               0.3.0              pyhd8ed1ab_0    conda-forge
pynacl                    1.4.0            py36h3a74357_2    conda-forge
pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyqt                      5.12.3           py36ha15d459_6    conda-forge
pyqt-impl                 5.12.3           py36he2d232f_6    conda-forge
pyqt5-sip                 4.19.18          py36he2d232f_6    conda-forge
pyqtchart                 5.12             py36he2d232f_6    conda-forge
pyqtwebengine             5.12.1           py36he2d232f_6    conda-forge
pyreadline                2.1             py36h9f0ad1d_1002    conda-forge
pyrsistent                0.17.3           py36h68aa20f_1    conda-forge
pysocks                   1.7.1            py36hd36e781_2    conda-forge
python                    3.6.12          h39d44d4_0_cpython    conda-forge
python-dateutil           2.8.1                      py_0    conda-forge
python-jsonrpc-server     0.4.0              pyh9f0ad1d_0    conda-forge
python-language-server    0.36.2             pyhd8ed1ab_0    conda-forge
python_abi                3.6                     1_cp36m    conda-forge
pytz                      2020.5             pyhd8ed1ab_0    conda-forge
pywin32                   228              py36h779f372_0    conda-forge
pywin32-ctypes            0.2.0           py36h9f0ad1d_1002    conda-forge
pyyaml                    5.3.1            py36hc753bc4_1    conda-forge
pyzmq                     20.0.0           py36hb0157bd_1    conda-forge
qdarkstyle                2.8.1              pyhd8ed1ab_2    conda-forge
qt                        5.12.9               h5909a2a_2    conda-forge
qtawesome                 1.0.2              pyhd8ed1ab_0    conda-forge
qtconsole                 5.0.1              pyhd8ed1ab_0    conda-forge
qtpy                      1.9.0                      py_0    conda-forge
regex                     2020.11.13       py36h68aa20f_0    conda-forge
requests                  2.25.1             pyhd3deb0d_0    conda-forge
requests-oauthlib         1.3.0              pyh9f0ad1d_0    conda-forge
rope                      0.18.0             pyh9f0ad1d_0    conda-forge
rsa                       4.6                pyh9f0ad1d_0    conda-forge
rtree                     0.9.4            py36h089df06_2    conda-forge
scipy                     1.5.3            py36h7ff6e69_0    conda-forge
setuptools                49.6.0           py36hd36e781_2    conda-forge
six                       1.15.0             pyh9f0ad1d_0    conda-forge
snowballstemmer           2.0.0                      py_0    conda-forge
sortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge
sphinx                    3.4.2              pyhd8ed1ab_0    conda-forge
sphinxcontrib-applehelp   1.0.2                      py_0    conda-forge
sphinxcontrib-devhelp     1.0.2                      py_0    conda-forge
sphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge
sphinxcontrib-jsmath      1.0.1                      py_0    conda-forge
sphinxcontrib-qthelp      1.0.3                      py_0    conda-forge
sphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge
spyder                    4.2.1            py36ha15d459_0    conda-forge
spyder-kernels            1.10.1           py36ha15d459_0    conda-forge
sqlite                    3.34.0               h8ffe710_0    conda-forge
tensorboard               2.1.1                    pypi_0    pypi
tensorboard-plugin-wit    1.7.0              pyh9f0ad1d_0    conda-forge
tensorflow                2.1.0           eigen_py36hdbbabfe_0
tensorflow-base           2.1.0           eigen_py36h49b2757_0
tensorflow-estimator      2.1.0                    pypi_0    pypi
termcolor                 1.1.0                      py_2    conda-forge
testpath                  0.4.4                      py_0    conda-forge
textdistance              4.2.0              pyhd8ed1ab_0    conda-forge
three-merge               0.1.1              pyh9f0ad1d_0    conda-forge
tk                        8.6.10               h8ffe710_1    conda-forge
toml                      0.10.2             pyhd8ed1ab_0    conda-forge
tornado                   6.1              py36h68aa20f_0    conda-forge
traitlets                 4.3.3            py36h9f0ad1d_1    conda-forge
typed-ast                 1.4.2            py36h68aa20f_0    conda-forge
typing-extensions         3.7.4.3                       0    conda-forge
typing_extensions         3.7.4.3                    py_0    conda-forge
ujson                     4.0.1            py36h003fed8_1    conda-forge
urllib3                   1.26.2             pyhd8ed1ab_0    conda-forge
vc                        14.2                 hb210afc_2    conda-forge
vs2015_runtime            14.28.29325          h5e1d092_0    conda-forge
watchdog                  1.0.2            py36ha15d459_0    conda-forge
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
werkzeug                  0.16.1                     py_0    conda-forge
wheel                     0.36.2             pyhd3deb0d_0    conda-forge
win_inet_pton             1.1.0            py36h9f0ad1d_1    conda-forge
wincertstore              0.2             py36h9f0ad1d_1005    conda-forge
wrapt                     1.11.2           py36h779f372_1    conda-forge
yaml                      0.2.5                he774522_0    conda-forge
yapf                      0.30.0             pyh9f0ad1d_0    conda-forge
yarl                      1.6.3            py36h68aa20f_0    conda-forge
zeromq                    4.3.3                h0e60522_3    conda-forge
zipp                      3.4.0                      py_0    conda-forge
zlib                      1.2.11            h62dcd97_1010    conda-forge"
46174,Failed to build tensorflow 2.3.0 from source on Nvidia Jetson Nano,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack 4.4 (Ubuntu 18.04 LTS)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0
- Python version: 2.7.17 and 3.6.9
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): Build label: 3.1.0- (@non-git)
- GCC/Compiler version (if compiling from source): (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: 10.2.89 / 8.0.0.180
- GPU model and memory: Nvidia Jetson Nano



**Describe the problem**

ERROR: /home/minhduc/src/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:6109:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4)
aarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow:libtensorflow.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 10499.940s, Critical Path: 654.48s
INFO: 5016 processes: 5016 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed this [tutorial](https://jkjung-avt.github.io/build-tensorflow-2.0.0/) ([github](https://github.com/jkjung-avt/jetson_nano)). I set up my virtual screen, start the build (NOT install) script (build_libtensorflow-2.3.0.sh). Go to school and come back home with the error log above

**Any other info / logs**
I also install both protobuf 3.8.0 and bazel using shell scripts from the above github repo
"
46173,Failed to build with CUDA 11.1 + TensorRT 7.2,"**System information**
Ubuntu 20.04
Building branch r2.4
Python 3.8.5 (conda)
Bazel 3.1.0
GCC 9
CUDA 11.1
TensorRT 7.2.2
GPU RTX 2080ti

I was able to build r2.4 a week ago, with a similar config (but in docker, with nvidia cuda-11.1 base image, without tensorrt).  
I tried with or without MKL (but I would like to build both cuda and mkl support).

**Bazel command**
```bash
export GCC_HOST_COMPILER_PATH=$(which gcc)
export PYTHON_BIN_PATH=$(which python)
export PYTHON_LIB_PATH=""$($PYTHON_BIN_PATH -c 'import site; print(site.getsitepackages()[0])')""
export TF_ENABLE_XLA=1
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_KAFKA=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export TF_NEED_ROCM=0
export TF_SET_ANDROID_WORKSPACE=0

# GPU
export CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1
export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:$CUDA_TOOLKIT_PATH/lib64:$CUDA_TOOLKIT_PATH/lib64/stubs""
export TF_CUDA_VERSION=11.1
export TF_CUDA_CLANG=0
export TF_CUDA_COMPUTE_CAPABILITIES=7.5
export TF_NEED_CUDA=1
export CUDNN_INSTALL_PATH=/usr/
export TF_CUDNN_VERSION=8
export TF_NCCL_VERSION=2
export TF_NEED_TENSORRT=1
export TF_TENSORRT_VERSION=7

# Set search path: /usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3
./configure

export TMP=/tmp/bazel
bazel build --jobs 15 --compilation_mode opt --verbose_failures --copt='-march=native' --copt='-mfpmath=both' --config=nohdfs --config=mkl --config=opt \
    //tensorflow:libtensorflow_framework.so //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package
```

**Bazel output (verbose failure)**
```
Configuration finished
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=271
INFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python --action_env PYTHON_LIB_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/lib/python3.8/site-packages --python_path=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_PATHS=/usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env CUDNN_INSTALL_PATH=/usr/ --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/opt/TensorRT-7.2.2.3/lib::/opt/tensorflow/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64/stubs --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/vidlb/Code/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:tensorrt in file /home/vidlb/Code/git/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /home/vidlb/Code/git/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:nohdfs in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:mkl in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:opt in file /home/vidlb/Code/git/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true
INFO: Found applicable config definition build:linux in file /home/vidlb/Code/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2020/sqlite-amalgamation-3340000.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed 3 targets (414 packages loaded, 33212 targets configured).
INFO: Found 3 targets...
ERROR: /home/vidlb/Code/git/tensorflow/tensorflow/stream_executor/cuda/BUILD:254:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cublas_lt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 \
    CUDNN_INSTALL_PATH=/usr/ \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/opt/TensorRT-7.2.2.3/lib::/opt/tensorflow/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64/stubs \
    PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin:/opt/tensorflow/bin:/home/vidlb/Applications/anaconda3/condabin:/home/vidlb/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python \
    PYTHON_LIB_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
    TF_CUDA_PATHS=/usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3 \
    TF_CUDA_VERSION=11.1 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_TENSORRT=1 \
    TF_TENSORRT_VERSION=7 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' '-mfpmath=both' '-march=native' -Wno-sign-compare '-std=c++14' -c tensorflow/stream_executor/cuda/cublasLt_stub.cc -o bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:576:5: error: â€˜cublasComputeType_tâ€™ has not been declared
  576 |     cublasComputeType_t computeType,
      |     ^~~~~~~~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:586:5: error: â€˜cublasComputeType_tâ€™ has not been declared
  586 |     cublasComputeType_t computeType,
      |     ^~~~~~~~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:597:60: error: â€˜cublasComputeType_tâ€™ has not been declared
  597 | cublasLtMatmulDescCreate(cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType, cudaDataType_t scaleType);
      |                                                            ^~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1069:5: error: â€˜cublasComputeType_tâ€™ has not been declared
 1069 |     cublasComputeType_t computeType,
      |     ^~~~~~~~~~~~~~~~~~~
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1087:26: error: â€˜cublasComputeType_tâ€™ has not been declared
 1087 |                          cublasComputeType_t computeType,
      |                          ^~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:58:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:135:5: error: â€˜cublasComputeType_tâ€™ has not been declared
  135 |     cublasComputeType_t computeType, cudaDataType_t scaleType) {
      |     ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function â€˜cublasStatus_t cublasLtMatmulDescInit_internal(cublasLtMatmulDesc_t, size_t, int, cudaDataType_t)â€™:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:137:37: error: â€˜cublasComputeType_tâ€™ has not been declared
  137 |       cublasLtMatmulDesc_t, size_t, cublasComputeType_t, cudaDataType_t);
      |                                     ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:144:39: error: â€˜cublasComputeType_tâ€™ has not been declared
  144 |     cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType,
      |                                       ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function â€˜cublasStatus_t cublasLtMatmulDescCreate(cublasLtMatmulDescOpaque_t**, int, cudaDataType_t)â€™:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:147:31: error: â€˜cublasComputeType_tâ€™ has not been declared
  147 |       cublasLtMatmulDesc_t *, cublasComputeType_t, cudaDataType_t);
      |                               ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:309:35: error: â€˜cublasComputeType_tâ€™ has not been declared
  309 |     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,
      |                                   ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function â€˜cublasStatus_t cublasLtMatmulAlgoGetIds(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, int*, int*)â€™:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:314:25: error: â€˜cublasComputeType_tâ€™ has not been declared
  314 |       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,
      |                         ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:323:35: error: â€˜cublasComputeType_tâ€™ has not been declared
  323 |     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,
      |                                   ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function â€˜cublasStatus_t cublasLtMatmulAlgoInit(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, cublasLtMatmulAlgo_t*)â€™:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:328:25: error: â€˜cublasComputeType_tâ€™ has not been declared
  328 |       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,
      |                         ^~~~~~~~~~~~~~~~~~~
INFO: Elapsed time: 1574.977s, Critical Path: 135.25s
INFO: 10072 processes: 10072 local.
FAILED: Build did NOT complete successfully
```
Do I really need to build with 11.0, even if I was able to build with v11.1 inside a docker container ?
Or is it possible to patch some files in order to build with latest CUDA drivers ?
"
46171,Conda_Verification_Error for installing Tensor_flow 2.3.0,"Hello Folks, 
I am new to Deep Learning Libraries and stuff. I tried installing tensorflow with conda in a new environment and 
I am getting the following error while verifying.

CondaVerificationError: The package for tensorflow-base located at C:\Users\yuvid\Downloads\Softs\Installed\Miniconda3\pkgs\tensorflow-base-2.3.0-eigen_py37h17acbac_0
appears to be corrupted. The path 'Lib/site-packages/tensorflow/include/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.cpp.inc'
specified in the package manifest cannot be found.


ClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a
path collision for 'lib/site-packages/wheel/__pycache__/__init__.cpython-37.pyc'.
This path already exists in the target prefix, and it won't be removed
by an uninstall action in this transaction. The path is one that conda
doesn't recognize. It may have been created by another package manager.


ClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a
path collision for 'lib/site-packages/wheel/__pycache__/bdist_wheel.cpython-37.pyc'.
This path already exists in the target prefix, and it won't be removed
by an uninstall action in this transaction. The path is one that conda
doesn't recognize. It may have been created by another package manager.


ClobberError: The package 'defaults/win-64::markdown-3.3.3-py37haa95532_0' cannot be installed due to a
path collision for 'lib/site-packages/wheel/__pycache__/pkginfo.cpython-37.pyc'.
This path already exists in the target prefix, and it won't be removed
by an uninstall action in this transaction. The path is one that conda
doesn't recognize. It may have been created by another package manager.

ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/win-64::six-1.15.0-py37haa95532_0, defaults/win-64::win_inet_pton-1.1.0-py37haa95532_0, defaults/win-64::cffi-1.14.4-py37hcd4344a_0, defaults/win-64::yarl-1.6.3-py37h2bbff1b_0, defaults/win-64::markdown-3.3.3-py37haa95532_0
  path: 'lib/site-packages/wheel/__pycache__/wheelfile.cpython-37.pyc'

N many more similar errors just there is change in the file name of pyc


Hope anyone can help me out."
46170,"Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate","Apologies if this is in the incorrect category, but is more clarification that I am looking for rather than an issue with the implemented code. 

**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.1 LTS**
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): **Source**
TensorFlow version (use command below): **v2.3.1**
Python version: N/A
Bazel version (if compiling from source): **3.1.0**
GCC/Compiler version (if compiling from source): **6.3.0**
CUDA/cuDNN version: N/A
GPU model and memory: N/A

**Describe the current behavior**

I'm currently looking into benchmarking a custom delegate against XNNPACK. I've done so through the second option specified in the tensorflow documentation (using the external delegate options within the benchmark tool). 

I downloaded the binary for the benchmarking tool itself here (linux aarch64) rather than building source: https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary

When I run the benchmarking tool for the custom delegate, I get the info message specified in the title: 
_""Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate""_

I'm not sure what this is telling me. I've looked through the the tensorflow codebase and, from where I've looked, these output messages are no where in **_tensorflow/tensorflow/lite_**. Going up a level or two, I still can't find anything of similar. 

When running with xnnpack set to true, I get the output message _**""Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.""**_. This is a little more self explanatory, and I can see that some of the code for this message was generated at tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:666.

I added some debug information within the custom delegate and I can see that, when running the benchmark_model tool, I am creating the optimized delegate graph I expect. I am also seeing the numbers that I expect to see for the benchmarking against XNNPACK.

However, the message seems to make it out as if the custom delegate has been created, but is not used during the benchmarking. Is this true? I would appreciate any clarity at your earliest convenience. Thank you!"
46169,The training job is stuck because the rpc server is not started in the evaluator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
tf version: 2.4
cluster spec: 
```
{
    ""ps"":[
        ""localhost:13344""
    ],
    ""chief"":[
        ""localhost:15881""
    ],
    ""worker"":[
        ""localhost:37309"",
        ""localhost:30812""
    ],
    ""evaluator"":[
        ""localhost:22944""
    ]
}
```
example code:
```python
def main(argv):
    cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
    if cluster_resolver.task_type in ('ps', 'worker'):
        logging.info(""[{}] Start {}({})..."".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))
        server = tf.distribute.Server(
            cluster_resolver.cluster_spec(),
            job_name=cluster_resolver.task_type,
            task_index=cluster_resolver.task_id,
            protocol=cluster_resolver.rpc_layer or ""grpc"",
            start=True)
        server.join()

    if cluster_resolver.task_type == 'evaluator':
        ...
        checkpoint = tf.train.Checkpoint(model=model)
        ...

    if cluster_resolver.task_type == 'chief':
        logging.info(""[{}] Start {}({})..."".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))

        variable_partitioner = (
            tf.distribute.experimental.partitioners.FixedShardsPartitioner(
                num_shards=NUM_PS))

        strategy = tf.distribute.experimental.ParameterServerStrategy(
            cluster_resolver,
            variable_partitioner=variable_partitioner)

        ...
```
When i run tensorflow2.4 job with Custom Training Loop using ParameterServerStrategy on Yarn and use the same cluster spec on every process, it is stucked. 
Finally, I found that the GRPC Server was not started by the **evaluator** process, which caused the **coordinator** to wait all the time.

**Describe the expected behavior**
According to the problem described above, there are two solutionsï¼š
1. in evaluator process, it start the grpc server. like the following code. Besides, it should add some related doc.
```python
if cluster_resolver.task_type == 'evaluator':
        server = tf.distribute.Server(
            cluster_resolver.cluster_spec(),
            job_name=cluster_resolver.task_type,
            task_index=cluster_resolver.task_id,
            protocol=cluster_resolver.rpc_layer or ""grpc"",
            start=True)
        # dont need to call server.join()
        checkpoint = tf.train.Checkpoint(model=model)
        ...
```
2. In the PS strategy code of tf2.4, the **evaluator** type should be dynamically ignored to avoid waiting for a long time in **coordinator**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46168,Test TensorFloat32 with conv2d,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
```
import tensorflow as tf
import numpy as np
tf.config.experimental.enable_tensor_float_32_execution(False)
x_in = np.array([[
  [[2], [1], [2], [0], [1]],
  [[1], [3], [2], [2], [3]],
  [[1], [1], [3], [3], [0]],
  [[2], [2], [0], [1], [1]],
  [[0], [0], [3], [1], [2]], ]])
kernel_in = np.array([
 [ [[2, 0.1]], [[3, 0.2]] ],
 [ [[0, 0.3]],[[1, 0.4]] ], ])
x = tf.constant(x_in, dtype=tf.float32)
kernel = tf.constant(kernel_in, dtype=tf.float32)
out = tf.nn.conv2d(x, kernel, strides=[1, 1, 1, 1], padding='VALID')
```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): pip3 install tensorflow-gpu
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda11.1/cudnn8.0.5
- GPU model and memory: GeForce RTX 3090 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I ran this on an RTX 3090 with Nsight system. Compared with  tf.config.experimental.enable_tensor_float_32_execution(False), the  conv2d kernels don`t have higher performance with tf.config.experimental.enable_tensor_float_32_execution(True).

**Describe the expected behavior**
With tf.config.experimental.enable_tensor_float_32_execution(True), the conv2d kernels should have higher performance.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46165,train_on_batch is much slower than custom training loop on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: Colab
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab

**Describe the current behavior**

On Colab, `train_on_batch` is 3 times slower than custom training loop on GPU

**Describe the expected behavior**

Should be comparable.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1s6O4MyfqyCFI8uEM6JlTVNF18BLvYfka?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to

I think it's because `train_on_batch` uses `tf.data.Dataset`, which places tensor on CPU, on all kinds of input. The data transfer cost from host to device is not negligible resulting in slow down."
46164,Missing header when building Android CMake TF Lite 2.4 ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: git/source
- Bazel version (if compiling from source):3.1.0 (not used)
- GCC/Compiler version (if compiling from source): clang 12.0.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A, 32GB ram


I am not sure but I think the tensorflow 2.4 branch is missing the file `tensorflow/lite/delegates/gpu/cl/serialization_generated.h`?

Following steps from https://www.tensorflow.org/lite/guide/build_cmake I get the error messages

```
gmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:960: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/softmax1x1.cc.o] Error 1
gmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:895: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/relu.cc.o] Error 1
1 warning and 1 error generated.
In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.cc:16:
In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.h:20:
In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:22:
In file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/arguments.h:24:
/Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/gpu_object.h:26:10: fatal error:
      'tensorflow/lite/delegates/gpu/cl/serialization_generated.h' file not found
#include ""tensorflow/lite/delegates/gpu/cl/serialization_generated.h""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

And many others. On master I can see the header exist, I assume the CMakeLists file isn't downloading or generating this file on compilation.

"
46163,GPU-delegate null EGLDisplay: lack of Wayland support?,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Mendel Linux 5.0 Eagle** ([Mendel Linux Versions](https://coral.ai/software/#mendel-dev-board))
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Coral Dev Board 1GB** ([details](https://coral.ai/products/dev-board/))
- TensorFlow installed from (source or binary): **source, TensorFlow Lite only**
- TensorFlow version (use command below): **released version 2.3.1**
- Python version: **N/A**
- Bazel version (if compiling from source): **3.10** (installed via `sudo apt install bazel-3.10`)
- GCC/Compiler version (if compiling from source): **gcc 7.5.0**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **Vivante GC7000Lite, ~1GB**

**Describe the current behavior**

When attempting to run a C++ program linked against `libtensorflowlite_gpu_delegate.so`, an EGLDisplay is
not properly assigned nor initialized, thus the overall gpu-delegate fails to run.

This is the pertinent error message:
```
EGL: Warning: No default display support on wayland
ERROR: TfLiteGpuDelegate Init: eglGetDisplay returned nullptr
```
which I believe is generated via [line 35 of egl_environment.cc](https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/delegates/gpu/gl/egl_environment.cc#L35)

**Describe the expected behavior**

The Coral Dev Board runs Mendel Linux, which as I understand it runs the Wayland compositor.  Thus _I believe_
`wl_display_connect()` (and possibly other functions) must be called to properly initialize EGL
within the TensorFlow Lite gpu-delegate.

**Standalone code to reproduce the issue**
The following code specifically shows the above error:
```
#include <stdio.h>
#include <assert.h>
#include <math.h>

#include <EGL/egl.h>
#include <EGL/eglext.h>

// gcc simple-egl.cpp -lEGL -lGLESv2 -o simple-egl-display
    
int main(int argc, char *argv[]) {

    EGLDisplay display;
    EGLSurface surface;
    EGLContext context;
    EGLConfig config;
    EGLBoolean result;

    display = eglGetDisplay(EGL_DEFAULT_DISPLAY);
    
    if (display == EGL_NO_DISPLAY) {
        fprintf(stderr, ""EGL_NO_DISPLAY...\n"");
        return 1;
    } 

    // initialize the EGL display connection
    result = eglInitialize(display, NULL, NULL);
    
    if (result == EGL_FALSE) {
        fprintf(stderr, ""Cannot initialize EGL via the default display...\n"");
        return 1;
    }        
    return 0;
}   
```
Versus this simple program with Wayland support:
```
#include <stdio.h>
#include <assert.h>
#include <math.h>

#include <EGL/egl.h>
#include <EGL/eglext.h>

#include <errno.h>
#include <cstring>
#include <wayland-egl.h>

// gcc simple-egl.cpp -lEGL -lGLESv2 -lwayland-egl -lwayland-client -o simple-egl-display

int main(int argc, char *argv[]) {

    EGLNativeDisplayType egl_native_display;
    EGLDisplay egl_display_wayland;
    EGLint major_version, minor_version;

    EGLDisplay display;
    EGLSurface surface;
    EGLContext context;
    EGLConfig config;
    EGLBoolean result;

    display = eglGetDisplay(EGL_DEFAULT_DISPLAY);

    if (display == EGL_NO_DISPLAY) {
        printf(""EGL NO DISPLAY (Wayland support?)\n"");
        //return 1;
    } else {

        // initialize the EGL display connection
        result = eglInitialize(display, NULL, NULL);

        if (result == EGL_FALSE) {
            fprintf(stderr, ""Can't initialise EGL via the default display...\n"");
            // return 1;
        }
    }

    // Requires wayland
    egl_native_display = EGLNativeDisplayType(wl_display_connect(NULL));
    if (egl_native_display == NULL) {
        printf(""wl_display_connect failed: %s\n"", strerror(errno));
        return 1;
    } else {

        // Now initialize EGL
        egl_display_wayland = eglGetDisplay(egl_native_display);

        if (egl_display_wayland == EGL_NO_DISPLAY){
            printf(""Could not obtain EGL display (wayland)\n"");
            return 1;
        }

        if (!eglInitialize(egl_display_wayland, &major_version, &minor_version))
        {
            printf(""Could not initialize EGL (wayland)"");
            egl_display_wayland = EGL_NO_DISPLAY;
            return 1;
        }
    }
    return 0;
}
```
"
46162,micro: port op ADD_N from lite,"@tensorflow/micro

This issue tracks my work porting operator ADD_N from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro making minimal changes and not including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test
"
46161,micro: port op LEAKY_RELU from lite,"@tensorflow/micro

This issue tracks my work porting operator LEAKY_RELU from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro making minimal changes and not including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test"
46148,"micro: in CONTRIBUTING.md, the path a test script is wrong","@tensorflow/micro
- Tensorflow version (commit SHA if source): 556fa126
 
In micro's CONTRIBUTING.md, the path to a script to run tests prior to submitting a PR uses (what is now, at least) an invalid path.

https://github.com/tensorflow/tensorflow/blob/7958e0cbfa55b779c3682aaaa37f6e7c55f55bc2/tensorflow/lite/micro/CONTRIBUTING.md#L204-L208

The fix is trivial and on its way."
46146,Model loaded from a SavedModel format in a distribution strategy has weights whose names are not unique,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- TensorFlow installed from (source or binary): colab
- TensorFlow version (use command below): 2.2 and 2.4
- Python version: 3.6
- CUDA/cuDNN version: problem is on CPU and GPU
- GPU model and memory: colab

**Describe the current behavior**

I am using a distribution strategy to train my model on several GPUs, and using the `ModelCheckpointCallback` to save my model.
I train my model in 2 steps, in the first I use the SavedModel format and in the second, I want to save my model using the HDF5 format.
However, I encounter a problem when loading my saved model in a distribution strategy, where the names of the different weights of my model are stripped down the last part (i.e. for example `'kernel:0'`).
Therefore, my model has several weights with the same name and cannot be saved in the HDF5 format.

**Describe the expected behavior**

I would like my model to be loaded in the distribution strategy with the proper names.

**Standalone code to reproduce the issue**
You can find a colab to illustrate this issue [here](https://colab.research.google.com/drive/1VqhqimaF3zxkj1mBrjugITtRC5XYJrW9?usp=sharing).

**Other info / logs** 
For reference the typical error that you get is `RuntimeError: Unable to create link (name already exists)`, with the full traceback being:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-10-debcda389c22> in <module>()
      1 # and as a consequence, we cannot save the weights of our model
----> 2 new_model.save_weights('new_model_weights.hdf5')

3 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in save_weights(self, filepath, overwrite, save_format, options)
   2106     if save_format == 'h5':
   2107       with h5py.File(filepath, 'w') as f:
-> 2108         hdf5_format.save_weights_to_hdf5_group(f, self.layers)
   2109     else:
   2110       if context.executing_eagerly():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)
    640     save_attributes_to_hdf5_group(g, 'weight_names', weight_names)
    641     for name, val in zip(weight_names, weight_values):
--> 642       param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
    643       if not val.shape:
    644         # scalar

/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds)
    137             dset = dataset.Dataset(dsid)
    138             if name is not None:
--> 139                 self[name] = dset
    140             return dset
    141 

/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py in __setitem__(self, name, obj)
    371 
    372             if isinstance(obj, HLObject):
--> 373                 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
    374 
    375             elif isinstance(obj, SoftLink):

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5o.pyx in h5py.h5o.link()

RuntimeError: Unable to create link (name already exists)
```

However, this issue is different than [this one](https://github.com/tensorflow/tensorflow/issues/27688#issuecomment-595950270) because originally the weights do have different names (and even when saved, the weights have different names, it's just that loading them inside the distribution strategy erases that).
"
46145,Pandas pct_change() function results in nan loss when training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: Python 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Running on CPU
- GPU model and memory:


**Describe the current behavior**
When training a model with data from a pandas DataFrame, the model trains fine unless I use the df.pct_change() pandas function on it. If I do so, the loss is always nan. I specifically made sure to remove any NaN or inf values from the dataset but the problem persists. I don't know if I'm just missing something obvious and the issue is on my end.

I created a simplified jupyter notebook to demonstrate the issue.

**Describe the expected behavior**
The loss should be a real number in the last cell of the notebook (after I remove all NaN and inf values) but it is still nan.

**Standalone code to reproduce the issue**
Here is the notebook:
https://drive.google.com/file/d/1k3dHHtFI4tGTKswF0d0TJdj9TfaqfBnX/view?usp=sharing
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46144,TensorFlowLiteSwift build error in the Bazel project,"**System information**
- OS Platform and Distribution: macOS Big Sur 11.1
- TensorFlow version: 2.4.0
- Bazel version: 3.7.2-homebrew
- Compiler version: Apple clang 12.0
- Xcode version: 12.3


I want to connect to mediapipe app for ios TensorFlowLiteSwift framework.

I added a bazel dependency to my project:
BUILD file:
```
swift_library(
    deps = [
      ""@tensorflow_my//tensorflow/lite/swift:TensorFlowLite"",
    ]
)
```

My WORKSPACE file (fragment for tensorflow):
```
#Tensorflow repo should always go after the other external dependencies.
# 2020-10-30
_TENSORFLOW_GIT_COMMIT = ""84384703c0d8b502e33ff6fd7eefd219dca5ff8e""
_TENSORFLOW_SHA256= ""23fb322fc15a20f7a7838d9a31f8b16f60700a494ea654311a0aa8621769df98""
http_archive(
    name = ""org_tensorflow"",
    urls = [
      ""https://github.com/tensorflow/tensorflow/archive/%s.tar.gz"" % _TENSORFLOW_GIT_COMMIT,
    ],
    patches = [
        ""@//third_party:org_tensorflow_compatibility_fixes.diff"",
    ],
    patch_args = [
        ""-p1"",
    ],
    strip_prefix = ""tensorflow-%s"" % _TENSORFLOW_GIT_COMMIT,
    sha256 = _TENSORFLOW_SHA256,
)

load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
tf_workspace(tf_repo_name = ""org_tensorflow"")

local_repository(
    name = ""tensorflow_my"",
    path = ""third_party/tensorflow"", # < -- cloned repository
)


load(""@tensorflow_my//tensorflow:workspace3.bzl"", ""workspace"")
workspace()
load(""@tensorflow_my//tensorflow:workspace2.bzl"", ""workspace"")
workspace()
load(""@tensorflow_my//tensorflow:workspace1.bzl"", ""workspace"")
workspace()
load(""@tensorflow_my//tensorflow:workspace0.bzl"", ""workspace"")
workspace()
```

When building a project, it outputs the following:
```
Showing All Messages
Queuing Tulsi build...
<*> Parsing options completed in 0.261 ms
Running ""/usr/local/bin/bazel build --verbose_failures --bes_outerr_buffer_size=0 --apple_platform_type=ios --cpu=ios_arm64 --watchos_cpus=armv7k --announce_rc '--override_repository=tulsi=/Users/dmitry/Library/Application Support/Tulsi/0.20190814.88/Bazel' --compilation_mode=dbg --define=apple.add_debugger_entitlement=1 --define=apple.propagate_embedded_extra_outputs=1 --define=apple.experimental.tree_artifact_outputs=1 --features=debug_prefix_map_pwd_is_dot --tool_tag=tulsi:bazel_build --build_event_json_file=/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/8792_build_events.json --noexperimental_build_event_json_file_path_conversion --aspects @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect --output_groups=tulsi_outputs,default //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp"", patching output for workspace root at ""/Users/dmitry/Documents/mediapipeNew/mediapipe"" with project path at ""/Users/dmitry/Documents/mediapipeNew"".
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:
  'build' options: --jobs 128 --define=absl=1 --enable_platform_specific_config --apple_platform_type=macos --apple_generate_dsym
INFO: Found applicable config definition build:macos in file /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc: --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --copt=-w
Loading: 
Loading: 0 packages loaded
DEBUG: Rule 'rules_foreign_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""3e6b0691fc57db8217d535393dcc2cf7c1d39fc87e9adb6e7d7bab1483915110""
DEBUG: Repository rules_foreign_cc instantiated at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:39:13: in <toplevel>
Repository rule http_archive defined at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
Analyzing: target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (0 packages loaded, 0 targets configured)
DEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""fc58ff069f150c81abd10231bb1d9fbff0ba9322e03c9396518db4d054d5f2e6""
DEBUG: Repository rules_cc instantiated at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:33:13: in <toplevel>
Repository rule http_archive defined at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:401:10: in <toplevel>
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/workspace0.bzl:65:34: in workspace
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (1 packages loaded, 15 targets configured).
INFO: Found 1 target...

[0 / 7] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[25 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc; 1s darwin-sandbox ... (33 actions, 4 running)
[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 2s darwin-sandbox ... (44 actions, 4 running)
[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 3s darwin-sandbox ... (44 actions, 4 running)
[31 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/cpu_backend_gemm_eigen.cc; 4s ... (44 actions, 4 running)
[34 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/experimental/resource/static_hashtable.cc; 6s ... (44 actions, 4 running)
[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 7s ... (43 actions, 4 running)
[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 9s ... (43 actions, 4 running)
[38 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/tflite_with_xnnpack_optional.cc; 11s ... (42 actions, 4 running)
[41 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/simple_memory_arena.cc; 13s ... (42 actions, 4 running)
[46 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/deprecated_backends.cc; 15s ... (42 actions, 4 running)
[55 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/core/api/op_resolver.cc; 17s ... (128 actions, 4 running)
[57 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/utils.cc; 20s ... (128 actions, 4 running)
[63 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/eigen_support.cc; 24s ... (128 actions, 4 running)
[68 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc; 28s ... (128 actions, 4 running)
INFO: From Linking external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a:
warning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: archive library: bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a the table of contents is empty (no object file members in the library define global symbols)
[94 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/interpreter.cc; 16s ... (124 actions, 4 running)
ERROR: /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/BUILD:22:11: C++ compilation of rule '@tensorflow_my//tensorflow/lite/delegates/xnnpack:xnnpack_delegate' failed (Exit 1): wrapped_clang failed: error executing command 
  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \
  exec env - \
    APPLE_SDK_PLATFORM=iPhoneOS \
    APPLE_SDK_VERSION_OVERRIDE=14.3 \
    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \
    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)
Execution platform: @local_execution_config_platform//:platform

Use --sandbox_debug to see verbose messages from the sandbox wrapped_clang failed: error executing command 
  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \
  exec env - \
    APPLE_SDK_PLATFORM=iPhoneOS \
    APPLE_SDK_VERSION_OVERRIDE=14.3 \
    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \
    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)
Execution platform: @local_execution_config_platform//:platform

Use --sandbox_debug to see verbose messages from the sandbox
/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:277:49: error: use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'
    const uint32_t flags = has_sparse_weights ? XNN_FLAG_SPARSE_INFERENCE : 0;
                                                ^
/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1449:33: error: use of undeclared identifier 'xnn_define_depth_to_space'
      const xnn_status status = xnn_define_depth_to_space(
                                ^
/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1538:11: error: use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?
          xnn_define_elu(subgraph, /*alpha=*/1.0f,
          ^~~~~~~~~~~~~~
          xnn_define_prelu
/Users/dmitry/Documents/mediapipeNew/mediapipe/external/XNNPACK/include/xnnpack.h:831:17: note: 'xnn_define_prelu' declared here
enum xnn_status xnn_define_prelu(
                ^
3 errors generated.
Aspect @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect of //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp up-to-date:
  bazel-bin/mediapipe/examples/ios/handtrackinggpu/HandTrackingGpuApp.tulsiouts
INFO: Elapsed time: 40,984s, Critical Path: 38,06s
INFO: 205 processes: 133 internal, 72 darwin-sandbox.
FAILED: Build did NOT complete successfully
<*> Running Bazel completed in 41186.495 ms
/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/Scripts/bazel_build.py:549: error: Bazel build failed with exit code 1. Please check the build log in Report Navigator (âŒ˜9) for more information.
<*> Everything completed in 41246.470 ms


```

Xcode highlights the following in red (a fragment from the above log):
```
 use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'
      const xnn_status status = xnn_define_depth_to_space(

 use of undeclared identifier 'xnn_define_depth_to_space'
          xnn_define_elu(subgraph, /*alpha=*/1.0f,
          ^~~~~~~~~~~~~~
          xnn_define_prelu

 use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?
         enum xnn_status xnn_define_prelu(
                        ^
3 errors generated.
```


"
46143,Tensorflow and Keras installation issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Here is the version of Tensorflow and keras installed on python 3.7 in win 10

.C:\Users\ZS>pip show tensorflow
Name: tensorflow
Version: 2.4.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\zs\appdata\local\programs\python\python37\lib\site-packages
Requires: termcolor, absl-py, google-pasta, typing-extensions, tensorboard, grpcio, six, wrapt, astunparse, numpy, keras-preprocessing, opt-einsum, gast, flatbuffers, tensorflow-estimator, wheel, protobuf, h5py
Required-by:

C:\Users\ZS>pip show keras
Name: Keras
Version: 2.4.3
Summary: Deep Learning for humans
Home-page: https://github.com/keras-team/keras
Author: Francois Chollet
Author-email: francois.chollet@gmail.com
License: MIT
Location: c:\users\zs\appdata\local\programs\python\python37\lib\site-packages
Requires: numpy, h5py, scipy, pyyaml
Required-by:
**But..........................................
******
import tensorflow
Traceback (most recent call last):
  File ""C:\Users\ZS\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\ZS\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\ZS\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\ZS\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ZS\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>  from tensorflow import keras
 
SyntaxError: unexpected indent
>>> 


"
46142,TensorFlow and TensorFlowLite model produces different results,"
### System information

-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Simple test script included
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Google Colaboratory
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device**:
N/A
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
v2.4.0-0-g582c8d236cb 2.4.0
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:
test script attached

### Describe the problem

I have a TensorFlow model (attached as a zip file) which is converted to TensorFlowLite using the attached script.
Both models are evaluated using a single test exemplar.
Expected: The two models should produce similar, if not identical, predictions.
Actual: The two models produce completely different predictions

(the tflite model produces the same 'wrong' predictions when running on a mobile device but the 
test can entirely reproduced in Colaboratory using the attached files).
[model.zip](https://github.com/tensorflow/tensorflow/files/5765363/model.zip)

[tflite_model_test.ipynb.txt](https://github.com/tensorflow/tensorflow/files/5765368/tflite_model_test.ipynb.txt)

### Source code / logs
Attached model.zip and tflite_model_test.ipynb"
46137,MetalDelegate.swift : Unexpected version number in 'available' attribute for non-specific platform '*',"Warning in line
 @available(*, deprecated: 2.4, renamed: ""isPrecisionLossAllowed"")"
46136,java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5',"I am going through the following codelab:
https://developer.android.com/codelabs/digit-classifier-tflite#5

## Description of issue:
I get the following error

  2021-01-04 14:58:02.547 27206-27283/org.tensorflow.lite.codelabs.digitclassifier E/AndroidRuntime: FATAL EXCEPTION: pool-1-thread-1
      Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 27206
      java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'
      
      Registration failed.
      
          at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
          at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:70)
          at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:61)
          at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.initializeInterpreter(DigitClassifier.kt:64)
          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.access$initializeInterpreter(DigitClassifier.kt:31)
          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier$initialize$1.run(DigitClassifier.kt:48)
          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
          at java.lang.Thread.run(Thread.java:919)

### Clear description

I have looked this up online. It points to some differences in the TF and TFLite versions. But I followed the steps exactly in the codelab and did not expect something like this.
"
46135,"When activation='tanh' is used, the training gradient no drop","
- TensorFlow installed from (source or binary): pip install tensorflow==2.4.0rc4
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.8
- CUDA/cuDNN version: CUDA11.0/cuDNN8.0.4
- GPU model and memory: NVIDIA RTX3070 8G
- OS version: windows 10

ISSUE:  When activation='tanh' is used, the training gradient no drop. the follow test code from: https://www.tensorflow.org/tutorials/generative/dcgan


import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow.keras import layers,Model
import time

from IPython import display

import tensorflow as tf



class ResNet(Model):
    def __init__(self):
        super(ResNet, self).__init__()
        self.d1=layers.Dense(7*7*256, use_bias=False, input_shape=(100,))
        self.b1=layers.BatchNormalization()
        self.a1=layers.LeakyReLU()
        self.r1=layers.Reshape((7, 7, 256))
        self.c1=layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)
        self.r2=layers.Reshape((7, 7, 128))
        self.c2=layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)
        self.r3=layers.Reshape((14, 14, 64))
        self.c3=layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)
        self.r4=layers.Reshape((28, 28, 1))

    def call(self, inputs):
        x=self.d1(inputs)
        x=self.b1(x)
        x=self.a1(x)
        x=self.r1(x)
        x=self.c1(x)
        assert Model.output_shape == (None, 7, 7, 128)
        x=self.b1(x)
        x=self.a1(x)
        x=self.c2(x)
        
        x=self.b1(x)
        x=self.a1(x)
        x=self.c3(x)
        x=tf.math.sinh(x)/tf.math.cosh(x)
        
        return x

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # æ³¨æ„ï¼šbatch size æ²¡æœ‰é™åˆ¶

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model


def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model


def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)


@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))


def train(dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch in dataset:
      train_step(image_batch)

    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epoch + 1,
                             seed)


    if (epoch + 1) % 15 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))


  display.clear_output(wait=True)
  generate_and_save_images(generator,
                           epochs,
                           seed)

def generate_and_save_images(model, epoch, test_input):

  predictions = model(test_input, training=False)

  fig = plt.figure(figsize=(4,4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
      plt.axis('off')
  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
 # plt.show()

def display_image(epoch_no):
  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))

config = tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.compat.v1.Session(config=config)

(train_images, train_labels), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_images = (train_images - 127.5) / 127.5 # å°†å›¾ç‰‡æ ‡å‡†åŒ–åˆ° [-1, 1] åŒºé—´å†…

BUFFER_SIZE = 60000
BATCH_SIZE = 256


train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)


generator =make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

plt.imshow(generated_image[0, :, :, 0], cmap='gray')
plt.show()


discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
generator_optimizer = tf.keras.optimizers.Adam(1e-2)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-2)

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 50
noise_dim = 100
num_examples_to_generate = 16



seed = tf.random.normal([num_examples_to_generate, noise_dim])


train(train_dataset, EPOCHS)

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

anim_file = 'dcgan.gif'

with imageio.get_writer(anim_file, mode='I') as writer:
  filenames = glob.glob('image*.png')
  filenames = sorted(filenames)
  last = -1
  for i,filename in enumerate(filenames):
    frame = 2*(i**0.5)
    if round(frame) > round(last):
      last = frame
    else:
      continue
    image = imageio.imread(filename)
    writer.append_data(image)
  image = imageio.imread(filename)
  writer.append_data(image)

import IPython
if IPython.version_info > (6,2,0,''):
  display.Image(filename=anim_file)

try:
  from google.colab import files
except ImportError:
   pass
else:
  files.download(anim_file)
"
46134,NotFoundError: No CPU devices are available in this process,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.2
- Python version: 2.7.17
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc 4.8


Hello!
I have built TF-cpu 1.13.2 from source code and I'm trying to run a benchmark [tensorflow/benchmark](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.13_compatible/scripts/tf_cnn_benchmarks)
An error (NotFoundError) occured after I run the following commands:
`python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &`

However tf can detect my CPU device correctly:
```
Python 2.7.17 (default, Sep 30 2020, 13:38:04)
[GCC 7.5.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python.client import device_lib
>>> device_lib.list_local_devices()
2021-01-04 08:35:20.085746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 15041826958121108015
]
>>>
```

Any help would be appreciated. Thanksï¼

Here is my log:

```
root@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &
[1] 12899
root@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# Traceback (most recent call last):
  File ""tf_cnn_benchmarks.py"", line 74, in <module>
    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""tf_cnn_benchmarks.py"", line 63, in main
    params = benchmark_cnn.setup(params)
  File ""/home/cluster/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py"", line 3503, in setup
    with tf.Session(config=create_config_proto(params)) as sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1551, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 676, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.NotFoundError: No CPU devices are available in this process
```"
46133,Disable Eager Execution for the OPs related to Graphs and Sessions in Tensorflow 2.x,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope

## Description of issue (what needs changing): 
The command, **`tf.compat.v1.disable_eager_execution()`** should be added in the Code Examples of [the documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) as the `Variable Scope` of `Graphs` is not applicable during `Eager Execution`.

### Clear description

For example, why should someone use this method? How is it useful? : For setting the **`Variable Scope`**.

### Correct links

Is the link to the source code correct? : Yes

### Parameters defined

Are all parameters defined and formatted correctly? : Yes

### Returns defined

Are return values defined? : NA

### Raises listed and defined

Are the errors defined? : Yes

### Usage example

Is there a usage example? : Yes"
46132,Android: Instantiating TFLite Interpreter with GPU delegate takes 2.5 seconds.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Catalina v10.15.7**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Pixel 3a**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **v2.4.0**
- Python version: **3.8.0**
- Bazel version (if compiling from source): **-** 
- GCC/Compiler version (if compiling from source): **-**
- CUDA/cuDNN version:
- GPU model and memory: **Adreno 615**

**Describe the current behavior**
The instantiation time of tiflite interpreter with GPU delegate on Android is extremely high. For some models It takes about 2.5 seconds to instantiate it. This behavior was tested with various segmentation models of different sizes (500Kb - 12Mb) and on different  mobile devices. The instantiation time very slightly dependent on the model size and is faster on flagship devices (such as Samsung Galaxy S20) but overall it is very slow. Initializing an interpreter with the same model without GPU delegate is sub millisecond.


**Standalone code to reproduce the issue**
```
// interpreterBinary: ByteBuffer direct allocated byte buffer.
val options = Interpreter.Options().addDelegate(GpuDelegate())
val stopwatch = Stopwatch.createStarted() // start trace
val interpreter = Interpreter(interpreterBinary, options)
stopwatch.stop() // end trace
```

"
46131,TensorFlow,
46130,ValueError: Invalid checkpoint state loaded from,"Running Tensorflow 1.13.0  through Anaconda on Ubuntu 16.04 (Full install on external SSD)
Python 3.6.11

```
import tensorflow as tf
result = tf.train.get_checkpoint_state('')
```
I booted to Ubuntu 16.04 installed on external SSD from my Windows PC. When I run the above commands in my PC Internal drive path in anaconda environment, I get a warning 
```
WARNING:tensorflow:FailedPreconditionError: checkpoint; Is a directory
WARNING:tensorflow:checkpoint: Checkpoint ignored
```

But when I run the exact commands in my external SSD drive path in the same anaconda environment, I get a `ValueError: Invalid checkpoint state loaded from`

What's the cause for this and how can I fix it?"
46129,"""Table not initialized"" when loading model in Java","- TensorFlow version (use command below):2.3.0
- Python version:3.7

I am trying to use the tensorflow model in java,I convert a text classification model (with tf.lookup) to fomat .pb and want to load it in JAVA.But got ""Table not initialized"" error.
```
2021-01-04 14:00:10.713588: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.
Exception in thread ""main"" java.lang.IllegalStateException: Table not initialized.
	 [[{{node graph/hash_table_Lookup/LookupTableFindV2}}]]
	at org.tensorflow.Session.run(Native Method)
	at org.tensorflow.Session.access$100(Session.java:48)
	at org.tensorflow.Session$Runner.runHelper(Session.java:326)
	at org.tensorflow.Session$Runner.run(Session.java:276)
	at ctest.Ttest.predict(Ttest.java:32)
	at ctest.Ttest.main(Ttest.java:13)
```

**here is my code:**
***In PYTHON***

```
import os
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
from tensorflow.python.framework.graph_util import convert_variables_to_constants
from tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
OUTPUT_FOLDER = ''
OUTPUT_NAME = 'hash_table.pb'
OUTPUT_NAMES = ['graph/output', 'init_all_tables']


def build_graph():
    d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}
    init = KeyValueTensorInitializer(list(d.keys()), list(d.values()))
    hash_table = HashTable(init, default_value=-1)
    data = tf.placeholder(tf.string, (None,), name='data')
    values = hash_table.lookup(data)
    output = tf.identity(values * 2, 'output')

def freeze_graph():
    with tf.Graph().as_default() as graph:
        with tf.name_scope('graph'):
            build_graph()

        with tf.Session(graph=graph) as sess:
            sess.run(tf.tables_initializer())
            print(sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']}))
            frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)
            tf.train.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_NAME, as_text=False)

def load_frozen_graph():
    with open(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME), 'rb') as f:
        output_graph_def = tf.GraphDef()
        output_graph_def.ParseFromString(f.read())

    with tf.Graph().as_default() as graph:
        tf.import_graph_def(output_graph_def, name='')
        with tf.Session(graph=graph) as sess:
            try:
                sess.run(graph.get_operation_by_name('init_all_tables'))
            except KeyError:
                pass
            print(sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']}))


if __name__ == '__main__':
    freeze_graph()
    load_frozen_graph()
```

***In JAVA***

```
package ctest;

import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import java.nio.file.Files;
import java.nio.file.Paths;

public class Ttest {
    public static void main(String[] args) throws Exception {
        predict();
    }
    public static void predict() throws Exception {
        try (Graph graph = new Graph()) {
            graph.importGraphDef(Files.readAllBytes(Paths.get(
                    ""/opt/resources/hash_table.pb""
            )));
            try (Session sess = new Session(graph)) {
                byte[][] matrix = new byte[1][];
                matrix[0] = ""a"".getBytes(""UTF-8"");
                Tensor< ? > out = sess.runner()
                        .feed(""graph/data:0"", Tensor.create(matrix)).fetch(""graph/output:0"").run().get(0);
                float[][] output = new float[1][(int) out.shape()[1]];
                out.copyTo(output);
                for(float i:output[0])
                    System.out.println(i);


            }
        }
    }
}
```

Any suggestions would be greatly appreciated.

"
46128,"This throws ERROR: features = tf.io.parse_example(..., features=make_parse_example_spec(columns))","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Latest from this week
- Python version: 3.8.x
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2
- GPU model and memory: 

**Describe the current behavior**
I was trying this code, but it throws exception

```https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures```

```return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.```


Here is the full code from that page,


```# Behavior of some cells or feature columns may depend on whether we are in
# training or inference mode, e.g. applying dropout.
training = True
rating = sequence_numeric_column('rating')
watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [rating, watches_embedding]

sequence_input_layer = SequenceFeatures(columns)
features = tf.io.parse_example(...,
                               features=make_parse_example_spec(columns))
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size, training=training)
rnn_layer = tf.keras.layers.RNN(rnn_cell, training=training)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
```"
46127,Installing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 enterprise
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: nightly or 2.4
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: normal pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.0 (Recomemded
- GPU model and memory: gtx 1060 6gb



**Describe the problem**

I run pip install tf-nightly or pip install tensorflow and get the following error
 ERROR: Command errored out with exit status 1:
     command: 'c:\users\kai\appdata\local\programs\python\python38\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\kai\\AppData\\Local\\Temp\\pip-install-w2yenss2\\wrapt\\setup.py'""'""'; __file__='""'""'C:\\Users\\kai\\AppData\\Local\\Temp\\pip-install-w2yenss2\\wrapt\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base 'C:\Users\kai\AppData\Local\Temp\pip-pip-egg-info-bql118ug'
         cwd: C:\Users\kai\AppData\Local\Temp\pip-install-w2yenss2\wrapt\
    Complete output (26 lines):
    Traceback (most recent call last):
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\pkg_resources\__init__.py"", line 2866, in get_entry_map
        ep_map = self._ep_map
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\pkg_resources\__init__.py"", line 2824, in __getattr__
        raise AttributeError(attr)
    AttributeError: _ep_map

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""C:\Users\kai\AppData\Local\Temp\pip-install-w2yenss2\wrapt\setup.py"", line 102, in <module>
        run_setup(with_extensions=True)
      File ""C:\Users\kai\AppData\Local\Temp\pip-install-w2yenss2\wrapt\setup.py"", line 72, in run_setup
        setup(**setup_kwargs_tmp)
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\distutils\core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\setuptools\dist.py"", line 427, in __init__
        for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'):
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\pkg_resources\__init__.py"", line 655, in <genexpr>
        for entry in dist.get_entry_map(group).values()
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\pkg_resources\__init__.py"", line 2868, in get_entry_map
        ep_map = self._ep_map = EntryPoint.parse_map(
      File ""c:\users\kai\appdata\local\programs\python\python38\lib\site-packages\pkg_resources\__init__.py"", line 2549, in parse_map
        raise ValueError(""Entry points must be listed in groups"")
    ValueError: Entry points must be listed in groups
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
46126,Performance 5x degradation after converting numpy to tf.function equivalent,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu (Google colab) & macOs 11.1
- TensorFlow installed from (source or binary): pip installation.
- TensorFlow version (use command below): 2.3.1 / 2.4
- Python version: 3.6+
- CUDA/cuDNN version: 10.1
- GPU model and memory: Varies

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I converted this function, that performs gradient updates for a DQN agent. It takes a batch of numpy arrays, calculates target values and updates gradients using `model.fit()` for 1 epoch, it usually processes 90-100 frames per second on google colab.

    def update(self, batch):
        """"""
        Update gradients given a batch.
        Args:
            batch: A batch of observations in the form of
                [[states], [actions], [rewards], [dones], [next states]]
    
        Returns:
            None
        """"""
        states, actions, rewards, dones, new_states = batch
        q_states = self.main_model.predict(states)
        if self.double:
            new_state_actions = np.argmax(self.main_model.predict(new_states), 1)
            new_state_q_values = self.target_model.predict(new_states)
            new_state_values = new_state_q_values[
                np.arange(self.batch_size), new_state_actions
            ]
        else:
            new_state_values = self.target_model.predict(new_states).max(1)
        new_state_values[dones] = 0
        target_values = np.copy(q_states)
        target_value_update = new_state_values * self.gamma ** self.n_steps + rewards
        state_action_values = target_values[np.arange(self.batch_size), actions]
        target_values[np.arange(self.batch_size), actions] = target_value_update
        self.main_model.fit(states, target_values, verbose=0)
        if self.buffer.priorities:
            squared_loss = (state_action_values - target_value_update) ** 2
            priorities = (
                self.buffer.current_weights * squared_loss + self.buffer.priority_bias
            )
            self.buffer.update_priorities(priorities)

**After the conversion:**

    @tf.function
    def get_targets(self, batch):
        """"""
        Get target values for gradient updates.
        Args:
            batch: A batch of observations in the form of
                [[states], [actions], [rewards], [dones], [next states]]
        Returns:
            None
        """"""
        states, actions, rewards, dones, new_states = batch
        q_states = self.main_model(states)
        if self.double:
            new_state_actions = tf.argmax(self.main_model(new_states), 1)
            new_state_q_values = self.target_model(new_states)
            a = self.get_action_indices(new_state_actions)
            new_state_values = tf.gather_nd(new_state_q_values, a)
        else:
            new_state_values = tf.reduce_max(self.target_model(new_states), axis=1)
        new_state_values *= tf.cast(~dones, tf.float32)
        target_values = tf.identity(q_states)
        target_value_update = new_state_values * self.gamma ** self.n_steps + tf.cast(
            rewards, tf.float32
        )
        indices = self.get_action_indices(actions)
        state_action_values = tf.gather_nd(target_values, indices)
        tf.tensor_scatter_nd_update(target_values, indices, target_value_update)
        if self.buffer.priorities:
            squared_loss = (state_action_values - target_value_update) ** 2
            priorities = (
                self.buffer.current_weights * squared_loss + self.buffer.priority_bias
            )
            self.buffer.update_priorities(priorities)
        return target_values

And I get `model.fit()` outside the method which results in a severe performance degradation ~= 20 frames per seconds.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Here's the colab [notebook](https://colab.research.google.com/drive/1PNSURh-Hhd2CtQ5vn82lG0rsJjYkYPqR?usp=sharing) for the original version(numpy, no `tf.function`)

Here's the less performant / worse version colab [notebook](https://colab.research.google.com/drive/1qSzJSepVSYyA3dRpdpgHo2cRWeOg2nSJ?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46125,Unhandled Rejecting Error ~ Implicit Shape Can't Be a Fractional Number,"Hello everyone, sos

I am following an online tutorial on how to run gesture recognition using react and tensor flow. However, I am always seeing this error whenever I play around with the webcam in chrome.

Here is my github for what I am working on btw. And here is the tutorial video I'm watching. I got stuck right around minute 10

https://github.com/riccrdo5/help

https://youtu.be/f7uBsb-0sGQ

Ty and happy holidays

![ErrorVSC](https://user-images.githubusercontent.com/67179440/103488631-02733c00-4dc3-11eb-81d1-70c75b9f9ffa.png)
![ErrorWeb](https://user-images.githubusercontent.com/67179440/103488632-043cff80-4dc3-11eb-8446-0a80b0258b26.jpg)
"
46124,Installation/Import issue. Help required.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  -  OS Name:                   Microsoft Windows 10 Home Single Language
  -  OS Version:                10.0.18363 N/A Build 18363
  -  OS Manufacturer:           Microsoft Corporation
  -  OS Configuration:          Standalone Workstation
  -  OS Build Type:             Multiprocessor Free

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.x
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Installed without any issue. But when trying to import, ImportError occurs.**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 1. ```pip install tensorflow --verbose --user```
 2. Then opened a .py file with first and only line as : ```import tensorflow```
 3. Saved the .py file and opened a new cmd window in the same directory.
 4. Executed ```python name_of_the_file.py```


**Any other info / logs**

Traceback (most recent call last):
  File ""C:\Users\ddsme\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <
module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\ddsme\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\ddsme\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\ddsme\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <
module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ddsme\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <
module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime."
46123,Small last batches for TripletSemiHardLoss,"I am using Tensorflow 2.3

I am using the following code for tfrecord

    dataset = tf.data.TFRecordDataset(filenames=filenames)
    dataset = dataset.shuffle(buffer_size=100000)
    # dataset handling API
    dataset = dataset.cache()
    dataset = dataset.map(parse_examples,
                          num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(50)

Say if I have 101 images, then I will have 3 batches where the last batch contains only 1 element.

This will give error if I use TripletSemiHardLoss as it requires both positive and negative examples. I would like to request a convenient way to ensure 
a. all batches have 50 elements or 
b. to ensure each batch has both positive and negative examples

"
46121,always got the time out when try to download upb file. ,"Firstly try to build TP  2.3 branch in windows 10 using the following command, always got the time out when download upb file. 
can I manully download the file or disable the download operation? or modify the connect_time_out option?
bazel build //tensorflow/tools/pip_package:build_pip_package
ERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to C:/users/86178/_bazel_86178/26orbg4z/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: connect timed out
INFO: Elapsed time: 69.014s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)"
46120,Bazel out of memory at checking cache actions when building for ARMv8,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

### System information
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.4.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: None
- GPU model and memory: None

### Describe the problem

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Hello.

I am tried to build tensorflow v2.4.0 from source for aarch64 / ARMv8 devices. I did this on a quad core A53 single board with 1GB RAM.

The process was always terminated at the time when about 3000 targets were done. At that moment, Bazel showed **Checking cache actions**, stuck for a while and raise java out of memory error.

I thought that the RAM and swap might be not enough, so I picked a 8GB USB drive and use it as swap. But the building process still crashed.

So, how much memory does it need when building tensorflow? Should I at least double the swap size, or any step is wrong?

Thank you.
"
46119,Will TensorFlow update new optimizer?: Adabelief,"**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes

I have read this article and it is quite interesting? At this time, I use Adam optimizer in TensorFlow but the current accuracy is just 88%. The Adabelief gave me an idea about getting the model accuracy to be over 90% which inspires me. Thus I am wondering will TensorFlow will update this? Thank you

https://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e
https://arxiv.org/pdf/1412.6980.pdf
"
46118,[Build] Building on Windows 10 fails with Bad address error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Windows 10_
- TensorFlow installed from (source or binary): _source_
- TensorFlow version: _v2.3.0_
- Python version: _CPython 3.8_
- Installed using virtualenv? pip? conda?: _pip_
- Bazel version (if compiling from source): _3.7.0_
- CUDA version: _10.1_
- cuDNN version: _7.6.5_
- GPU model and memory: _GeForce GTX 760_


**Describe the problem**
Failed with error _Bad address_


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```console
PS C:\tensorflow> python.exe .\configure.py
You have bazel 3.7.0 installed.
Please specify the location of python. [Default is C:\Python\CPython38\python.exe]:

Found possible Python library paths:
  C:\Python\CPython38\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Python\CPython38\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Could not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
        'local/cuda/extras/CUPTI/include'
of:
        'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1

Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.6.5

Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/tools/cuda/lib/x64
    C:/tools/cuda/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.8

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
PS C:\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Python/CPython38/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Python/CPython38/python.exe --action_env PYTHON_LIB_PATH=C:/Python/CPython38/lib/site-packages --python_path=C:/Python/CPython38/python.exe --config=xla --action_env TF_CUDA_VERSION=10.1 --action_env TF_CUDNN_VERSION=7.6.5 --action_env TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.8 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (412 packages loaded, 26538 targets configured).
INFO: Found 1 target...
ERROR: C:/users/{User}/_bazel_{User}/xv6zejqw/external/libjpeg_turbo/BUILD.bazel:394:8: Executing genrule @libjpeg_turbo//:simd_win_x86_64_assemble failed (Exit 126): bash.exe failed: error executing command
  cd C:/users/{User}/_bazel_{User}/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PYTHON_BIN_PATH=C:/Python/CPython38/python.exe
    SET PYTHON_LIB_PATH=C:/Python/CPython38/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.8
    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.5
    SET TF_NEED_CUDA=1
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jchuff-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcphuff-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctflt-sse.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctfst-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctflt-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctfst-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctred-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquantf-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jsimdcpu.obj; do
  bazel-out/x64_windows-opt/bin/external/nasm/nasm.exe -fwin64 -DWIN64 -D__x86_64__    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jdct.inc)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jdct.inc)/../../win/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.obj}.asm)
done
Execution platform: @local_execution_config_platform//:platform
/usr/bin/bash: line 1: bazel-out/x64_windows-opt/bin/external/nasm/nasm.exe: Bad address
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: C:/users/{User}/_bazel_{User}/xv6zejqw/external/libjpeg_turbo/BUILD.bazel:347:11 Executing genrule @libjpeg_turbo//:simd_win_x86_64_assemble failed (Exit 126): bash.exe failed: error executing command
  cd C:/users/{User}/_bazel_{User}/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PYTHON_BIN_PATH=C:/Python/CPython38/python.exe
    SET PYTHON_LIB_PATH=C:/Python/CPython38/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.8
    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7.6.5
    SET TF_NEED_CUDA=1
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jccolor-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcgray-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jchuff-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcphuff-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jcsample-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jdsample-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctflt-sse.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctfst-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctflt-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctfst-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctint-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jidctred-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquantf-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-avx2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jquanti-sse2.obj bazel-out/x64_windows-opt/bin/external/libjpeg_turbo/simd/x86_64/jsimdcpu.obj; do
  bazel-out/x64_windows-opt/bin/external/nasm/nasm.exe -fwin64 -DWIN64 -D__x86_64__    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jdct.inc)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jdct.inc)/../../win/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.obj}.asm)
done
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 81.600s, Critical Path: 16.20s
INFO: 322 processes: 41 internal, 281 local.
FAILED: Build did NOT complete successfully
```
"
46116,How to convert this numpy to tf.function compatible code?,"I'm trying to convert numpy to tensorflow equivalent code to be compatible with `tf.function` ...

Given have a `(32, 6)` numpy array `target_values` that looks like this:

    array([[-0.01656106,  0.04762066,  0.05735449, -0.0284767 , -0.02237438,
            -0.00042562],
           [-0.01420249,  0.0477839 ,  0.0563598 , -0.02971786, -0.02367548,
             0.00001262],
           [-0.01695916,  0.04826669,  0.05893629, -0.03067053, -0.02261235,
             0.00345904],
           [-0.01953977,  0.04540274,  0.05829531, -0.02759781, -0.02390759,
            -0.00487727],
           [-0.01708016,  0.04894669,  0.0606699 , -0.02576046, -0.02461138,
            -0.00068538],
           [-0.01604217,  0.04770135,  0.05761468, -0.02858265, -0.02624938,
            -0.00084356],
           [-0.01527106,  0.04699571,  0.05959677, -0.02956396, -0.02510098,
            -0.00223234],
           [-0.01448676,  0.04620824,  0.05775366, -0.03008122, -0.02655901,
            -0.00159649],
           [-0.0172577 ,  0.04814827,  0.05807308, -0.02916523, -0.02367857,
            -0.00100602],
           [-0.01690523,  0.0484785 ,  0.05807881, -0.02960616, -0.02560546,
            -0.00065042],
           [-0.0166171 ,  0.0488232 ,  0.05776291, -0.03231864, -0.02132723,
            -0.00033605],
           [-0.01541627,  0.04840397,  0.0580376 , -0.02927143, -0.02461101,
             0.00121263],
           [-0.01685588,  0.047661  ,  0.05873172, -0.02989979, -0.02574112,
            -0.00126612],
           [-0.01333553,  0.05043796,  0.05915743, -0.02990219, -0.02657976,
            -0.0007656 ],
           [-0.01531163,  0.04781894,  0.05637252, -0.02968849, -0.02225551,
            -0.00151382],
           [-0.01357749,  0.04807179,  0.05955081, -0.02748637, -0.02498721,
            -0.00040934],
           [-0.01606943,  0.04768877,  0.05455931, -0.03136749, -0.02475093,
             0.00245846],
           [-0.01609829,  0.04687681,  0.05982678, -0.02886578, -0.02608151,
             0.00015348],
           [-0.01503662,  0.04740106,  0.05958583, -0.03141545, -0.02522127,
            -0.00063602],
           [-0.01697148,  0.04910276,  0.05744712, -0.02858391, -0.02481578,
            -0.00072039],
           [-0.01503395,  0.04843756,  0.05773868, -0.03061879, -0.02586869,
            -0.00025573],
           [-0.0152991 ,  0.04847359,  0.05739099, -0.0299796 , -0.02552593,
            -0.00334571],
           [-0.01324895,  0.04529134,  0.05534273, -0.03109139, -0.02304241,
            -0.00143186],
           [-0.01280282,  0.05004944,  0.05856398, -0.0314032 , -0.02394999,
            -0.00030306],
           [-0.01677033,  0.04876196,  0.05794405, -0.02888608, -0.02658239,
            -0.00015171],
           [-0.01572544,  0.04779808,  0.05939355, -0.03048976, -0.02896303,
            -0.00090334],
           [-0.01542805,  0.04709881,  0.05839922, -0.02894112, -0.02240603,
            -0.00188624],
           [-0.01493233,  0.0476524 ,  0.0581631 , -0.0297201 , -0.02485022,
            -0.00087418],
           [-0.01804641,  0.04739738,  0.06070606, -0.02981704, -0.02543145,
            -0.00115484],
           [-0.01518638,  0.04843838,  0.05744548, -0.02980216, -0.02420005,
             0.00036349],
           [-0.01442349,  0.04673778,  0.05804737, -0.03062913, -0.02476445,
            -0.00066772],
           [-0.01598305,  0.04622466,  0.0588723 , -0.03096713, -0.02364032,
            -0.00005574]])

Given another `(32,)` array of indices `actions` with values being in range(5) inclusive:

    array([0, 2, 5, 5, 1, 1, 3, 4, 0, 5, 4, 3, 4, 5, 1, 0, 3, 0, 0, 2, 2, 2,
           0, 1, 4, 1, 4, 4, 0, 4, 1, 0])

I'm expecting this result:

    array([-0.01656106,  0.0563598 ,  0.00345904, -0.00487727,  0.04894669,
            0.04770135, -0.02956396, -0.02655901, -0.0172577 , -0.00065042,
           -0.02132723, -0.02927143, -0.02574112, -0.0007656 ,  0.04781894,
           -0.01357749, -0.03136749, -0.01609829, -0.01503662,  0.05744712,
            0.05773868,  0.05739099, -0.01324895,  0.05004944, -0.02658239,
            0.04779808, -0.02240603, -0.02485022, -0.01804641, -0.02420005,
            0.04673778, -0.01598305], dtype=float32)

For `self.batch_size == 32`, I'm able to achieve what I need in numpy using:

    state_action_values = target_values[np.arange(self.batch_size), actions]

For `target_value_update` being another `(32,)` array of new values, I will need to assign the new values to this slice using:

    target_values[np.arange(self.batch_size), actions] = target_value_update

However in tensorflow under `tf.function`, this is not possible and I get the following error:

    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])

So I try:

    target_values = tf.Variable(target_values)
    state_action_values = tf.gather(target_values, actions, axis=1)

However here's the value of `state_action_values` which should be `(32,)` not `(32, 32)`

    Tensor(""GatherV2:0"", shape=(32, 32), dtype=float32)


  [1]: https://www.tensorflow.org/api_docs/python/tf/function
"
46114,C++ Gradient for ReadVariableOp,"**System information**
- TensorFlow version (you are using): `master`
- Are you willing to contribute it: Yes



**Describe the feature and the current behavior/state.**

Currently, `ReadVariableOp`'s gradient is implemented in Python, making using resource variables for training from the C++ API or other APIs based on it impossible.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Anyone using resource variables from the C++ API or APIs based on it.
"
46113,RPi Zero(ARMv6l) _FusedConv2D <no registered kernels>,"**System information**
- Using Keras MobileNet model
- Platform: RPi Zero (armv6l)
- Tested on binaries: v1.14.0 from piwheel / v2.4.0 from [https://github.com/lhelontra/tensorflow-on-arm/releases/tag/v2.4.0](url)
- Python version: 3.7

**Describe the current behavior**
Error when attempting model.predict on RPi zero using keras.apps MobileNet model
Attempting _FusedConv2D calculation via a non existing kernel

**Describe the expected behavior**
Attempted unfused conv2D + relu since armv6l doesnt support it
Successful forward pass calculation of model

**Standalone code to reproduce the issue**
https://pastebin.com/4syM43S1

Reproducable when applied on RPi Zero 
libs:
- atlasbase
- hdf5
- openjp2
- tiff5

pip: 
- tf
- keras
- h5py
- pillow

**Other info / logs**
Trace (TF v2.4.0): https://pastebin.com/QNFsJjkJ

Having same issue as open:  https://github.com/tensorflow/tensorflow/issues/24732

Could a possible work around for this issue be to enter the TFLite path?
Or can FusedConv2D be disabled before building binary?

Thank you.

**EDIT**: TFLite path was great 
https://github.com/google-coral/pycoral/issues/7
https://github.com/google-coral/edgetpu/issues/229
https://drive.google.com/file/d/1mW8QGmhfk4kRYXqUTEH1ckPUNIsl7S3d/view
https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"
46112,ImageDataGenerator().batch_index starts from zero even though manually setting it,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: Cuda 11.0 update 1 + cudnn 8.0.5
- GPU model and memory: RTX 3080 10GB


**Describe the current behavior**
I define my datagenerator by `ImageDataGenerator(preprocessing_function=preprocess_input)` then I define the generator:        

```
 generator = datagen.flow_from_directory(
            directory=data_dir,
            target_size=(112, 112),
            color_mode=""rgb"",
            class_mode=None,
            batch_size=batch_size,
            shuffle=False,
        )
```
I want to **start at a particular batch_index**, so I set the `generator.batch_index = 500`. When I loop for the first time over the generator, it will ignore this set batch_index and start at zero:  
```
for batch, label in generator:
    pred = model.predict(batch, verbose=0)
    features.append(scipy.sparse.csr_matrix(pred))
```

But if just loop over it once with 
```
for batch, label in generator:
    break
```
 and then set the `generator.batch_index = 500`, and then loop over it again with the previous code it will start at the correct index.

I don't know if this is a feature or not. But if it is, it's not very practical to loop over it once before being able to adjust the batch_index.

**Describe the expected behavior**
I should be able to change the batch_index before looping over the generator. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

"
46111,Resource exhausted:  OOM when allocating tensor with TF 2.4.0,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: The code is from PyImageSearch.com, it runs fine on TF 2.3.1
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro, version 20H2, OS build: 19042.685
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: -
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.4.0 (sorry, 2.4 is uninstalled now, I cannot run that command)
-   **Python version**: 3.7.5
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: 11.0.2_451.48 / 11.0
-   **GPU model and memory**: NVIDIA GeForce RTX 2060, 6 GB RAM
-   **Exact command to reproduce**: run my python script with my data, after around 940-960 epochs it runs out of memory


### Describe the problem
There is a bug around TensorFlow 2.4.0: the exact same code runs fine with the following components in python:
tensorflow==2.3.1
tensorflow-gpu==2.3.1
cuda_10.1.105_418.96_win10.exe
cudnn-10.1-windows10-x64-v7.6.5.32.zip

But give a ""Resource exhausted:  OOM when allocating tensor"" error after 900 and some epochs with TF 2.4.0:
tensorflow==2.4.0
tensorflow-gpu==2.4.0
cuda_11.0.2_451.48_win10.exe
cudnn-11.0-windows-x64-v8.0.5.39.zip

In the logs, I found this:
/job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu

Can it be, that TF is switching to cpu instead of gpu at one point?

### Source code / logs
Logs of two runs:
[bug.txt](https://github.com/tensorflow/tensorflow/files/5760806/bug.txt)
[bug2.txt](https://github.com/tensorflow/tensorflow/files/5760807/bug2.txt)
"
46110,what CUDA 11 and cuDNN i have to install to use TF 2.4?,"Hello i just want know what CUDA 11 and cuDNN i have to install to use TF 2.4
I'ts not clear in doc
![image](https://user-images.githubusercontent.com/73416709/103465568-4885bc80-4d3d-11eb-8438-4ccf6f940133.png)
Thanks

"
46109,tensorflow lite model hangs in tflite.run in android emulator,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
     Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device**:
     Android emulator.
-   **TensorFlow installed from (source or binary)**:
    Installed from source
-   **TensorFlow version (use command below)**:
    Tensorflow version used to convert tf model to tflite: 2.5.0
    Tensorflow version and tf op version:  2.3.0

-   **Python version**: 3.8.0
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
46108,Illegal instruction (core dumped) using tensorflow==2.4.0 with AVX instruction when importing,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.9
- CUDA/cuDNN version: 11.0/8.0.5
- GPU model and memory: GTX 2080Ti


**Describe the current behavior**

Attempting to import tensorflow produces an ""Illegal instruction(core dumped)"" error.

**Describe the expected behavior**

Import tensorflow without error.

**Standalone code to reproduce the issue**
```
conda create -n test-tf2.4 python=3.7
conda activate test-tf2.4
pip install tensorflow==2.4.0
```
The following command exits with an ""Illegal instruction"" error:
```
python -c ""import tensorflow as tf""
```

**Other info / logs** 
This will be not an issue with the last development version of 2.5.0 (tf-nightly 2.5.0.dev20210102) from tf-nightly, installed with:
```
pip install tf-nightly
```

MOST IMPORTANTLY, My machine has AVX instructions. So this is not the same as the previous ""old cpu"", while it is THE SAME AS the [#44668 ](https://github.com/tensorflow/tensorflow/issues/44668)problem  which your team has NOT SOLVED."
46107,TFlite interpreter stuck after call to invoke(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): example code + adaptations
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: tf-nightly 2.5.0.dev20210102
- Python version: 3.6.9
- GPU model and memory: no GPU

**Describe the current behavior**
TFlite interpreter invoke() method never returns

**Describe the expected behavior**
invoke() method returns with results

Hi,
i've successfully trained an object detection model using tensorflow 2.3.0 with SSD ResNet50 V1 FPN pretrained model.
By loading the model as a saved_model format i can run it as expected on my PC and on raspberry pi 4 as well.

Because performance on the rasperryPI was not good i looked into TFLite optimiziations and got some issues with the conversion from saved_model to .tflite format. The model was trained with the keep_aspect_ratio_resizer imge_resizer that required dynamic input size support from tflite. This is when i've moved to tf-nightly as suggested in a discussion found here on github. After the update the conversion was successfull, this is the code:
```
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(""./1024PRE_v2/saved_model"") # path to the SavedModel directory
converter.experimental_new_converter = True
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

# Save the model
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

In the object detection code i added a call to resize_tensor_input as suggested but the execution never gets past the invoke() method:

 ```
  interpreter = interpreter_wrapper.Interpreter(model_path=model_file)
  interpreter.resize_tensor_input(0, [1, 1024, 1024, 3])
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  category_index = label_map_util.create_category_index_from_labelmap(label_file, use_display_name=True)

  # Load a 1024x1024 image
  img = cv2.imread(file_name)

  # add N dim
  input_data = np.expand_dims(img, axis=0)

  interpreter.set_tensor(input_details[0]['index'], input_data)

  interpreter.invoke()
  
 # Code never reaches this point
 ```

Thanks in advance for the support"
46106, Request: offer image-enlarging library that uses AI (TensorFlow),"**System information**
- TensorFlow version (you are using):
tensorflow-lite-2.3.0
- Are you willing to contribute it (Yes/No):
Not sure how. 

**Describe the feature and the current behavior/state.**
https://issuetracker.google.com/issues/176311044

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Everyone who wishes to enlarge/enhance images.

**Any Other info.**
I know we already have a sample of it, but it's very restricted (only allows from 50x50 input images) and isn't comfortable at all to use on new projects."
46104,"Hello guys , does any one know how to solve this issue ? Thanks in advance!","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46103,ProfilerNotRunningError: Cannot stop profiling. No profiler is running. raised using profiler in tf.function,"Hello. I'm trying to use profiling in my custom tf.function, but I'm getting the following error calling `tf.summary.trace_export` 

`tensorflow.python.eager.profiler.ProfilerNotRunningError: Cannot stop profiling. No profiler is running.`

I followed the tutorial in [https://www.tensorflow.org/tensorboard/graphs](https://www.tensorflow.org/tensorboard/graphs) and I realized the error is raised in the tutorial too in line `tf.summary.trace_export(
      name=""my_func_trace"",
      step=0,
      profiler_outdir=logdir)`. I ran it using google colab in remote enviroment so I guess is a bug in version 2.4

Does anybody another way to achieve it?
"
46102,ImportError: cannot import name 'function_pb2' from 'tensorflow.core.framework' (unknown location),"**System information**
- ubuntu20.04
- build tf from source
- TensorFlow version (use command below):2.5
- Python version:3.8.5
- Bazel version (if compiling from source):3.7.2
- GCC/Compiler version (if compiling from source):gcc-9
- CUDA/cuDNN version:11.1

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory
"
46101,Building TF 2.3.1 with CUDA 11.1 fails with undefined protobuf symbol,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 20.04 LTS
- TensorFlow installed from: Trying to build from source
- TensorFlow version: 2.3.1
- Python version: 3.8 with pip
- Bazel version: 3.1.0 (also tried with 3.7.2)
- GCC/Compiler version: 7.5.0 (also tried with 8.x, 9.x)
- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8 / TensorRT 7
- GPU model and memory: RTX 3070 8GB
<br>

**Describe the problem**
I've tried to build Tensorflow 2.3.1 with CUDA 11.1 to support RTX 3070,
but it always fails because of the problem related to undefined symbol in protobuf.
(I need to use 2.3.1 instead of 2.4.0 or 2.5.0 because of the compatibility with other framework now.)
<br>

**Provide the exact sequence of commands / steps that you executed before running into the problem**
These are my commands which were used to build TF.
```
$ cd ~/utils/tensorflow
$ bazel clean
$ ./configure
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.8/dist-packages
  /usr/lib/python3.8/dist-packages
  /home/sjlee/utils/tvm/python
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Found CUDA 11.1 in:
    /usr/local/cuda-11.1/targets/x86_64-linux/lib
    /usr/local/cuda-11.1/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 8.6]: 8.6,7.5,6.1


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package
```
<br>

**Any other info / logs**
Build fails with these error messages. It says there are undefined symbol named ""fixed_address_empty_string"".
I've referenced other issues related to my case, but nothing helped for me.
I think sources for protobuf were already compiled before tensorflow kernels, so I cannot understand why the error occurs.
```
ERROR: /home/sjlee/utils/tensorflow/tensorflow/python/BUILD:2823:29: Executing genrule //tensorflow/python:clustering_ops_pygenrule failed (Exit 127): bash failed: error executing command /bin/bash bazel-out/k8-opt/bin/tensorflow/python/clustering_ops_pygenrule.genrule_script.sh
bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/python/gen_clustering_ops_py_wrappers_cc: symbol lookup error: bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/python/gen_clustering_ops_py_wrappers_cc: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2049.987s, Critical Path: 94.77s
INFO: 26147 processes: 11918 internal, 14229 local.
FAILED: Build did NOT complete successfully
```
<br>

These info messages also can be found during build process.
Could these ones be related to the error above?
```
INFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/graph_debug_info.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/tensor_bundle.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/saver.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/trackable_object_graph.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/transport_options.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h:
bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/protobuf/device_filters.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
INFO: From ProtoCompile tensorflow/core/framework/node_def.pb.h [for host]:
bazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.
```
<br>

Thank you :)"
46100,"Exported TFLite Inference Graph (Step 1), but can't Convert to TFLite (Step 2)","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): from Anaconda command using pip install
- TensorFlow version (or github SHA if from source): 2.4.0

**What I did**

Based on:
[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md)

### Step 1 : Export TFLite inference graph
**Step 1.1 : Input via Anaconda command**
```
cd D:\Documents\AI\Github Trials\models-master\research

set PYTHONPATH=`pwd`:`pwd`/slim
python setup.py build
python setup.py install
pip install tf-slim
protoc object_detection/protos/*.proto --python_out=.
pip install .
pip install tf-models-official

python object_detection/export_tflite_graph_tf2.py  --pipeline_config_path object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config  --trained_checkpoint_dir object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint  --output_directory object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/tflite_inference_graph
```
**Step 1.2 : Output from Anaconda Command**
Emphasis on the following part of the Output
```
WARNING:tensorflow:AutoGraph could not transform <bound method SSDModule.inference_fn of <tensorflow.python.eager.functi
on.TfMethodTarget object at 0x00000236A95C61C8>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
```
Full version of the **same** Output
```
Requirement already satisfied: tf-models-official in d:\anaconda3\envs\tf\lib\site-packages (2.3.0)
Requirement already satisfied: kaggle>=1.3.9 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.5.10
)
Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in d:\anaconda3\envs\tf\lib\site-packages (from tf-m
odels-official) (0.5.0)
Requirement already satisfied: py-cpuinfo>=3.3.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (7.
0.0)
Requirement already satisfied: opencv-python-headless in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official
) (4.4.0.46)
Requirement already satisfied: pandas>=0.22.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.1.4
)
Requirement already satisfied: six in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.15.0)
Requirement already satisfied: tensorflow-hub>=0.6.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official)
 (0.10.0)
Requirement already satisfied: scipy>=0.19.1 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.4.1)

Requirement already satisfied: tensorflow-addons in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (0.
12.0)
Requirement already satisfied: pyyaml in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (5.3.1)
Requirement already satisfied: psutil>=5.4.3 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (5.7.2)

Requirement already satisfied: matplotlib in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (3.3.3)
Requirement already satisfied: google-cloud-bigquery>=0.31.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-o
fficial) (2.6.1)
Requirement already satisfied: tensorflow-datasets in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (
4.1.0)
Requirement already satisfied: dataclasses in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (0.6)
Requirement already satisfied: tf-slim>=1.1.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.1.0
)
Requirement already satisfied: google-api-python-client>=1.6.7 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models
-official) (1.12.8)
Requirement already satisfied: Cython in d:\anaconda3\envs\tf\lib\site-packages\cython-3.0a6-py3.7-win-amd64.egg (from t
f-models-official) (3.0a6)
Requirement already satisfied: numpy>=1.15.4 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (1.19.4
)
Requirement already satisfied: Pillow in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (8.0.1)
Requirement already satisfied: gin-config in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (0.4.0)
Requirement already satisfied: tensorflow>=2.3.0 in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (2.
4.0)
Requirement already satisfied: sentencepiece in d:\anaconda3\envs\tf\lib\site-packages (from tf-models-official) (0.1.94
)
Requirement already satisfied: urllib3 in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-official
) (1.25.11)
Requirement already satisfied: python-slugify in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-o
fficial) (4.0.1)
Requirement already satisfied: requests in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-officia
l) (2.24.0)
Requirement already satisfied: python-dateutil in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-
official) (2.8.1)
Requirement already satisfied: certifi in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-official
) (2020.6.20)
Requirement already satisfied: tqdm in d:\anaconda3\envs\tf\lib\site-packages (from kaggle>=1.3.9->tf-models-official) (
4.54.0)
Requirement already satisfied: dm-tree~=0.1.1 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-model-optimizat
ion>=0.2.1->tf-models-official) (0.1.5)
Requirement already satisfied: pytz>=2017.2 in d:\anaconda3\envs\tf\lib\site-packages (from pandas>=0.22.0->tf-models-of
ficial) (2020.1)
Requirement already satisfied: protobuf>=3.8.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-hub>=0.6.0->tf
-models-official) (3.13.0)
Requirement already satisfied: typeguard>=2.7 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-addons->tf-mode
ls-official) (2.10.0)
Requirement already satisfied: cycler>=0.10 in d:\anaconda3\envs\tf\lib\site-packages (from matplotlib->tf-models-offici
al) (0.10.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\anaconda3\envs\tf\lib\site-packages (from
matplotlib->tf-models-official) (2.4.7)
Requirement already satisfied: kiwisolver>=1.0.1 in d:\anaconda3\envs\tf\lib\site-packages (from matplotlib->tf-models-o
fficial) (1.3.1)
Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.23.0 in d:\anaconda3\envs\tf\lib\site-packages (from g
oogle-cloud-bigquery>=0.31.0->tf-models-official) (1.24.1)
Requirement already satisfied: proto-plus>=1.10.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-cloud-bigquery>
=0.31.0->tf-models-official) (1.13.0)
Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in d:\anaconda3\envs\tf\lib\site-packages (from goo
gle-cloud-bigquery>=0.31.0->tf-models-official) (1.2.0)
Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in d:\anaconda3\envs\tf\lib\site-packages (from google-c
loud-bigquery>=0.31.0->tf-models-official) (1.5.0)
Requirement already satisfied: termcolor in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-models-
official) (1.1.0)
Requirement already satisfied: typing-extensions; python_version < ""3.8"" in d:\anaconda3\envs\tf\lib\site-packages (from
 tensorflow-datasets->tf-models-official) (3.7.4.3)
Requirement already satisfied: importlib-resources; python_version < ""3.9"" in d:\anaconda3\envs\tf\lib\site-packages (fr
om tensorflow-datasets->tf-models-official) (3.3.0)
Requirement already satisfied: dill in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-models-offic
ial) (0.3.3)
Requirement already satisfied: absl-py in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-models-of
ficial) (0.11.0)
Requirement already satisfied: attrs>=18.1.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-mod
els-official) (20.3.0)
Requirement already satisfied: future in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-models-off
icial) (0.18.2)
Requirement already satisfied: promise in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->tf-models-of
ficial) (2.3)
Requirement already satisfied: tensorflow-metadata in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow-datasets->
tf-models-official) (0.25.0)
Requirement already satisfied: uritemplate<4dev,>=3.0.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-api-pytho
n-client>=1.6.7->tf-models-official) (3.0.1)
Requirement already satisfied: google-auth-httplib2>=0.0.3 in d:\anaconda3\envs\tf\lib\site-packages (from google-api-py
thon-client>=1.6.7->tf-models-official) (0.0.4)
Requirement already satisfied: httplib2<1dev,>=0.15.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-api-python-
client>=1.6.7->tf-models-official) (0.18.1)
Requirement already satisfied: google-auth>=1.16.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-api-python-cli
ent>=1.6.7->tf-models-official) (1.23.0)
Requirement already satisfied: wheel~=0.35 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-models-
official) (0.35.1)
Requirement already satisfied: gast==0.3.3 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-models-
official) (0.3.3)
Requirement already satisfied: tensorboard~=2.4 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-mo
dels-official) (2.4.0)
Requirement already satisfied: google-pasta~=0.2 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-m
odels-official) (0.2.0)
Requirement already satisfied: astunparse~=1.6.3 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-m
odels-official) (1.6.3)
Requirement already satisfied: grpcio~=1.32.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-mode
ls-official) (1.32.0)
Requirement already satisfied: wrapt~=1.12.1 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-model
s-official) (1.12.1)
Requirement already satisfied: keras-preprocessing~=1.1.2 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.
3.0->tf-models-official) (1.1.2)
Requirement already satisfied: h5py~=2.10.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-models
-official) (2.10.0)
Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in d:\anaconda3\envs\tf\lib\site-packages (from ten
sorflow>=2.3.0->tf-models-official) (2.4.0)
Requirement already satisfied: flatbuffers~=1.12.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf
-models-official) (1.12)
Requirement already satisfied: opt-einsum~=3.3.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorflow>=2.3.0->tf-m
odels-official) (3.3.0)
Requirement already satisfied: text-unidecode>=1.3 in d:\anaconda3\envs\tf\lib\site-packages (from python-slugify->kaggl
e>=1.3.9->tf-models-official) (1.3)
Requirement already satisfied: chardet<4,>=3.0.2 in d:\anaconda3\envs\tf\lib\site-packages (from requests->kaggle>=1.3.9
->tf-models-official) (3.0.4)
Requirement already satisfied: idna<3,>=2.5 in d:\anaconda3\envs\tf\lib\site-packages (from requests->kaggle>=1.3.9->tf-
models-official) (2.10)
Requirement already satisfied: setuptools in d:\anaconda3\envs\tf\lib\site-packages (from protobuf>=3.8.0->tensorflow-hu
b>=0.6.0->tf-models-official) (50.3.1.post20201107)
Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in d:\anaconda3\envs\tf\lib\site-packages (from g
oogle-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.52.0)
Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= ""3.5"" in d:\anaconda3\envs\tf\lib\site-pack
ages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.1.0)
Requirement already satisfied: zipp>=0.4; python_version < ""3.8"" in d:\anaconda3\envs\tf\lib\site-packages (from importl
ib-resources; python_version < ""3.9""->tensorflow-datasets->tf-models-official) (3.4.0)
Requirement already satisfied: rsa<5,>=3.1.4; python_version >= ""3.5"" in d:\anaconda3\envs\tf\lib\site-packages (from go
ogle-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.6)
Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\anaconda3\envs\tf\lib\site-packages (from google-auth>=1.16.0
->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-auth>=1.16.
0->google-api-python-client>=1.6.7->tf-models-official) (4.1.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\anaconda3\envs\tf\lib\site-packages (from tensorboard
~=2.4->tensorflow>=2.3.0->tf-models-official) (1.6.0)
Requirement already satisfied: markdown>=2.6.8 in d:\anaconda3\envs\tf\lib\site-packages (from tensorboard~=2.4->tensorf
low>=2.3.0->tf-models-official) (3.3.2)
Requirement already satisfied: werkzeug>=0.11.15 in d:\anaconda3\envs\tf\lib\site-packages (from tensorboard~=2.4->tenso
rflow>=2.3.0->tf-models-official) (0.16.1)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\anaconda3\envs\tf\lib\site-packages (from tensorbo
ard~=2.4->tensorflow>=2.3.0->tf-models-official) (0.4.2)
Requirement already satisfied: cffi>=1.0.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-crc32c<2.0dev,>=1.0; p
ython_version >= ""3.5""->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.14.
3)
Requirement already satisfied: pyasn1>=0.1.3 in d:\anaconda3\envs\tf\lib\site-packages (from rsa<5,>=3.1.4; python_versi
on >= ""3.5""->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)
Requirement already satisfied: importlib-metadata; python_version < ""3.8"" in d:\anaconda3\envs\tf\lib\site-packages (fro
m markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->tf-models-official) (2.0.0)
Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\anaconda3\envs\tf\lib\site-packages (from google-auth-oaut
hlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->tf-models-official) (1.3.0)
Requirement already satisfied: pycparser in d:\anaconda3\envs\tf\lib\site-packages (from cffi>=1.0.0->google-crc32c<2.0d
ev,>=1.0; python_version >= ""3.5""->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-offic
ial) (2.20)
Requirement already satisfied: oauthlib>=3.0.0 in d:\anaconda3\envs\tf\lib\site-packages (from requests-oauthlib>=0.7.0-
>google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->tf-models-official) (3.1.0)

(tf) D:\Documents\AI\Github Trials\models-master\research>python object_detection/export_tflite_graph_tf2.py  --pipeline
_config_path object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config  --trained_checkpoint_dir object_det
ection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint  --output_directory object_detection/ssd_mobilenet_v2_320x320_co
co17_tpu-8/tflite_inference_graph
2021-01-01 15:35:13.672770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:02.206834: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_
devices not set
2021-01-01 15:36:02.245547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library nvcuda.dll
2021-01-01 15:36:02.586806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2021-01-01 15:36:02.607594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:03.530245: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublas64_11.dll
2021-01-01 15:36:03.532987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublasLt64_11.dll
2021-01-01 15:36:03.896998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cufft64_10.dll
2021-01-01 15:36:04.091753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library curand64_10.dll
2021-01-01 15:36:04.663251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusolver64_10.dll
2021-01-01 15:36:05.073078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusparse64_11.dll
2021-01-01 15:36:05.093508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudnn64_8.dll
2021-01-01 15:36:05.218116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-01 15:36:05.287330: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized wit
h oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:
 AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-01 15:36:05.334165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2021-01-01 15:36:05.339857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:05.342356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublas64_11.dll
2021-01-01 15:36:05.351041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublasLt64_11.dll
2021-01-01 15:36:05.353842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cufft64_10.dll
2021-01-01 15:36:05.356236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library curand64_10.dll
2021-01-01 15:36:05.358652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusolver64_10.dll
2021-01-01 15:36:05.362098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusparse64_11.dll
2021-01-01 15:36:05.370079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudnn64_8.dll
2021-01-01 15:36:05.373200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-01 15:36:13.128203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor
with strength 1 edge matrix:
2021-01-01 15:36:13.131097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-01 15:36:13.132404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-01 15:36:13.161953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus
 id: 0000:01:00.0, compute capability: 6.1)
2021-01-01 15:36:13.202675: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_
devices not set
WARNING:tensorflow:AutoGraph could not transform <bound method SSDModule.inference_fn of <tensorflow.python.eager.functi
on.TfMethodTarget object at 0x00000236A95C61C8>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:14.384240 10772 ag_logging.py:146] AutoGraph could not transform <bound method SSDModule.inference_fn of <te
nsorflow.python.eager.function.TfMethodTarget object at 0x00000236A95C61C8>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method FreezableBatchNorm.call of <object_detection.core.freezab
le_batch_norm.FreezableBatchNorm object at 0x00000236B5ACB608>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:14.768384 10772 ag_logging.py:146] AutoGraph could not transform <bound method FreezableBatchNorm.call of <o
bject_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x00000236B5ACB608>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method SSDKerasFeatureExtractor.call of <object_detection.models
.ssd_mobilenet_v2_keras_feature_extractor.SSDMobileNetV2KerasFeatureExtractor object at 0x00000236A2F5BE08>> and will ru
n it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:16.034504 10772 ag_logging.py:146] AutoGraph could not transform <bound method SSDKerasFeatureExtractor.call
 of <object_detection.models.ssd_mobilenet_v2_keras_feature_extractor.SSDMobileNetV2KerasFeatureExtractor object at 0x00
000236A2F5BE08>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method KerasMultiResolutionFeatureMaps.call of <object_detection
.models.feature_map_generators.KerasMultiResolutionFeatureMaps object at 0x00000236C7AAA248>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:16.354562 10772 ag_logging.py:146] AutoGraph could not transform <bound method KerasMultiResolutionFeatureMa
ps.call of <object_detection.models.feature_map_generators.KerasMultiResolutionFeatureMaps object at 0x00000236C7AAA248>
> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.339918 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.342931 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.347878 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.350869 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.357949 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0101 15:36:17.358946 10772 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0
WARNING:tensorflow:AutoGraph could not transform <bound method KerasBoxPredictor.call of <object_detection.predictors.co
nvolutional_keras_box_predictor.ConvolutionalBoxPredictor object at 0x00000236A9538508>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:17.366203 10772 ag_logging.py:146] AutoGraph could not transform <bound method KerasBoxPredictor.call of <ob
ject_detection.predictors.convolutional_keras_box_predictor.ConvolutionalBoxPredictor object at 0x00000236A9538508>> and
 will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <bound method KerasHead.call of <object_detection.predictors.heads.kera
s_box_head.ConvolutionalBoxHead object at 0x00000236A9165D48>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:17.366203 10772 ag_logging.py:146] AutoGraph could not transform <bound method KerasHead.call of <object_det
ection.predictors.heads.keras_box_head.ConvolutionalBoxHead object at 0x00000236A9165D48>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-01-01 15:36:17.494573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2021-01-01 15:36:17.501030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:17.507623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublas64_11.dll
2021-01-01 15:36:17.513051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublasLt64_11.dll
2021-01-01 15:36:17.516730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cufft64_10.dll
2021-01-01 15:36:17.519172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library curand64_10.dll
2021-01-01 15:36:17.521521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusolver64_10.dll
2021-01-01 15:36:17.523880: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusparse64_11.dll
2021-01-01 15:36:17.533205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudnn64_8.dll
2021-01-01 15:36:17.536259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-01 15:36:17.537924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor
with strength 1 edge matrix:
2021-01-01 15:36:17.540286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-01 15:36:17.541693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-01 15:36:17.543160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus
 id: 0000:01:00.0, compute capability: 6.1)
2021-01-01 15:36:17.553303: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_
devices not set
2021-01-01 15:36:17.618925: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimizatio
n passes are enabled (registered 0 passes)
WARNING:tensorflow:AutoGraph could not transform <function SSDModule._get_postprocess_fn.<locals>.dummy_post_processing
at 0x00000236C7D27798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:19.335623 10772 ag_logging.py:146] AutoGraph could not transform <function SSDModule._get_postprocess_fn.<lo
cals>.dummy_post_processing at 0x00000236C7D27798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-01-01 15:36:20.775396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2021-01-01 15:36:20.780405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:20.783048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublas64_11.dll
2021-01-01 15:36:20.785961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublasLt64_11.dll
2021-01-01 15:36:20.788581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cufft64_10.dll
2021-01-01 15:36:20.790940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library curand64_10.dll
2021-01-01 15:36:20.793317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusolver64_10.dll
2021-01-01 15:36:20.795926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusparse64_11.dll
2021-01-01 15:36:20.798359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudnn64_8.dll
2021-01-01 15:36:20.801092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-01 15:36:20.802806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor
with strength 1 edge matrix:
2021-01-01 15:36:20.811645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-01 15:36:20.814407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-01 15:36:20.816910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus
 id: 0000:01:00.0, compute capability: 6.1)
2021-01-01 15:36:20.821653: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_
devices not set
WARNING:tensorflow:AutoGraph could not transform <function SSDModule._get_postprocess_fn.<locals>.dummy_post_processing
at 0x000002377C7D3EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:20.924464 10772 ag_logging.py:146] AutoGraph could not transform <function SSDModule._get_postprocess_fn.<lo
cals>.dummy_post_processing at 0x000002377C7D3EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x00000
2377E245E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:21.259392 10772 ag_logging.py:146] AutoGraph could not transform <function canonicalize_signatures.<locals>.
signature_wrapper at 0x000002377E245E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMeta
Arch object at 0x00000236A956D4C8>, because it is not built.
W0101 15:36:21.489312 10772 save_impl.py:78] Skipping full serialization of Keras layer <object_detection.meta_architect
ures.ssd_meta_arch.SSDMetaArch object at 0x00000236A956D4C8>, because it is not built.
2021-01-01 15:36:33.415476: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this
may change in the future, so consider avoiding using them.
2021-01-01 15:36:56.497517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2021-01-01 15:36:56.502998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudart64_110.dll
2021-01-01 15:36:56.506094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublas64_11.dll
2021-01-01 15:36:56.508562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cublasLt64_11.dll
2021-01-01 15:36:56.511047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cufft64_10.dll
2021-01-01 15:36:56.514148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library curand64_10.dll
2021-01-01 15:36:56.522609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusolver64_10.dll
2021-01-01 15:36:56.525053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cusparse64_11.dll
2021-01-01 15:36:56.527458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic
library cudnn64_8.dll
2021-01-01 15:36:56.530278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-01-01 15:36:56.533155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor
with strength 1 edge matrix:
2021-01-01 15:36:56.541155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-01-01 15:36:56.543258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-01-01 15:36:56.544827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 2982 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus
 id: 0000:01:00.0, compute capability: 6.1)
2021-01-01 15:36:56.550138: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_
devices not set
WARNING:tensorflow:AutoGraph could not transform <function SSDModule._get_postprocess_fn.<locals>.dummy_post_processing
at 0x0000023794F6BAF8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:36:56.624512 10772 ag_logging.py:146] AutoGraph could not transform <function SSDModule._get_postprocess_fn.<lo
cals>.dummy_post_processing at 0x0000023794F6BAF8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236B8372708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.904431 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236B8372708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236A2F54DC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.924087 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236A2F54DC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236B8372D38> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.934363 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236B8372D38> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236B83728B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.950443 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236B83728B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.964281 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.974267 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:08.989209 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC891F8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.003207 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC891F8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.014103 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.024310 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236B8372C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.039472 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236B8372C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236A2D0B4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.054223 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236A2D0B4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236A2D04F78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.064049 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236A2D04F78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89A68> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.080380 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89A68> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89438> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.094172 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89438> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D98318> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.104253 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D98318> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89F78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.119428 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89F78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.129079 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC27EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.139221 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC27EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC273A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.154466 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC273A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D984C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.164408 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D984C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D988B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.178624 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D988B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D98D38> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.193627 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D98D38> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277D0D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.206599 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277D0D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D98948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.218703 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D98948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D98B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.229380 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D98B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236B83729D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.249172 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236B83729D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.259261 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277D4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.274536 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277D4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277D168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.284113 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277D168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277D5E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.299229 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277D5E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.313076 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277D948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.329065 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277D948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277DB88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.342951 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277DB88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC278B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.354170 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC278B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC274C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.364410 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC274C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C393A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.374466 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C393A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C39168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.384218 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C39168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C39C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.394142 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C39C18> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D98558> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.414140 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D98558> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C398B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.424285 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C398B8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C39438> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.444151 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C39438> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D981F8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.454444 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D981F8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277DF78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.464429 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277DF78> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C4C39678> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.480985 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C4C39678> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7649678> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.494034 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7649678> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C277DE58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.504319 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C277DE58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7649EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.514178 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7649EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7649B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.524438 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7649B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7649DC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.544341 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7649DC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236BDC89E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.554081 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236BDC89E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C0D985E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.564515 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C0D985E8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7C83E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.604518 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7C83E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CA7288> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.614604 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CA7288> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CA7798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.624274 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CA7798> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CCD048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.644068 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CCD048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CCD3A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.654093 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CCD3A8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CCDA68> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.664325 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CCDA68> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CFE168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.681007 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CFE168> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7CFE9D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:09.684165 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7CFE9D8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7D27E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.666164 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7D27E58> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7D27B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.689193 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7D27B88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377A9EF948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.704203 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377A9EF948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377A9EFDC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.714448 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377A9EFDC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377AA1B708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.734195 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377AA1B708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377AA1BDC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.757413 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377AA1BDC8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x00000236C7D27AF8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.779386 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x00000236C7D27AF8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377A9EF4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.795366 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377A9EF4C8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377A9EFB88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.809165 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377A9EFB88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377AA1B288> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.827819 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377AA1B288> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377AA1B948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.844805 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377AA1B948> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function Layer._handle_weight_regularization.<locals>._loss_for_variab
le at 0x000002377AA1BEE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:10.860762 10772 ag_logging.py:146] AutoGraph could not transform <function Layer._handle_weight_regularizati
on.<locals>._loss_for_variable at 0x000002377AA1BEE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VER
BOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0101 15:37:12.150229 10772 save.py:241] Found untraced functions such as BoxPredictor_layer_call_and_return_conditional
_losses, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_and_return_conditional_losses,
BoxPredictor_layer_call_and_return_conditional_losses while saving (showing 5 of 125). These functions will not be direc
tly callable after loading.
W0101 15:37:13.090939 10772 save.py:241] Found untraced functions such as BoxPredictor_layer_call_and_return_conditional
_losses, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_fn, BoxPredictor_layer_call_and_return_conditional_losses,
BoxPredictor_layer_call_and_return_conditional_losses while saving (showing 5 of 125). These functions will not be direc
tly callable after loading.
INFO:tensorflow:Assets written to: object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/tflite_inference_graph\saved_m
odel\assets
I0101 15:37:27.996973 10772 builder_impl.py:775] Assets written to: object_detection/ssd_mobilenet_v2_320x320_coco17_tpu
-8/tflite_inference_graph\saved_model\assets

```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

### Step 2 : Convert to TFLite (via Spyder)

in D:\Documents\AI\Github Trials\models-master\research\object_detection\ssd_mobilenet_v2_320x320_coco17_tpu-8\step2_convert_to_tflite.py
```
import tensorflow as tf

saved_model_dir = 'D:\Documents\AI\Github Trials\models-master\research\object_detection\ssd_mobilenet_v2_320x320_coco17_tpu-8\tflite_inference_graph\saved_model'

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```
**Failure details**

Step 2 above could not find {saved_model.pbtxt|saved_model.pb} in the folder.
Below is the full Traceback:
```
Traceback (most recent call last):

  File ""D:\Documents\AI\Github Trials\models-master\research\object_detection\ssd_mobilenet_v2_320x320_coco17_tpu-8\step2_convert_to_tflite.py"", line 6, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

  File ""D:\Anaconda3\envs\tf\lib\site-packages\tensorflow\lite\python\lite.py"", line 1069, in from_saved_model
    saved_model = _load(saved_model_dir, tags)

  File ""D:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\saved_model\load.py"", line 859, in load
    return load_internal(export_dir, tags, options)[""root""]

  File ""D:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\saved_model\load.py"", line 871, in load_internal
    loader_impl.parse_saved_model_with_debug_info(export_dir))

  File ""D:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 56, in parse_saved_model_with_debug_info
    saved_model = _parse_saved_model(export_dir)

  File ""D:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 114, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))

esearch\object_detection\ssd_mobilenet_v2_320x320_coco17_tpu-8	flite_inference_graph\saved_model/{saved_model.pbtxt|saved_model.pb}
```

Any help is **greatly** appreciated!"
46098,quant transpose convolution layer is not supported in nnapi acceleration,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Not related
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
  gtihub master branch, but this problem exist on v2.3.0 v2.2.0 etc.

**Describe the current behavior**
Quant transpose convolution layer not supported in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2228
The layer version of INT8 transpose convolution layer had a version of 2 which failed the above version checking. See below:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc#L261

i.e. All deep learning model (esp. segmentation) using quant transpose convolution will not get nnapi acceleration on  all Android phone when they are using tensorflow lite to do the inferencing instead of Qualcomm SNPE DL engine to do the inference.

Related to https://github.com/tensorflow/tensorflow/issues/46084"
46096,Bert Preprocess Model not working on windows 10,"I have the same issue described here [error-with-using-bert-model-from-tensorflow](https://stackoverflow.com/questions/65298391/error-with-using-bert-model-from-tensorflow)

I get this exception when i try to use the bert preprocessor on windows 10

`Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface`

**Stack trace**
```
File ""C:\work\vpython\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\function.py"", line 555, in call
    outputs = execute.execute(
  File ""C:\work\vpython\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface
	 [[{{node prediction/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_tokenizer/StatefulPartitionedCall/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]] [Op:__inference_train_function_52076]
Function call stack:
train_function
```"
46094,cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory,"hello,

I'm trying to run inference on a tensorrt converted graph (tf-trt) and received the error in the title.
I've followed instructions for ""TF-TRT 2.0 Workflow With A SavedModel"" at https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html.
but i'm afraid i have not been able to reach the part of the code the performs the inference, I get the OOM error a after loading the saved model , during preparation of the input data. 

I'm running tensorflow 2.0.0 (eager execution). on ubuntu 16.04, cuda 10.0, geforce gtx 1050 ( 4gb ram )

here is my conversion code 
from tensorflow.python.compiler.tensorrt import trt_convert as trt
ConvParams = trt.DEFAULT_TRT_CONVERSION_PARAMS
ConvParams = ConvParams._replace(max_workspace_size_bytes=1*1024*1024*1024)
ConvParams = ConvParams._replace(precision_mode=""FP32"")
ConvParams = ConvParams._replace(max_batch_size=1)
converter = trt.TrtGraphConverterV2(input_saved_model_dir=args.InputToConvert, conversion_params=ConvParams)

TensorRT_graph = converter.convert()

print('convert completed, building')
def my_input_fn():
  import numpy as np
  Inp1 = np.random.normal(size=(1,250, 250, 3)).astype(np.uint8)
  yield [Inp1]
converter.build(input_fn=my_input_fn)

print('build completed, saveing results to {}'.format(args.Output))
converter.save(args.Output)


the conversion process generates warnings but succeeds. 

**in another program/script/""execution session"" I perform the following:**
load the saved model using the following code

saved_model_loaded = tf.saved_model.load('my saved model directory name',tags=[tf.compat.v1.saved_model.tag_constants.SERVING])
    graph_func = saved_model_loaded.signatures[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
    frozen_func = convert_variables_to_constants_v2(graph_func)
    print('created concreate function')


try to load an image for inference 

ReadFile = tf.io.read_file('/home/omerbrandis/ffrobotics/AppleMask/mask_rcnn/test_images/img3095_RECT2_2.jpg')
print('after read file')
DecodedImg = tf.io.decode_jpeg(ReadFile)
print('after decode ')
image_expanded = tf.expand_dims(DecodedImg, axis=0)
print('read file')


**at runtime the code does not reach ""read file"" , only ""after decode"" is written.**

if i comment out the first section of the program that deals with reading the graph , the part that reads the file works without any problem.

as stated before, i have 4GB of ram on my gpu , and the conversion process should have limited the graphs usage to 1gb thus the OOM error is surprising. ( the image is only 210X210 pixels).

please advise
Omer.

 
"
46093,When can we expect Tensorflow builds with Cuda 11.1 or Cuda 11.2?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Tensorflow 2.4 and 2.5 is only built with cuda 11.0.  Can there please be an updated build, which is built with Cuda 11.1 or Cuda 11.2. 

**Will this change the current api? How?**

Tensorflow will point at cuda 11.1/2 files instead of cuda 11.0.

**Who will benefit with this feature?**

At least everyone with RTX 3000 series graphics card, since only cuda 11.1+ officially supports compute capability 8.6.

**Any Other info.**
"
46092,Wrong output dimension calculation of StrideSlice operation on TensorFlow Lite.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both pip and source build
- TensorFlow version (use command below): `v2.4.0-rc4-71-g582c8d236cb 2.4.0`
- Python version: `3.6.8`
- Bazel version (if compiling from source): `3.7.1`
- GCC/Compiler version (if compiling from source): `8.3.1`
- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8
- GPU model and memory: RTX3090 24GB

**Describe the current behavior**
The `StridedSlice` operation on TensorFlow Lite calculates output dimension incorrectly.

I'm developing some custom operations for both TensorFlow and TensorFlow Lite.
While I debugging my custom operation, inference fails only on TensorFlow Lite.
I found that `StridedSlice` operation of TensorFlow Lite calculates output dimension incorrectly.
I added some `printf` function to [`tflite::ops::builtin::strided_slice::Eval`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L185):
```c++
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  StridedSliceContext op_context(context, node);

  printf(""**** [STRIDED_SLICE] Input ["");
  for (int i = 0; i < NumDimensions(op_context.input); i++) {
    printf(""%d, "", SizeOfDimension(op_context.input, i));
  }
  printf(""]\n**** [STRIDED_SLICE] Begin: "");
  for (int i = 0; i < SizeOfDimension(op_context.begin, 0); i++) {
    printf(""%d, "", GetTensorData<int32_t>(op_context.begin)[i]);
  }
  printf(""\n**** [STRIDED_SLICE] End: "");
  for (int i = 0; i < SizeOfDimension(op_context.end, 0); i++) {
    printf(""%d, "", GetTensorData<int32_t>(op_context.end)[i]);
  }
  printf(""\n**** [STRIDED_SLICE] Strides: "");
  for (int i = 0; i < SizeOfDimension(op_context.strides, 0); i++) {
    printf(""%d, "", GetTensorData<int32_t>(op_context.strides)[i]);
  }
  printf(""\n"");

  if (IsDynamicTensor(op_context.output)) {
    TF_LITE_ENSURE_OK(context, ResizeOutputTensor(context, &op_context));
  }
  StridedSliceParams op_params = BuildStridedSliceParams(&op_context);

  printf(""**** [STRIDED_SLICE] Output ["");
  for (int i = 0; i < NumDimensions(op_context.output); i++) {
    printf(""%d, "", SizeOfDimension(op_context.output, i));
  }
  printf(""]\n"");
```

And I got following logs:
```
**** [STRIDED_SLICE] Input [1656, 8, 32, ]
**** [STRIDED_SLICE] Begin: 0, 0, 0, 
**** [STRIDED_SLICE] End: 893, 
**** [STRIDED_SLICE] Strides: 1, 1, 1, 
**** [STRIDED_SLICE] Output [893, 8, 0, ]
```

**Describe the expected behavior**
After strided slicing, output dimension should be `[893, 8, 32, ]` in above example.


**Standalone code to reproduce the issue**
I wrote a smallest [reproducible code](https://github.com/kukdh1/tflite_buf_report/blob/master/test_strided_slice.py).
The `SimpleLayer` keras layer creates read-only `Tensor` with size of [128, 8, 32].
After ten `model.call`s, I saved the model into tflite model file.

On TensorFlow (python, no code modification -- installed by pip `tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl`) it calculates output dimension correctly.
```
TEST: Input: [128, 8, 32] SiliceTo: 53 Output: (53, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 38 Output: (38, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 64 Output: (64, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 100 Output: (100, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 106 Output: (106, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 90 Output: (90, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 126 Output: (126, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 122 Output: (122, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 62 Output: (62, 8, 32)
TEST: Input: [128, 8, 32] SiliceTo: 99 Output: (99, 8, 32)
```

I checked the stored model with [netron](https://netron.app/):
![image](https://user-images.githubusercontent.com/6904750/103407030-52be8400-4ba0-11eb-8344-2e2d1b85e0d2.png)
And it shows that `StridedSlice` has input `Tensor` of size `[128, 8, 32]` and output `Tensor` has size `[Unknown, 8, 32]`.

To test on TensorFlow Lite, I wrote [simple inference code](https://github.com/kukdh1/tflite_buf_report/blob/master/strided_slice.cc) with TensorFlow Lite C++ API.
It randomly selects input `Tensor` (slice index) and print the output dimension.

I used the source code (582c8d23 == v2.4.0) with above `StrideSlice` `printf` modification.
I built `libtensorflowlite.so` with following command:
`bazel build --verbose_failures -c opt --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_nccl_support=true --define=build_with_mkl=false --config=monolithic //tensorflow/lite:libtensorflowlite.so`
And the content of `.tf_configure.bazelrc` is:
```
build --action_env PYTHON_BIN_PATH=""/home/kukdh1/.virtualenvs/tf_develop/bin/python3""
build --action_env PYTHON_LIB_PATH=""/home/kukdh1/.virtualenvs/tf_develop/lib/python3.6/site-packages""
build --python_path=""/home/kukdh1/.virtualenvs/tf_develop/bin/python3""
build --config=xla
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-11.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.5,8.6""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-11.1/lib64:""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```

I built test program with `mkdir build && cmake -DTF_SOURCE_DIR=<source directory> ..` ([CMakeLists.txt](https://github.com/kukdh1/tflite_buf_report/blob/master/CMakeLists.txt))
When I ran the test program, I got following result:
```
$ CUDA_VISIBLE_DEVICES=-1 ./tflite-strided-slice strided_slice.tflite 
2020-12-31 19:47:18.088431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
**** [STRIDED_SLICE] Input [128, 8, 32, ]
**** [STRIDED_SLICE] Begin: 0, 0, 0, 
**** [STRIDED_SLICE] End: 103, 
**** [STRIDED_SLICE] Strides: 1, 1, 1, 
**** [STRIDED_SLICE] Output [103, 0, 32, ]
Input: 103
Output: [103 0 32 ]
########## PRINT INTERPRETER STATE BEGIN ##########
Interpreter has 6 tensors and 2 nodes
Inputs: 0
Outputs: 5

Tensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB) 
Tensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32
Tensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  103 0 32

Node   0 Operator Builtin Code  83 PACK
  Inputs: 0
  Outputs: 4
Node   1 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 1 2 4 3
  Outputs: 5
########### PRINT INTERPRETER STATE END ###########
```

The output Tensor has size of `[103, 0, 32]` not `[103, 8, 32]`.

I think this is a bug on [`tflite::ops::builtin::strided_slice::ResizeOutputTensor`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L101).
Please let me know if I did something wrong.

**Other info / logs**
You can find all the code snippets I used to reproduce the problem at [here](https://github.com/kukdh1/tflite_buf_report).
(Download all dependencies by running `./download_dependencies.sh` at `tensorflow/lite/tools/make`)
You can find tflite model too.

P.S. The dimension error is quite random. Sometimes it calculates correctly. Sometimes innermost (axis=2) dimension becomes zero. Sometimes middle (axis=1) dimension becoms zero, Sometimes both (axis=1, axis=2) dimensions become zero.

P.S. 2. I double checked with unmodified tensorflow source downloaded with:
`wget https://github.com/tensorflow/tensorflow/archive/v2.4.0.tar.gz`
on Ubuntu 18.04.5 (different machine) with GCC 7.5.0, bazel 3.7.2 and Python 3.6.9.
Same command used to build `libtensorflowlite.so` and `.tf_configure.bazelrc` is:
```
build --action_env PYTHON_BIN_PATH=""/home/kukdh1/.virtualenvs/tensorflow/bin/python3""
build --action_env PYTHON_LIB_PATH=""/home/kukdh1/.virtualenvs/tensorflow/lib/python3.6/site-packages""
build --python_path=""/home/kukdh1/.virtualenvs/tensorflow/bin/python3""
build --config=xla
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```

The output on Ubuntu machine was (one example):
```
Input: 41
Output: [41 0 0 ]
########## PRINT INTERPRETER STATE BEGIN ##########
Interpreter has 6 tensors and 2 nodes
Inputs: 0
Outputs: 5

Tensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB)
Tensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32
Tensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  41 0 0

Node   0 Operator Builtin Code  83 PACK
  Inputs: 0
  Outputs: 4
Node   1 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 1 2 4 3
  Outputs: 5
########### PRINT INTERPRETER STATE END ###########
```
"
46091,Missing functions from TF 2.4 vs TF 2.3 (tf.keras.backend),"**System information**
- Used code from another repository : https://github.com/LeonLok/Deep-SORT-YOLOv4
- Windows 10 Pro
- pip install via anaconda
- TensorFlow v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python 3.6
- CUDA 11.0 cuDNN 8.04
- GeForce RTX 3060 Ti 8192 MB

I am trying to run the repository stated above on my 3060Ti and made some modifications to run it on tf 2.4. I noticed that alot of functions from tf.keras.backend for tf 2.4 (https://www.tensorflow.org/api_docs/python/tf/keras/backend) are missing compared to tf 2.3 (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/backend) in the API docs . I tried to look for alternatives and managed to replace some of them. However, I am not able to find some of the functions mentioned below. Are they obsolete or are they any other alternatives.

K.arange
K.concatenate 
K.dtype
K.min
K,gather 
K.sum
K.switch 
K.max
K.control_flow_ops.while_loop
K.learning_phase

**Code**
https://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo.py
https://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo4/model.py
"
46090,call() missing 1 required positional argument,"Tensorflow Version 2.4 pip wheel
Python Version: 3.6
OS: official docker
As the title, when I try to export a multi-input model, it failed with an error message ""call() missing 1 required positional argument: 'y'"".Thanks for your help.
Here comes the code:
```python
import tensorflow as tf
import numpy as np
class CustomModel(keras.Model):
    def __init__(self, hidden_units):
        super(CustomModel, self).__init__()
        self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]

    def call(self, x, y):
        x = tf.concat((x, y), axis=1)
        for layer in self.dense_layers:
            x = layer(x)
        return {'x': x}


model = CustomModel([16, 16, 10])
# Build the model by calling it
input_arr = tf.random.uniform((1, 5))
input_arr_2 = tf.random.uniform((1, 10))
outputs = model(input_arr, input_arr_2)
print(outputs)
model.save(""my_model"")
```"
46089,Big SparseTensor constant and Datasets either turn into a large graph or a slow execution.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Testing/Sid
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.243/7.6.5
- GPU model and memory: Quadro T2000 

I am loading a sparse matrix via `numpy.load` with shape `[16777216  5416537]` and 335548396 values (this is the TFIDF matrix of DrQA). I want to use a dataset and use the matrix at some point of my computation. I have tried to different ways, one is simply super slow and the other one hits the 2GB limit (I assume the optimizer is kicking in and unfolding the constant matrix and overpopulation the graph, but I am not sure and my test with the config for Grappler did not work)

The slow version (I have 32GB and 16 cores available) is something like this:
```python
indices, values, dense_shape = load_parts_of_sparse_matrix_as_np_arrays()

def f(x, indices, values, dense_shape):
  m = tf.SparseTensor(indices, values, dense_shape=dense_shape)
  return do_something(x, m)

ds = load_ds()

ds = tf.data.Dataset.zip((
  ds,
  tf.data.Dataset.from_tensors(indices).repeat(),
  tf.data.Dataset.from_tensors(values).repeat(),
  tf.data.Dataset.from_tensors(dense_shape).repeat(),
))

ds.map(f)
```
I guess memory is being copied at every iteration.


The following modification hits the 2gb limit
```python
indices, values, dense_shape = load_parts_of_sparse_matrix_as_np_arrays()

m = tf.SparseTensor(indices, values, dense_shape=dense_shape)

def f(x):
  return do_something(x, m)

ds = load_ds()

ds.map(f)
```
This is a bit harder for me to understand. My only guess is that TF wraps the function passed to map by a `tf.function` then it trace the function, detects the constant and expands it overpopulating the graph somehow. I turned constant_folding off without success.

So far the only way I get good results is by disabling eager execution and use a placeholder for the parts of the sparse tensor, in any other way I hit the 2GB result. Sadly, I can't control this when using datasets.

I tried asking for help in Stack Overflow first, but I have received no answer yet.

"
46088,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.0
- Python version: 3.7.0
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: GeForce GTX 1060 6GB



**Describe the problem**
Whenever I build or use tensorflow it gives me an Import error, I tried searching the web for solutions but I can't find any, I even tried reinstalling tensorflow just to be greeted  with the same error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf

https://user-images.githubusercontent.com/49057329/103400118-ec753980-4b7e-11eb-81f1-29a2e2ab7da2.mp4



**Any other info / logs**
```
Traceback (most recent call last):
  File ""C:\Users\LEE_FAMILY\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\LEE_FAMILY\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\LEE_FAMILY\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\LEE_FAMILY\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\LEE_FAMILY\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
```

"
46086,dlerror: cusparse64_11.dll not found. Cannot detetct GPU.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64bit
- TensorFlow version: tensorflow-gpu=2.4.0
- Python version: 3.8.5
- Installed using pip 
- CUDA/cuDNN version: CUDA 11.1 | cuDNN v8.0.4 for CUDA 11.1
- GPU model and memory: NVIDIA GeForce GTX 1050

**I try to use NVIDIA GeForce GTX 1050 by installing all the ablove mnetioned specs. Cannot find the cusparse64_11.dll. The GPU is not being detected for running deep learning models. 


Could not load dynamic library 'cusparse64_11.dll'; **dlerror: cusparse64_11.dll not found**

Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices... **

![image](https://user-images.githubusercontent.com/48597846/103396793-2c2a2a00-4b5b-11eb-9499-8b66759fba37.png)

"
46084,u-net always not able to run in a single delegate,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  No, just use  prebuilt  android_aarch64_benchmark_model
- OS Platform and Distribution: 
  ested on Android 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  Qualcomm dragon 855
- TensorFlow installed from (source or binary):
  google tensorflow prebuilt android_aarch64_benchmark_model


**Describe the current behavior**
I tried many u-net model for semantic segmentation, none of those can be run on nnapi delegate fully. and most likely the transpose conv will be fallback on CPU. Though the underlying nnapi accel supports transport conv.


**Describe the expected behavior**
It should run the model fully on one single delegate without fallback to CPU which extremely slow. Or have some kinds of indication  why t is not  running fully in nnapi delegate and fallback to CPU. The TRANSPOSE_CONV and CONV2D are clearly supported by the underly nnapi acceleration. But it just fallback to CPU for unknown reason. 

**Standalone code to reproduce the issue**
Download android_aarch64_benchmark_model from https://www.tensorflow.org/lite/performance/measurement
adb push that into /data/local/tmp/
created one using unet with post quant.
Run 
/android_aarch64_benchmark_model  --graph=model.tflite  --use_nnapi=true  --nnapi_accelerator_name=qti-gpu --enable_op_profiling=true                                                               

Expected:
Explicitly applied NNAPI delegate, and the model graph will be completely executed by the delegate.

Current:
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
STARTING!
Log parameter values verbosely: [0]
Graph: [model.tflite]
Enable op profiling: [1]
Use NNAPI: [1]
NNAPI accelerator name: [qti-gpu]
NNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,qti-hta,nnapi-reference]
Loaded model model.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.
The input model file size (MB): 0.398212
Initialized session in 185.23ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=10 first=72646 curr=55109 min=43832 max=72646 avg=50987.8 std=7966

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=50 first=57407 curr=57830 min=39546 max=63652 avg=54400.3 std=5091

Inference timings in us: Init: 185230, First inference: 72646, Warmup (avg): 50987.8, Inference (avg): 54400.3
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=3.77344 overall=14.918
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	  184.511	  184.511	 50.938%	 50.938%	  3864.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	           95.737	  177.694	   88.858	 49.062%	100.000%	     0.000	        2	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	  184.511	  184.511	 50.938%	 50.938%	  3864.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	           95.737	  177.694	   88.858	 49.062%	100.000%	     0.000	        2	AllocateTensors/0

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	 ModifyGraphWithDelegate	        1	   184.511	    50.938%	    50.938%	  3864.000	        1
	         AllocateTensors	        1	   177.715	    49.062%	   100.000%	     0.000	        2

Timings (microseconds): count=1 curr=362226
Memory (bytes): count=0
2 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	            0.000	    7.375	    6.549	 12.047%	 12.047%	     0.000	        1	[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25
	                 CONV_2D	            6.554	   11.632	    6.679	 12.286%	 24.333%	     0.000	        1	[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10
	                 CONV_2D	           13.236	    7.296	    6.370	 11.717%	 36.050%	     0.000	        1	[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11
	                 CONV_2D	           19.609	    4.624	    5.985	 11.009%	 47.059%	     0.000	        1	[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12
	     TfLiteNnapiDelegate	           25.596	    3.368	    3.671	  6.753%	 53.812%	     0.000	        1	[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26
	          TRANSPOSE_CONV	           29.271	    3.924	    4.570	  8.407%	 62.218%	     0.000	        1	[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15
	                     ADD	           33.843	    0.238	    0.267	  0.490%	 62.709%	     0.000	        1	[unet/conv2d_transpose/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose/BiasAdd]:16
	     TfLiteNnapiDelegate	           34.111	    4.639	    4.690	  8.627%	 71.335%	     0.000	        1	[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27
	          TRANSPOSE_CONV	           38.804	    7.393	    8.092	 14.884%	 86.219%	     0.000	        1	[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20
	                     ADD	           46.899	    0.915	    1.073	  1.974%	 88.194%	     0.000	        1	[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21
	     TfLiteNnapiDelegate	           47.974	    5.968	    6.418	 11.806%	100.000%	     0.000	        1	[Identity]:28

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	          TRANSPOSE_CONV	           38.804	    7.393	    8.092	 14.884%	 14.884%	     0.000	        1	[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20
	                 CONV_2D	            6.554	   11.632	    6.679	 12.286%	 27.170%	     0.000	        1	[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10
	     TfLiteNnapiDelegate	            0.000	    7.375	    6.549	 12.047%	 39.217%	     0.000	        1	[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25
	     TfLiteNnapiDelegate	           47.974	    5.968	    6.418	 11.806%	 51.023%	     0.000	        1	[Identity]:28
	                 CONV_2D	           13.236	    7.296	    6.370	 11.717%	 62.740%	     0.000	        1	[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11
	                 CONV_2D	           19.609	    4.624	    5.985	 11.009%	 73.749%	     0.000	        1	[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12
	     TfLiteNnapiDelegate	           34.111	    4.639	    4.690	  8.627%	 82.376%	     0.000	        1	[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27
	          TRANSPOSE_CONV	           29.271	    3.924	    4.570	  8.407%	 90.783%	     0.000	        1	[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15
	     TfLiteNnapiDelegate	           25.596	    3.368	    3.671	  6.753%	 97.535%	     0.000	        1	[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26
	                     ADD	           46.899	    0.915	    1.073	  1.974%	 99.510%	     0.000	        1	[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21

Number of nodes executed: 11
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteNnapiDelegate	        4	    21.327	    39.233%	    39.233%	     0.000	        4
	                 CONV_2D	        3	    19.033	    35.013%	    74.246%	     0.000	        3
	          TRANSPOSE_CONV	        2	    12.661	    23.291%	    97.537%	     0.000	        2
	                     ADD	        2	     1.339	     2.463%	   100.000%	     0.000	        2

Timings (microseconds): count=50 first=57372 curr=57796 min=39517 max=63611 avg=54364.5 std=5088
Memory (bytes): count=0
11 nodes observed
"
46083,ESP32-EYE person_detection example will not build,"@tensorflow/micro

Ubuntu 20.04.1 LTS
GNU Make 4.2.1
ESP IDF v4.2 checked out tag and ran install.sh and source export.sh
Cloned esp32-camera into idf components

I tried several different TF commits. I ran git clean -f -x -d between TF versions.

**TF master at 49524d68892d13265ec33b0e1cf7422fd71e316c**
Making the ESP project works but running ""idf.py build"" gives:
```
CMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):
  Cannot find source file:

    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc
```
I edited the file tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/CMakeLists.txt   and added  ""../components/tfmicro/""  to the front of all the paths.

I can now run idf.py menuconfig to set (necessary) support for array rtc_gpio_desc.  
Now I run ""idf.py build"" and get:

```
 #error ""No camera module configured, please configure in menuconfig""
```
The camera module selection is not showing in the menuconfig system when running tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/idf.py menuconfig. (should I run it from somewhere else?) 

I manually added ""#define CONFIG_CAMERA_MODEL_ESP_EYE 1"" in /tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/components/tfmicro/tensorflow/lite/micro/examples/person_detection/esp/app_camera_esp.h. 

I then run idf.py build again.  This time I get a link error:
```
/home/felix/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/main/libmain.a(main_functions.cc.obj): in function `loop':
/home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/build/../components/tfmicro/tensorflow/lite/micro/examples/person_detection/main_functions.cc:101: undefined reference to `GetImage(tflite::ErrorReporter*, int, int, int, signed char*)'
```

In ""tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/components/tfmicro/tensorflow/lite/micro/examples/person_detection/esp/image_provider.cc""  Change the definitions of 
```
TfLiteStatus PerformCapture(tflite::ErrorReporter* error_reporter, int8_t* image_data) 
```
and 
```
TfLiteStatus GetImage(tflite::ErrorReporter* error_reporter, int image_width, int image_height, int channels, int8_t* image_data) 
```
The image_data pointer was wrongly typed as uint8_t*, changing to int8_t* fixes the undefined reference issue and the example now builds and runs! Hurray!  That was not easy.

**TF v2.4.0**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_int8_esp_project
```
tensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:378: tensorflow/lite/micro/tools/make/targets/esp_makefile.inc: No such file or directory
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/esp_makefile.inc'.  Stop.
```
I renamed esp32_makefile.inc to esp_makefile.inc and now generation works.

cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/

idf.py build fails with....
```
CMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):
  Cannot find source file:

    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc
```
Edit the file tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/CMakeLists.txt   add ../components/tfmicro/  to the front of all the paths
I can now run idf.py menuconfig to set (necessary) support for array rtc_gpio_desc.  
idf.py build now results in
```
fatal_exception  esp-idf/app_trace/libapp_trace.a  -lgcov  esp-idf/app_trace/libapp_trace.a  -lgcov  -lc  -lm && :
/home/felix/.espressif/tools/xtensa-esp32-elf/esp-2020r3-8.4.0/xtensa-esp32-elf/bin/../lib/gcc/xtensa-esp32-elf/8.4.0/../../../../xtensa-esp32-elf/bin/ld: esp-idf/esp32/libesp32.a(cpu_start.c.obj):(.literal.main_task+0x18): undefined reference to `app_main'
```

**TF v2.3.0**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_int8_esp_project
cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/
idf.py build fails with....
```
CMake Error at /home/felix/esp-idf/tools/cmake/component.cmake:468 (add_library):
  Cannot find source file:

    /home/felix/source/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection_int8/esp-idf/main/tensorflow/lite/micro/examples/person_detection_experimental/detection_responder.cc
``` 

Can anyone point to a combination of dependencies and TF that this example will actually build and run for esp32? I've managed to run the hello world but I'd like a more sophisticated example to work from."
46082,undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_,"when I run .tflite file in my Raspberry Pi 3B+ ,some errors happened.
Python version: 3.7.3
Tensorflow version: 1.13.1
GCC: 8.3.0
Description:    Raspbian GNU/Linux 10 (buster)
Release:        10
Codename:       buster
I have changed tf version but it did not worK. 
Please help!!
here is the error:
`Traceback (most recent call last):
  File ""/home/pi/project/test.py"", line 62, in <module>
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py"", line 154, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
  File ""/usr/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 696, in _load
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf
`
"
46081,Is there have API to automatically calculate #flops (e.g. like flop_counter in pytorch) in tensorflow2?,"This template is for miscellaneous issues not covered by the other issue categories.
***
Environment: python3.6+tensorflow2.1, 
Question:  Is there have API to automatically calculate #flops (e.g.  flop_counter) in tensorflow2?
Note: in Pytorch, this could be easily realized by calling API, e.g. thop.profile(),  flop_counterï¼Œ etc.
***

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
46080,[RNN] only LSTM not working,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 


**Command used to run the converter or code if youâ€™re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
[kaggle notebook](https://www.kaggle.com/davidweingut/notebook0189756df4)

```
# Copy and paste here the exact command
```

**The output from the converter invocation**
long strings are replaced by `[long string]`
<details>
<summary>massive log omitting weights(?)</summary>


```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198                                                  debug_info_str,
--> 199                                                  enable_mlir_converter)
    200       return model_str

/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     37       debug_info_str,
---> 38       enable_mlir_converter)
     39 

Exception: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at ""sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268"") at ""StatefulPartitionedCall@__inference_signature_wrapper_7157"") at ""StatefulPartitionedCall"")): We cannot duplicate the value since it's not constant.

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(callsite(unknown at ""sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268"") at ""StatefulPartitionedCall@__inference_signature_wrapper_7157"") at ""StatefulPartitionedCall"")): see current operation: %6 = ""tfl.unidirectional_sequence_lstm""(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>
<unknown>:0: error: Failed to duplicate values for the stateful op

<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<1x?xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<""[long string]""> : tensor<49x128xf32>} : () -> tensor<49x128xf32>
  %cst_0 = ""std.constant""() {value = dense<[0.0391180739, -0.035346359, -0.0317100026, -0.0274537932, -2.994480e-02, -0.0300172977, -0.0454840586, -0.0257392451, -0.030700814, -0.0278897285, -0.0355975218, -0.0340279937, -0.0132417707, -0.0220720414, -0.0170032419, 0.0107899494, -0.0109964302, 0.0110128662, -0.00513247494, 0.0186021626, 0.0219315775, 0.0235501211, 0.0213683285, 0.0133540835, 3.695680e-02, 0.0311833415, 0.0377812907, 0.0342479311, 0.0296398606, 0.0385234021, 0.0154196564, 0.0205324832, 0.0297776293, 0.00336526474, 0.0244599823, 0.00573094795, 0.0279730614, 0.0106867487, -0.011602954, 0.0209239982, -0.0380180553, 0.00730686448, -0.0307899173, -0.0395634323, -0.0275964253, -0.0415802076, -5.012860e-02, -0.0420668162, 0.021869421]> : tensor<49xf32>} : () -> tensor<49xf32>
  %cst_1 = ""std.constant""() {value = dense<49> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_2 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_3 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_4 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_5 = ""std.constant""() {value = dense<512> : tensor<i32>} : () -> tensor<i32>
  %cst_6 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_7 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_8 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_9 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_10 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_11 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_12 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_13 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_14 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_15 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_16 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_17 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_18 = ""std.constant""() {value = dense<""[long string]""> : tensor<49x512xf32>} : () -> tensor<49x512xf32>
  %cst_19 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_20 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value} : () -> none
  %0 = ""tfl.cast""(%arg0) : (tensor<1x?xf32>) -> tensor<1x?xi32>
  %1 = ""tfl.gather""(%cst, %0) {axis = 0 : i32} : (tensor<49x128xf32>, tensor<1x?xi32>) -> tensor<1x?x128xf32>
  %2 = ""tfl.shape""(%1) : (tensor<1x?x128xf32>) -> tensor<3xi32>
  %3 = ""tfl.strided_slice""(%2, %cst_19, %cst_20, %cst_20) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %4 = ""tfl.pack""(%3, %cst_5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %5 = ""tfl.fill""(%4, %cst_4) : (tensor<2xi32>, tensor<f32>) -> tensor<?x512xf32>
  %6 = ""tfl.unidirectional_sequence_lstm""(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>
  %7 = ""tfl.shape""(%6) : (tensor<1x?x512xf32>) -> tensor<3xi32>
  %8 = ""tfl.gather""(%7, %cst_3) {axis = 0 : i32} : (tensor<3xi32>, tensor<2xi32>) -> tensor<2xi32>
  %9 = ""tfl.reduce_prod""(%8, %cst_19) {keep_dims = false} : (tensor<2xi32>, tensor<1xi32>) -> tensor<i32>
  %10 = ""tfl.concatenation""(%8, %cst_1) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<2xi32>, tensor<1xi32>) -> tensor<3xi32>
  %11 = ""tfl.gather""(%7, %cst_2) {axis = 0 : i32} : (tensor<3xi32>, tensor<1xi32>) -> tensor<1xi32>
  %12 = ""tfl.reduce_prod""(%11, %cst_19) {keep_dims = false} : (tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %13 = ""tfl.pack""(%9, %12) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %14 = ""tfl.reshape""(%6, %13) : (tensor<1x?x512xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %15 = ""tfl.fully_connected""(%14, %cst_18, %cst_21) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x?xf32>, tensor<49x512xf32>, none) -> tensor<?x49xf32>
  %16 = ""tfl.reshape""(%15, %10) : (tensor<?x49xf32>, tensor<3xi32>) -> tensor<?x?x?xf32>
  %17 = ""tfl.add""(%16, %cst_0) {fused_activation_function = ""NONE""} : (tensor<?x?x?xf32>, tensor<49xf32>) -> tensor<?x?x49xf32>
  ""std.return""(%17) : (tensor<?x?x49xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""embedding_1_input""]}, result0 = {tf_saved_model.index_path = [""dense_1""]}, sym_name = ""serving_default"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_embedding_1_input:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<1x?xf32>) -> tensor<?x?x49xf32>} : () -> ()


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
<ipython-input-20-6750f334da79> in <module>
      1 converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
----> 2 tflite_model = converter.convert()
      3 
      4 with open('chopin.tflite', 'wb') as f:
      5   f.write(tflite_model)

/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    708     return super(TFLiteSavedModelConverterV2,
    709                  self).convert(meta_graph.graph_def, input_tensors,
--> 710                                output_tensors)
    711 
    712 

/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)
    631         input_tensors=input_tensors,
    632         output_tensors=output_tensors,
--> 633         **converter_kwargs)
    634 
    635     calibrate_and_quantize, flags = quant_mode.quantizer_flags(

/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    572       input_data.SerializeToString(),
    573       debug_info_str=debug_info_str,
--> 574       enable_mlir_converter=enable_mlir_converter)
    575   return data
    576 

/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at ""sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268"") at ""StatefulPartitionedCall@__inference_signature_wrapper_7157"") at ""StatefulPartitionedCall"")): We cannot duplicate the value since it's not constant.

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(callsite(unknown at ""sequential_1/lstm_1/PartitionedCall@__inference__wrapped_model_4268"") at ""StatefulPartitionedCall@__inference_signature_wrapper_7157"") at ""StatefulPartitionedCall"")): see current operation: %6 = ""tfl.unidirectional_sequence_lstm""(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>
<unknown>:0: error: Failed to duplicate values for the stateful op

<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<1x?xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<""[long string]""> : tensor<49x128xf32>} : () -> tensor<49x128xf32>
  %cst_0 = ""std.constant""() {value = dense<[0.0391180739, -0.035346359, -0.0317100026, -0.0274537932, -2.994480e-02, -0.0300172977, -0.0454840586, -0.0257392451, -0.030700814, -0.0278897285, -0.0355975218, -0.0340279937, -0.0132417707, -0.0220720414, -0.0170032419, 0.0107899494, -0.0109964302, 0.0110128662, -0.00513247494, 0.0186021626, 0.0219315775, 0.0235501211, 0.0213683285, 0.0133540835, 3.695680e-02, 0.0311833415, 0.0377812907, 0.0342479311, 0.0296398606, 0.0385234021, 0.0154196564, 0.0205324832, 0.0297776293, 0.00336526474, 0.0244599823, 0.00573094795, 0.0279730614, 0.0106867487, -0.011602954, 0.0209239982, -0.0380180553, 0.00730686448, -0.0307899173, -0.0395634323, -0.0275964253, -0.0415802076, -5.012860e-02, -0.0420668162, 0.021869421]> : tensor<49xf32>} : () -> tensor<49xf32>
  %cst_1 = ""std.constant""() {value = dense<49> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_2 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_3 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_4 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_5 = ""std.constant""() {value = dense<512> : tensor<i32>} : () -> tensor<i32>
  %cst_6 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_7 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_8 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_9 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x512xf32>} : () -> tensor<512x512xf32>
  %cst_10 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_11 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_12 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_13 = ""std.constant""() {value = dense<""[long string]""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_14 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_15 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_16 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_17 = ""std.constant""() {value = dense<""[long string]""> : tensor<512x128xf32>} : () -> tensor<512x128xf32>
  %cst_18 = ""std.constant""() {value = dense<""[long string]""> : tensor<49x512xf32>} : () -> tensor<49x512xf32>
%cst_19 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_20 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value} : () -> none
  %0 = ""tfl.cast""(%arg0) : (tensor<1x?xf32>) -> tensor<1x?xi32>
  %1 = ""tfl.gather""(%cst, %0) {axis = 0 : i32} : (tensor<49x128xf32>, tensor<1x?xi32>) -> tensor<1x?x128xf32>
  %2 = ""tfl.shape""(%1) : (tensor<1x?x128xf32>) -> tensor<3xi32>
  %3 = ""tfl.strided_slice""(%2, %cst_19, %cst_20, %cst_20) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %4 = ""tfl.pack""(%3, %cst_5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %5 = ""tfl.fill""(%4, %cst_4) : (tensor<2xi32>, tensor<f32>) -> tensor<?x512xf32>
  %6 = ""tfl.unidirectional_sequence_lstm""(%1, %cst_14, %cst_15, %cst_16, %cst_17, %cst_6, %cst_7, %cst_8, %cst_9, %cst_21, %cst_21, %cst_21, %cst_10, %cst_11, %cst_12, %cst_13, %cst_21, %cst_21, %5, %5, %cst_21, %cst_21, %cst_21, %cst_21) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<1x?x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x128xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, tensor<512x512xf32>, none, none, none, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, none, none, tensor<?x512xf32>, tensor<?x512xf32>, none, none, none, none) -> tensor<1x?x512xf32>
  %7 = ""tfl.shape""(%6) : (tensor<1x?x512xf32>) -> tensor<3xi32>
  %8 = ""tfl.gather""(%7, %cst_3) {axis = 0 : i32} : (tensor<3xi32>, tensor<2xi32>) -> tensor<2xi32>
  %9 = ""tfl.reduce_prod""(%8, %cst_19) {keep_dims = false} : (tensor<2xi32>, tensor<1xi32>) -> tensor<i32>
  %10 = ""tfl.concatenation""(%8, %cst_1) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<2xi32>, tensor<1xi32>) -> tensor<3xi32>
  %11 = ""tfl.gather""(%7, %cst_2) {axis = 0 : i32} : (tensor<3xi32>, tensor<1xi32>) -> tensor<1xi32>
  %12 = ""tfl.reduce_prod""(%11, %cst_19) {keep_dims = false} : (tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %13 = ""tfl.pack""(%9, %12) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %14 = ""tfl.reshape""(%6, %13) : (tensor<1x?x512xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %15 = ""tfl.fully_connected""(%14, %cst_18, %cst_21) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x?xf32>, tensor<49x512xf32>, none) -> tensor<?x49xf32>
  %16 = ""tfl.reshape""(%15, %10) : (tensor<?x49xf32>, tensor<3xi32>) -> tensor<?x?x?xf32>
  %17 = ""tfl.add""(%16, %cst_0) {fused_activation_function = ""NONE""} : (tensor<?x?x?xf32>, tensor<49xf32>) -> tensor<?x?x49xf32>
  ""std.return""(%17) : (tensor<?x?x49xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""embedding_1_input""]}, result0 = {tf_saved_model.index_path = [""dense_1""]}, sym_name = ""serving_default"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_embedding_1_input:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<1x?xf32>) -> tensor<?x?x49xf32>} : () -> ()
```

</details>

**Failure details**
Conversion of SimpleRNN and GRU works when not stateful, but LSTM does not.
"
46079,benchmark_model build failed,"**System information**
- OS Platform and Distribution ubuntu 18.04
- TensorFlow installed from (source or binary): git hub master branch 
- TensorFlow version: master in github, tried also v2.3.0 v2.2.0 
- Python version: 2
- Installed using virtualenv? pip? conda?:NA 
- Bazel version (if compiling from source): Bazelisk version: v1.7.4 Build label: 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the problem**
Build benchmark tool according to README.md  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md failed :


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel  build -c opt   --config=android_arm64   tensorflow/lite/tools/benchmark:benchmark_model  --verbose_failures

**Any other info / logs**
Repository rule git_repository defined at:
  /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
ERROR: /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' **does not contain a toolchain for cpu 'arm64-v8a'**
ERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed
INFO: Elapsed time: 0.175s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
"
46078,tensorflow.math.abs crashes Python on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Trivial example given.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
- TensorFlow installed from (source or binary): using pip
- TensorFlow version (use command below): v1.12.1-48227-g77eafffd690 2.5.0-dev20201230 tf-nightly==2.5.0.dev20201230
- Python version: 3.8.5
- CUDA/cuDNN version: CUDA 11.1, cuDNN 8.0.5
- GPU model and memory: RTX 3080 10 GB

EDIT:
I've also managed to reproduce the issue using the same software, but on a GTX 970, as well as on a GTX 1060.

**Describe the current behavior**
Any usage of tensorflow.math.abs crashes Python. Functions which depend on tensorflow.math.abs on the backend also crash, such as keras.losses.Huber and keras.losses.MeanAbsoluteError.

**Describe the expected behavior**
It should just compute absolute values without crashing.

**Standalone code to reproduce the issue**
This trivial example should yield the error:
```
import tensorflow
x = tensorflow.convert_to_tensor([0], dtype='float32')
y = tensorflow.math.abs(x)
```

**Other info / logs**
```
2020-12-30 12:47:20.566855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-30 12:47:20.597895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2020-12-30 12:47:20.598347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 12:47:20.626950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 12:47:20.627203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-30 12:47:20.631687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-30 12:47:20.635184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-30 12:47:20.639243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 12:47:20.642267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-30 12:47:20.643017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 12:47:20.643278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0
2020-12-30 12:47:20.644054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-30 12:47:20.644953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2020-12-30 12:47:20.645412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 12:47:20.645637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 12:47:20.645846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-30 12:47:20.646045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-30 12:47:20.646244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-30 12:47:20.646443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 12:47:20.646648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-30 12:47:20.646859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 12:47:20.647092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0
2020-12-30 12:47:21.201575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-30 12:47:21.201806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2020-12-30 12:47:21.201945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2020-12-30 12:47:21.202263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6703 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6)
Process finished with exit code -1073741571 (0xC00000FD)
```
"
46077,Assigning values to variables in Tensorflow graphs using the C API,"I am attempting to assign a value to a variable in a Tensorflow graph loaded using the C API. When I export a graph using Tensorflow 2.x it contains the following ops related to a particular variable:

```
variable_name type: VarHandleOp device:  number inputs: 0 number outputs: 1
Number inputs: 0
Number outputs: 1
 dims: 0 []

variable_name/Read/ReadVariableOp type: ReadVariableOp device:  number inputs: 1 number outputs: 1
Number inputs: 1
Number outputs: 1
 dims: 0 []
```
According to this video (https://www.youtube.com/watch?v=uaRO0AV6Tto) variables in TF 2.x now use VarHandleOps in order to mediate write access to them. The TF test suite for the C API is the only example I've found so far of using a VarHandleOp to access a variable but it is created from scratch using the Eager API (https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L409).

I would like to be able to load a graph and assign to a variable already contained within it, is this even possible with the Eager API? When I try using the older graph API I can fetch the VarHandleOp like so:
```
TF_Operation* op = TF_GraphOperationByName(graph, ""variable_name"");
```
But then I can't figure out how to run this op to return a TFE_TensorHandle (like here https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/c/eager/c_api_test.cc#L418), a type which doesn't even seem to exist in the graph API. TF_SessionRun()'s inputs and outputs deal in TF_Output and TFE_Execute expects a TFE_Op rather than the TF_Operation I have access to.

I think this may be a possible bug caused by the introduction of VarHandleOps in the Eager API but no corresponding mechanism in the graph API, not sure if this is the correct place to post it - please let me know if there's a better place.

Any tips or pointers gratefully received!"
46076,"Receiving the error: ""ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 7, 169)""","Hello,

I would like to apologize if this is posted in the wrong location.  If that is the case, any guidance as to where I can ask this question would be greatly appreciated.  I am new to coding and GitHub, but have found many answers to posts on this forum very helpful to me.  I am trying to setup a code to utilize the Keras model for deep learning with a 3d dataset, and am uncertain how to deal with the error I'm receiving (in the title).  I have found a similar error reported on this forum, but the response was that it would be fixed in TensorFlow v 2.1.  I am using v2.4.0, so I expect I'm probably doing something wrong here.  The code I am using is as follows:

```
# define the keras model
model = Sequential()
model.add(Dense(12, input_dim=7, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# separate the data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

# fit the keras model on the dataset
model.fit(X_train, y_train, epochs=150, batch_size=10)

# evaluate the keras model
_, accuracy = model.evaluate(X_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))
```

Information on the variables as follows:
_X is an array of float64, with size (152,7,169)
Y is an array of int32, with size (152,)_

Information on versions as follows:
_Python version 3.8.5
Keras version 2.4.3
TensorFlow version 2.4.0
Windows 10
Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz   1.90 GHz  (AVX capable)
Spyder version 4.2.0
 _

The full error I am getting is as follows:

> Epoch 1/150
> Traceback (most recent call last):
> 
>   File ""C:\Users\pdeol\Documents\CodingPractice\KerasCombined.py"", line 171, in <module>
>     model.fit(X_train, y_train, epochs=150, batch_size=10)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1100, in fit
>     tmp_logs = self.train_function(iterator)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
>     result = self._call(*args, **kwds)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 871, in _call
>     self._initialize(args, kwds, add_initializers_to=initializers)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 725, in _initialize
>     self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
>     graph_function, _ = self._maybe_define_function(args, kwargs)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 3361, in _maybe_define_function
>     graph_function = self._create_graph_function(args, kwargs)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 3196, in _create_graph_function
>     func_graph_module.func_graph_from_py_func(
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 990, in func_graph_from_py_func
>     func_outputs = python_func(*func_args, **func_kwargs)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 634, in wrapped_fn
>     out = weak_wrapped_fn().__wrapped__(*args, **kwds)
> 
>   File ""C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 977, in wrapper
>     raise e.ag_error_metadata.to_exception(e)
> 
> ValueError: in user code:
> 
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:805 train_function  *
>         return step_function(self, iterator)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:795 step_function  **
>         outputs = model.distribute_strategy.run(run_step, args=(data,))
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1259 run
>         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2730 call_for_each_replica
>         return self._call_for_each_replica(fn, args, kwargs)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3417 _call_for_each_replica
>         return fn(*args, **kwargs)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:788 run_step  **
>         outputs = model.train_step(data)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:754 train_step
>         y_pred = self(x, training=True)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py:998 __call__
>         input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
>     C:\Users\pdeol\anaconda3\lib\site-packages\tensorflow\python\keras\engine\input_spec.py:255 assert_input_compatibility
>         raise ValueError(
> 
>     ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 7 but received input with shape (None, 7, 169)

The above error is what I receive every time I run this code.  I just restarted the software and ran it again, and in addition to the above error, I received the following error immediately below:

> 2020-12-30 10:17:26.556667: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
> 2020-12-30 10:17:26.557141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
> 2020-12-30 10:26:08.613023: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2020-12-30 10:26:08.616994: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
> 2020-12-30 10:26:08.617895: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
> 2020-12-30 10:26:08.628902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LT-B831B58344E3
> 2020-12-30 10:26:08.629094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LT-B831B58344E3
> 2020-12-30 10:26:08.634489: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2020-12-30 10:26:08.636834: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
> 2020-12-30 10:26:08.863911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)

I'm not sure if this is related, or a completely separate issue.  Any information or guidance you can provide would be greatly appreciated."
46074,Are there any excellent github projects on action recognition implemented by TF2?,"I have been searching for official or non-official awesome code projects of action recognition written by TF2 for a long time, but unfortunately fail. I'd like to integrate video classification with other TF2 elements more conviniently. The most majority of video-relevant resources are under the community of Pytorch, though only a few projects were done by TF1, which seem fairly passe and only attract very few stars. Since the TF 2.X version has been released for over 2 years, why does its current ecosystem yet appear to be frustrated? Does anybody else cope with similar questions?"
46073,Cannot quantize part of a model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0-dev20201208
- Python version: 3.7

**Standalone code to reproduce the issue**
I follow the tutorial [here](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide) to see if I can only quantize part of a model. Here is my code:

```
i = tf.keras.Input(shape=(20,))
x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(i)
x = tf.keras.layers.Dense(10)(x)
x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(x)
o = tf.keras.layers.Flatten()(x)
annotated_model = tf.keras.Model(inputs=i, outputs=o)
quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)
converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()
pathlib.Path('/tmp/tmp.tflite').write_bytes(quantized_tflite_model)
```
Basically I want to quantize the 1st and 3rd dense layers of a model. And here is how the resultant model looks like:
<img width=""237"" alt=""Screen Shot 2020-12-30 at 8 02 19 PM"" src=""https://user-images.githubusercontent.com/23546158/103350164-0741a280-4ada-11eb-9c0e-713bb2ceddca.png"">
Apparently, the 2nd quantization goes to the wrong place..."
46072,installing Keras package with tensorflow 2.3.1,"I am a beginner in this, I want to make OpenCV and TensorFlow project and I have some issues

I downloaded python 3.7.6 with pip 19.2.3

after that, I used CMD to download Tensorflow 2.3.1 and I made the path in a python project  where I am coding 
_C:\Users\Desktop\PycharmProjects\SudokuSolver\venv\Lib\site-packages\tensorflow>_**pip install tensorflow==2.3.1**

now when I am importing the libraries:

import TensorFlow
from TensorFlow.Keras.models import load_model

That gives me an error
**""ModuleNotFoundError: No module named 'tensorflow.keras'""**

So what should I do and where is the wrong how to fix it?
Many thanks."
46071,How the parameters of decay_rate & decay_steps are taken into consideration while computing InverseTimeDecay(),"During the last couple of days, I am experimenting with the different schedulers of learning rate decay offered by Keras (link [here][1]). Specifically, I have been using *InverseTimeDecay: A LearningRateSchedule that uses an inverse time decay schedule.* This type of scheduler uses the following two parameters:
1) decay_steps
2) decay_rate

*Note that those two parameters are also used by ```class ExponentialDecay```*

I would like to understand how those two parameters are initialized and taken into consideration by the InverseTimeDecay class. So far, what I know is that the following formula is used (based on the documentation):

```
def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)
```

The parameter initialization I use:

```
def optimizer_adam_v2(hparams):
    # Inverse Time Decay
    initial_learning_rate = 0.001
    decay_steps = int(np.ceil(len(x_train)//32))*10 #len(x_train) = 31334, and 10 represents the epoch. So after 10 epochs or similarly 9791 steps the decay rate will be applied.
    decay_rate = 0.5
    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate)

    return keras.optimizers.Adam(learning_rate=learning_rate_fn)
```

Based on the values above and the formula, the ```decay_learning_rate``` should be equal to

```
decay_learning_rate = 0.001 / (1 + 0.5 * step / 9791) = 0.0009999489353010264
# 9791 = int(np.ceil(31334//32))*10 #decay_steps parameter
```

When the training process begins the learning rate starts to drop by the decay_learning_rate at every step of the training. However, based on the [documentation][2] (	```decay_steps``` is ""How often to apply decay""). the learning rate should start dropping after the training pass the value of the decay_steps. So after 9791 steps (approximately 10 epochs). Instead, the learning rate starts to drop from epoch 0. 

Why is this happening? Why the decay starts from epoch 0 and not from epoch 10 (9791 steps have passed).

P.S 1: Similar [question][3]

**[UPDATE] - 02.01.2021**

This is also observed with the Piecewise Constant Decay (link [here][3])

Piecewise constant decay is a similar scheduler to InverseTimeDecay with a much simpler application. The concept is that for every N number of steps the learning rate is constant and changes based on an array of values.

For example I have implemented a piecewise decay scheduler like below:

```
def optimizer_adam_v2():
    # PiecewiseConstantDecay
    step = tf.Variable(0, trainable=False)
    boundaries = [1000, 5000]
    values = [0.001, 0.0005, 0.00025]

    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)

    # Later, whenever we perform an optimization step, we pass in the step.
    learning_rate_adam = learning_rate_fn(step)

    return keras.optimizers.Adam(learning_rate=learning_rate_adam)
```

In short, *based always on the relevant documentation written by Tensorflow experts*, what the code above does is to use a learning rate of 0.001 for the first 1000 steps, 0.0005 for the next 5000 steps (until 6000 steps completed), and the for the rest of the training use 0.00025 learning rate.
Since I monitored the learning rate of my scheduler I got the following result [here ](https://drive.google.com/file/d/1jvDzOZMB9YkuUxQ4I0yCapI5Hb9LaQM-/view?usp=sharing)in my Drive folder

With the indication ```lr``` I monitor the learning rate of the Adam optimizer. Until epoch 15 the learning rate didn't change whatsoever. Based on step calculation methodology, 15 epochs is approximately equal to ```len(x_train)//batch_size * epochs = (31335//32)*15 = 979*15 = 14,687 steps.```

After all, my question is still on the table. Why the learning rate of Adam does not obey the change rules of each scheduler.

You may find the implementation of the ```Piecewise constant decay``` scheduler on my colab notebook [here](https://colab.research.google.com/drive/1BsdrgZDtoDEASTikKa4LEVAz7IBzp3jw?usp=sharing)


  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules
  [2]: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay
  [3]: https://stackoverflow.com/questions/60029027/decay-parameter-of-adam-optimizer-in-keras"
46070,Make `TextVectorization` support pickling,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version Tensorflow 2.2:
- Are you willing to contribute it No:



**Describe the feature and the current behavior/state.**

Make `TextVectorization` support pickling, currently it does not support pickling:
```
import cloudpickle
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
ptf = cloudpickle.dumps(TextVectorization())
```
Get error like `Cannot convert a Tensor of dtype resource to a NumPy array.`


**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Support pickling will make it work with some distributed framework such as `Horovod`. Such framework usually require to pickle the user defined training function to remote and executing. So pickling is required.

**Any Other info.**

N/A"
46069,No matching distribution found during installation suing dockerfile,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Ubuntu 18.04 (Docker)
- TensorFlow installed from (source or binary): binary
- TensorFlow version:1.12
- Python version:2.7
- Installed using virtualenv? pip? conda?: virtualenv

**Describe the problem**
Hi, i am building this data analytics project in which i need to use tensorflow using dockerfile running on raspberry pi 4 python 2.7. I did not encounter this issue when i was running the dockerfile in amd64 architecture however in raspberry pi 4 python 2.7 it gives out error no matching distribution found for this platform. P
![tensorflow2](https://user-images.githubusercontent.com/76122954/103345254-d9556180-4acb-11eb-90d7-09a6229fe3be.png)
lease do help me figure this out. Thank you in advance. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

RUN pip install --no-cache-dir pandas sklearn keras tensorflow \
    && apt-get autoremove -y \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
46068,Error when build plugin,"When I try to compile TF with a plugin, I run the following commands:
c++ -O3 -rdynamic -std=c++11 -I/usr/local/lib/python2.7/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -Wall -c plugin.cc -o plugin.o
c++ -shared -fPIC -Wl,-soname,libplugin.so.1 -L/usr/local/lib/python2.7/dist-packages/tensorflow -ltensorflow_framework -o libplugin.so plugin.o

However, when I try to tensorflow.load_library('libplugin.so'), error occurs: 
tensorflow.python.framework.errors_impl.NotFoundError: ./libplugin.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

I use the following command: ldd libconfig.so, but no libtensorflow_framework.so is included:
	linux-vdso.so.1 (0x00007ffceaa5e000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fd2cc4f2000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fd2cc2da000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd2cbee9000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd2cbb4b000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fd2ccaa7000)
I find libtensorflow_framework.so in /usr/local/lib/python2.7/dist-packages/tensorflow.
So what's wrong with the compilation process?

Any help would be appreciated! Thanks

The system information: ubuntu18.04, cuda10.0, cudnn7, tf1.13.2, gcc 4.8.5"
46067,"Data input pipeline does not implement Batch for ""from_tensor_slices(dict(df))""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): latest from source
- TensorFlow version (use command below): latest from source
- Python version: 3.8
- Bazel version (if compiling from source): 3.6.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 2070

**Describe the current behavior**
```https://www.tensorflow.org/guide/data#consuming_csv_data```

```titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))```

That code will have dictionary object slice.
element_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,

```def make_window_dataset(ds, window_size=5, shift=1, stride=1):
  windows = ds.window(window_size, shift=shift, stride=stride)

  def sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

  windows = windows.flat_map(sub_to_batch)
  return windows
```

"
46066,Input pipe line with consuming CSV data (dictionary object) does not implement Batch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): latest from source
- TensorFlow version (use command below): latest from source
- Python version: 3.8
- Bazel version (if compiling from source): 3.6.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 2070

**Describe the current behavior**
```https://www.tensorflow.org/guide/data#consuming_csv_data```

titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))
That code will have dictionary object slice.
element_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,

def make_window_dataset(ds, window_size=5, shift=1, stride=1):
  windows = ds.window(window_size, shift=shift, stride=stride)

  def sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

  windows = windows.flat_map(sub_to_batch)
  return windows


"
46065,Support Relu activation function with CUDA cuDNN gpu accleeration  instead of just tanh,"**System information**
- TensorFlow version (you are using):
   2.4
- Are you willing to contribute it (Yes/No):
Yes


**Describe the feature and the current behavior/state.**

The docs say that cuDNN requires tanh for some reason although its not clear why this limitation exists:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#used-in-the-notebooks_1

Tensorflow should support more activation functions like Relu when using cuDNN.  On the Nvidia cuDNN side, there is no limitation with using relu

https://developer.nvidia.com/blog/optimizing-recurrent-neural-networks-cudnn-5/

Right now I am being forced to use tanh instead of relu simply because of the fact that the performance of cuDNN with tanh is like 5 orders of magnitude better for training in terms of speed compared to the generic kernel. 

In summary, please remove the validation and requirements check for cuDNN that prevents you from using relu. Thank you.

**Will this change the current api? How?**
It should not affect the API at all

**Who will benefit with this feature?**
Huge performance benefits if able to use cuDNN instead of the generic tensorflow kernel, so anyone with an nvidia GPU would benefit.
**Any Other info.**
"
46064,Wrong casting (mixed precision) in attention layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0
- Python version: 3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
When using Attention or AdditiveAttention with mixed precision policy issue occurred due to wrong casting (mask casted to floatx but should be casted to scores.dtype)

**Describe the expected behavior**
Layers should work without issues with mixed_fp16

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1R2MKXAIrmYBBGkij11m9aG7hLm2AEHjF?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The issue comes from here https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/layers/dense_attention.py#L129
Should be this:
```python
scores -= 1.e9 * math_ops.cast(padding_mask, dtype=scores.dtype)
```"
46062,Modularize and consolidate the Docker images for downstream usage,"**System information**
- TensorFlow version (you are using): 2.4.0+
- Are you willing to contribute it (Yes/No): I can help, but it needs ownership from TF Dev-Infra team.

**Describe the feature and the current behavior/state.**
Currently there are 3 or 4 Dockerfiles which are maintained independently and with different levels of support. This has been a time sink for the TF team, and a headache for downstream consumers. It should be (relatively) easy to refactor these as docker targets that build from one another. Information below is for  GPU containers (though it applies to CPU and TF versions as well):

*  DockerHub tensorflow/tensorflow:gpu
    * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile
    * Builds from nvidia/cuda  
    * Installs TF pip package to base python3
*  DockerHub tensorflow/tensorflow:devel-gpu
    * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile
    * Builds from nvidia/cuda 
    * Installs build tools and bazel
    * Clones TF source code; 
    * Installs python deps to base python3
 *  DockerHub tensorflow/tensorflow:custom_op_gpu
    * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.custom_op_ubuntu_16_cuda10.1
    * Builds from nvidia/cuda 
    * Installs build tools and bazel
    * Installs devtoolset7 and devtoolset8 for manylinux2010 builds
    * Explicitly installs select python versions
    * Installs python deps to base python3
 *  GCR tensorflow-testing/nosla-cuda11.0-cudnn8-ubuntu18.04-manylinux2010-multipython
    * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rbe.cuda11.0-cudnn8-ubuntu18.04-manylinux2010-multipython
    * Builds from nvidia/cuda 
    * Installs build tools and bazel
    * Installs devtoolset7 and devtoolset8 for manylinux2010 builds
    * A nice modular installation of all support python versions
    * Installs python deps to all python installations
    * **This is currently used by SIG Addons and SIG IO -- though there is no SLA for these images**

As you can see there is a ton of duplication, not reusing of modular scripts when they can be, and with these 4 options there is still a ton of bloat in the images that should be refactored.

**Will this change the current api? How?**
We should use Docker targets to progressively build the containers and publish their intermediate stages. There would be no need to modify tags or anything. Prototype:

```
FROM nvidia/cuda${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base
RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        cuda-command-line-tools-${CUDA/./-} \
        libcublas-${CUDA/./-} \
        ....

RUN ln -s $(which python3) /usr/local/bin/python

COPY install/build_and_install_python.sh /install/
RUN /install/build_and_install_python.sh ""3.6.9""
RUN /install/build_and_install_python.sh ""3.7.7""
RUN /install/build_and_install_python.sh ""3.8.2""

# Install bazel
ARG BAZEL_VERSION=3.7.2
RUN mkdir /bazel && \
    wget -O /bazel/installer.sh ""https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh"" && \
    wget -O /bazel/LICENSE.txt ""https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE"" && \
    chmod +x /bazel/installer.sh && \
    /bazel/installer.sh && \
    rm -f /bazel/installer.sh

# -------------------------------------------------------------------
FROM base as tensorflow_gpu
RUN python3 -m pip install --no-cache-dir ${TF_PACKAGE}${TF_PACKAGE_VERSION:+==${TF_PACKAGE_VERSION}}

# -------------------------------------------------------------------
FROM base as tensorflow_devel_gpu

RUN apt-get update && apt-get install -y \
    openjdk-8-jdk \
    ....

RUN python3 -m pip --no-cache-dir install \
    Pillow \
    h5py \
    keras_preprocessing \
    matplotlib \
    mock \
    'numpy<1.19.0' \
    scipy \
    sklearn \
    pandas \
    future \
    portpicker \
    enum34

# -------------------------------------------------------------------
FROM base as tensorflow_custom_op_gpu
ADD devtoolset/fixlinks.sh fixlinks.sh
ADD devtoolset/build_devtoolset.sh build_devtoolset.sh
ADD devtoolset/rpm-patch.sh rpm-patch.sh

# Set up a sysroot for glibc 2.12 / libstdc++ 4.4 / devtoolset-7 in /dt7.
RUN /build_devtoolset.sh devtoolset-7 /dt7
# Set up a sysroot for glibc 2.12 / libstdc++ 4.4 / devtoolset-8 in /dt8.
RUN /build_devtoolset.sh devtoolset-8 /dt8

FROM nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04
COPY --from=devtoolset /dt7 /dt7
COPY --from=devtoolset /dt8 /dt8

RUN /install/install_bootstrap_deb_packages.sh
RUN /install/install_deb_packages.sh
RUN /install/install_clang.sh
RUN /install/install_bazel.sh
RUN /install/install_buildifier.sh
RUN /install/install_pip_packages.sh
RUN /install/install_auditwheel.sh

ENV TF_NEED_CUDA=1

# -------------------------------------------------------------------
FROM tensorflow_custom_op_gpu as tensorflow_build_manylinux2010_multipython

COPY install/install_pip_packages_by_version.sh /install/
RUN /install/install_pip_packages_by_version.sh ""/usr/local/bin/pip2.7""
RUN /install/install_pip_packages_by_version.sh ""/usr/local/bin/pip3.8""
RUN /install/install_pip_packages_by_version.sh ""/usr/local/bin/pip3.5""
RUN /install/install_pip_packages_by_version.sh ""/usr/local/bin/pip3.6""
RUN /install/install_pip_packages_by_version.sh ""/usr/local/bin/pip3.7""

# -------------------------------------------------------------------

```

**Who will benefit with this feature?**
SIGs, developers, and downstream libraries looking for well managed Docker containers to build from. 

Example benefits:
* Currently the custom_op container has a lot of installations that are not needed see #38352
* Currently the custom_op container gets updated when time permits, this would make it update alongside the rest of the containers
* Currently the manylinux2010_multipython is the most comprehensive build container, but it has no SLA and is also unnecessarily bulky (can remove all the pip package installations for every py version)
* Single Dockerfile to maintain (can have owners for seperate pieces etc.)

**Any Other info.**
With this being refactored properly we could close #38352, https://github.com/tensorflow/addons/issues/2326 and https://github.com/tensorflow/build/issues/6
"
46061,Error during converting the OpenNMT-tf model to TensorFlow Lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.4.0 (have tested on tf-nightly as well)


**Command used to run the converter or code if youâ€™re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
https://colab.research.google.com/gist/sayakpaul/b3a06119884e0b0bf9cb7f509e4b341e/scratchpad.ipynb
```

**The output from the converter invocation**

```
Exception: <unknown>:0: error: loc(""transformer_base_1/while@__inference__run_14521""): 'tf.While' op body result type tensor<?x8x1x64xf32> is incompatible with result type tensor<?x8x0x64xf32> at index 6


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    214       return model_str
    215     except Exception as e:
--> 216       raise ConverterError(str(e))
    217 
    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(""transformer_base_1/while@__inference__run_14521""): 'tf.While' op body result type tensor<?x8x1x64xf32> is incompatible with result type tensor<?x8x0x64xf32> at index 6
```

**Also, please include a link to the saved model or GraphDef**

```
https://s3.amazonaws.com/opennmt-models/averaged-ende-export500k-v2.tar.gz
```

[OpenNMT](https://opennmt.net/) is a pretty well-known open-source project in the area of Neural Machine Translation. It's used across industries and many academic institutions. 

I was trying to convert their latest SavedModel to TensorFlow Lite but currently, it's failing. From the initial error logs, it looks like it's because of the pesky `tf.While` op.  When I switched to `tf-nightly` it gets changed to the following - 

```
ConverterError: <unknown>:0: error: loc(callsite(callsite(""transformer_base_1/TensorArrayV2@__inference__run_14521"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15063"") at ""StatefulPartitionedCall_4"")): requires element_shape to be 1D tensor during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall_4""): called from
<unknown>:0: error: loc(callsite(callsite(""transformer_base_1/TensorArrayV2@__inference__run_14521"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15063"") at ""StatefulPartitionedCall_4"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall_4""): called from
```

I have tried with the flex ops too but that does not help. I was wondering if the TensorFlow Lite team could look into this and share some suggestions to fix this much like https://github.com/TensorSpeech/TensorflowTTS/pull/84. 

Please note that OpenNMT folks have tried to convert TensorFlow Lite before but they have been unsuccessful so far as per  https://github.com/OpenNMT/OpenNMT-tf/issues/490. 

Cc: @abattery "
46058,micro: port op POW from Lite,"@tensorflow/micro

**System information**
Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): master
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op POW to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.
PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.
PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.
PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded.
"
46054,Bug: Converting quantized Keras models to TFLite,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.3.1
-   **Python version**: 3.8.2

### Describe the problem
Appears to be a bug when trying to convert quantized Keras models to TFLite. Goal is to be able to generate TFLite models using quantized TFHub models.

### Source code / logs
Source Code
import tensorflow as tf
import tensorflow_hub as hub

input_val = tf.keras.layers.Input(shape=(320,320,3), dtype=tf.uint8)
module = hub.KerasLayer('https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1')(input_val)
model = tf.keras.models.Model(inputs=[input_val], outputs=[module])
model.summary()
converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_out = converter.convert()

Error
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(""Func/functional_1/keras_layer/StatefulPartitionedCall/input/_1""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/functional_1/keras_layer/StatefulPartitionedCall/input/_1""): see current operation: %1 = ""tf.Identity""(%arg0) {_class = [""loc:@Func/functional_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/input/_411""], device = """"} : (tensor<?x320x320x3x!tf.quint8>) -> tensor<?x320x320x3xui8>

TF 2.4.0 Error
line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(""Func/model/keras_layer/StatefulPartitionedCall/input/_1""): requires all operands and results to have compatible element types

line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(""Func/model/keras_layer/StatefulPartitionedCall/input/_1""): requires all operands and results to have compatible element types
"
46052,Ability to parse non-Example protos without Eager Execution,"**System information**
- TensorFlow version 2.4.0:
- Are you willing to contribute it (No):

**Describe the feature and the current behavior/state.**
I am trying to use the Waymo open dataset in tensorflow, without using eager execution. The data is not formatted as tf.data.examples, but instead as a custom proto that is serialized. The eager-compatible approach is something like this: `example.ParseFromString(raw_record.numpy())`, which relies on the numpy compatibility of eager tensors. I thought this issue could be solved by the introduction of tf.experimental.numpy, but it does not seem to support iterating like normal numpy arrays. Are there any plans to add iteration support to tensorflow numpy, or support for deserializing protos other than tf.Examples?
 
**Will this change the current api? How?**
If the iteration approach is taken, it will not change the api as iteration methods currently exist but throw errors when called. Other approaches may involve additions to the API for proto desierialization.

**Who will benefit with this feature?**
Users of the Waymo Open Dataset will benefit, as well as anyone else who would like to format their data as custom protos. Tf.example format can be difficult for structured or hierarchical data."
46049,micro: port op ZEROS_LIKE from lite,"@tensorflow/micro

**System information**
Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): master
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op ZEROS_LIKE to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes (? zeros_like doesn't seem to need a reference implementation).
PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.
PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.
PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded."
46048,micro: port op WHERE from lite,"@tensorflow/micro

**System information**
Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): master
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op WHERE to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.
PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.
PR 4: delete all the extra code from the lite reference kernel (but from the new location in micro. Extra code includes the integer ops, generic optimized calls, and other cleanup.
PR 5: make the necessary changes to the reference kernel, reference header and test to make it work with micro. This is where we add it into the TFLM build, and make all the necessary changes to make the code ready for embedded."
46047,"    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3)) AttributeError: module 'tensorflow' has no attribute 'placeholder'","**Describe the current behavior**
```
[3306:3298 0:1022] 01:57:24 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet
$ python train.py 
Traceback (most recent call last):
  File ""train.py"", line 260, in <module>
    train()
  File ""train.py"", line 96, in train
    pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)
  File ""/home/mona/research/code/DJ-RN/pointnet/models/pointnet_cls.py"", line 13, in placeholder_inputs
    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))
AttributeError: module 'tensorflow' has no attribute 'placeholder'
```

Also:

```
[3306:3298 0:1023] 01:57:31 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet
$ python
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.__version__
'2.2.0'
>>> quit()
12149/31772MB
[3306:3298 0:1024] 01:59:05 Tue Dec 29 [mona@goku:pts/0 +1] ~/research/code/DJ-RN/pointnet
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
12149/31772MB

$ lsb_release -a
LSB Version:	core-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.1 LTS
Release:	20.04
Codename:	focal

```


Please also check https://github.com/charlesq34/pointnet/issues/265 for the code repo



- TensorFlow installed from (source or binary): installed using
`conda install tensorflow-gpu cudatoolkit=10.1`

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
```
$ python 
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python.platform import build_info as tf_build_info
>>> tf_build_info.cudnn_version_number
'7.6'
```

- GPU model and memory: `NVIDIA GeForce 1650 Ti :  4G`

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
```
$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""

unknown 2.2.0
```


--------------------------------------------------

$ ./tf_env_collect.sh 
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_os.py"", line 18, in <module>
    platform.linux_distribution(),
AttributeError: module 'platform' has no attribute 'linux_distribution'
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 5, in <module>
    with tf.Session() as sess:
AttributeError: module 'tensorflow' has no attribute 'Session'
./tf_env_collect.sh: line 168: bazel: command not found
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

12515/31772MB
[8547:3298 0:1043] 02:08:05 Tue Dec 29 [mona@goku:pts/1 +1] ~/Downloads
$ cat tf_env.txt

== check python ===================================================
python version: 3.8.5
python branch: 
python build version: ('default', 'Sep  4 2020 07:30:14')
python compiler version: GCC 7.3.0
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                              1.19.2
numpydoc                           1.1.0
protobuf                           3.14.0
tensorflow                         2.2.0
tensorflow-estimator               2.4.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.2.0
tf.version.GIT_VERSION = unknown
tf.version.COMPILER_VERSION = 5.4.0
      9014:	find library=libpthread.so.0 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib/tls/haswell/x86_64:/home/mona/anaconda3/bin/../lib/tls/haswell:/home/mona/anaconda3/bin/../lib/tls/x86_64:/home/mona/anaconda3/bin/../lib/tls:/home/mona/anaconda3/bin/../lib/haswell/x86_64:/home/mona/anaconda3/bin/../lib/haswell:/home/mona/anaconda3/bin/../lib/x86_64:/home/mona/anaconda3/bin/../lib(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/tls/haswell/x86_64/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/tls/haswell/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/tls/x86_64/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/tls/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/haswell/x86_64/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/haswell/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/x86_64/libpthread.so.0
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libpthread.so.0
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/libpthread.so.0
      9014:	
      9014:	find library=libc.so.6 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libc.so.6
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
      9014:	
      9014:	find library=libdl.so.2 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libdl.so.2
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/libdl.so.2
      9014:	
      9014:	find library=libutil.so.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libutil.so.1
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/libutil.so.1
      9014:	
      9014:	find library=librt.so.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/librt.so.1
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/librt.so.1
      9014:	
      9014:	find library=libm.so.6 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libm.so.6
      9014:	 search cache=/etc/ld.so.cache
      9014:	  trying file=/lib/x86_64-linux-gnu/libm.so.6
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/libpthread.so.0
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/libc.so.6
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/libm.so.6
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/librt.so.1
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/libutil.so.1
      9014:	
      9014:	
      9014:	calling init: /lib/x86_64-linux-gnu/libdl.so.2
      9014:	
      9014:	
      9014:	initialize program: /home/mona/anaconda3/bin/python
      9014:	
      9014:	
      9014:	transferring control: /home/mona/anaconda3/bin/python
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libffi.so.7 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell:/home/mona/anaconda3/lib/python3.8/lib-dynload/../../x86_64:/home/mona/anaconda3/lib/python3.8/lib-dynload/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/x86_64/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/haswell/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/x86_64/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../tls/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/x86_64/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../haswell/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../x86_64/libffi.so.7
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libmkl_rt.so [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../..(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/x86_64/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/haswell/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/x86_64/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../tls/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/x86_64/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../haswell/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../x86_64/libmkl_rt.so
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	/home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so: error: symbol lookup error: undefined symbol: omp_get_num_threads (fatal)
      9014:	find library=libiomp5.so [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so
      9014:	
      9014:	find library=libgcc_s.so.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/bin/../lib		(RPATH from file /home/mona/anaconda3/bin/python)
      9014:	  trying file=/home/mona/anaconda3/bin/../lib/libgcc_s.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/bin/../lib/libgcc_s.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_py_mkl_service.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libz.so.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=liblzma.so.5 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libcrypto.so.1.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_core.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_thread.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_lp64.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_avx2.so
      9014:	
      9014:	find library=libtensorflow_framework.so.2 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/..:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/tls/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../tls/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../haswell/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2
      9014:	
      9014:	find library=libstdc++.so.6 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/..:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/x86_64/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/haswell/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/x86_64/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../tls/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/x86_64/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../haswell/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../x86_64/libstdc++.so.6
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	
      9014:	find library=libprotobuf.so.24 [0]; searching
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_tf_stack.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfe.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_session.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_exception_registry.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_utils.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/wrapt/_wrappers.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_bfloat16.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_dtypes.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/fast_tensor_util.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_op_def_registry.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_python_op_gen.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_file_io.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_func.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_events_writer.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_record_io.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_checkpoint_reader.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libssl.so.1.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/lib-dynload/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_device_lib.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_optimizer.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_cluster.so
      9014:	
      9014:	find library=libhdf5.so.103 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/x86_64/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/haswell/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/x86_64/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../tls/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/x86_64/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../haswell/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../x86_64/libhdf5.so.103
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libhdf5_hl.so.100 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_objects.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_conv.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5r.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5t.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/utils.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5z.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5a.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5s.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5p.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ac.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_proxy.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5d.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ds.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5f.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5g.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5i.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5fd.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5pl.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5o.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5l.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/brotli/_brotli.abi3.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/_cffi_backend.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libyaml-0.so.2 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/x86_64/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/haswell/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/x86_64/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../tls/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/x86_64/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../haswell/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../x86_64/libyaml-0.so.2
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_ccallback_c.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_uarray/_uarray.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_sparsetools.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_csparsetools.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_tools.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_traversal.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_flow.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_matching.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_reordering.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/interval.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashtable.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/missing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/dtypes.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/base.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/nattype.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/np_datetime.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/tzconversion.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/ccalendar.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/parsing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/fields.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/strptime.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/period.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/vectorized.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops_dispatch.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslib.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/reduce.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce_axis.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/move.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/sparse.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reshape.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/indexing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/writers.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/mmap.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/aggregations.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/indexers.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/groupby.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reduction.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/parsers.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/json.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/testing.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfprof.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_quantize_training.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_stacktrace_handler.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_util_port.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_debug_events_writer.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_mlir.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_toco_api.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	find library=libjpeg.so.9 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../..(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/x86_64/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/haswell/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/x86_64/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../tls/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/x86_64/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../haswell/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../x86_64/libjpeg.so.9
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9
      9014:	
      9014:	find library=libtiff.so.5 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../..		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5
      9014:	
      9014:	find library=libzstd.so.1 [0]; searching
      9014:	 search path=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././x86_64:/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../.		(RPATH from file /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5)
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/x86_64/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/haswell/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/x86_64/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././tls/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/x86_64/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././haswell/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././x86_64/libzstd.so.1
      9014:	  trying file=/home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_nd_image.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_ni_label.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_fblas.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flapack.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flinalg.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_solve_toeplitz.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_decomp_update.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_blas.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling init: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_lapack.cpython-38-x86_64-linux-gnu.so
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/bin/python [0]
      9014:	
      9014:	
      9014:	calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_mklinit.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libiomp5.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/_py_mkl_service.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_decimal.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_avx2.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_lp64.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_intel_thread.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_core.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/pyexpat.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_tf_stack.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfe.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_session.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_exception_registry.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_utils.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/wrapt/_wrappers.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_bfloat16.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_dtypes.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/fast_tensor_util.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_op_def_registry.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_python_op_gen.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_file_io.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_py_func.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_queue.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_events_writer.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_record_io.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_checkpoint_reader.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_json.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_ssl.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libssl.so.1.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libcrypto.so.1.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_contextvars.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_device_lib.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_optimizer.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tf_cluster.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_errors.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5_hl.so.100 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_objects.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_conv.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5r.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5t.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/utils.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5z.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5a.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5s.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5p.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ac.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/_proxy.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5d.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/array.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5ds.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5f.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5g.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5i.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5fd.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/../../../libhdf5.so.103 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5pl.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5o.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/h5py/h5l.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/brotli/_brotli.abi3.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/_cffi_backend.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libffi.so.7 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/unicodedata.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/_yaml.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/../../libyaml-0.so.2 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_ccallback_c.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/_lib/_uarray/_uarray.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_sparsetools.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/_csparsetools.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_tools.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_traversal.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_flow.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_matching.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/sparse/csgraph/_reordering.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/interval.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashtable.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/missing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/dtypes.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/base.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/nattype.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/np_datetime.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/tzconversion.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/ccalendar.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/parsing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/fields.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/strptime.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/properties.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/period.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslibs/vectorized.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops_dispatch.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/tslib.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/hashing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/ops.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/reduce.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/nonreduce_axis.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/bottleneck/move.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/sparse.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reshape.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/indexing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/writers.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/mmap.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/aggregations.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/window/indexers.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/groupby.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/reduction.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/parsers.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/json.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/pandas/_libs/testing.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfprof.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_quantize_training.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_stacktrace_handler.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_util_port.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_debug_events_writer.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_mlir.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_toco_api.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/tensorflow/python/../../../../libstdc++.so.6 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/bin/../lib/libgcc_s.so.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/_elementtree.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/_imaging.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libtiff.so.5 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../../libjpeg.so.9 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../liblzma.so.5 [0]
      9014:	
      9014:	
      9014:	calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/lib-dynload/../../libz.so.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/PIL/../../.././libzstd.so.1 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_nd_image.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/ndimage/_ni_label.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_fblas.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flapack.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_flinalg.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_solve_toeplitz.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/_decomp_update.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_blas.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/scipy/linalg/cython_lapack.cpython-38-x86_64-linux-gnu.so [0]
      9014:	
      9014:	
      9014:	calling fini: /home/mona/anaconda3/lib/python3.8/site-packages/mkl/../../../libmkl_rt.so [0]
      9014:	
      9014:	
      9014:	calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
      9014:	

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Dec 29 14:08:04 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 165...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   43C    P8     4W /  N/A |    690MiB /  3911MiB |      6%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       981      G   /usr/lib/xorg/Xorg                133MiB |
|    0   N/A  N/A      1735      G   /usr/lib/xorg/Xorg                400MiB |
|    0   N/A  N/A      1906      G   /usr/bin/gnome-shell              122MiB |
|    0   N/A  N/A      2264      G   ...gAAAAAAAAA --shared-files        9MiB |
|    0   N/A  N/A      3148      G   /usr/lib/firefox/firefox            1MiB |
|    0   N/A  N/A      3724      G   /usr/lib/firefox/firefox            1MiB |
|    0   N/A  N/A      4056      G   /usr/lib/firefox/firefox            1MiB |
|    0   N/A  N/A      4209      G   /usr/lib/firefox/firefox            1MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/mona/anaconda3/lib/python3.8/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 5, 'final', 0)

== bazel version  ===============================================
12476/31772MB
[8547:3298 0:1044] 02:08:12 Tue Dec 29 [mona@goku:pts/1 +1] ~/Downloads
$ 




"
46044,How to run tensorflow 2.4.0 on ARM Mac using Rosetta 2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacBook Pro Apple M1 chip, macOS Big Sur version 11.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.0
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: I don't know what that means
- GPU model and memory: No GPU



**Describe the problem**

Python code `import tensorflow` leads to `zsh: illegal hardware instruction  python3` and the Python Console stops. I would like to enter additional commands after importing tensorflow rather than the Python Console stopping. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Install Python 3.8 using homebrew in a Rosetta 2 terminal
2. In Rosetta 2 terminal:
```
which python3
/usr/bin/python3
```
3. Create virtual environment:
`python3 -m venv /Users/daniel/TFTest/venv`

4. Activate virtual environment
`/Users/daniel/TFTest/venv/bin/activate`

5.
```
which python3
/Users/daniel/TFTest/venv/bin/python3
```

6. `pip install --upgrade pip`

7. `pip3 install --upgrade tensorflow`

8.
```
python3
import tensorflow
```

gives output:

`zsh: illegal hardware instruction  python3`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I am aware of this repository: https://github.com/apple/tensorflow_macos . But I would like to use tensorflow 2.4.0 rather than the experimental version from the Apple repository."
46043,RTX 3060 Ti is approximately x1.5 slower compared to RTX 2080 Super,"**System information**
- Tested using a simple script and sample code from Tensorflow Object Detection API:
- Windows 10 Pro
- Installed using pip in Anaconda
- v2.4.0-rc4-71-g582c8d236cb 2.4.0 (3060 Ti)/v2.2.0-rc4-8-g2b96f3662b 2.2.0 (2080 Super)
- Python version: 3.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA 11.0 cuDNN 8.04 (3060 Ti)/ CUDA 10.1 cuDNN 7.6.5 (2080 Super)
- GeForce RTX 3060 Ti 8192 MB/GeForce RTX 2080 Super 8192 MB


**Describe the current behavior**
Tested the performance of my 3060 Ti by training a simple model and running it with Tensorflow Object Detection tutorial
Training 10 iteration of simple model: 30 seconds (3060 Ti)/ 18 seconds (2080 Super)
Inference speed of the 2 sample image provided: 7.15 s (3060Ti)/5.54 s (2080 Super))

**Describe the expected behavior**
Performance of +-15% in terms of speed

**Standalone code to reproduce the issue**
Custom script:
```
import tensorflow as tf
import time

if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print(""Please install GPU version of TF"")

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

start = time.time()

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

end = time.time()
print(""Time delta: "", end - start)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])

start = time.time()
model.fit(x_train, y_train, epochs=10)
end = time.time()
print(""Time delta: "", end - start)
```

Tensorflow Object Detection API:
https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb

**Attempts done**
Initially had the problem indicated in this issue https://github.com/tensorflow/tensorflow/issues/45170 and followed the suggestions but no improvement to the speed.

**Other info / logs**
**logs from terminal when running on object detection api (3060 Ti):**
```
2020-12-30 00:23:16.972492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 00:23:18.909521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-30 00:23:18.912363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-30 00:23:18.932425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-30 00:23:18.932532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 00:23:18.944202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 00:23:18.944282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-30 00:23:18.947437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:23:18.948443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:23:18.955239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:23:18.957428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-30 00:23:18.958003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 00:23:18.958093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-30 00:23:19.358911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-30 00:23:19.358988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-12-30 00:23:19.359579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-12-30 00:23:19.359887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2020-12-30 00:23:19.360476: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-30 00:23:20.842690: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-30 00:23:20.842856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-30 00:23:20.843420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 00:23:20.843686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 00:23:20.843868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-30 00:23:20.844089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:23:20.844257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:23:20.844440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:23:20.844608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-30 00:23:20.844782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 00:23:20.844981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-30 00:23:20.845492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-30 00:23:20.845542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-30 00:23:20.845717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 00:23:20.845888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-30 00:23:20.846081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:23:20.846310: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:23:20.846483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:23:20.846768: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-30 00:23:20.846957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 00:23:20.847157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-30 00:23:20.847318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-30 00:23:20.847521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-12-30 00:23:20.847710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-12-30 00:23:20.847788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2020-12-30 00:23:20.847924: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-30 00:23:20.880594: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-30 00:23:26.283777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-30 00:23:27.288825: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2020-12-30 00:23:27.321549: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2020-12-30 00:23:27.335160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-30 00:23:27.915665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
```




**logs from terminal when running on object detection api (2080 Super):**
```
2020-12-30 00:16:36.575415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-30 00:16:38.228703: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-12-30 00:16:38.237835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x271c5ab6f50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-30 00:16:38.240279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-12-30 00:16:38.242462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-12-30 00:16:38.270196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s
2020-12-30 00:16:38.274723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-30 00:16:38.279122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-30 00:16:38.283598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:16:38.286135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:16:38.292216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:16:38.295924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-30 00:16:38.303861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-30 00:16:38.305815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-12-30 00:16:38.752621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-30 00:16:38.755254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-12-30 00:16:38.756368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-12-30 00:16:38.757586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-12-30 00:16:38.763118: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x271890480c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-12-30 00:16:38.765334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 SUPER, Compute Capability 7.5
2020-12-30 00:16:40.654793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s
2020-12-30 00:16:40.658737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-30 00:16:40.660559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-30 00:16:40.662372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:16:40.664757: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:16:40.666563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:16:40.668383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-30 00:16:40.670203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-30 00:16:40.672337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-12-30 00:16:40.674537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s
2020-12-30 00:16:40.677940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-30 00:16:40.679743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-30 00:16:40.682034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-30 00:16:40.683834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-30 00:16:40.685635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-30 00:16:40.687685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-30 00:16:40.689888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-30 00:16:40.692332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-12-30 00:16:40.693697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-30 00:16:40.695596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-12-30 00:16:40.696726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-12-30 00:16:40.697905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6553 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)
```"
46042,Tensorflow related error when deploying shiny app on shinyapps.io,"I am trying to deploy shiny app, that uses reticulate and keras packages. I do not have any problem to run it locally, but real troubles appear, when I try to deploy it to shinyapps.io. My app.r file is as follows:
```
virtualenv_dir = Sys.getenv(""VIRTUALENV_NAME"")
python_path = Sys.getenv(""PYTHON_PATH"")
reticulate::virtualenv_create(envname = virtualenv_dir, python = python_path)
reticulate::virtualenv_install(virtualenv_dir, packages = c(""numpy"", ""h5py"", ""scipy"", ""scikit-image"", ""pyyaml"", ""pillow""), ignore_installed = TRUE)
reticulate::use_virtualenv(virtualenv = virtualenv_dir)

library(shiny)
library(keras)
library(reticulate)
library(magick)
library(raster)
library(EBImage)
library(rdrop2)
library(plotly)

np <- import(""numpy"", convert=FALSE)
ndi <- import(""scipy.ndimage"", convert=FALSE)
segment <- import(""skimage.segmentation"", convert=FALSE)
feature <- import(""skimage.feature"", convert=FALSE)

model = load_model_hdf5(""model_v02122020.h5"")

ui <- 
tagList(
    fluidPage(
        sidebarLayout(sidebarPanel(
            fileInput(""upload"", ""Choose a file"", accept = c('image/png', 'image/jpeg')),
            actionButton('click', 'Start')
        ),
        mainPanel(
            tabsetPanel(type=""tabs"",
                tabPanel(""Input image"", plotOutput(""InputImagePlot"", height=""100%"")),
                tabPanel(""Output image"", plotOutput(""OutputImagePlot"", height=""100%"")),
            )
        )
        )
    )
)
server <- 
function(input, output, session) {

    observeEvent(input$click, {
## some code for image processing
})
}
shinyApp(ui = ui, server = server)
```
My .Rprofile file is as follows (credit to [this source](https://github.com/ranikay/shiny-reticulate-app/blob/master/.Rprofile)):
```
VIRTUALENV_NAME = ""virt_tf""

if (Sys.info()[[""user""]] == ""shiny""){
  
  # Running on shinyapps.io
  Sys.setenv(PYTHON_PATH = 'python3')
  Sys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) # Installs into default shiny virtualenvs dir
  Sys.setenv(RETICULATE_PYTHON = paste0('/home/shiny/.virtualenvs/', VIRTUALENV_NAME, '/bin/python'))
  
} else if (Sys.info()[[""user""]] == ""rstudio-connect""){
  
  # Running on remote server
  Sys.setenv(PYTHON_PATH = '/opt/python/3.7.6/bin/python')
  Sys.setenv(VIRTUALENV_NAME = paste0(VIRTUALENV_NAME, '/')) # include '/' => installs into rstudio-connect/apps/
  Sys.setenv(RETICULATE_PYTHON = paste0(VIRTUALENV_NAME, '/bin/python'))
  
} else {
  
  # Running locally
  options(shiny.port = 7450)
  Sys.setenv(PYTHON_PATH = 'python 3.6.12')
  Sys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) # exclude '/' => installs into ~/.virtualenvs/
  # RETICULATE_PYTHON is not required locally, RStudio infers it based on the ~/.virtualenvs path
}
```
The deployment process seems to run completely according to R log:
```
rsconnect::deployApp()
Preparing to deploy application...Update application currently deployed at
https://name.shinyapps.io/appname/? [Y/n] y
DONE
Uploading bundle for application: 3428026...DONE
Deploying bundle: 4035381 for application: 3428026 ...
Waiting for task: 846214175
  building: Parsing manifest
  building: Building image: 4594673
  building: Installing system dependencies
  building: Fetching packages
  building: Installing packages
  building: Installing files
  building: Pushing image: 4594673
  deploying: Starting instances
  terminating: Stopping old instances
Application successfully deployed to https://name.shinyapps.io/appname/
```
The error I get from the bottom of the log:

> Error in value[[3L]](cond) : Installation of TensorFlow not found.
> Python environments searched for 'tensorflow' package:
> You can install TensorFlow using the install_tensorflow() function.
> /home/shiny/.virtualenvs/virt_tf/bin/python3
> Calls: local ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>
> Execution halted

When I try to include `tensorflow` into the list of packages required to be installed into my virtualenvironment I get the following error message:

> Downloading tensorflow-2.3.1-cp35-cp35m-manylinux2010_x86_64.whl (320.4 MB)
> Collecting tensorflow
> Killed
> Calls: local ... tryCatch -> tryCatchList -> tryCatchOne -> <Anonymous>
> Error in value[[3L]](cond) : 
> Error installing package(s): 'numpy', 'h5py', 'scipy', 'scikit-image', 'pyyaml', 'pillow', 'tensorflow'
> Execution halted
> Out of memory!

As far as I understand, shinyapps.io pushes me to install tensorflow package into virtualenvironment. However, I guess, it should be in the list of available packages. But how to force using it?"
46041,keras can't save model when useing featurecolumns embding,"`# -*- coding: utf-8 -*-
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input, Model
from tensorflow.keras import regularizers
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import Input, Dense, Multiply
from tensorflow.keras.layers import DenseFeatures

from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import *
import numpy as np
import os

# æ¨¡åž‹ç»†èŠ‚æœ‰å…³çš„è®¾ç½®
dnn_activation = 'tanh'
dropout_rate = 0.5
dnn_l1 = 0.01
dnn_l2 = 0.01
data_use_num = 1
data_2pre_num = 1
pred_label = 'pred_d'
weights_path = 'model_dir/jarvis_super'
batch_size = 1
lr = 0.01

use_feature_columns = True


def lrs(epoch):
    if epoch < 5:
        learning_rate = lr
    elif epoch < 10:
        learning_rate = lr / 3
    elif epoch < 20:
        learning_rate = lr / 10
    else:
        learning_rate = lr / 100
    print('learning rate : {}'.format(learning_rate))
    return learning_rate


def get_model(shape_deep, shape_times, pred_num):
    ##############################################################
    if use_feature_columns:

        feature_columns, feature_layer_inputs = feature_columns_list(
            numeric_column_list=['30day_press_dnn', '60day_press_dnn', '5day_press', ], buckets_column_list=[])

        feature_columns2, feature_layer_inputs2 = feature_columns_list(
            numeric_column_list=['30day_press', '60day_press', ], buckets_column_list=[])

        feature_layer = DenseFeatures(feature_columns)
        input_deep = feature_layer(feature_layer_inputs)
        ##############################################################
        ###############input_layer
        feature_layer = DenseFeatures(feature_columns2)
        input_times = feature_layer(feature_layer_inputs2)

    else:
        input_deep = tf.keras.Input(shape=(3), name='input_deep_list')
        input_times = tf.keras.Input(shape=(2), name='input_times_list')

    dnn1 = (keras.layers.Dense(units=500, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                               activity_regularizer=regularizers.l1(dnn_l1))(input_deep))
    # dnn1 = keras.layers.LayerNormalization(axis=1)(dnn1)
    dnn1 = tf.keras.layers.Dropout(dropout_rate)(dnn1)

    #######################
    dnn2 = (keras.layers.Dense(units=400, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                               activity_regularizer=regularizers.l1(dnn_l1))(dnn1))
    # dnn2 = keras.layers.LayerNormalization(axis=1)(dnn2)
    dnn2 = tf.keras.layers.Dropout(dropout_rate)(dnn2)

    ###############################
    dnn3 = (keras.layers.Dense(units=200, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                               activity_regularizer=regularizers.l1(dnn_l1))(dnn2))
    concat = (keras.layers.concatenate([input_deep, dnn3]))
    # dnn3 = keras.layers.BatchNormalization(axis=1)(concat)
    dnn3 = tf.keras.layers.Dropout(dropout_rate)(concat)

    ###########################################
    dnn4 = (keras.layers.Dense(units=150, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                               activity_regularizer=regularizers.l1(dnn_l1), name='dnn4')(dnn3))

    ################æ—¶é—´å˜é‡ç»“æž„

    attention_probs_time = (keras.layers.Dense(units=input_deep.shape[1], activation='softmax', )(input_times))
    dnn_time1 = Multiply()([input_deep, attention_probs_time])

    dnn_time1 = (keras.layers.Dense(units=500, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time1))
    # dnn_time1 = keras.layers.LayerNormalization(axis=1)(dnn_time1)
    dnn_time1 = tf.keras.layers.Dropout(dropout_rate)(dnn_time1)

    #######################
    dnn_time2 = (keras.layers.Dense(units=400, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time1))
    # dnn_time2 = keras.layers.LayerNormalization(axis=1)(dnn_time2)
    dnn_time2 = tf.keras.layers.Dropout(dropout_rate)(dnn_time2)

    ###############################
    dnn_time3 = (keras.layers.Dense(units=200, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time2))
    concat = (keras.layers.concatenate([input_deep, dnn_time3]))
    # dnn_time3 = keras.layers.LayerNormalization(axis=1)(concat)
    dnn_time3 = tf.keras.layers.Dropout(dropout_rate)(concat)

    ###########################################
    # tf.keras.layers.SpatialDropout3D(
    dnn_time4 = (keras.layers.Dense(units=150, activation=dnn_activation, kernel_regularizer=regularizers.l2(dnn_l2),
                                    activity_regularizer=regularizers.l1(dnn_l1))(dnn_time3))

    ############################################
    finall_concat = keras.layers.concatenate([dnn4, dnn_time4])

    x = Dense(128, activation=""relu"")(finall_concat)
    x = BatchNormalization()(x)

    outputs = Dense(1, name='outputs')(x)

    input_deep_list = []
    if use_feature_columns:
        for v in feature_layer_inputs.values():
            print(v)
            input_deep_list.append(v)

        input_times_list = []
        for v in feature_layer_inputs2.values():
            input_times_list.append(v)
        print(input_deep_list, input_times_list)
    else:
        input_times_list = input_times
        input_deep_list = input_deep
    model = Model(inputs=[input_deep_list, input_times_list], outputs=outputs)
    return model


def feature_columns_list(numeric_column_list, buckets_column_list):
    feature_columns = []
    feature_layer_inputs = {}

    for header in numeric_column_list:
        print(header)
        day_press=tf.feature_column.numeric_column(header)
        day_press_buket = tf.feature_column.bucketized_column(day_press, [1,100,1000])#è¿™é‡Œå› ä¸ºæ²¡æœ‰åµŒå…¥dnn æ‰€å·²è¦åŽ»æŽ‰
        day_press_emb = tf.feature_column.embedding_column(categorical_column=day_press_buket,dimension=8)
        feature_columns.append(day_press_emb)

        feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)  ######æ‰€æœ‰å•ä¸ªçš„éƒ½ç”¨è¿™ä¸ª

    return feature_columns, feature_layer_inputs


def feature_columns_list2(numeric_column_list, buckets_column_list):
    feature_columns = []
    feature_layer_inputs = {}

    for header in numeric_column_list:
        print(header)
        numeric_column = tf.feature_column.numeric_column(header)
        feature_columns.append(numeric_column)
        feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=(header + 'time'))  ######æ‰€æœ‰å•ä¸ªçš„éƒ½ç”¨è¿™ä¸ª
    return feature_columns, feature_layer_inputs


df1 = np.random.random((100, 3))
df2 = np.random.random((100, 2))
y_train = np.random.random((100, 1))

list1 = ['30day_press_dnn', '60day_press_dnn',  '5day_press', ]
list2 = ['30day_press', '60day_press', ]
df1 = pd.DataFrame(df1, columns=list1)
df2 = pd.DataFrame(df2, columns=list2)
y_train = pd.DataFrame(y_train, columns=['pred'])

dataset1 = tf.data.Dataset.from_tensor_slices((dict(df1), dict(df2)))
dataset2 = tf.data.Dataset.from_tensor_slices(y_train)
train_dataset = tf.data.Dataset.zip((dataset1, dataset2))
train_dataset = train_dataset.batch(96, drop_remainder=True)

if __name__ == '__main__':
    model = get_model(shape_deep=(1,), shape_times=(1,), pred_num=1)
    # model.summary()

    plateau = LearningRateScheduler(lrs)

    checkpoint = ModelCheckpoint(weights_path,
                                 monitor='val_loss',
                                 verbose=0,
                                 mode='max',
                                 save_best_only=True)
    early_stopping = EarlyStopping(monitor='val_loss', patience=15, mode='max')
    opt = Adam(lr=lr)
    model.compile(
        loss='mse',
        optimizer=opt,
    )  # metrics='loss'
    model.fit(train_dataset, epochs=10, callbacks=[checkpoint, plateau],  # #plateau   early_stopping
              shuffle=False,  # æ—¶é—´åºåˆ—ä¸ºä»€ä¹ˆè¦æ‰“ä¹±  æ˜¯å†™é”™é‡Œå—ï¼Ÿ #å¦å¤–xæ˜¯å¦åº”è¯¥ä¸º38000*300 è¡Œï¼Ÿ
              batch_size=1, verbose=2)
    model.save('model.h5')

<em>Please` make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary):conda install
- TensorFlow version (use command below):tensorflow 2.3.0-gpu
- Python version:python 3.6
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: gpu 2080s   8g 

When I use keras to save the model, if I don't use embedding in feature columns_ The column model can be saved normally, but if embedding is used_ The model after column training cannot be saved. The error is as follows:


Traceback (most recent call last):
  File ""/home/zy/Project_save/BTC_project/code_program/keras_bug.py"", line 212, in <module>
    model.save('model.h5')
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1979, in save
    signatures, options)
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 131, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 119, in save_model_to_hdf5
    save_weights_to_hdf5_group(model_weights_group, model_layers)
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 640, in save_weights_to_hdf5_group
    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/h5py/_hl/group.py"", line 139, in create_dataset
    self[name] = dset
  File ""/home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/h5py/_hl/group.py"", line 373, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46038,Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows,"After installing tensorflow2.4rc4 version on windows10, cudnn initialization error appears when running the training code. 
error info: Unimplemented: kernel reported driver version not implemented on Windows

tensorflow version:  tensorflow 2.4.0rc4
os :  windows 10
cuda version: 11.0
python version: 3.8
GPU: RTX3070 



2020-12-29 18:23:45.222526: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2020-12-29 18:23:45.222972: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2020-12-29 18:23:45.225297: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2020-12-29 18:23:45.225401: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2020-12-29 18:23:45.225508: F tensorflow/core/kernels/conv_grad_input_ops.cc:1173] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()), &algorithms)
"
46037,tensorflow 1.15.4 contrib.slim.python.slim.nets,"i use tensorflow 1.15.4<br>
`from tensorflow.contrib.slim.python.slim.nets import resnet_v2
ImportError: No module named contrib.slim.python.slim.nets`"
46036,Tensorflow crashes when running the profiler with CUDA 11,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11
- GPU model and memory: NVIDIA GeForce GTX 1050 Ti

**Describe the current behavior**
When using `tf.profiler.experimental.Profile` to profile tensorflow code on Jupyter notebook, a warning shows up saying cupti is not loaded. It turns out that in CUDA 11, the cupti library has been renamed to `cupti64_2020.2.0.dll` instead of `cupti64_110.dll`. I made a copy of the library file and renamed it to `cupti64_110.dll` and re-ran the code. This time it loaded correctly, but tensorflow crashed when the code finished running, and no profile data was written. It did not crash when I ran the code without the profiler.

**Describe the expected behavior**
Tensorflow does not crash and profile data is written to the disk, as it has been before.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
with tf.profiler.experimental.Profile('profile'):
    foo = tf.random.normal((100, 100))
    bar = foo * foo
```

**Other info / logs**
Here's the log from jupyter.
```
[I 17:37:43.509 LabApp] Kernel started: 38200cad-ad00-4083-b8be-e15872934699
2020-12-29 17:37:47.950560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-29 17:37:50.254885: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2020-12-29 17:37:50.268058: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2020-12-29 17:37:50.278782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-29 17:37:51.365917: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2020-12-29 17:37:51.378912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cupti64_110.dll
2020-12-29 17:37:51.396378: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-29 17:37:51.409157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-12-29 17:37:51.437201: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-29 17:37:51.459132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-29 17:37:51.472536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-29 17:37:51.490913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-29 17:37:51.509229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-29 17:37:51.528279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-29 17:37:51.546872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-29 17:37:51.562046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-29 17:37:51.576068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-29 17:37:51.586997: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-29 17:37:51.623031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-12-29 17:37:51.649021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-29 17:37:51.663404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-29 17:37:51.677417: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-29 17:37:51.691208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-29 17:37:51.704849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-29 17:37:51.717234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-29 17:37:51.739085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-29 17:37:51.754932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-29 17:37:52.065794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-29 17:37:53.194216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-29 17:37:53.206184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-12-29 17:37:53.214585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-12-29 17:37:53.222182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2974 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-12-29 17:37:53.249346: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-29 17:37:53.436880: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2020-12-29 17:37:53.450762: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
[I 17:37:55.491 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports
kernel 38200cad-ad00-4083-b8be-e15872934699 restarted
```
"
46035,map_fn can't generate ragged tensor on arbitrary dimension,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04 lts
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

map_fn can't generate ragged tensor on arbitrary dimension.

**Describe the expected behavior**

map_fn should generate ragged tensor on arbitrary dimension.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3

import tensorflow as tf;

elems = tf.constant([3, 5, 0, 2]);
a=tf.map_fn(lambda x: tf.zeros((x)), elems, fn_output_signature = tf.RaggedTensorSpec(shape = (None), dtype = tf.float32, ragged_rank = 0));
print(a);
# error occurs below
b=tf.map_fn(lambda x: tf.zeros((4,x)), elems, fn_output_signature = tf.RaggedTensorSpec(shape = (4, None), dtype = tf.float32, ragged_rank = 1));
print(b);
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
46034,Error when build TF 1.13.2 from the source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04/18.04
- TensorFlow installed from source
- TensorFlow version: 1.13.2
- Python version: 2.7
- Installed using: docker
- Bazel version: 0.19.2
- GCC/Compiler version: 4.9
- CUDA/cuDNN version: 10.0, 7
- GPU model and memory: T4, 32GB



**Describe the problem**
I tried to install TF from the source following instructions here, https://github.com/tensorflow/docs/blob/r1.13/site/en/install/source.md#build-from-source. 
However some error occurs: ERROR: /home/cluster/tensorflow/tensorflow/core/kernels/BUILD:593:1: C++ compilation of rule '//tensorflow/core/kernels:eigen_contraction_kernel' failed (Exit 1). 
Any help would be appreciated! Thanks!
"
46032,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'is_fully_defined',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.4.0
- Python version:3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.0
- GPU model and memory:24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Can not reshapes SparseTensor
**Describe the expected behavior**
Reshapes a SparseTensor
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf

sp = tf.SparseTensor([[1,1],[2,2]],[1.,2.],[4,4])
new_sp = tf.sparse.reshape(sp,[8,-1])
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:\Program Files\Python37\lib\site-packages\tensorflow\python\ops\sparse_ops.py"", line 902, in sparse_reshape
    and sp_input.shape.is_fully_defined()):
AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'is_fully_defined'

"
46031,dataset_fn() must return a tf.data.Dataset when using a tf.distribute.Strategy.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
46030,I need to pass a buffer in the tensorFlow library when the buffer is of the type byteArray,"Please, help me. I trying to use the android demo (https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android) version with speech recognition and i need to pass a buffer in the tensorFlow library when the buffer is of the type byteArray. When sending the buffer of type byteArray, the model stops speech recognition. How to correctly pass the buffer in the tensorFlow library with byteArray type?

```
class MainActivity : AppCompatActivity() {

    companion object {

        private const val TAG = ""MainActivity""
        private const val SAMPLE_RATE = 16_000
        private const val BUFFER_SIZE_SECONDS = 0.3F
        private const val DETECTION_THRESHOLD = 0.50F
        private const val SUPPRESSION_MS = 1500
        private const val MINIMUM_COUNT = 3
        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L
        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L
        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200
    }

    private val labels = listOf(""_silence_"", ""_unknown_"", ""yes"", ""no"", ""up"", ""down"", ""left"", ""right"", ""on"", ""off"", ""stop"", ""go"")
    private val tfLiteOptions = Interpreter.Options()
    private val recordingBufferLock = ReentrantLock()

    private var recordingOffset = 0
    private var shouldContinue = true
    private var recordingThread: Thread? = null
    private var shouldContinueRecognition = true
    private var recognitionThread: Thread? = null
    private var recordingBuffer = ByteArray(SAMPLE_RATE)

    private lateinit var tfLite: Interpreter
    private lateinit var tfLiteModel: MappedByteBuffer
    private lateinit var recognizeCommands: RecognizeCommands

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        recognizeCommands = RecognizeCommands(
            labels,
            AVERAGE_WINDOW_DURATION_MS,
            DETECTION_THRESHOLD,
            SUPPRESSION_MS,
            MINIMUM_COUNT,
            MINIMUM_TIME_BETWEEN_SAMPLES_MS
        )

        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {
            initTfLite()
        } else {
            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)
        }
    }

    private fun initTfLite() {
        try {
            tfLiteModel = loadModelFile()
            tfLite = Interpreter(tfLiteModel, tfLiteOptions)

            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))
            tfLite.resizeInput(1, intArrayOf(1))

            startRecording()
            startRecognition()
        } catch (exc: IOException) {
            Log.e(TAG, ""Error: ${exc.message}"")
        }
    }

    private fun startRecording() {
        if (recordingThread == null) {
            shouldContinue = true
            recordingThread = Thread { record() }
            recordingThread?.start()
        }
    }

    private fun record() {
        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)

        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2
        val record = AudioRecord(
            MediaRecorder.AudioSource.DEFAULT,
            SAMPLE_RATE,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            bufferSize
        )

        if (record.state != AudioRecord.STATE_INITIALIZED) {
            Log.e(TAG,""Audio Record can't initialize!"")
            return
        }

        record.startRecording()

        while (shouldContinue) {
            val audioBuffer = ByteArray(bufferSize)
            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)
            val newRecordingOffset = recordingOffset + numberRead
            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)
            val firstCopyLength = numberRead - secondCopyLength

            recordingBufferLock.lock()
            try {
                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)
                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)
                recordingOffset = newRecordingOffset % recordingBuffer.size
            } finally {
                recordingBufferLock.unlock()
            }
        }

        record.stop()
        record.release()
    }

    private fun startRecognition() {
        if (recognitionThread == null) {
            shouldContinueRecognition = true
            recognitionThread = Thread { recognize() }
            recognitionThread?.start()
        }
    }

    private fun recognize() {
        val inputBuffer = ByteArray(SAMPLE_RATE)
        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }
        val outputScores = Array(1) { FloatArray(labels.size) }
        val sampleRateList = intArrayOf(SAMPLE_RATE)

        while (shouldContinueRecognition) {
            recordingBufferLock.lock()

            try {
                val maxLength = recordingBuffer.size
                val firstCopyLength = maxLength - recordingOffset
                val secondCopyLength = recordingOffset
                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)
                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)
            } finally {
                recordingBufferLock.unlock()
            }

            for (i in 0 until SAMPLE_RATE) {
                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()
            }

            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)
            val outputMap: MutableMap<Int, Any> = HashMap()
            outputMap[0] = outputScores

            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)

            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())

            if (!result.foundCommand.startsWith(""_"") && result.isNewCommand) {
                Log.d(TAG, ""Command: ${result.foundCommand} (${result.score})"")
            }

            try {
                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)
            } catch (exc: InterruptedException) {
                Log.d(TAG, ""Error: ${exc.message}"")
            }
        }
    }

    @Throws(IOException::class)
    private fun loadModelFile(): MappedByteBuffer {
        val fileDescriptor = assets.openFd(""conv_actions_frozen.tflite"")
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    override fun onRequestPermissionsResult(
        requestCode: Int,
        permissions: Array<out String>,
        grantResults: IntArray
    ) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults)
        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {
            initTfLite()
        } else {
            Toast.makeText(this, ""ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ"", Toast.LENGTH_LONG).show()
            finish()
        }
    }
}
```"
46029, java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/schema/Model;,"java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/schema/Model;
        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222)
        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64)
        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75)
        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)
        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84)
        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:192)
        at android.app.ActivityThread.main(ActivityThread.java:6738)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825)
     Caused by: java.lang.ClassNotFoundException: org.tensorflow.lite.schema.Model
        at java.lang.VMClassLoader.findLoadedClass(Native Method)
        at java.lang.ClassLoader.findLoadedClass(ClassLoader.java:738)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:363)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)
        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222)Â 
        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64)Â 
        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75)Â 
        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)Â 
        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84)Â 
        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147)Â 
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512)Â 
        at android.os.Handler.dispatchMessage(Handler.java:106)Â 
        at android.os.Looper.loop(Looper.java:192)Â 
        at android.app.ActivityThread.main(ActivityThread.java:6738)Â 
        at java.lang.reflect.Method.invoke(Native Method)Â 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525)Â 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825)Â 
     Caused by: java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/flatbuffers/Table;
        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222)Â 
        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64)Â 
        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75)Â 
        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)Â 
        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84)Â 
        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147)Â 
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512)Â 
        at android.os.Handler.dispatchMessage(Handler.java:106)Â 
        at android.os.Looper.loop(Looper.java:192)Â 
        at android.app.ActivityThread.main(ActivityThread.java:6738)Â 
        at java.lang.reflect.Method.invoke(Native Method)Â 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525)Â 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825)Â 
     Caused by: java.lang.ClassNotFoundException: Didn't find class ""com.google.flatbuffers.Table"" on path: DexPathList[[zip file ""/data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/base.apk""],nativeLibraryDirectories=[/data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/lib/arm64, /data/app/com.gc.testt-RmxHcdzbPpW7fPFnmeS8Rw==/base.apk!/lib/arm64-v8a, /system/lib64, /system/vendor/lib64, /system/vendor/lib64/hw]]
        at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:125)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:379)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)
        at org.tensorflow.lite.support.metadata.ModelInfo.assertTFLiteModel(ModelInfo.java:222)Â 
        at org.tensorflow.lite.support.metadata.ModelInfo.<init>(ModelInfo.java:64)Â 
        at org.tensorflow.lite.support.metadata.MetadataExtractor.<init>(MetadataExtractor.java:75)Â 
        at com.gc.testt.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)Â 
        at com.gc.testt.ADetectorActivity.onPreviewSizeChosen(ADetectorActivity.java:84)Â 
        at com.gc.testt.ACameraActivity.onPreviewFrame(ACameraActivity.java:147)Â 
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1512)Â 
        at android.os.Handler.dispatchMessage(Handler.java:106)Â 
        at android.os.Looper.loop(Looper.java:192)Â 
        at android.app.ActivityThread.main(ActivityThread.java:6738)Â 
        at java.lang.reflect.Method.invoke(Native Method)Â 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:525)Â 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:825)Â 

Androidstudio using the Tensorflow-lite-Metadata - 0.0.0.0-Nightly JAR file, the Table file could not be foundï¼›"
46028,"When use  activation='tanh' , the training program will crash","def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) 

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    model.add(layers.LeakyReLU())
    assert model.output_shape == (None, 28, 28, 1)

    return model

generator =make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

###################
issue:  when use activation='tanh' , the training program will crash, use other activation function is normal.
tensorflow version:   from tf-nightly2.5.0.dev20201217 to tf-nightly2.5.0.dev20201228
cuda version:  11.0
python version: 3.8
os:  windows 10"
46027,gpu out of mempry,"i difined a simple model use tensorflow2.0, but it run slowly. then i code gpustat in terminal, i found gpu out of memory. i run the same code use tensorflow tensorflow1.15, it runs normaly, and gpu memory cost correctly. the model defined as follow:

model = keras.Sequential([
    layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),
    layers.Dense(100, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(10)])

what should i do?"
46025,micro: port op DEPTH_TO_SPACE from lite,"@tensorflow/micro

This issue tracks my work porting operator DEPTH_TO_SPACE from lite to micro. The port will be submitted in a number of PRs. Here's a rough flight plan:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro without making any changes or including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test"
46023,AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift',"
On MacOS 10.13.6 (High Sierra), I get:

AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'


% python bg1.py

2020-12-28 18:32:50.699423: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
o enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-28 18:32:50.699890: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""bg1.py"", line 65, in <module>
    chol_precision_tril=true_chol_precision)
  File ""<decorator-gen-351>"", line 2, in __init__
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py"", line 276, in wrapped_init
    default_init(self_, *args, **kwargs)
  File ""bg1.py"", line 36, in __init__
    tfb.Shift(shift=loc),
**AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'**

====================================================================



**Versions**

MacOS 10.13.6 (High Sierra)

tensorflow                2.0.0           mkl_py37hda344b4_0  
tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  
tensorflow-estimator      2.0.0              pyh2649769_0  
tensorflow-probability    0.8.0                      py_0    conda-forge
jupyter_client            6.1.7                      py_0  
jupyter_core              4.7.0            py37hecd8cb5_0  
jupyterlab_pygments       0.1.2                      py_0  
ipython                   7.19.0           py37h01d92e1_0  
ipython_genutils          0.2.0              pyhd3eb1b0_1  
python                    3.7.9                h26836e1_0  
python-dateutil           2.8.1                      py_0  
python_abi                3.7                     1_cp37m    conda-forge

====================================================================

**The source-code:**

```

import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

from pprint import pprint
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

#import tensorflow as tf
#print(tf.__version__)

import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

import tensorflow_probability as tfp

sns.reset_defaults()
sns.set_context(context='talk',font_scale=0.7)
plt.rcParams['image.cmap'] = 'viridis'

#%matplotlib inline

tfd = tfp.distributions
tfb = tfp.bijectors

class MVNCholPrecisionTriL(tfd.TransformedDistribution):
  """"""MVN from loc and (Cholesky) precision matrix.""""""

  def __init__(self, loc, chol_precision_tril, name=None):
    super(MVNCholPrecisionTriL, self).__init__(
        distribution=tfd.Independent(tfd.Normal(tf.zeros_like(loc),
                                                scale=tf.ones_like(loc)),
                                     reinterpreted_batch_ndims=1),
        bijector=tfb.Chain([
            tfb.Shift(shift=loc),
            tfb.Invert(tfb.ScaleMatvecTriL(scale_tril=chol_precision_tril,
                                           adjoint=True)),
        ]),
        name=name)

def compute_sample_stats(d, seed=42, n=int(1e6)):
  x = d.sample(n, seed=seed)
  sample_mean = tf.reduce_mean(x, axis=0, keepdims=True)
  s = x - sample_mean
  sample_cov = tf.linalg.matmul(s, s, adjoint_a=True) / tf.cast(n, s.dtype)
  sample_scale = tf.linalg.cholesky(sample_cov)
  sample_mean = sample_mean[0]
  return [
      sample_mean,
      sample_cov,
      sample_scale,
  ]

dtype = np.float32
true_loc = np.array([1., -1.], dtype=dtype)
true_chol_precision = np.array([[1., 0.],
                                [2., 8.]],
                               dtype=dtype)
true_precision = np.matmul(true_chol_precision, true_chol_precision.T)
true_cov = np.linalg.inv(true_precision)

d = MVNCholPrecisionTriL(
    loc=true_loc,
    chol_precision_tril=true_chol_precision)

[sample_mean, sample_cov, sample_scale] = [
    t.numpy() for t in compute_sample_stats(d)]

print('true mean:', true_loc)
print('sample mean:', sample_mean)
print('true cov:\n', true_cov)
print('sample cov:\n', sample_cov)

```
"
46020,Common location for portable bash helper functions / aliases,"@tensorflow/micro

PR #46011 fixes the use of `md5sum` to be compatible with a Mac. We are already using `md5sum` in additional places too.

While we should get #46011 merged, it would be better to have a common location for these helper functions / aliases.

I can imagine collecting these into a common helper script (for example `micro/tools/bash_helpers.sh`) and have something like:

```bash

UNAME_S=`uname -s`

if [ UNAME_S == Linux]; then
  alias tflm_md5sum='md5sum'
else if [ UNAME_S == Darwin ]; then
  alias tflm_md5sum='md5 -r'
fi

```

We would then need to change the different download scripts to determine the directory in which the script lives, something like: https://github.com/tensorflow/tensorflow/blob/59f5abfbc8dc5559c361f80f4fa4a006db825e40/tensorflow/lite/micro/tools/ci_build/test_bluepill.sh#L21-L23

and then have
```bash
source tensorflow/lite/micro/tools/bash_helper.sh
```"
46017,"class_id support for PrecisionAtRecall, RecallAtPrecision, and related metrics","**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.keras.metrics.Precision` and `tf.keras.metrics.Recall` both include an optional `class_id` keyword argument to compute metrics in a multi-class setting for a given `class_id`. It would be nice if other metrics shared this feature including `PrecisionAtRecall` and `RecallAtPrecision` (could probably easily be added to any metric that uses `metrics_utils.update_confusion_matrix_variables`).

**Will this change the current api? How?** No, `class_id` is an optional keyword argument.

**Who will benefit with this feature?** Users wanting to monitor class-specific metrics in a multi-class setting.
"
46016,TypeError: sample_chain() got an unexpected keyword argument 'seed',"Error with Tensorflow 2.0 using MCMC on MacOS 10.13.6

**The error on the console:**

```
2020-12-27 22:06:48.253835: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 22:06:48.254353: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
objc[69111]: Class zmAppHelper is implemented in both /Library/ScriptingAdditions/zOLPluginInjection.osax/Contents/MacOS/zOLPluginInjection (0x1a48eaf4f0) and /Library/Application Support/Microsoft/ZoomOutlookPlugin/zOutlookPlugin64.bundle/Contents/MacOS/zOutlookPlugin64 (0x1a490e0518). One of the two will be used. Which one is undefined.
objc[69111]: class `ERCalendarEventEditorWindowController' not linked into application
Traceback (most recent call last):
  File ""dc7.py"", line 131, in <module>
    chains, kernel_results = run_chain(initial_state)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    dc7.py:117 run_chain  *
        return tfp.mcmc.sample_chain(

    **TypeError: sample_chain() got an unexpected keyword argument 'seed'**
```

**Versions**

```
MacOS 10.13.6 High Sierra

tensorflow                2.0.0           mkl_py37hda344b4_0  
tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  
tensorflow-estimator      2.0.0              pyh2649769_0  
tensorflow-probability    0.8.0                      py_0    conda-forge
jupyter_client            6.1.7                      py_0  
jupyter_core              4.7.0            py37hecd8cb5_0  
jupyterlab_pygments       0.1.2                      py_0  
ipython                   7.19.0           py37h01d92e1_0  
ipython_genutils          0.2.0              pyhd3eb1b0_1  
python                    3.7.9                h26836e1_0  
python-dateutil           2.8.1                      py_0  
python_abi                3.7                     1_cp37m    conda-forge
```

**The source-code:**


```
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

from pprint import pprint
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

#import tensorflow as tf
#print(tf.__version__)

import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

import tensorflow_probability as tfp

sns.reset_defaults()
sns.set_context(context = 'talk', font_scale = 0.7)
plt.rcParams['image.cmap'] = 'viridis'

#%matplotlib inline

tfd = tfp.distributions
tfb = tfp.bijectors


#### ============================================

#@title Utils { display-mode: ""form"" }
def print_subclasses_from_module(module, base_class, maxwidth=80):
  import functools, inspect, sys
  subclasses = [name for name, obj in inspect.getmembers(module)
                if inspect.isclass(obj) and issubclass(obj, base_class)]
  def red(acc, x):
    if not acc or len(acc[-1]) + len(x) + 2 > maxwidth:
      acc.append(x)
    else:
      acc[-1] += "", "" + x
    return acc
  print('\n'.join(functools.reduce(red, subclasses, [])))

# Generate some data
def f(x, w):
  # Pad x with 1's so we can add bias via matmul
  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)
  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])
  result = linop.matmul(x, adjoint=True)
  return result[..., 0, :]

num_features = 2
num_examples = 50
noise_scale = .5
true_w = np.array([-1., 2., 3.])

xs = np.random.uniform(-1., 1., [num_features, num_examples])
ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)

# Visualize the data set
plt.scatter(*xs, c=ys, s=100, linewidths=0)

grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))
xs_grid = np.stack(grid, axis=0)
fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)
fs_grid = np.reshape(fs_grid, [100, 100])
plt.colorbar()
plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)
plt.show()

### Sampling the noise scale

# Define the joint_log_prob function, and our unnormalized posterior.
def joint_log_prob(w, sigma, x, y):
  # Our model in maths is
  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))
  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N

  rv_w = tfd.MultivariateNormalDiag(
    loc=np.zeros(num_features + 1),
    scale_diag=np.ones(num_features + 1))
  
  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))

  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])
  return (rv_w.log_prob(w) +
          rv_sigma.log_prob(sigma) +
          tf.reduce_sum(rv_y.log_prob(y), axis=-1))

# Create our unnormalized target density by currying x and y from the joint.
def unnormalized_posterior(w, sigma):
  return joint_log_prob(w, sigma, xs, ys)


# Create an HMC TransitionKernel
hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(
  target_log_prob_fn=unnormalized_posterior,
  step_size=np.float64(.1),
  num_leapfrog_steps=4)



# Create a TransformedTransitionKernl
transformed_kernel = tfp.mcmc.TransformedTransitionKernel(
    inner_kernel=hmc_kernel,
    bijector=[tfb.Identity(),    # w
              tfb.Invert(tfb.Softplus())])   # sigma


# Apply a simple step size adaptation during burnin
@tf.function
def run_chain(initial_state, num_results=1000, num_burnin_steps=500):
  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(
      transformed_kernel,
      num_adaptation_steps=int(.8 * num_burnin_steps),
      target_accept_prob=np.float64(.75))

  return tfp.mcmc.sample_chain(
    num_results=num_results,
    num_burnin_steps=num_burnin_steps,
    current_state=initial_state,
    kernel=adaptive_kernel,
    seed=(0, 1),
    trace_fn=lambda cs, kr: kr)


# Instead of a single set of initial w's, we create a batch of 8.
num_chains = 8
initial_state = [np.zeros([num_chains, num_features + 1]),
                 .54 * np.ones([num_chains], dtype=np.float64)]

chains, kernel_results = run_chain(initial_state)

r_hat = tfp.mcmc.potential_scale_reduction(chains)
print(""Acceptance rate:"", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())
print(""R-hat diagnostic (per w variable):"", r_hat[0].numpy())
print(""R-hat diagnostic (sigma):"", r_hat[1].numpy())

w_chains, sigma_chains = chains
```

"
46015,Cross-compilation for Raspberry Pi fails on macOS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4
- Python version: 3.7
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**

Cross-compilation for Raspberry Pi fails on macOS. The same steps work well on an Ubuntu server.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout r2.4
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

> WORKSPACE: /Users/aldemim/ws/tensorflow
> CI_DOCKER_BUILD_EXTRA_PARAMS: 
> CI_DOCKER_EXTRA_PARAMS: 
> COMMAND: tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
> CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured pi-python37
> CONTAINER_TYPE: pi-python37
> BUILD_TAG: tf_ci
>   (docker container name will be tf_ci.pi-python37)
> 
> Building container (tf_ci.pi-python37)...
> [+] Building 2.0s (18/18) FINISHED                                                                                                                                                                       
>  => [internal] load build definition from Dockerfile.pi-python37                                                                                                                                    0.0s
>  => => transferring dockerfile: 49B                                                                                                                                                                 0.0s
>  => [internal] load .dockerignore                                                                                                                                                                   0.0s
>  => => transferring context: 2B                                                                                                                                                                     0.0s
>  => [internal] load metadata for docker.io/library/ubuntu:16.04                                                                                                                                     1.9s
>  => [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                       0.0s
>  => [ 1/12] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5fe9742042a2f10b257c908fbdfac81912a16eb463879                                                                             0.0s
>  => [internal] load build context                                                                                                                                                                   0.0s
>  => => transferring context: 1.74kB                                                                                                                                                                 0.0s
>  => CACHED [ 2/12] COPY install/*.sh /install/                                                                                                                                                      0.0s
>  => CACHED [ 3/12] RUN /install/install_bootstrap_deb_packages.sh                                                                                                                                   0.0s
>  => CACHED [ 4/12] RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:george-edison55/cmake-3.x                                                                           0.0s
>  => CACHED [ 5/12] RUN /install/install_deb_packages.sh                                                                                                                                             0.0s
>  => CACHED [ 6/12] RUN /install/install_pi_python3x_toolchain.sh ""3.7""                                                                                                                              0.0s
>  => CACHED [ 7/12] RUN /install/install_bazel.sh                                                                                                                                                    0.0s
>  => CACHED [ 8/12] RUN /install/install_proto3.sh                                                                                                                                                   0.0s
>  => CACHED [ 9/12] RUN /install/install_buildifier.sh                                                                                                                                               0.0s
>  => CACHED [10/12] RUN /install/install_auditwheel.sh                                                                                                                                               0.0s
>  => CACHED [11/12] RUN /install/install_golang.sh                                                                                                                                                   0.0s
>  => CACHED [12/12] COPY install/.bazelrc /etc/bazel.bazelrc                                                                                                                                         0.0s
>  => exporting to image                                                                                                                                                                              0.0s
>  => => exporting layers                                                                                                                                                                             0.0s
>  => => writing image sha256:0102277c114ff3d600ff1e82e705a7522bf542d9a22125c9c3fa391b0838979b                                                                                                        0.0s
>  => => naming to docker.io/library/tf_ci.pi-python37                                                                                                                                                0.0s
> Running 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...
> Reading package lists...
> Building dependency tree...
> Reading state information...
> sudo is already the newest version (1.8.16-0ubuntu1.9).
> 0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.
> dialout:x:20:
> adduser: Warning: The home directory `/Users/aldemim/ws/tensorflow/bazel-ci_build-cache' does not belong to the user you are currently creating.
> /workspace /workspace
> You have bazel 3.1.0 installed.
> Found possible Python library paths:
>   /usr/lib/python3/dist-packages
>   /usr/local/lib/python3.7/dist-packages
> Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]
> Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.
> 
> Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.
> 
> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
>         --config=mkl            # Build with MKL support.
>         --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
>         --config=monolithic     # Config for mostly static monolithic build.
>         --config=ngraph         # Build with Intel nGraph support.
>         --config=numa           # Build with NUMA support.
>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
>         --config=v2             # Build TensorFlow 2.x instead of 1.x.
> Preconfigured Bazel build configs to DISABLE default on features:
>         --config=noaws          # Disable AWS S3 filesystem support.
>         --config=nogcp          # Disable GCP support.
>         --config=nohdfs         # Disable HDFS support.
>         --config=nonccl         # Disable NVIDIA NCCL support.
> /workspace
> Extracting Bazel installation...
> TF_BUILD_INFO = {container_type: ""pi-python37"", command: ""tensorflow/tools/ci_build/pi/build_raspberry_pi.sh"", source_HEAD: ""5485ec964e7d51b5e51139943e02f8241aa56cfa"", source_remote_origin: ""https://github.com/tensorflow/tensorflow.git"", OS: ""Linux"", kernel: ""4.19.121-linuxkit"", architecture: ""x86_64"", processor: ""Intel(R) Core(TM) i7-8569U CPU @ 2.80GHz"", processor_count: ""4"", memory_total: ""2036452 kB"", swap_total: ""1048572 kB"", Bazel_version: ""Build label: 3.1.0"", Java_version: ""1.8.0_275"", Python_version: ""2.7.12"", gpp_version: ""g++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: """", CUDA_device_count: ""0"", CUDA_device_names: """", CUDA_toolkit_version: """"}
> You have bazel 3.1.0 installed.
> Found possible Python library paths:
>   /usr/local/lib/python3.7/dist-packages
>   /usr/lib/python3/dist-packages
> Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.7/dist-packages]
> Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.
> 
> Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.
> 
> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
>         --config=mkl            # Build with MKL support.
>         --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
>         --config=monolithic     # Config for mostly static monolithic build.
>         --config=ngraph         # Build with Intel nGraph support.
>         --config=numa           # Build with NUMA support.
>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
>         --config=v2             # Build TensorFlow 2.x instead of 1.x.
> Preconfigured Bazel build configs to DISABLE default on features:
>         --config=noaws          # Disable AWS S3 filesystem support.
>         --config=nogcp          # Disable GCP support.
>         --config=nohdfs         # Disable HDFS support.
>         --config=nonccl         # Disable NVIDIA NCCL support.
> Configuration finished
> Building for the Pi Two/Three, with NEON acceleration
> INFO: Options provided by the client:
>   Inherited 'common' options: --isatty=0 --terminal_columns=80
> INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
>   Inherited 'common' options: --color=yes
> INFO: Reading rc options for 'build' from /workspace/.bazelrc:
>   Inherited 'common' options: --experimental_repo_remote_exec
> INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:
>   'build' options: --verbose_failures --spawn_strategy=standalone --strategy=Genrule=standalone
> INFO: Reading rc options for 'build' from /workspace/.bazelrc:
>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
> INFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:
>   'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/dist-packages --python_path=/usr/local/bin/python3.7 --action_env TF_CONFIGURE_IOS=0
> INFO: Found applicable config definition build:short_logs in file /workspace/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
> INFO: Found applicable config definition build:v2 in file /workspace/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
> INFO: Found applicable config definition build:monolithic in file /workspace/.bazelrc: --define framework_shared_object=false
> INFO: Found applicable config definition build:linux in file /workspace/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
> INFO: Found applicable config definition build:dynamic_kernels in file /workspace/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
> Loading: 
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> Loading: 0 packages loaded
> DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
> DEBUG: Repository io_bazel_rules_go instantiated at:
>   no stack (--record_rule_instantiation_callstack not enabled)
> Repository rule git_repository defined at:
>   /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
> Loading: 0 packages loaded
> Analyzing: 4 targets (3 packages loaded, 0 targets configured)
> Analyzing: 4 targets (60 packages loaded, 36 targets configured)
> DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
> DEBUG: Repository io_bazel_rules_docker instantiated at:
>   no stack (--record_rule_instantiation_callstack not enabled)
> Repository rule git_repository defined at:
>   /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
> Analyzing: 4 targets (183 packages loaded, 3775 targets configured)
> 
> Analyzing: 4 targets (194 packages loaded, 3818 targets configured)
> 
> Analyzing: 4 targets (196 packages loaded, 3818 targets configured)
> Analyzing: 4 targets (196 packages loaded, 3818 targets configured)
> INFO: Repository aws instantiated at:
>   no stack (--record_rule_instantiation_callstack not enabled)
> Repository rule third_party_http_archive defined at:
>   /workspace/third_party/repo.bzl:216:28: in <toplevel>
> WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
> ERROR: An error occurred during the fetch of repository 'aws':
>    java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)
> INFO: Repository aarch64_compiler instantiated at:
>   no stack (--record_rule_instantiation_callstack not enabled)
> Repository rule tf_http_archive defined at:
>   /workspace/third_party/repo.bzl:131:19: in <toplevel>
> ERROR: /workspace/tensorflow/tools/pip_package/BUILD:175:1: //tensorflow/tools/pip_package:licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@aws//': java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)
> INFO: Elapsed time: 357.129s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (206 packages loaded, 3958 targets configured)
> "
46014,How do I pass an array of bytes instead of an array of shorts,"Good evening, I started to deal with this example https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android but ran into a problem. Help me please, I want to pass an array of bytes, not shorts.
```
class MainActivity : AppCompatActivity() {

    companion object {

        private const val TAG = ""MainActivity""
        private const val SAMPLE_RATE = 16_000
        private const val BUFFER_SIZE_SECONDS = 0.3F
        private const val DETECTION_THRESHOLD = 0.50F
        private const val SUPPRESSION_MS = 1500
        private const val MINIMUM_COUNT = 3
        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L
        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L
        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200
    }

    private val labels = listOf(""_silence_"", ""_unknown_"", ""yes"", ""no"", ""up"", ""down"", ""left"", ""right"", ""on"", ""off"", ""stop"", ""go"")
    private val tfLiteOptions = Interpreter.Options()
    private val recordingBufferLock = ReentrantLock()

    private var recordingOffset = 0
    private var shouldContinue = true
    private var recordingThread: Thread? = null
    private var shouldContinueRecognition = true
    private var recognitionThread: Thread? = null
    private var recordingBuffer = ByteArray(SAMPLE_RATE)

    private lateinit var tfLite: Interpreter
    private lateinit var tfLiteModel: MappedByteBuffer
    private lateinit var recognizeCommands: RecognizeCommands

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        recognizeCommands = RecognizeCommands(
            labels,
            AVERAGE_WINDOW_DURATION_MS,
            DETECTION_THRESHOLD,
            SUPPRESSION_MS,
            MINIMUM_COUNT,
            MINIMUM_TIME_BETWEEN_SAMPLES_MS
        )

        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {
            initTfLite()
        } else {
            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)
        }
    }

    private fun initTfLite() {
        try {
            tfLiteModel = loadModelFile()
            tfLite = Interpreter(tfLiteModel, tfLiteOptions)

            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))
            tfLite.resizeInput(1, intArrayOf(1))

            startRecording()
            startRecognition()
        } catch (exc: IOException) {
            Log.e(TAG, ""Error: ${exc.message}"")
        }
    }

    private fun startRecording() {
        if (recordingThread == null) {
            shouldContinue = true
            recordingThread = Thread { record() }
            recordingThread?.start()
        }
    }

    private fun record() {
        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)

        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2
        val record = AudioRecord(
            MediaRecorder.AudioSource.DEFAULT,
            SAMPLE_RATE,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            bufferSize
        )

        if (record.state != AudioRecord.STATE_INITIALIZED) {
            Log.e(TAG,""Audio Record can't initialize!"")
            return
        }

        record.startRecording()

        while (shouldContinue) {
            val audioBuffer = ByteArray(bufferSize)
            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)
            val newRecordingOffset = recordingOffset + numberRead
            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)
            val firstCopyLength = numberRead - secondCopyLength

            recordingBufferLock.lock()
            try {
                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)
                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)
                recordingOffset = newRecordingOffset % recordingBuffer.size
            } finally {
                recordingBufferLock.unlock()
            }
        }

        record.stop()
        record.release()
    }

    private fun startRecognition() {
        if (recognitionThread == null) {
            shouldContinueRecognition = true
            recognitionThread = Thread { recognize() }
            recognitionThread?.start()
        }
    }

    private fun recognize() {
        val inputBuffer = ByteArray(SAMPLE_RATE)
        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }
        val outputScores = Array(1) { FloatArray(labels.size) }
        val sampleRateList = intArrayOf(SAMPLE_RATE)

        while (shouldContinueRecognition) {
            recordingBufferLock.lock()

            try {
                val maxLength = recordingBuffer.size
                val firstCopyLength = maxLength - recordingOffset
                val secondCopyLength = recordingOffset
                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)
                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)
            } finally {
                recordingBufferLock.unlock()
            }

            for (i in 0 until SAMPLE_RATE) {
                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()
            }

            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)
            val outputMap: MutableMap<Int, Any> = HashMap()
            outputMap[0] = outputScores

            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)

            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())

            if (!result.foundCommand.startsWith(""_"") && result.isNewCommand) {
                Log.d(TAG, ""Command: ${result.foundCommand} (${result.score})"")
            }

            try {
                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)
            } catch (exc: InterruptedException) {
                Log.d(TAG, ""Error: ${exc.message}"")
            }
        }
    }

    @Throws(IOException::class)
    private fun loadModelFile(): MappedByteBuffer {
        val fileDescriptor = assets.openFd(""conv_actions_frozen.tflite"")
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    override fun onRequestPermissionsResult(
        requestCode: Int,
        permissions: Array<out String>,
        grantResults: IntArray
    ) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults)
        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {
            initTfLite()
        } else {
            Toast.makeText(this, ""ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ"", Toast.LENGTH_LONG).show()
            finish()
        }
    }
}
```"
46012,tf.function retracing,"**System information**
- OS: Ubuntu 18.04.5 LTS (Google Colab)
- tf version: 2.4.0
- tf git version: v2.4.0-0-g582c8d236cb

Code:
```
import tensorflow as tf
import numpy as np
from keras import *
from keras.layers import *

X = np.random.uniform(-1, 1, size = (1, 1000))
y = np.array([[0.7974]])

for _ in range(6):
  model = Sequential([
    Input(shape = 1000),
    Dense(1, activation = 'sigmoid'),
  ])

  model.compile(loss = 'mse', optimizer = 'adam')
  model.fit(X, y, batch_size = 1, epochs = 100, verbose = 0)
  print(model.predict(X))
```

Output:
```
[[0.7982576]]
[[0.7960699]]
[[0.7987139]]
[[0.79762185]]
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dc45f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[[0.79733586]]
WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dbbf3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[[0.79833543]]
```

Is this warning a bug, or am I doing something wrong?"
46010,Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 ,"**Descriptions**
We successfully converted the Matterpot Mask RCNN object detection model (using hdf5 weight file) into tflite format using tensorflow (version 2.3.0). We got the prediction from the converted tflite model using python API of tf-nightly () due to Flex delegate issue raised when using tensorflow version 2.3.0.

**Converted tflite model link** : https://drive.google.com/file/d/1kJjQnuf5FYdn0DcEAIkeMoWFXZFuM2eb/view?usp=sharing

When we tried to get the prediction using our converted tflite model using the android application (https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) but the following error is raised at **runForMultipleInputsOutputs()** function.

**Error raised in android :**
```
2020-12-28 11:30:01.263 6838-6857/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 6838
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.
    No
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:334)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)
        at android.os.Handler.handleCallback(Handler.java:761)
        at android.os.Handler.dispatchMessage(Handler.java:98)
        at android.os.Looper.loop(Looper.java:156)
        at android.os.HandlerThread.run(HandlerThread.java:61)
```

**Used python code used for converting the model into tflite format**
```
model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)
keras_model = `model.keras_model`
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.optimizations = [ tf.lite.Optimize.DEFAULT ]
tflite_model = converter.convert()
```


**Java code used in android studio to the get the prediction**
We tested the code for single image and set input tensors accordly (images, img_metas and anchors).

```
// File:  org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel

Interpreter.Options options = new Interpreter.Options();

// Model file is loaded from assets.
Interpreter Tflite = new Interpreter(modelFile, options);

float[][][][] images = new float[1][1024][1024][3];
float[][] img_metas = new float[1][14];
float[][][] anchors = new float[1][261888][4];
// above arrays are populated. 

Object[] inputArray = {images, img_metas, anchors};
tensor0 = new float[1][1][1];
tensor1 = new float[1][1000][2][4];
tensor2 = new float[1][1000][2];
tensor3 = new float[1][100][6];
tensor4 = new float[1][100][28][28][2];
tensor5 = new float[1][1][4];
tensor6 = new float[1][1][2];
Map<Integer, Object> outputMap = new HashMap<>();
    outputMap.put(0, tensor0);
    outputMap.put(1, tensor1);
    outputMap.put(2, tensor2);
    outputMap.put(3, tensor3);
    outputMap.put(4, tensor4);
    outputMap.put(5, tensor5);
    outputMap.put(6, tensor6);
tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
```

"
46009,ADDING DARK MODE TO TensorFlow WEBSITE,"**   FEATURE REQUEST FOR DARK MODE ON WEBSITE **

As the bright interphase of our website affects our eyes i would request to put an dark mode feature to our TensorFlow website.
  "
46008,SparseTensor mul. broadcasting gradient fails,"## System information
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219
- Python version: 3.7

## Current behavior

Computing gradients of scaled `tf.SparseTensor`s fails. This is resolved be adding dimensions to scalar.

## Expected behavior

Gradient computation compatible with automatic dimension adding when broadcasting leading dimensions.

## Standalone code to reproduce the issue

[Notebook](https://colab.research.google.com/drive/1R-HV0570iNzbY3yUGXSuqnikPWZd6aDO?usp=sharing)

Code copied below for convenience

```python
import tensorflow as tf

n = 5
values = tf.Variable(tf.random.uniform((n,)))
indices = tf.sparse.eye(n).indices
with tf.GradientTape() as tape:
    tape.watch(values)
    st = tf.SparseTensor(indices, values, (n, n))
    st = st * 2.                        # doesn't work
    # st = st * tf.reshape(2., (1, 1))  # works
    loss = tf.sparse.reduce_sum(st)

grad = tape.gradient(loss, values)
print(grad)
```

## Stack Trace

```txt
Traceback (most recent call last):
  File ""main.py"", line 13, in <module>
    grad = tape.gradient(loss, values)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 162, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py"", line 247, in _SparseDenseCwiseMulGrad
    return _SparseDenseCwiseMulOrDivGrad(op, grad, True)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py"", line 227, in _SparseDenseCwiseMulOrDivGrad
    dense_vals = array_ops.gather_nd(y, scaled_indices)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 5348, in gather_nd
    return gen_array_ops.gather_nd(params, indices, name=name)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3695, in gather_nd
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6870, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: params must be at least a vector [Op:GatherNd]
```"
46007,"Error on TPU V3-8 with XLA: RPC failed with status = ""Unavailable: Socket closed""","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I am using the XLA example of RoBERTa Pytorch-XLA https://cloud.google.com/tpu/docs/tutorials/roberta-pytorch
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian-9-torch-xla-v20201225 (GCP image)
- GCP Machine type: Custom 8vCPUs, 256GB memory
- TensorFlow installed from (source or binary): Provided on GCP image
- TensorFlow version (use command below): torch-xla-1.7
- Python version: Python 3.6.10 :: Anaconda, Inc.
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: TPU V3-8

**Describe the current behavior**

The following error is happening after training with Pythorch-XLA  for some time: 

```
2020-12-28 01:56:05.252085: W    1417 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.251970000"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC


*** Begin stack trace ***
        tensorflow::CurrentStackTrace()
        xla::XrtComputationClient::ReleaseHandles(std::vector<xla::XrtComputationClient::DeviceHandle, std::allocator<xla::XrtComputationClient::DeviceHandle> >*, std::function<xla::XrtSession::CachedNode const& (xla::XrtSession*, tensorflow::Scope const&, std::string const&)> const&, xla::metrics::Metric*, xla::metrics::Counter*)
        xla::XrtComputationClient::HandleReleaser()
        xla::util::TriggeredTask::Runner()


        clone
*** End stack trace ***
```


I was following the steps described here: https://cloud.google.com/tpu/docs/tutorials/roberta-pytorch with the same network parameters, just a different dataset.  
I encountered an error before, but it was happening due to OOM on the VM after restoring a checkpoint, which reason why I increased VM memory.

It seems that the TPU is getting pre-empted somehow, but I don't have access to the runtime log as this happened overnight and it got automatically deleted by TFRC.

**Describe the expected behavior**

The training should continue as expected.


**Standalone code to reproduce the issue**
https://cloud.google.com/tpu/docs/tutorials/roberta-pytorch

Training data was around 40GB.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
| epoch 002 | training on xla:0/1:   4151 / 10099 loss=1.804, nll_loss=1.804, wps=17811, ups=0, wpb=117675.783, bsz=296.068, num_updates=14249, lr=0.000491148, gnorm=0.345, oom=0.000, wall=27948, train_wall=92610, now=01:54:20
| epoch 002 | training on xla:0/7:   4151 / 10099 loss=1.805, nll_loss=1.805, wps=17811, ups=0, wpb=117678.381, bsz=296.074, num_updates=14249, lr=0.000491148, gnorm=0.345, oom=0.000, wall=27948, train_wall=92609, now=01:54:20
| epoch 002 | training on xla:0/3:   4151 / 10099 loss=1.805, nll_loss=1.805, wps=17810, ups=0, wpb=117668.251, bsz=296.137, num_updates=14249, lr=0.000491148, gnorm=0.345, oom=0.000, wall=27949, train_wall=92608, now=01:54:20
2020-12-28 01:56:05.252059: W    1436 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.251908254"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252085: W    1417 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.251970000"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252085: W    1416 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.251940620"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252162: W    1379 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252025037"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252205: W    1438 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252117134"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252251: W    1465 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252143973"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252279: W    1483 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252130522"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252398: W    1464 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252301762"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252431: W    1428 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252333413"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252452: W    1341 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252361631"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252472: W    1400 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252380523"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252456: W    1345 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252299434"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252541: W    1378 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252405772"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252553: W    1423 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252493206"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252618: W    1397 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252489431"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
2020-12-28 01:56:05.252674: W    1480 tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:160] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1609120565.252561700"",""description"":""Error received from peer ipv4:10.180.83.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC
terminate called after throwing an instance of 'std::runtime_error'
  what():  tensorflow/compiler/xla/xla_client/xrt_computation_client.cc:1110 : Check failed: session->session()->Run( feed_inputs, {}, {cached_node.operations[0]}, &outputs) == ::tensorflow::Status::OK() (Aborted: Session a57840b79b1bd972 is not found. vs. OK)
*** Begin stack trace ***
        tensorflow::CurrentStackTrace()
        xla::XrtComputationClient::ReleaseHandles(std::vector<xla::XrtComputationClient::DeviceHandle, std::allocator<xla::XrtComputationClient::DeviceHandle> >*, std::function<xla::XrtSession::CachedNode const& (xla::XrtSession*, tensorflow::Scope const&, std::string const&)> const&, xla::metrics::Metric*, xla::metrics::Counter*)
        xla::XrtComputationClient::HandleReleaser()
        xla::util::TriggeredTask::Runner()


        clone
*** End stack trace ***
```
  
  



Currently, I resumed training on another TPU node, but checking the memory usage it seems it is increasing at each training step. Is it possible that OOM happened on the TPU and it got unavailable?

![image](https://user-images.githubusercontent.com/14294190/103204324-039df800-48ef-11eb-98cb-8feca300d76a.png)

"
46006,"ConverterError: <unknown>:0: error: loc(""lstm_bias_lstm_17""): is not immutable (RNN)","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): 2.4.0


**Command used to run the converter or code if youâ€™re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
`import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()`

**The output from the converter invocation**
`---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    212                                                  debug_info_str,
--> 213                                                  enable_mlir_converter)
    214       return model_str

4 frames
Exception: <unknown>:0: error: loc(""lstm_bias_lstm_17""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    214       return model_str
    215     except Exception as e:
--> 216       raise ConverterError(str(e))
    217 
    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(""lstm_bias_lstm_17""): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable`

"
46005,windows 10 build failed (ERROR: An error occurred during the fetch of repository 'local_config_cuda':),"I've tried to install tensorflow step by step from [source_windows](https://www.tensorflow.org/install/source_windows)
but got error
**System information**
- OS Platform and Distribution : windows10 x64
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.7.7
- Bazel version : 3.7.2
- GCC/Compiler version (if compiling from source): visual studio 2019
- CUDA/cuDNN version: 10.2
- GPU model and memory: 960m, 4G

output of `python ./configure.py`:
```
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is C:\Users\127051\AppData\Local\Programs\Python\Python37\python.exe]:


Found possible Python library paths:
  C:\Users\127051\AppData\Local\Programs\Python\Python37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\127051\AppData\Local\Programs\Python\Python37\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.2 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.0


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```
Install error:
```
bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=164
INFO: Reading rc options for 'build' from d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe
INFO: Reading rc options for 'build' from d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from d:\softwareinstaltion\tensorflow-2.4.0\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\softwareinstaltion\tensorflow-2.4.0\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  D:/softwareinstaltion/tensorflow-2.4.0/WORKSPACE:19:16: in <toplevel>
  D:/softwareinstaltion/tensorflow-2.4.0/tensorflow/workspace.bzl:96:19: in tf_repositories
Repository rule cuda_configure defined at:
  D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl:1430:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl"", line 1400, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl"", line 1244, column 56, in _create_local_cuda_repository
                host_compiler_includes + _cuda_include_path(
        File ""D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl"", line 364, column 32, in _cuda_include_path
                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + ""/include""))
        File ""D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl"", line 277, column 19, in realpath
                return execute(repository_ctx, [bash_bin, ""-c"", ""realpath \""%s\"""" % path]).stdout.strip()
        File ""D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl"", line 217, column 13, in execute
                fail(
Error in fail: Repository command failed
/usr/bin/realpath: missing operand
Try '/usr/bin/realpath --help' for more information.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed
/usr/bin/realpath: missing operand
Try '/usr/bin/realpath --help' for more information.
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Repository command failed
/usr/bin/realpath: missing operand
Try '/usr/bin/realpath --help' for more information.
INFO: Elapsed time: 1.781s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package

```"
46003,keras.Model.fit error with 2.4 and TPUv3,"My code is worked in 2.3 and 2.4 with GPU. And it is failed when doing ""keras.Model.fit"" in Cloud TPU with 2.4, but worked in 2.3.1 and nightly.

In the past, this error is only encountered when the TensorFlow version in TPU does not fully matched the version in VM."
46002,TFlite Poor Performance on Pixel 3 CPU when channel not multiple of 4,"**System information**
- Have I written custom code: Only to generate conv2d tflite models
- Host OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Pixel 3
- TensorFlow installed from (source or binary): [native benchmark binary](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary) nightly pre-built (no Flex delegate) android_aarch64
- Host TensorFlow version: 2.3.0
- Mobile device TensorFlow version: nightly
- Python version: 3.8.3
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
The tflite conv2d op performs poorly on Pixel 3 CPU when the input or output channel numbers are not multiples of 4. Selective inference latency measured on Pixel 3 is shown in the following table to support 3 major observations:

|In/Out Channel|Latency|In/Out Channel|Latency|In/Out Channel|Latency|
|-|-|-|-|-|-|
|16/14|21760.5|30/43|137299.0|4/44|14541.9|
|16/15|24486.5|31/43|166752.0|44/4|38141.8|
|16/16|21479.2|32/43|86515.3|11/43|67771.0|
|16/17|24136.2|33/43|156961.0|43/11|72423.9|
|16/18|26734.9|34/43|152460.0|22/30|66271.4|
|44/37|26193.0|35/43|158265.0|30/22|77171.4|
|44/64|124600.0|36/43|112036.0|

1. The first two columns show that with a fixed input channel, output channels of no multiples of 4 (no4x) perform even worse than those of more channels but of multiples of 4 (4x). Notably, 44/37 has a larger latency than 44/64.
2. Similarly, the second two columns show that with a fixed output channel, no4x input channels perform worse than 4x input channels. Notably, 30/43 has a larger latency than 36/43.
3. The last two columns show that input and output channels are not created equal. Regardless of 4x or no4x, input channels turn to induce greater latency than output channels.

**Describe the expected behavior**
[TFlite guide on GPU](https://www.tensorflow.org/lite/performance/gpu_advanced#tips_and_tricks) mentions:

> On a GPU, tensor data is sliced into 4-channels. Thus, a computation on a tensor of shape [B, H, W, 5] will perform about the same on a tensor of shape [B, H, W, 8], but significantly worse than [B, H, W, 4].

Our questions:

1. Should CPU observe the same behavior as the above-described GPU case?
2. The above example seems to indicate that the latency of a no4x channel should perform about the same as when it is rounded up to the next 4x, but in our measurements, it is not the case. no4x can sometimes perform worse than much larger 4x. Is there any reasoning behind this?
3. In our third observation, input channel impacts latency more than output channel. Is there any reasoning behind this?

**Standalone code to reproduce the issue**
The conv2d models used for the benchmark are defined with keras and converted to tflite with the following code:
```
# TF model generation
model = Sequential()
model.add(Conv2D(output_channel, (3, 3), padding=""same""))
model.build(input_shape=(1, 180, 180, input_channel))
shutil.rmtree(""./temp"")
save_model(model, ""./temp"")
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=""./temp"", signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
tflite_model = converter.convert()
tf_filename = 'b{}_ic{}_oc{}_ih{}_iw{}_tf.tflite'.format(1, input_channel, output_channel, 180, 180)
with open(model_dir + tf_filename, 'wb') as f:
    f.write(tflite_model)
```
Where `input_channel` and `output_channel` are variables, batch size is fixed to 1, input shape is fixed to 180x180, kernel size is 3x3, and stride is 1. In this way, the generated tflite models contain only a single conv2d operator with various input and output channel numbers. The models are then loaded to Pixel 3 phones for the benchmark.

**Other info / logs**
The inference latency was measured using the [native benchmark binary](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary) nightly pre-built (no Flex delegate) android_aarch64 with the following command:
`adb shell taskset f0 /data/local/tmp/android_aarch64_benchmark_model --graph=/data/local/tmp/{model}.tflite --num_threads=1 --num_runs=100`
In the report generated by the benchmark binary, `inference (avg)` is kept as the inference latency of a conv2d model. For example, when the report contains
`Inference timings in us: Init: 8241, First inference: 4084, Warmup (avg): 1584.61, Inference (avg): 1573.36`
1573.36 is kept as the latency for the model.
"
46001,Weight Normalization layer doesn't work with mixed precision,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minimal
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 'v2.4.0-rc4-71-g582c8d236cb'
- Python version: 3.8
- CUDA/cuDNN version: CUDA 11  cudnn 8
- GPU model and memory: RTX 2080 11gb

**Describe the current behavior**
when using mixed precision and the weight normalization layer from tensorflow_addons, it throws an error 
`    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'sequential/weight_normalization/cond/data_dep_init/mul:0' shape=(10,) dtype=float16>`
from
```
assign_tensors = self._data_dep_init(inputs)
    \tensorflow_addons\layers\wrappers.py:204 _data_dep_init
        g_tensor = self.g.assign(self.g * scale_init)
    \tensorflow\python\keras\mixed_precision\autocast_variable.py:225 assign
```


**Describe the expected behavior**
layer to build and run normally as it does in float32 and float16 policies


**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np

policy = tf.keras.mixed_precision.Policy(""mixed_float16"")
#runs fine with all the same type
#policy = tf.keras.mixed_precision.Policy(""float16"")

tf.keras.mixed_precision.set_global_policy(policy)

model = tf.keras.Sequential()

model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(10, activation=""relu"")))

model.compile(loss=tf.keras.losses.MeanSquaredError())

X = np.random.randn(1,20)
Y = np.random.randn(1,10)

model.fit(X,Y)
```



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

putting a tf.cast inside each call to assign in wrappers.py -> WeightNormalization stops the error, but I'm not certain if that is correct from a numerical stability perspective, and it seems like adding some functionality to the assign function in mixedPrecision/autocast_variable.py -> AutocastVariable would be better"
46000,Incorrect conda dependency information for tensorflow-gpu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro + Anaconda
- TensorFlow version: 2.3.0
- Python version: 3.8.5
- Installed using conda
- CUDA/cuDNN version: 11.0


**Describe the problem**

Tensorflow 2.3.0 is only compatible with CUDA 10.x but not 11.0. But a conda installation is unable to figure out the incompatibility during installation.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Minimal example to reproduce the problem:

`conda create --name tf_gpu python=3.8 tensorflow-gpu cudatoolkit --dry-run`

As shown in the list, CUDA 11.0 will be installed with tensorflow 2.3.0.

**Any other info / logs**
There is another problem of the default conda dependency: it differs (and conflicts on certain constraints) with the pip dependency. Details of tensorflow 2.3.0 are shown in the following.

<table style=""width:100%"">
  <tr>
    <th>conda dependency</th>
    <th>pip dependency</th>
  </tr>
  <tr>
    <td>gast >=0.4.0,<0.4.1.0a0</td>
    <td>gast==0.3.3</td>
  </tr>
  <tr>
    <td>scipy >=1.5.2</td>
    <td>scipy==1.4.1</td>
  </tr>
  <tr>
    <td>keras-preprocessing >=1.1.0</td>
    <td>keras-preprocessing<1.2,>=1.1.1</td>
  </tr>
  <tr>
    <td>numpy >=1.16.6,<2.0a0</td>
    <td>numpy<1.19.0,>=1.16.0</td>
  </tr>
</table>

If only one of `conda` or `pip` is used, it would be fine. However, in some cases I may need to use `pip` to install packages in a conda environment. If I need to install a package built on top of `tensorflow` using `pip`, a conflict would cause problems.

It would be nice if this problem could also be fixed.

"
45999,Bug in Transpose Convolution Padding,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **2.3.0**
- Python version: **3.7.6**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **10.2**
- GPU model and memory: **Nvidia GeForce GTX 1080 8GB**

**Describe the current behavior**
Padding for transpose convolutions under certain parameter combinations fails to distribute the padding as expected. This can, in extreme cases, lead to highly skewed outputs. This can happen regardless of whether the filter or signal are even or odd.

Here, the output is subtly different but quite simple to verify that it must have been generated with a padding of [1, 0] rather than [0, 1]:
```Python
TESTING 2 x 2 INPUT WITH 2 x 2 KERNEL, (1, 1) STRIDE AND ""SAME"" PADDING:
Input:
[[1 1]
 [1 1]]

Kernel:
[[1 1]
 [1 1]]


Dilated Input: (=> 2 x 2 array)
[[1 1]
 [1 1]]

Dilated Input + Padding: (=> 3 x 3 array)
[[1 1 0]
 [1 1 0]
 [0 0 0]]


Manual Result: (=> 2 x 2 array)
[[4 2]
 [2 1]]

Standard Result: (=> 2 x 2 array)
[[1 2]
 [2 4]]
```

Here's an example with an odd kernel and valid padding instead:
```Python
TESTING 2 x 2 INPUT WITH 3 x 3 KERNEL, (5, 5) STRIDE AND ""VALID"" PADDING:
Input:
[[1 1]
 [1 1]]

Kernel:
[[1 1 1]
 [1 1 1]
 [1 1 1]]


Dilated Input: (=> 6 x 6 array)
[[1 0 0 0 0 1]
 [0 0 0 0 0 0]
 [0 0 0 0 0 0]
 [0 0 0 0 0 0]
 [0 0 0 0 0 0]
 [1 0 0 0 0 1]]

Dilated Input + Padding: (=> 12 x 12 array)
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]


Manual Result: (=> 10 x 10 array)
[[0 0 0 0 0 0 0 0 0 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 1 1 1 0 0 1 1 1 0]
 [0 0 0 0 0 0 0 0 0 0]]

Standard Result: (=> 10 x 10 array)
[[1 1 1 0 0 1 1 1 0 0]
 [1 1 1 0 0 1 1 1 0 0]
 [1 1 1 0 0 1 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [1 1 1 0 0 1 1 1 0 0]
 [1 1 1 0 0 1 1 1 0 0]
 [1 1 1 0 0 1 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
```

Here's an example with an odd signal and even kernel:
```Python
TESTING 3 x 3 INPUT WITH 2 x 2 KERNEL, (4, 4) STRIDE AND ""VALID"" PADDING:
Input:
[[1 1 1]
 [1 1 1]
 [1 1 1]]

Kernel:
[[1 1]
 [1 1]]


Dilated Input: (=> 9 x 9 array)
[[1 0 0 0 1 0 0 0 1]
 [0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0]
 [1 0 0 0 1 0 0 0 1]
 [0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0]
 [1 0 0 0 1 0 0 0 1]]

Dilated Input + Padding: (=> 13 x 13 array)
[[0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]]


Manual Result: (=> 12 x 12 array)
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 1 1 0 0 1 1 0 0 1 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]

Standard Result: (=> 12 x 12 array)
[[1 1 0 0 1 1 0 0 1 1 0 0]
 [1 1 0 0 1 1 0 0 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [1 1 0 0 1 1 0 0 1 1 0 0]
 [1 1 0 0 1 1 0 0 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [1 1 0 0 1 1 0 0 1 1 0 0]
 [1 1 0 0 1 1 0 0 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]
```

Finally, here the skew is quite large and noticeable due to the large amount of padding, which clearly seems like it could only have stemmed from the padding being allocated entirely to the bottom and right only:
```Python
TESTING 2 x 2 INPUT WITH 2 x 2 KERNEL, (6, 6) STRIDE AND ""SAME"" PADDING:
Input:
[[1 1]
 [1 1]]

Kernel:
[[1 1]
 [1 1]]


Dilated Input: (=> 7 x 7 array)
[[1 0 0 0 0 0 1]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0]
 [1 0 0 0 0 0 1]]

Dilated Input + Padding: (=> 13 x 13 array)
[[0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0]]


Manual Result: (=> 12 x 12 array)
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 1 0 0 0 0 1 1 0 0]
 [0 0 1 1 0 0 0 0 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 1 1 0 0 0 0 1 1 0 0]
 [0 0 1 1 0 0 0 0 1 1 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]

Standard Result: (=> 12 x 12 array)
[[1 1 0 0 0 0 1 1 0 0 0 0]
 [1 1 0 0 0 0 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [1 1 0 0 0 0 1 1 0 0 0 0]
 [1 1 0 0 0 0 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]
```

**Describe the expected behavior**
Padding for transpose convolutions should be distributed evenly with at most an extra zero on the right when not evenly divisible by 2. This is TensorFlow convention for regular convolutions, but not here, oddly enough.

All of my examples are easy to compute and verify by hand. It is not clear what the exact method of allocating the padding is for transpose convolutions, as the documentation is quite scarce. Just to clarify, my code does actually match up for many parameter combinations I did not show here for brevity, but it disagrees with enough to make me suspicious that something's up. I would love it if someone could explain the algorithm better!

**Standalone code to reproduce the issue**
I have provided code to replicate the above examples below:
```Python
# Import modules
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose

# Define convenience functions
def dilate_array(x, kernel, dilation_rate, padding = 'valid'): # manually dilate array by inserting zeros
    # Get padding for after dilation
    rank = len(x)
    x = x[None, ..., None]
    paddings, _ = get_deconv_padding(x, kernel, strides, padding = padding) # calculate BEFORE dilation
    
    # Create ""upsampled"" array via repetition then cut off extra values by slicing
    shp, strd, slc = (1, ), (slice(None, None, None), ), (slice(None, None, None), )
    dil = inp = Input(x.shape[1:])
    for i in range(rank):
        shp += (dilation_rate[i]*dil.shape[i + 1] - dilation_rate[i] + 1, )
        dil = tf.repeat(dil, dilation_rate[i], axis = i + 1)
        strd += (slice(None, None, dilation_rate[i]), )
        slc += (slice(None, shp[i + 1], None), )
    shp += (1, )
    strd += (slice(None, None, None), )
    slc += (slice(None, None, None), )
    dil = dil[slc]
    
    # Create mask to replace repeats with zeros a la ""proper"" resampling and transpose convolution
    mask = np.zeros(shp)
    mask[strd] = 1.0
    mask = tf.cast(mask, dil.dtype)
    
    # Mask dilated input
    dil = mask*dil
    
    # Pad dilated input
    pad = tf.pad(dil, paddings, mode = 'CONSTANT')
    
    # Execute dilation + padding
    model = Model(inp, [dil, pad])
    xd, xp = model.predict(x)
    xd, xp = xd[0, ..., 0], xp[0, ..., 0]
    
    return xd, xp, paddings
def deconv_padding_after_dilation(input_length, kernel_length, stride, padding = 'valid'): # adjusted padding for manual dilation (== stride)
    # Compute minimum padding length assuming manually dilated array
    output_length = stride*input_length + (max(kernel_length - stride, 0) if padding == 'valid' else 0)
    dilated_length = stride*input_length - (stride - 1)
    pad_length = output_length - dilated_length + kernel_length - 1
    
    return pad_length, output_length
def get_deconv_padding(x, kernel, strides, padding = 'valid'):
    # Calculate needed paddings with PRE-DILATED input to apply to DILATED input
    rank = len(x.shape) - 2
    paddings, output_shape = [[0, 0]], (1, )
    for i in range(rank):
        pad_length, output_length = deconv_padding_after_dilation(int(x.shape[i + 1]), kernel.shape[i], strides[i], padding = padding)
        paddings += [[pad_length//2, pad_length//2 + pad_length%2]]
        output_shape += (output_length, )
    paddings += [[0, 0]]
    output_shape += (1, )
    
    return paddings, output_shape
def execute_manual_transpose_convolution(x, kernel, strides, padding = 'valid'):
    # Manually dilate and pad output then convolve normally
    xd, xp, paddings = dilate_array(x, kernel, strides, padding = padding)
    inp = Input(xp.shape + (1, ))
    conv = Conv2D(1, kernel.shape, padding = 'valid')(inp)
    model = Model(inp, conv)
    model.set_weights([kernel[..., None, None], np.zeros((1, ))])
    y = model.predict(xp[None, ..., None])[0, ..., 0]
    
    return xd, xp, paddings, y
def execute_standard_transpose_convolution(x, kernel, strides, padding = 'valid'):
    # Use standard transpose convolution
    inp = Input(x.shape + (1, ))
    convT = Conv2DTranspose(1, kernel.shape, strides = strides, padding = padding)(inp)
    model = Model(inp, convT)
    model.set_weights([kernel[..., None, None], np.zeros((1, ))])
    y = model.predict(x[None, ..., None])[0, ..., 0]
    
    return y
```
```Python
# Define inputs
rank = 2
x = np.ones(rank*(2, ), dtype = 'float32') # CHANGE THIS
kernel = np.ones(rank*(2, ), dtype = 'float32') # CHANGE THIS
strides = rank*(1, ) # CHANGE THIS
padding = 'same' # CHANGE THIS

# Test transpose convolution
xd, xp, paddings, y_manual = execute_manual_transpose_convolution(x, kernel, strides, padding = padding)
y_standard = execute_standard_transpose_convolution(x, kernel, strides, padding = padding)

# Show results
print('TESTING {} INPUT WITH {} KERNEL, {} STRIDE AND ""{}"" PADDING:'.format(' x '.join([str(s) for s in x.shape]),
                                                                            ' x '.join([str(s) for s in kernel.shape]),
                                                                            strides,
                                                                            padding.upper()))
print('Input:')
print(x.astype(int))
print('')
print('Kernel:')
print(kernel.astype(int))
print('\n')
print('Dilated Input: (=> {} array)'.format(' x '.join([str(s) for s in xd.shape])))
print(xd.astype(int))
print('')
print('Dilated Input + Padding: (=> {} array)'.format(' x '.join([str(s) for s in xp.shape])))
print(xp.astype(int))
print('\n')
print('Manual Result: (=> {} array)'.format(' x '.join([str(s) for s in y_manual.shape])))
print(y_manual.astype(int))
print('')
print('Standard Result: (=> {} array)'.format(' x '.join([str(s) for s in y_standard.shape])))
print(y_standard.astype(int))
print('')
```

"
45998," When I use AndroidStudio to import Image Segementation TensorsorflowLite model, there is a problem...","@tensorflow/micro

**System information**
-Pixel 2xl  Android 11

**Describe the problem**
Model:
[ImageSegmentation](https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/metadata/2)

Dependenciesï¼š
```
implementation(""org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly"")
implementation(""org.tensorflow:tensorflow-lite-support:0.0.0-nightly"")
implementation(""org.tensorflow:tensorflow-lite-metadata:0.0.0-nightly"")
```

Code:
```
 val options = Model.Options.Builder()
            .setDevice(Model.Device.GPU)
            .setNumThreads(4)
            .build()
val inputImage = TensorImage.fromBitmap(originBitmap)
val model = ImageSegmentationModel.newInstance(context, options)
val result = model.process(inputImage)
result.segmentationMasksAsCategoryList
model.close()
```
Error: `java.lang.IllegalArgumentException: Label number 21 mismatch the shape on axis 1`"
45997,java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/support/image/ColorSpaceType,"Error when using ImageSegmentation in Android

Maybe my ability is not enough and I am prone to make some low-level mistakes, but this problem has really troubled me for a long time, and there is no problem with using other types of models. Thank you."
45994,TF 2.4.0 crashes on startup with IndexError assuming that sys.argv[0] exists when it may be hosted by C++ (regression from 2.3.1),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux Debian 10
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.4.0
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.7.9
- CUDA/cuDNN version: 11.0, 8.0.4 for 11.0
- GPU model and memory: NVIDIA GTX 1080

**Describe the current behavior**

TF 2.4.0 crashes on startup with:
```
File ""...\site-packages\tensorflow\python\distribute\combinations.py"", line 159, in GPUCombination
GPU_TEST = re.search(r""(test_gpu|test_xla_gpu)$"", sys.argv[0])
IndexError: list index out of range
```

**Describe the expected behavior**
sys.argv is checked to be non-empty before indexing (no bug on TF 2.3.1)

**Standalone code to reproduce the issue**

1. Ensure that PYTHONHOME is set
2. Py_Initialize()
3. Append our module's directory to the system path: (PyImport_ImportModule(""sys""), PyObject_GetAttrString(sys, ""path""), PyUnicode_FromString(""...""), PyList_Append(sysPath, ...))
4. Import our module: PyImport_ImportModule(""..."")
5. Our module calls ""import tensorflow as tf"", which then crashes

**Python-only mock repro**
```
import sys
sys.argv.clear()
import tensorflow as tf
```

**Workaround**
```
# Work around https://github.com/tensorflow/tensorflow/issues/45994
import sys
if not sys.argv:
  sys.argv.append(""(C++)"")

import tensorflow as tf
```

**Notes**

It may help to write a unit test/build verification test that mocks this scenario by clearing sys.argv before importing tensorflow."
45993,How to access operations of the tf graph from a keras model? (for tf 2.4.0),"**System information**
- TensorFlow installed from (source or binary): pypi
- TensorFlow version (use command below): 2.4.0

**Describe the current behavior**
Starting from tensorflow 2.4.0, it seems that the keras model cannot access the operations of the backend tf graph. I am looking at `keras.backend.get_session().graph`, is this the right way to access tf.graph? If I do `keras.backend.get_session().graph.get_operations()`, it returns an empty array.
Note that we can do this successfully for tf 1.x. And for tf 2.3, we can access tf graph by `model.outputs[0].graph`, however, `model.ouputs[0]` does not have `graph` for tf 2.4.
That is:
```python
# keras == 2.3.1, tensorflow == 2.3.0
from tensorflow.python import keras  
model = keras.models.Sequential()
model.add(keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
tf_operations = model.outputs[0].graph.get_operations()
print(len(tf_operations))
# result: 8

# keras == 2.3.1, tensorflow == 1.15.0
from tensorflow.python import keras  
model = keras.models.Sequential()
model.add(keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
tf_operations = keras.backend.get_session().graph.get_operations()
print(len(tf_operations))
# result: 25
```
but for tensorflow==2.4.0, if I use:
```python
tf_operations = model.outputs[0].graph.get_operations()
```
then it throws errors `AttributeError: 'KerasTensor' object has no attribute 'graph'`
and it returns 0 when I `print(len(tf_operations))` for `tf_operations = keras.backend.get_session().graph.get_operations()`"
45992,from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer,"`from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer`<br>
ImportError: No module named slim.layers.python.layers.initializers<br>
is there a new way to do this ? "
45991,Tensorflow 2.4 requires BMI2 CPU extension (AVX is not enough),"**System information**
- OS Platform and Distribution : Debian GNU/Linux 10 (buster) (and Ubuntu)
- TensorFlow installed from (source or binary): binary

```
== check python ===================================================
python version: 3.6.9
python branch: 
python build version: ('default', 'Oct  8 2020 12:12:24')
python compiler version: GCC 8.4.0
python implementation: CPython

== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Debian 4.19.160-2 (2020-11-28)
os release version: 4.19.0-13-amd64
os platform: Linux-4.19.0-13-amd64-x86_64-with-Ubuntu-18.04-bionic
linux distribution: ('Ubuntu', '18.04', 'bionic')
linux os distribution: ('Ubuntu', '18.04', 'bionic')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='90e48bf24072', release='4.19.0-13-amd64', version='#1 SMP Debian 4.19.160-2 (2020-11-28)', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== check pips ===================================================
numpy                  1.19.4
protobuf               3.14.0
tensorflow             2.4.0
tensorflow-estimator   2.4.0rc0

== check for virtualenv =========================================
False

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.4.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.6/dist-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 9, 'final', 0)
```

**Describe the current behavior**
import tensorflow fails with ""Illegal Instruction"" on a CPU with AVX

**Describe the expected behavior**
according to the documentation (https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements), all CPU with AVX are supported, so the import should not fail

**Standalone code to reproduce the issue**
python3
`>>> import tensorflow`

**Other info / logs**
The problem comes from an instruction from the ""BMI2"" extension. Output from GDB : 
```
Program received signal SIGILL, Illegal instruction.
0x00007fb5fe658c48 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tensorflow::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
```

Which points to instruction **SHLX**.

Some older CPUs do have AVX, but not BMI2 : 
- Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz
- Intel(R) Core(TM) i3-2120 CPU @ 3.30GHz

example output of lscpu : 
`Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d`

Best Regards"
45990,How to read .bin file using Tensorflow C++ api?,"Hey all,

I need to read lidar point cloud from .bin folder. But I could not able to find an API in c++ to achieve my task. could anyone help me with this? 

Tensorflow version 1.4.0

Thanks in advance.
"
45988,cannot execute exe of tensorflow py compiled with pyinstaller,"**System information**
- I used the basic example of tensorflow installing procedure:
`  import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))
`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  I don't know
- TensorFlow installed from (source or binary):
  installed via pip. pip install tensorflow and pip install --upgrade tensorflow
- TensorFlow version (use command below):
  2.4.0
- Python version:
  Python 3.8.7rc1
- Bazel version (if compiling from source):
  Not compiled from source
- GCC/Compiler version (if compiling from source):
 Not compiled from source
- CUDA/cuDNN version:
  Tried:
  1- Without CUDA
  2- CUDA 11.1, no CUDNN
  3- CUDA 11.1,  CUDNN 8.1
  4- CUDA 11.1,  CUDNN 8.1, CUDA 10.1 over CUDA 11.1
Same result
- GPU model and memory:
   NVIDIA GeForce MX250


**Describe the current behavior**
I run python with the script:
```python
import tensorflow as tf
print(tf.reduce_sum(tf.random.normal([1000, 1000])))
```
The right output is:
tf.Tensor(-29.82965, shape=(), dtype=float32)

I launch pyinstaller --onefile prova.py and this is the output:
 C:\Users\fabio\VSC>pyinstaller --onefile C:\Users\fabio\VSC\prova.py
103 INFO: PyInstaller: 4.1
103 INFO: Python: 3.8.7rc1
104 INFO: Platform: Windows-10-10.0.19041-SP0
106 INFO: wrote C:\Users\fabio\VSC\prova.spec
108 INFO: UPX is not available.
108 INFO: Extending PYTHONPATH with paths
['C:\\Users\\fabio\\VSC', 'C:\\Users\\fabio\\VSC']
131 INFO: checking Analysis
735 INFO: checking PYZ
1053 INFO: checking PKG
1061 INFO: Building because C:\Users\fabio\VSC\build\prova\prova.exe.manifest changed
1069 INFO: Building PKG (CArchive) PKG-00.pkg
22364 INFO: Building PKG (CArchive) PKG-00.pkg completed successfully.
22370 INFO: Bootloader c:\windows\system32\venv\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe
22373 INFO: checking EXE
22374 INFO: Rebuilding EXE-00.toc because pkg is more recent
22376 INFO: Building EXE from EXE-00.toc
22383 INFO: Appending archive to EXE C:\Users\fabio\VSC\dist\prova.exe
22465 INFO: Building EXE from EXE-00.toc completed successfully.

Then I run the exe and this is the output:
(venv) C:\Users\fabio\VSC>dist\prova.exe
Traceback (most recent call last):
  File ""prova.py"", line 1, in <module>
    import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))
  File ""c:\windows\system32\venv\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 493, in exec_module
    exec(bytecode, module.__dict__)
  File ""tensorflow\__init__.py"", line 41, in <module>
  File ""c:\windows\system32\venv\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 493, in exec_module
    exec(bytecode, module.__dict__)
  File ""tensorflow\python\__init__.py"", line 41, in <module>
  File ""c:\windows\system32\venv\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 493, in exec_module
    exec(bytecode, module.__dict__)
  File ""tensorflow\python\eager\context.py"", line 35, in <module>
  File ""c:\windows\system32\venv\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 493, in exec_module
    exec(bytecode, module.__dict__)
  File ""tensorflow\python\pywrap_tfe.py"", line 29, in <module>
ImportError: DLL load failed while importing _pywrap_tfe: Impossibile trovare il modulo specificato.
[20820] Failed to execute script prova

**Describe the expected behavior**
The exe file should return the tensor as the python script

**Standalone code to reproduce the issue**
Create a python script prova.py:
import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))

Run in the directory of the script
pyinstaller --onefile prova.py

Then run the exe resulting

How can I build a onefile exe from a script using tensorflow?
Thanks.
"
45987,Tensorflow 2.4  CUDA 11   CUDA_ERROR_LAUNCH_FAILED,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Win10  -2004
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**:  2.4
-   **Python version**: 3.6.1
-   **CUDA/cuDNN version**: CUDA 11.0 -  cudNN 8.0.4
-   **GPU model and memory**: GTX 1660 Super 6GB (+ GT 730 2GB) with 456.55 driver.
-   **Exact command to reproduce**: py -3.6 generate.py

### Describe the problem
Hello, I have had a problem with tensorflow and cuda for days ... the program works with the CPU but I would like to use the latter with my GPU. However, the program refuses to end. the program starts but as soon as it arrives at 5/54 or 6/54 it crashes with the message bellow.
I tried to change the version of cuda + cudnn, change the driver version of my gpu but nothing works, I still find this error.
The program is [here](https://github.com/Migateak/tensorflowtextgen) with data, it's not mine but it's for a generation of auto text.

### Source code / logs

> Epoch 1/50
>  5/54 [=>............................] - ETA: 18s - loss: 4.20602020-12-27 12:11:08.578867: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: 
> unspecified launch failure
> 2020-12-27 12:11:08.578954: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED
> in tensorflow/stream_executor/cuda/cuda_dnn.cc(1972): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'
> 2020-12-27 12:11:08.579262: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1
> 2020-12-27 12:11:08.579796: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 1024, 1, 200, 64, 1024]
> "
45986,TFLite Benchmark Binary Erroring with FastSpeech Model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

I used [nightly pre-built binary](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_plus_flex) from [here](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary)

FastSpeech Model used for benchmark can be downloaded from [here](https://github.com/tulasiram58827/TTS_TFLite/blob/main/models/fastspeech_quant.tflite)

Error : 

```
The input model file size (MB): 31.1646
 Initialized session in 13.661ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.
ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.

ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.
ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.

ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.
ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.

ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.
ERROR: Node number 553 (EXPAND_DIMS) failed to prepare.

ERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.
```

After this error I converted the model with fixed size and I got this error.

```
The input model file size (MB): 31.1646
Initialized session in 13.661ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.
ERROR: Node number 477 (RANGE) failed to invoke.

ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.
ERROR: Node number 477 (RANGE) failed to invoke.

ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.
ERROR: Node number 477 (RANGE) failed to invoke.

ERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.
ERROR: Node number 477 (RANGE) failed to invoke.
```
"
45985,"Install Error ""github.com/tensorflow/examples""","Would you tell me?
I want install ""tensorflow example"" from Anaconda for windows10.

However, no matter how many times I repeat it, I get an error.

Installed anaconda3

**Procedure so far
conda create -n insta1
conda activate insta1
conda install python=3.7.5
git clone https://github.com/tensorflow/examples
(insta1) C:\installation\insta2020-01\examples\tensorflow_examples\lite\model_maker\pip_package>pip install -e .

**Error contents
---------------------------------
(insta1) C:\installation\insta2020-01\examples\tensorflow_examples\lite\model_maker\pip_package>pip install -e .
Obtaining file:///C:/installation/insta2020-01/examples/tensorflow_examples/lite/model_maker/pip_package
    ERROR: Command errored out with exit status 1:
     command: 'C:\anaconda3\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py'""'""'; __file__='""'""'C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base 'C:\Users\toshi\AppData\Local\Temp\pip-pip-egg-info-su_59rf8'
         cwd: C:\installation\insta2020-01\examples\tensorflow_examples\lite\model_maker\pip_package\
    Complete output (11 lines):
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""C:\installation\insta2020-01\examples\tensorflow_examples\lite\model_maker\pip_package\setup.py"", line 232, in <module>
        setup_extra = prepare_package_src()
      File ""C:\installation\insta2020-01\examples\tensorflow_examples\lite\model_maker\pip_package\setup.py"", line 188, in prepare_package_src
        namespace_packages = find_namespace_packages(where=BUILD_ROOT)
      File ""C:\anaconda3\lib\site-packages\setuptools\__init__.py"", line 64, in find
        convert_path(where),
      File ""C:\anaconda3\lib\distutils\util.py"", line 121, in convert_path
        if pathname[0] == '/':
    TypeError: 'WindowsPath' object is not subscriptable
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output."
45984,Parallel execution of ops on tensors in unstacked list ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently, mapping/vector operations execute only batches in parallel, implying that if an op has to be performed on multiple tensors in parallel, all of them should be stacked, and therefore have the same shape.
This feature will allow execution of all tensors in a python iterable in parallel, without requiring them to be stacked. It will be analogous to the `map` function in python.

**Will this change the current api? How?**
This will add a new function to the API, accepting two arguments: A callable and a python iterable of tensors, not necessarily of the same shape.
The callable takes a single argument, which can be a single or nested tensor.

**Who will benefit with this feature?**
This feature has a wide range of applications, and can benefit the entire community.

**Any Other info.**
"
45983,Unrecognized DataType enum value 68 and crash in malloc,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux
- TensorFlow installed from (source or binary): Official repository: `python-tensorflow` (https://archlinux.org/packages/community/x86_64/python-tensorflow/) or `python-tensorflow-opt`
- TensorFlow version (use command below): Tensorflow 2.4.0 with Keras 2.4.3
- Python version: 3.9 (I know this isn't supported yet. Feel free to close this if it's version incompatibility)
- CUDA/cuDNN version: N/A (gpu not enabled)
- GPU model and memory: N/A (not enabled)


**Describe the current behavior**

1. Clone https://github.com/j05t/emnist
2. Run this file 
[train.zip](https://github.com/tensorflow/tensorflow/files/5744113/train.zip) in the directory with `gdb`
3. The program crashes with:

```
Starting program: /usr/bin/python train.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/usr/lib/libthread_db.so.1"".
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
2.4.3
0.17325132 0.3316614
[New Thread 0x7fff503ee640 (LWP 57446)]
2020-12-27 00:32:00.447866: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-27 00:32:00.448201: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[New Thread 0x7fff4fb2d640 (LWP 57447)]
[New Thread 0x7fff4f32c640 (LWP 57448)]
[New Thread 0x7fff4eb2b640 (LWP 57449)]
[New Thread 0x7fff4e32a640 (LWP 57450)]
[New Thread 0x7fff4db29640 (LWP 57451)]
[New Thread 0x7fff4d328640 (LWP 57452)]
[New Thread 0x7fff4cb27640 (LWP 57453)]
[New Thread 0x7fff2ffff640 (LWP 57454)]
2020-12-27 00:32:00.450681: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[New Thread 0x7fff2f7fe640 (LWP 57455)]
[New Thread 0x7fff2effd640 (LWP 57456)]
2020-12-27 00:32:00.757113: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-27 00:32:00.771377: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1996860000 Hz
[New Thread 0x7fff2d7fb640 (LWP 57457)]
[Thread 0x7fff2d7fb640 (LWP 57457) exited]
[New Thread 0x7fff2d7fb640 (LWP 57461)]
[New Thread 0x7fff2cefa640 (LWP 57462)]
[New Thread 0x7fff0bfff640 (LWP 57463)]
[New Thread 0x7fff0b7fd740 (LWP 57465)]
[New Thread 0x7fff0affb7c0 (LWP 57466)]
[New Thread 0x7fff0a7f9840 (LWP 57467)]
[New Thread 0x7fff09ff78c0 (LWP 57468)]
[New Thread 0x7fff097f5940 (LWP 57469)]
[New Thread 0x7fff08ff39c0 (LWP 57470)]
[New Thread 0x7ffeffffda40 (LWP 57471)]
[New Thread 0x7ffeff7fbb40 (LWP 57472)]
[New Thread 0x7ffefeff9bc0 (LWP 57473)]
[New Thread 0x7ffefe7f7c40 (LWP 57474)]
[New Thread 0x7ffefdff5cc0 (LWP 57475)]
[New Thread 0x7ffefd7f3d40 (LWP 57476)]
[New Thread 0x7ffefcff1dc0 (LWP 57477)]
[New Thread 0x7ffed3ffce40 (LWP 57478)]
2020-12-27 00:32:03.193765: E tensorflow/core/framework/types.cc:101] Unrecognized DataType enum value 68
malloc(): unaligned tcache chunk detected

Thread 12 ""python"" received signal SIGABRT, Aborted.
[Switching to Thread 0x7fff2effd640 (LWP 57456)]
0x00007ffff7a29615 in raise () from /usr/lib/libc.so.6
(gdb) bt
#0  0x00007ffff7a29615 in raise () from /usr/lib/libc.so.6
#1  0x00007ffff7a12862 in abort () from /usr/lib/libc.so.6
#2  0x00007ffff7a6b5e8 in __libc_message () from /usr/lib/libc.so.6
#3  0x00007ffff7a7327a in malloc_printerr () from /usr/lib/libc.so.6
#4  0x00007ffff7a77f8c in malloc () from /usr/lib/libc.so.6
#5  0x00007fffe674a53a in operator new (sz=64) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/new_op.cc:50
#6  0x00007ffff29718ee in tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&) ()
   from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffe462d157 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#8  0x00007fffe4634daa in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#9  0x00007fffe463b392 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#10 0x00007fffe9add029 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffe9ad9cc8 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffe5a59ee1 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#13 0x00007ffff79d33e9 in start_thread () from /usr/lib/libpthread.so.0
#14 0x00007ffff7aec293 in clone () from /usr/lib/libc.so.6

```

Alternative corruption message:

```
New Thread 0x7fff503ee640 (LWP 58610)]
2020-12-27 00:42:25.626607: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-27 00:42:25.626814: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 00:42:25.629252: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2020-12-27 00:42:25.901677: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-27 00:42:25.917771: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1996860000 Hz
python: malloc.c:4064: _int_malloc: Assertion `(unsigned long) (size) >= (unsigned long) (nb)' failed.

Thread 12 ""python"" received signal SIGABRT, Aborted.
[Switching to Thread 0x7fff2effd640 (LWP 58620)]
0x00007ffff7a29615 in raise () from /usr/lib/libc.so.6

// Crashed thread
(gdb) bt
#0  0x00007ffff7a29615 in raise () from /usr/lib/libc.so.6
#1  0x00007ffff7a12862 in abort () from /usr/lib/libc.so.6
#2  0x00007ffff7a73458 in __malloc_assert () from /usr/lib/libc.so.6
#3  0x00007ffff7a76f08 in _int_malloc () from /usr/lib/libc.so.6
#4  0x00007ffff7a77e51 in malloc () from /usr/lib/libc.so.6
#5  0x00007fffe674a53a in operator new (sz=40) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/new_op.cc:50
#6  0x00007fffe452b776 in tensorflow::Tensor::Tensor(tensorflow::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::AllocationAttributes const&) () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#7  0x00007fffe43986f2 in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tensorflow::AllocatorAttributes, tensorflow::AllocationAttributes const&) ()
   from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#8  0x00007fffe4398ebb in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tensorflow::AllocatorAttributes) () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#9  0x00007fffe43992e5 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#10 0x00007fffec4eb9e0 in tensorflow::AllocateOutputSetMklShape(tensorflow::OpKernelContext*, int, tensorflow::Tensor**, tensorflow::TensorShape const&, tensorflow::MklDnnShape const&, bool) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffec840fff in tensorflow::MklFusedBatchNormOp<Eigen::ThreadPoolDevice, float, float, true, false, false>::Compute(tensorflow::OpKernelContext*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffe463c3de in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#13 0x00007fffe9add029 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fffe9ad9cc8 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007fffe5a59ee1 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#16 0x00007ffff79d33e9 in start_thread () from /usr/lib/libpthread.so.0
#17 0x00007ffff7aec293 in clone () from /usr/lib/libc.so.6

// Main Thread

Thread 1 (Thread 0x7ffff7874740 (LWP 58605) ""python""):
#0  0x00007ffff7ae6d5d in syscall () from /usr/lib/libc.so.6
#1  0x00007ffff29bae09 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
--Type <RET> for more, q to quit, c to continue without paging--
#2  0x00007ffff29b7b43 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007ffff29b8043 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffee56824b in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tensorflow::CancellationManager*, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffe9c59f2b in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_2020_02_25::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::lts_2020_02_25::Span<tensorflow::TensorHandle*>) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffe9c5ac88 in tensorflow::ExecuteNode::Run() () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffee564f80 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fffe9c59107 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fffe9c59820 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fffe9c4917e in tensorflow::EagerOperation::Execute(absl::lts_2020_02_25::Span<tensorflow::AbstractTensorHandle*>, int*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffe9a9d099 in TFE_Execute () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffe9a2c2e5 in TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007fff9d2fe87f in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
#14 0x00007fff9d2fec79 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
#15 0x00007fff9d3227b5 in ?? () from /usr/lib/python3.9/site-packages/tensorflow/python/_pywrap_tfe.so
#16 0x00007ffff7d01253 in ?? () from /usr/lib/libpython3.9.so.1.0
#17 0x00007ffff7ce771d in _PyObject_MakeTpCall () from /usr/lib/libpython3.9.so.1.0
#18 0x00007ffff7ce3462 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#19 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#20 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#21 0x00007ffff7cdf577 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#22 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#23 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#24 0x00007ffff7cff8e4 in ?? () from /usr/lib/libpython3.9.so.1.0
#25 0x00007ffff7cdf577 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#26 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#27 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#28 0x00007ffff7cff8e4 in ?? () from /usr/lib/libpython3.9.so.1.0
#29 0x00007ffff7cdf577 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#30 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#31 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#32 0x00007ffff7ce6fe5 in _PyObject_FastCallDictTstate () from /usr/lib/libpython3.9.so.1.0
#33 0x00007ffff7cfc3d9 in _PyObject_Call_Prepend () from /usr/lib/libpython3.9.so.1.0
#34 0x00007ffff7ddaa7a in ?? () from /usr/lib/libpython3.9.so.1.0
#35 0x00007ffff7d001e2 in PyObject_Call () from /usr/lib/libpython3.9.so.1.0
#36 0x00007ffff7ce10c6 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#37 0x00007ffff7cddaaf in ?? () from /usr/lib/libpython3.9.so.1.0
#38 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#39 0x00007ffff7cffae4 in ?? () from /usr/lib/libpython3.9.so.1.0
#40 0x00007ffff7ce10c6 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#41 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#42 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
--Type <RET> for more, q to quit, c to continue without paging--
#43 0x00007ffff7ce6fe5 in _PyObject_FastCallDictTstate () from /usr/lib/libpython3.9.so.1.0
#44 0x00007ffff7cfc3d9 in _PyObject_Call_Prepend () from /usr/lib/libpython3.9.so.1.0
#45 0x00007ffff7ddaa7a in ?? () from /usr/lib/libpython3.9.so.1.0
#46 0x00007ffff7ce771d in _PyObject_MakeTpCall () from /usr/lib/libpython3.9.so.1.0
#47 0x00007ffff7ce3462 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#48 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#49 0x00007ffff7cef40e in _PyFunction_Vectorcall () from /usr/lib/libpython3.9.so.1.0
#50 0x00007ffff7cff8e4 in ?? () from /usr/lib/libpython3.9.so.1.0
#51 0x00007ffff7cdf577 in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
#52 0x00007ffff7cdd28d in ?? () from /usr/lib/libpython3.9.so.1.0
#53 0x00007ffff7cdcc51 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.9.so.1.0
#54 0x00007ffff7d9eb13 in PyEval_EvalCode () from /usr/lib/libpython3.9.so.1.0
#55 0x00007ffff7daeb9d in ?? () from /usr/lib/libpython3.9.so.1.0
#56 0x00007ffff7daa96b in ?? () from /usr/lib/libpython3.9.so.1.0
#57 0x00007ffff7c4c7c4 in PyRun_FileExFlags () from /usr/lib/libpython3.9.so.1.0
#58 0x00007ffff7c4c14c in PyRun_SimpleFileExFlags () from /usr/lib/libpython3.9.so.1.0
#59 0x00007ffff7dc0c99 in Py_RunMain () from /usr/lib/libpython3.9.so.1.0
#60 0x00007ffff7d91ba9 in Py_BytesMain () from /usr/lib/libpython3.9.so.1.0
#61 0x00007ffff7a14152 in __libc_start_main () from /usr/lib/libc.so.6
#62 0x000055555555504e in _start ()

```


**Describe the expected behavior**

The program doesn't crash.

**Standalone code to reproduce the issue**

(Described above)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45982,Simplify GPU usage model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0 - Python
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
IMO many new developers are using TF as it supports CUDA, and they have a single GPU they want to use. So one common use case is a single GPU. To utilize a GPU device script writers are using the following code 

```python
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
```

Can we have a simple API that enables the GPU first device ?

proposed API

experimental:
```python
configure_default_device(string physical_device_name, int device_number)
get_default_device()
```

example usage:
```python
tf.config.experimental.configure_default_device('GPU', 0)

get_default_device()
     example settings above would return
                  ""GPU"" device 0

```
**Will this change the current api? How?**
Yes
Simplify a common use case, and abstract away these settings so they can be changed in future without developers having to re-write their scripts ie when moving to later TF version

**Who will benefit with this feature?**
Many new developers with a single GPU

**Any Other info.**
It would be good to extend the API to handle specific GPU devices, or to set the engine to use all of them. Another API would show the current settings."
45981,"Running custom tflite model, segfault only on CPU, tflite 2.4.0 built from sources","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Meizu 16th
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0, also 5a8dc94c30a
- Python version: no 
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior**
trying to run custom [converted](https://github.com/pb-julian/liteface) tflite [model](https://www.dropbox.com/s/akxeqp99jvsd6z7/model-MobileFaceNet-arcface-ms1m-refine-v1.zip?dl=0) from insigthface on android. It works with standard tflite.aar from jcenter. Also works with built from sources libtensorflowlite_gpu_delegate.so. But when i tried to run on CPU with built from source libtensorflowlite.so, i got segfault.

**Describe the expected behavior**
Expected to work with manually built libtensorflowlite.so

**Standalone code to reproduce the issue**
- *.so files was built with docker from repo and [guide](https://www.tensorflow.org/lite/guide/build_android) in site.
- libtensorflowlite.so was built with commands: `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=arm64-v8a` and  `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=armeabi-v7a`
- code for running was adapted from [sample of usage](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)

**Other info / logs** Include any logs or source code that would be helpful to
traceback:
<img width=""1250"" alt=""Ð¡Ð½Ð¸Ð¼Ð¾Ðº ÑÐºÑ€Ð°Ð½Ð° 2020-12-27 Ð² 05 13 36"" src=""https://user-images.githubusercontent.com/32731602/103161348-56e64c80-4802-11eb-87de-e40365ea69e0.png"">

"
45980,TF2.3; tensorflow.python.framework.errors_impl.UnknownError: Failed to rename; : Access is denied. ; Input/output error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Conda; TF 2.3; 
- TensorFlow version (use command below): 2.3
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Stackoverflow: https://stackoverflow.com/questions/65461750/tensorflow-python-framework-errors-impl-unknownerror-failed-to-rename-access
I am not able to download and load tensorflow dataset on my Windows 10 machine. It works okay on Google colab. 


**Describe the expected behavior**
I should be able to download the datasets. 


**Standalone code to reproduce the issue**

```
import tensorflow_datasets as tfds
datasets, info = tfds.load(""imdb_reviews"", as_supervised=True, with_info=True
```

**Error:**


```

Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]
Shuffling...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:01<00:00, 14.15 shard/s]
Reading...: 0 examples [00:00, ? examples/s]
                                            
Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]
                                                           
Reading...: 0 examples [00:00, ? examples/s]
                                            
Writing...:   0%|          | 0/2500 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\IPython\core\interactiveshell.py"", line 3418, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-3b586bfe81d7>"", line 3, in <module>
    datasets, info = tfds.load(""imdb_reviews"", as_supervised=True, with_info=True)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow_datasets\core\api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow_datasets\core\registered.py"", line 300, in load
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow_datasets\core\api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 307, in download_and_prepare
    self.info.write_to_directory(self._data_dir)
  File ""C:\Anaconda3\envs\ml_tf\lib\contextlib.py"", line 119, in __exit__
    next(self.gen)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow_datasets\core\file_format_adapter.py"", line 200, in incomplete_dir
    tf.io.gfile.rename(tmp_dir, dirname)
  File ""C:\Anaconda3\envs\ml_tf\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 546, in rename_v2
    compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\Users\User\tensorflow_datasets\imdb_reviews\plain_text\0.1.0.incomplete5JQVCL to: C:\Users\User\tensorflow_datasets\imdb_reviews\plain_text\0.1.0 : Access is denied.
; Input/output error
```

**Conda list**

```
tensorboard               2.3.0              pyh4dce500_0
tensorboard-plugin-wit    1.6.0                      py_0
tensorflow                2.3.0           mkl_py37h3bad0a6_0
tensorflow-base           2.3.0           eigen_py37h17acbac_0
tensorflow-datasets       1.2.0                    py37_0
tensorflow-estimator      2.3.0              pyheb71bc4_0
tensorflow-metadata       0.14.0             pyhe6710b0_1
tensorflow-mkl            2.3.0                h93d2e19_0
```"
45979,tf.keras.preprocessing.image.save_img() flips dimensions of images created from tf.keras.preprocessing.image_dataset_from_directory(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.6
- CUDA/cuDNN version: n/a
- GPU model and memory: Intel Iris 1536 MB

**Describe the current behavior**

-> A directory hierarchy with the following format is created

/Project
|-- data
|    -- class-1
|        -- image1.jpg
|        -- image2.jpg
|        -- ...
|    -- class-2
|        -- image1.jpg
|        -- image2.jpg
|-- venv (tensorflow, pillow)
|    -- ...
|-- bug.py

in which the images have uneven dimensions (ie. 640x360 as opposed to 256x256)

-> A dataset is imported from a directory using tf.keras.preprocessing.image_dataset_from_directory()
-> A single image is taken from the dataset using next(dataset)
-> The image is saved using tf.keras.preprocessing.image.save_img()

**Unexpected Behavior:** the resulting image has dimensions 360x640 instead of 640x360 and is heavily warped

**Describe the expected behavior**

The image should have the same dimensions as when it was imported ie. 640x360

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Github repo (couldn't figure out how to get it into a Jupyter notebook)
https://github.com/atw1020/tensorflowBug

**Other info / logs**


"
45978,TF 2.4: Metrics not recognized as stateful in ProgbarLogger callback,"**System information**
- The vanilla example in https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric has the issue
- OS Platform and Distribution: Linux Ubuntu 21.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.9
- CUDA/cuDNN version: 11.0/8.0.5.39
- GPU model and memory: GeForce RTX 2080 Mobile

**Describe the current behavior**
Although the metric in the example, `CategoricalAccuracy`, is calculated correctly, it is not being recognized as a stateful metric by the `ProgbarLogger` callback. For more complicated metrics, that require proper handling by the `ProgbarLogger` callback, this causes errors such as `ValueError: operands could not be broadcast together with shapes (19,11) (38,11)` since they are concatenated instead of using the stateful metric value.

**Describe the expected behavior**
If the automatic addition of the `ProgbarLogger` callback did recognize the metrics added to the model, it would have been seen in the `ProgbarLogger.stateful_metrics` attribute, but it is currently always empty.

**Standalone code to reproduce the issue**
In the code example below, the `ProgbarLogger` is manually created and metrics are added to the constructor. Hence, the automatic creation of the `ProgbarLogger` is disabled and the `ProgbarLogger.stateful_metrics` will contain the `CategoricalAccuracy` metric and properly calculate the progress in a stateful manner.

```python
import types

import numpy as np
import tensorflow as tf
from tensorflow.python.keras.callbacks import ProgbarLogger

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

metrics = [tf.keras.metrics.CategoricalAccuracy()]
model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=metrics)

np.random.seed(0)
tf.random.set_seed(0)
data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset = dataset.batch(32)

metric_names = [
    m.lower() if isinstance(m, str) else
    m.__name__ if isinstance(m, types.FunctionType) else
    m.name for m in metrics]
callbacks = [ProgbarLogger(
    count_mode='steps',
    stateful_metrics=metric_names)]

result = model.fit(dataset, callbacks=callbacks, epochs=10)

```
"
45977,MaskRCNN TensorFlow Lite Inference Issue. No output from TFLite Model.,"**System information**
- OS Platform and Distribution ( Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1034-azure x86_64)):
- TensorFlow installed from (source- Pip Install):
- TensorFlow version (2.3.0):


**Command used to run the converter**
```
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.allow_custom_ops = True
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

converter.optimizations = [ tf.lite.Optimize.DEFAULT ]

tflite_model = converter.convert()
```

**link to Jupyter notebook and tflite model**

https://drive.google.com/drive/folders/1pTB33fTSo5ENzevobTvuG7hN4YmiCPF_?usp=sharing


**Commands used for inference**
```
### Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""model_2.3.tflite"")
interpreter.allocate_tensors()

### Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

### Test the model on random input data.
input_data_1 = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)
input_data_2 = np.array(np.random.random_sample(input_details[1]['shape']), dtype=np.float32)
input_data_3 = np.array(np.random.random_sample(input_details[2]['shape']), dtype=np.float32)

interpreter.set_tensor(input_details[0]['index'], input_data_1)
interpreter.set_tensor(input_details[1]['index'], input_data_2)
interpreter.set_tensor(input_details[2]['index'], input_data_3)

interpreter.invoke() ---> Kernel is getting stuck here. No output. I am executing the code from jupyter.

```

**The output from the converter invocation**

No output in Jupyter. 

Segmentation fault (core dumped) -- When executed in command line.

**Failure details**

Conversion is successful. But there is no output from model.

Could you guys please provide some ideas? I am stuck here and don't know how to proceed!
"
45976," No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=""dense/kernel"", _class=[""loc:@dense/kernel""], dtype=DT_FLOAT, container=""""]","I have created a tensorflow 1.13.1 model and try to used it in an android app. When I ""init"" the variables running   sess.runner().addTarget(""init"").run(); I got this error: No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=""dense/kernel"", _class=[""loc:@dense/kernel""], dtype=DT_FLOAT, container=""""]

This is the code I used to create the graph.pb file:

model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1, )),
tf.keras.layers.Dense(25, activation=tf.keras.activations.relu),  
tf.keras.layers.Dense(1, activation=tf.keras.activations.relu)])

model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mean_squared_error)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

init = tf.global_variables_initializer()
saver_def = tf.train.Saver().as_saver_def()
with open('graph.pb', 'wb') as f:
  f.write(tf.get_default_graph().as_graph_def().SerializeToString())

print('Operation to initialize variables:       ', init.name)
print('Tensor to be fed for checkpoint filename:', saver_def.filename_tensor_name)
print('Operation to save a checkpoint:          ', saver_def.save_tensor_name)
print('Operation to restore a checkpoint:       ', saver_def.restore_op_name)
print('Trainable variables: ', tf.trainable_variables())

Operation to initialize variables:        init
Tensor to be fed for checkpoint filename: save/Const:0
Operation to save a checkpoint:           save/control_dependency:0
Operation to restore a checkpoint:        save/restore_all
Trainable variables:  [<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(1, 25) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(25,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(25, 1) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32>]

The tensorflow android version is:  'org.tensorflow:tensorflow-android:1.13.1'

It worked with other linnear regression model but I dont know what is wrong,

Regards,
Alejandro."
45975,tf.debugging.assert_type raising error for wrong reason,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary (pip install)
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
```
>>> tf.debugging.assert_type(tf.constant(0.0), tf_type=(tf.float32,))
TypeError:  tensor must be of type (tf.float32,)
```
is incorrect.

**Describe the expected behavior**
Error should be raised because of incorrect argument type for `tf_type` (which needs to be a tf float type, not an iterable of them), not because tensor is not of type `(tf.float32,)`"
45974,TensorflowLite Android OpenCL delegate may produce invalid Conv2D result,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei P30 Lite
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: -
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Android NDK 21.3.6528147
- CUDA/cuDNN version: -
- GPU model and memory: Mali-G51 MP4 as per [gsmarena](https://www.gsmarena.com/huawei_p30_lite-9545.php) on an Android smartphone

I have been trying to run inference of some CNN model using TFLite 2.4.0 with OpenCL GPU delegate enabled and found that Conv2D operator may produce NaNs, Infs and other invalid values when running on the **Mali-G51 MP4** GPU if precision loss is allowed (I assume that getting NaNs is not considered as a reasonable precision loss) and Cond2D padding is set to `same`. For `valid` padding model produces valid results.
I've created a simple Conv2D-only ([simple_conv.zip](https://github.com/tensorflow/tensorflow/files/5743060/simple_conv.zip), shown on the illustration below) model to test via [inference_diff](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) util:

![image](https://user-images.githubusercontent.com/5624568/103150993-e2bc9200-478a-11eb-8a56-11c5566eeb33.png)

Here are some sample outputs of the `inference_diff/run_eval` util obtained using the described model on the Huawei P30 Lite (Mali-G51 MP4 GPU) smartphone:

```
$ adb shell /data/local/tmp/run_eval --model_file=/data/local/tmp/simple_conv.tflite --num_runs=1 --delegate=gpu
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
GPU delegate is created.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
native : inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : tflite_inference_stage.cc:128 
native : inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 1
Reference run latency: avg=55112(us), std_dev=17548(us)
Test run latency: avg=11990(us), std_dev=1488(us)
OutputDiff[0]: avg_error=inf, std_dev=nan
```

After disabling precision loss manually, model seemed to be working fine, but much slower obviously:

```
$ adb shell /data/local/tmp/run_eval --model_file=/data/local/tmp/simple_conv.tflite --num_runs=1 --delegate=gpu --gpu_precision_loss_allowed=false
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
GPU delegate is created.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
native : inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : tflite_inference_stage.cc:128 
native : inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 1
Reference run latency: avg=121364(us), std_dev=20829(us)
Test run latency: avg=28716(us), std_dev=1158(us)
OutputDiff[0]: avg_error=0.000120298, std_dev=0
```

After further investigation I found out that this kind of behavior may be fixed by manually commenting the [piece of code](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/lite/delegates/gpu/cl/selectors/operation_selector.cc#L257) responsible for selecting Winograd algorithm kernel as a Conv2D node implementation (i.e. `SelectConvolution` branch is always used). After this fix, model seemed to be working fine:

```
$ adb shell /data/local/tmp/run_eval --model_file=/data/local/tmp/simple_conv.tflite --num_runs=1 --delegate=gpu
GPU delegate is created.
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
native : inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : tflite_inference_stage.cc:128 
native : inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 1
Reference run latency: avg=113876(us), std_dev=17465(us)
Test run latency: avg=30590(us), std_dev=3084(us)
OutputDiff[0]: avg_error=0.304206, std_dev=0
```

Thus I assume that Winograd algorithm implementation for OpenCL delegate is the root cause of the issue. To sum up, here is the list of conditions to reproduce the bug, at least on the Mali-G51 MP4 GPU:
1. Create Conv2D node which is suitable for using Winograd algorithm as per [check](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/lite/delegates/gpu/cl/selectors/operation_selector.cc#L43).
2. Use `same` padding in Conv2D node.
3. Use OpenCL backend.
4. Allow precision loss.

The same behavior was also observed when running on Samsung Galaxy M31 (Mali-G72 MP3 GPU) and Huawei P20 (Mali-G72 MP12 GPU). However, running default build (i.e. without disabling Winograd manually) on Samsung Galaxy S20+ (Mali-G77 MP11 GPU) was successful. 

Please let me know, if you need more details/logs/code etc."
45973,"grpc 1.32.0 doesn't support python 3.9 in windows 10, dependency should be updated to 1.34.0","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.9.1
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):  Visual Studio 2019
- CUDA/cuDNN version: 11.2 / 8.0.5
- GPU model and memory:
RTX 3070 GDDR6 8GB


**Describe the problem**
cannot build grpcio 1.32.0
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build and install

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

grpcio 1.32.0 does not support python 3.9.1 in Windows 10

tensorflow dependency should be updated as with grpcio 1.34.0"
45972,Android Benchmark tool Build Failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: master branch
- Python version: 3.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version:
- GPU model and memory: GTX 1660 and 6GB RAM



**Describe the problem**

Build failed when building android benchmark tool for tflite models.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I just ran this command 

> bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark/android:benchmark_model

**Any other info / logs**
> INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=204
INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Found applicable config definition build:short_logs in file /home/ram/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/ram/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /home/ram/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /home/ram/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
ERROR: /home/ram/.cache/bazel/_bazel_ram/a1828cafbf783c13dba0538b4a59550c/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'
ERROR: Analysis of target '//tensorflow/lite/tools/benchmark/android:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed
INFO: Elapsed time: 0.180s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded, 1 target configured)

"
45971,Attempting to use uninitialized value ,"Hello, 
I have a trained tensorflow model which is trained by tensorflow v1.14 and the model is saved as meta graph model. What I'm trying to do is to convert this model to Keras since I cannot work with the model with SHAP package. The Shap package doesn't support the meta graph tensorflow model. So I want to convert it to Keras. I have done this with another TF model with VGG architecture and this works perfectly fine.  For this purpose I create the architecture in Keras and then I set the weight and bias values to the new model. 
Now I have a trained model with Resnet-18 architecture. But the problem is when I want to get the weights I get this error: 

Attempting to use uninitialized value scale2/block1/A/weights
	 [[{{node _retval_scale2/block1/A/weights_0_0}}]]

This works fine when I want to get the non-nested layer's values. 


```
with tf.compat.v1.Session() as sess, tf.device('/device:gpu:0'):
    saver = tf.compat.v1.train.import_meta_graph('./snapshot-110.meta')
    saver.restore(sess,tf.train.latest_checkpoint('./'))
    graph = tf.compat.v1.get_default_graph()

    vars_global = tf.compat.v1.global_variables()
    sess.as_default()
    model_vars = {}
    for var in vars_global:
        model_vars[var.name] = var.eval()
```"
45970,Timeseries example does NOT address the core of prediction,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/structured_data/time_series

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/tutorials/structured_data/time_series

## Description of issue (what needs changing):
I get the sequential data and the methods to get to next step or multi steps. But with the Time being converted to frequency, why do we have to have again equidistant events record. That beats the purpose of ANY timed series problem. In the example given, converting the time to frequency does not affect the model much because the series is Timed, not any value for the period. The period between records is equal. I still don't see this as a clear example of Time series. You can call this example as Series prediction, not Timed series prediction.

1) There seems to be some errors too,  In the below function, why do we have Shuffle to TRUE ? I thought timeseries is sequential ? 

```
def make_dataset(self, data):
  data = np.array(data, dtype=np.float32)
  ds = tf.keras.preprocessing.timeseries_dataset_from_array(
      data=data,
      targets=None,
      sequence_length=self.total_window_size,
      sequence_stride=1,
      shuffle=True,
      batch_size=32,)
  ds = ds.map(self.split_window)
  return ds
```

2 ) And also, how is the Validation done ? Do we have to window slide the validation data too ?
3)  Do we really need window slide for LSTM ? Seems to be over fitting for a lots of examples.
4)  If some input fields have categorical data (not the output), there is no way this will perform better especially if we use embedding
5) With this function timeseries_dataset_from_array, it is hard to do any embedding layer.
6) Timeseries involves large datasets, unfortunately using BatchNormalization is very confusing, blame it on the documentation.
7) How do we do feature engineered layer (such as mixed with embedding layer) in the first input with 3D tensor.


"
45969,Apollo3 project upload issue ,"@tensorflow/micro

**System information**
- Windows 10
- Arduino IDE
- TensorFlow Lite 2.1
- Sparkfun Apollo3 blue

I make all the steps correct from the sparkfun site to i can run the apollo3 from arduino ide.

When i upload the micro speech project into apollo3 from Arduino IDE i get this message #include ""tensorflow/lite/c/c_api_internal.h""
How i can resolve this ?
"
45968,Package build failure,"The package build using 
bazel build //tensorflow/tools/pip_package:build_pip_package

fails with the message: 

ERROR: 
//tensorflow/tools/pip_package: licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz, https://github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz] to /private/var/tmp/_bazel_kevinlano/58ba076afb76e123cd689325196ab4ac/external/aws/temp6281506492855719619/1.7.336.tar.gz: Tried to reconnect at offset 15,240,407 but server didn't support it


"
45967,"tflite android gpu delegate init error, what does it mean?  Init: MUL: 1x1088x1x1  cannot be reduced to linear.","@tensorflow/micro



**Describe the problem**
error log:
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: 
TfLiteGpuDelegate Init: MUL: 1x1088x1x1  cannot be reduced to linear.
TfLiteGpuDelegate Prepare: delegate is not initialized
 Node number 23608 (TfLiteGpuDelegateV2) failed to prepare.

when i run my own model convert from pytorch to TFLite.

device: HUAWEI Mate20
dependence:
implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly')
implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly')
implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')

**Please provide the exact sequence of commands/steps when you ran into the problem**
When I run the model on the android GPU
"
45966,sparse_categorical_crossentropy and temporal sample weight,"I am training a (BERT-like) language model with Keras, using _sparse_categorical_crossentropy_ loss. As with any training of language models, I have input sequences of different lengths, meaning that I need to add padding at the ends of most input sequences in order for them to fit a uniform batch size. When it comes to the outputs, however, I do not want to force the model to learn unnecessary padding tokens; rather, I'd just like it to ignore the outputs corresponding to positions where the input has a padding token.

The most sensible solution I was able to think of for doing that was using a _sample_weight_ with zeros at positions corresponding to padding in the input, and ones at all other (actual) positions. This requires setting _sample_weight_mode = 'temporal'_.

The problem is that _sparse_categorical_crossentropy_ doesn't seem to support that, and I get the following error:
_**ValueError: Found a sample_weight array for an input with shape (8108, 512). Timestep-wise sample weighting (use of sample_weight_mode=""temporal"") is restricted to outputs that are at least 3D, i.e. that have a time dimension.**_

Is there a way of using temporal sample weight with _sparse_categorical_crossentropy_  in Keras? 
Alternatively, is there another approach I should consider to achieve what I want, or any workaround I could resort to (for making Keras ignore positions with padding in the input during training)?

Thank you!"
45965,Download dependent 404ï¼ˆBuild did NOT complete successfullyï¼‰,"INFO: Found applicable config definition build:linux in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (190 packages loaded, 20055 targets configured).
INFO: Found 1 target...
ERROR: /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/aws/BUILD.bazel:12:1: C++ compilation of rule '@aws//:aws' failed (Exit 1): gcc failed: error executing command 
  (cd /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.d '-frandom-seed=bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o' -fPIC -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -g0 -w '-march=native' -g0 '-std=c++14' -DENABLE_OPENSSL_ENCRYPTION '-DAWS_SDK_VERSION_MAJOR=1' '-DAWS_SDK_VERSION_MINOR=7' '-DAWS_SDK_VERSION_PATCH=266' -DOPENSSL_IS_BORINGSSL -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/aws/aws-cpp-sdk-s3/source/S3Client.cpp -o bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o)
Execution platform: @local_execution_config_platform//:platform
gcc: fatal error: Killed signal terminated program cc1plus
compilation terminated.
Target //tensorflow/tools/lib_package:libtensorflow failed to build
INFO: Elapsed time: 2784.337s, Critical Path: 123.30s
INFO: 3070 processes: 3070 local.
FAILED: Build did NOT complete successfully
"
45964,tf.data.Data.as_numpy_iterator() creates unnecessary copies,"Does `tf.data.Data.as_numpy_iterator()` need to copy the data ([here](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L3942)), or can the tensor buffer be used directly? This copy can be avoided by using `x._numpy()` instead, like in this Flax ImageNet example [here](https://github.com/google/flax/blob/master/linen_examples/imagenet/train.py#L169-L170). Or, by using the numpy buffer interface, as shown [here](https://github.com/tensorflow/tensorflow/issues/33254#issuecomment-542379165). "
45963,NotFoundError: '_MklMatMul',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
 No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
c0-deeplearning-tf2-2-4-tpu-v20201215-debian-10
Description
Google, Deep Learning Image: TensorFlow 2.4, m60 TPU, A debian-10 Linux based image with TensorFlow 2.4 pre-installed.
Location
asia (Asia Pacific), eu (European Union), us (United States)
Labels
release : m60
Creation time
Dec 16, 2020, 5:10:12 PM UTC-08:00
Family
tf2-2-4-tpu-debian-10
Encryption type
Google managed

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
After initializing the tpu - I'm using version 2.4 on the tpu as well, tf.matmul doesn't work anymore.

I'm initializing the tpu with:

```
def get_strategy():

    try:
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-3')
        print('Running on TPU ', tpu.master())
    except:
        tpu = None

    if tpu:
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.TPUStrategy(tpu)
    
    else:
        strategy = tf.distribute.get_strategy()
        for d in tf.config.list_physical_devices():
            print(d)
            
    return strategy

strategy = get_strategy()
```

Here is the full error:

```
NotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-1f5a3a66-w-0. Make sure the operation or function is registered in the binary running in this process.
```

It works fine on the cpu before the tpu is initialized."
45962,on_epoch_end() being called multiple times when workers > 0,"**System information**
- Written custom code
- Linux 5.4.0-1034-azure x86_64
- TensorFlow version 2.3.0
- Python version: 3.6.9
- CUDA version:  10.1

**Current Behavior**
I have implemented a data generator based on the Sequence generator class. At the end of every epoch, I want to update a parameter. This parameter should change the preprocessing of my dataset for the next epoch. However at the end of every epoch the on_epoch_end() function is called multiple times. To better grasp this behavior I implemented a counter in the on_epoch_end() as this:

```
    def on_epoch_end(self):
        'Updates indices after each epoch'

        #My parameter should be updated here

        print(""counted"", self.counter, self.genMode)
        self.counter += 1

        self.indexes = np.arange(len(self.imageID))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
```
Where it prints the current count aswell as the mode of the generator that's printing this statement (as I have a validation and a training generator). 

self.counter is initiated at self.counter = 1

This results in the following output:

```
Epoch 1/15
counted 1 train
counted 1 val
counted 2 val
counted 2 train
Epoch 2/15
counted 3 train
counted 3 val
counted 4 train
Epoch 3/15
counted 5 train
counted 4 val
counted 6 train
Epoch 4/15
counted 7 train
counted 5 val
counted 8 train
Epoch 5/15
counted 9 train
counted 6 val
counted 7 val
counted 10 train
Epoch 6/15
counted 11 train
counted 8 val
counted 12 train
Epoch 7/15
counted 13 train
counted 9 val
counted 14 train
Epoch 8/15
counted 15 train
counted 10 val
counted 11 val
counted 16 train

...... etc
```
I currently cannot find a workaround to only update my parameter once at the end of every epoch. The parameter should be updated for both my validation generator and my training generator concurrently. 

**expected behavior**

Expected behavior would be that the on_epoch_end() function would be called only once at the end of every epoch. 

**Standalone code to reproduce the issue**
The code I use to create the generators plus my model.fit() function. Where scheduler is just a simple learning rate scheduler. It adjusts the learning rate every fifth epoch (5, 10 15). I have tried my code without the callbacks which produced exactly the same behavior. 
```

    validation_generator = DataGenerator(augParams, genMode = 'val', **params)
    train_generator = DataGenerator(augParams, genMode = 'train', **params)

    callbacks_list = [scheduler]
    model.fit(train_generator,  
                        validation_data=validation_generator,
                        verbose = 3,
                        max_queue_size=1000,
                        workers=5,
                        epochs=15,
                        callbacks= callbacks_list)
```

The generators themselves implement the following thee functions plus some additional functions that are used for preprocessing that are called from __data__generation(). (I left out all functions and initialization that are not connected to the on_epoch_end() function)

```
from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):
    def __init__(self, augParams, genMode, batchSize=32, dim=(360,640), shuffle=True):
          self.genMode = genMode
          self.counter = 1
  

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.imageID) / self.batchSize))

    
    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batchSize:(index+1)*self.batchSize]

        # Find list of IDs
        imageIDtemp = [self.imageID[k] for k in indexes]
        annoIDtemp = [self.annoID[k] for k in indexes]

        # Generate data
        X, Y = self.__data_generation(imageIDtemp, annoIDtemp)
        return X, Y
    
    def on_epoch_end(self):
        'Updates indices after each epoch'
        #My parameter should be updated here

        print(""counted"", self.counter, self.genMode)
        self.counter += 1

        self.indexes = np.arange(len(self.imageID))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
```

on_epoch_end() is not called when the generators are initialized. 

**Other info / logs**

My first guess was that the on_epoch_end() function wasn't thread-safe so I tried thread locking it but this had no effect. I have also run the exact same code only this time adjusting my on_epoch_end() to this:

```
    def on_epoch_end(self):
        'Updates indices after each epoch'
        traceback.print_stack()
        #My parameter should be updated here

        print(""counted"", self.counter, self.genMode)
        self.counter += 1

        self.indexes = np.arange(len(self.imageID))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
```

Printing the stack every time the function is called. This results in the following log for the first 5 epochs.

**Log with traceback.print_stack()**

```
Epoch 1/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 1 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 1 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 2 train
Epoch 2/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 3 train
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 2 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 3 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 4 train
Epoch 3/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 5 train
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 4 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 5 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 6 train
Epoch 4/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 7 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 6 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 8 train
Epoch 5/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 9 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 7 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 117, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 110, in main
    callbacks= callbacks_list)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 98, in on_epoch_end
    traceback.print_stack()
counted 10 train
```

**Log of first 5 epochs with traceback.print_stacks() when no callbacks have been given to the fit function.**

```
Epoch 1/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 1 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 1 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 2 train
Epoch 2/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 3 train
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 2 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 3 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 4 train
Epoch 3/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 5 train
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 4 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 5 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 6 train
Epoch 4/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 7 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 6 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 8 train
Epoch 5/15
  File ""/usr/lib/python3.6/threading.py"", line 884, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 876, in _run
    self.sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 9 train
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1133, in fit
    return_dict=True)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1373, in evaluate
    for _, iterator in data_handler.enumerate_epochs():  # Single epoch.
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 7 val
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/oddity/code/Train.py"", line 116, in <module>
    main()
  File ""/home/oddity/code/Train.py"", line 109, in main
    epochs=15)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1086, in fit
    for epoch, iterator in data_handler.enumerate_epochs():
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1145, in enumerate_epochs
    self._adapter.on_epoch_end()
  File ""/home/oddity/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 953, in on_epoch_end
    self._keras_sequence.on_epoch_end()
  File ""/home/oddity/code/DataGenFastest.py"", line 99, in on_epoch_end
    traceback.print_stack()
counted 10 train
``` 

"
45961,Programm ends Drasicly,"Works fine with other Code

I try to run a convolution script for my network, whan i was running it on my CPU it was fine, then i installed CUDA and CUDNN and now it stopped working. I tried searching for the issue got a few got a few fixes, but it still dosn't work. Now it only says:
2020-12-24 14:31:49.491561: I
and not: W or E ; so i guess thats a good thing?

**System information**
- OS: Windows 10
- TensorFlow: 2.4.0
- Python version: 3.8
- Installed using: pip install tensorflow-gpu
- CUDA/cuDNN version: 11.0 / 8.0.5.39
- GPU model and memory: 1050 Mobile 4GB

Im using pycharm IDE:

Output:
2020-12-24 14:31:42.522978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
3-conv-64-nodes-0-dense-1608816708
2020-12-24 14:31:48.503990: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-24 14:31:48.505733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-24 14:31:49.421165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-12-24 14:31:49.421594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-24 14:31:49.455737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-24 14:31:49.456022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-24 14:31:49.462110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-24 14:31:49.467640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-24 14:31:49.480366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-24 14:31:49.484829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-24 14:31:49.486190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-24 14:31:49.486542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-24 14:31:49.487316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-24 14:31:49.488676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-12-24 14:31:49.489674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-24 14:31:49.490059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-24 14:31:49.490445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-24 14:31:49.490811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-24 14:31:49.491164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-24 14:31:49.491561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-24 14:31:49.491986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-24 14:31:49.492414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-24 14:31:49.492930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-24 14:31:50.647138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-24 14:31:50.647372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-12-24 14:31:50.647516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-12-24 14:31:50.647835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-12-24 14:31:50.649096: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-24 14:31:51.001350: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2020-12-24 14:31:51.001582: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2020-12-24 14:31:51.001816: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2020-12-24 14:31:51.004339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cupti64_110.dll
2020-12-24 14:31:51.158879: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2020-12-24 14:31:51.159215: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2020-12-24 14:31:52.099653: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/10
2020-12-24 14:31:53.954017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-24 14:31:54.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-24 14:31:54.544413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll

Process finished with exit code -1073740791 (0xC0000409)


(If you'd need any additional information ask)

"
45960,A long period of GPU/CPU idle time in timeline,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.10.0
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tensorflow-gpu==1.15
- Python version: 3.6.8
- CUDA/cuDNN version: 10.2
- GPU model and memory: Tesla-V100, 16G

**Describe the current behavior**
There is a long period of GPU and CPU idle time in timeline. The model is trained in one GPU and there is no problem of network transmission.

**Describe the expected behavior**
There should not be such a long period of idle time.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
![å±å¹•å¿«ç…§ 2020-12-24 ä¸‹åˆ8 17 36](https://user-images.githubusercontent.com/5723913/103089708-c2c18d00-4629-11eb-9059-43aeb8b20136.png)

"
45959,Nontrainable Custom Convolution at the start of model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
I am creating a CNN model ,but at the start I want my image to be passed through high pass filter(based on a matrix convolution) how to do that,or How to add a custom non trainable Convolution layer with a constant matrix at the start of model
.Please refer GNCNN model , how to do the HIgh pass filter part in keras , using flow from directory method"
45958,Performance discrepency in an integer quantized model ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am integer quantizing (full-integer) the [EAST model](https://arxiv.org/abs/1704.03155) trained on COCO-text dataset. Results are very inconsistent with the integer quantized model with respect to the other variants such as dynamic-range and float16. I get the following results with both dynamic-range and float16 (which is consistent with the original model too) - 

![image](https://user-images.githubusercontent.com/22957388/103078882-cc7ecc80-45f8-11eb-8392-e2db4cf515cb.png)

But with the integer quantized model, I get - 

![download](https://user-images.githubusercontent.com/22957388/103078920-e3bdba00-45f8-11eb-9f15-3b846dbc189b.png)

During inference with the integer quantized model, I have also scaled the input image properly. My question is do I need to scale back the predictions before I post-process them?  

**Describe the expected behavior**

The results of the integer-quantized model should be consistent. 

**Standalone code to reproduce the issue**
 
Conversion and inference Colab Notebook - https://colab.research.google.com/gist/sayakpaul/0438ad116871cd860d0a25dcc9b21805/east_tflite.ipynb. "
45957,Is it possible to build tf2.4 with cuda 10.0?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.0.130, cudnn 7.5
- GPU model and memory: K80



**Describe the problem**

I would like to know if is it possible to build newest tensorflow against cuda 10.0? Is there any workaround with cuda 10.0? It looks like libcublasLt.so.10.0 is missing in cuda10.0 ...

**Provide the exact sequence of commands / steps that you executed before running into the problem**

During the build I get following error:

    $: bazel build //tensorflow/tools/pip_package:build_pip_package
    ....
    Repository command failed
    No library found under: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublasLt.so.10.0
    INFO: Elapsed time: 1.836s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (0 packages loaded)
        currently loading: tensorflow/tools/pip_package


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45956,Using Tensorflow 2.3 in r (conda environment) to reproduce the examples of Platypus R-package,"the example included in Platypus package: https://github.com/maju116/platypus/blob/yolo3_fix/examples/Blood%20Cell%20Detection/Blood-Cell-Detection.md

my session trying to reproduce the result:
> history <- blood_yolo %>%
+   yolo3_fit_generator(
+     generator = train_blood_yolo_generator,
+     epochs = 3,
+     steps_per_epoch = 3,
+     validation_generator = valid_blood_yolo_generator,
+     validation_steps_per_epoch = 9,
+     model_filepath = ""development/BCCD/blood_w.hdf5"",
+     save_best_only = TRUE,
+     monitor = ""val_loss""
+   )
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024526D8AB80> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244990D0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244E0EE0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F4C280> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F97D30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024562187C10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
45955,Tf.contrib.layers.Bias_add fails to operate due to uninitialized value,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from source:
- TensorFlow version (use command below): 1.14/1.15
- Python version: 3.7
- CUDA/cuDNN version: 10


**Describe the current behavior**
I want to add bias layers to slicing parts of a reshape layer. I conduct it with `tf.contrib.layers.bias_add` and then concatenate them back. However, I can't do that and it keeps raising the bug of uninitialized for the `Bias_add op` even I have given it

The code I use is below:

   ```
 input = keras.Input(shape=[shape, shape, 256])

    kernel_init = tf.keras.initializers.RandomNormal(0.0, 0.01)

    # seg_layer = _seg_layer(output_filters,num, f_l,kernel_init,bias_init)
    head = keras.layers.Conv2D(256, 3, padding=""same"", kernel_initializer=kernel_init)(input)
    head = keras.layers.ReLU()(head)
    for _ in range(3):
        head = keras.layers.Conv2D(256, 3, padding=""same"", kernel_initializer=kernel_init)(head)
        head = keras.layers.ReLU()(head)
    head = keras.layers.Conv2D(
        output_filters,
        3,
        1,
        padding=""same"",
        kernel_initializer=kernel_init,
        use_bias=False,
    )(head)
    # head = tf.identity(head)
    head = keras.layers.Reshape([-1, int(output_filters/9) ])(head)
    slicing = int(f_l/num)
    segment = []
    for i in range(num):
        tempt = head[:,slicing*i:slicing*(i+1),:]
        tempt = tf.contrib.layers.bias_add(
            tempt,
            activation_fn=None,
            initializer=tf.zeros_initializer(),
            regularizer=None,
            reuse=None,
            variables_collections=None,
            outputs_collections=None,
            trainable=True,
            data_format='NHWC',
            scope=None
        )
        segment.append(tempt)
    output = keras.layers.Concatenate(axis=1)(segment)
    model = keras.models.Model(input,output)
```

Error:
```
tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Attempting to use uninitialized value BiasAdd/biases
         [[{{node BiasAdd/biases/read}}]]
         [[BiasAdd/biases/read/_687]]
  (1) Failed precondition: Attempting to use uninitialized value BiasAdd/biases
         [[{{node BiasAdd/biases/read}}]]
0 successful operations.
0 derived errors ignored.
```"
45954,AttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices',"c:\users\gwinivac\.conda\envs\chatboty\lib\importlib\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject
  return f(*args, **kwds)
c:\users\gwinivac\.conda\envs\chatboty\lib\importlib\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject
  return f(*args, **kwds)
c:\users\gwinivac\.conda\envs\chatboty\lib\importlib\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject
  return f(*args, **kwds)
c:\users\gwinivac\.conda\envs\chatboty\lib\importlib\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject
  return f(*args, **kwds)
Traceback (most recent call last):
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Gwinivac\.conda\envs\chatboty\Scripts\rasa.exe\__main__.py"", line 7, in <module>
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\site-packages\rasa\__main__.py"", line 115, in main
    rasa.telemetry.initialize_error_reporting()
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\site-packages\rasa\telemetry.py"", line 230, in decorated
    return f(*args, **kwargs)
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\site-packages\rasa\telemetry.py"", line 651, in initialize_error_reporting
    default_context = _default_context_fields()
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\site-packages\rasa\telemetry.py"", line 480, in _default_context_fields
    ""gpu"": len(tf.config.list_physical_devices(""GPU"")),
  File ""c:\users\gwinivac\.conda\envs\chatboty\lib\site-packages\tensorflow_core\python\util\module_wrapper.py"", line 193, in __getattr__
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'


running on:
tensorflow version: 1.15.0
installed using conda

"
45953,Tensorflow Lite speed -> mobile vs desktop,"As I am currently building a mobile application with pretty heavy duty object detection, I switched to tensorflow lite as a way to speed up computing. Everything is lightning fast as expected, but I am seeing 99% of the computation time being overpowered by the interpretation.invoke() function which is obviously critical for inference as described in https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter. 

With over 1400 images to loop my model through, I am seeing the invoke() function take roughly 1 sec for each image extending the total inference time to over 20 min. One extremely important detail to note is that this is run through xcode on my desktop computer and not a mobile device. After extensive research, I have discovered to expect better performance on a mobile device with tensorflow lite.

My question is - are the performance gains from tensorflow lite on a mobile device exponentially faster than desktop? If my run time on desktop is over 20 min, is it even possible for this to decrease to a few minutes on mobile? Tons of forums out there are concluding tflite is obviously faster on mobile than desktop, but I am looking for logistics here. Generally HOW MUCH FASTER? Please help! Thanks!!"
45952,tf.read_file have different speed to read the same images,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):docker image: `docker pull tensorflow/tensorflow:1.13.1-gpu-py3`
- TensorFlow version (use command below):1.13.1
- Python version:3.5.2
- CUDA/cuDNN version:10.0
- GPU model and memory:1080TI/11G

**Describe the current behavior**
Hi,
I had pull tf1.13.1 docker image from dockerhub: 

```sh
$ docker pull tensorflow/tensorflow:1.13.1-gpu-py3
```
I use `tf.data.TextLineDataset` to load data from HHD, here is my code:

```python
def _parse_function(data):
    data_str_split = tf.string_split([data]).values
    filename = data_str_split[0]
    image_string = tf.read_file(filename)
    return image_string

def input_fn(is_training, filename, params):
    batch_size = params['batch_size']
    num_parallel_calls = params['num_parallel_calls']
    if num_parallel_calls is None:
        num_parallel_calls = tf.data.experimental.AUTOTUNE
    parse_fn = lambda data: _parse_function(data)    # anchor
    dataset = tf.data.TextLineDataset(filename)
    if is_training:
        dataset = dataset.apply(tf.data.experimental.map_and_batch(
            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))
        dataset = dataset.shuffle(300)
    else:
        dataset = dataset.apply(tf.data.experimental.map_and_batch(
            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))

    # create reinitializable iterator from dataset
    iterator = dataset.make_initializable_iterator()
    images = iterator.get_next()
    iterator_init_op = iterator.initializer
    inputs = {'images': images, 'iterator_init_op': iterator_init_op}
    return inputs
```
I tested the reading speed of the first file: `total.txt`(about 4,000,000 lines), the reading speed is about 2000ï½ž3000it/s(if i test it a second time, it will up to 8000~9000it/s):
```python
if __name__ == ""__main__"":
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    params = {
        'batch_size': 32,
        'num_parallel_calls': 64
    }

    test_txt = '${ROOT}/total.txt'
    # test_txt = '${ROOT}/train.txt'
    # test_txt = '${ROOT}/valid.txt'
    test_inputs = input_fn(True, test_txt, params)

    import time
    ts = time.time()
    last_processed = 0
    num_processed = 0
    with tf.Session() as sess:
        sess.run(test_inputs['iterator_init_op'])
        while True:
            try:
                images = sess.run(test_inputs['images'])
            except tf.errors.OutOfRangeError:
                sess.run(test_inputs['iterator_init_op'])
                continue
            num_processed += images.shape[0]
            tn = time.time()
            if tn-ts > 1.0:
                print(""\rnum_parallel_calls[{}]:{:5.2f}it/s"".format(params['num_parallel_calls'],
                                                                    (num_processed-last_processed)/(tn-ts)), end='')
                last_processed = num_processed
                ts = tn
```
Then, I shuffled the file `total.txt` and splited to `train.txt/valid.txt`, and test the reading speed,
but the speed drops to **100it/s**!

The 2 txt file pointed to the same images, why is there such a big difference?

Can you tell me how to solve this problem, I have tried to split the `train.txt` to several small `.txt` file, but it's useless.

Thanks a lot!"
45950,"Tensorflow does not use the GPU during training with eager execution, despite manual activation","**NOTE** This issue is being the same issue [here](https://github.com/tensorflow/tensorflow/issues/45546) and it's being re-opened because this [person](https://github.com/sanjoy) decided to close it without resolution / discussing his decision, so there we go again ...

<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux (Google colab)):
- TensorFlow installed using `pip install tensorflow-gpu`:
- TensorFlow version (2.3.1):
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.243
- GPU model and memory: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm experiencing a very slow training time when running a third party [script](https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py) which is the same(0% difference between GPU and CPU). I use the following command for activating the GPU on google colab after installing `tensorflow-gpu` using `pip`:

```
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
```
I add the lines above in `main()` in the script I referred to earlier and I use [wandb](https://github.com/wandb/client) for monitoring the training. Here are the [graphs](https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing) within a few minutes of training showing 0% GPU utilization.

**Describe the expected behavior**

A fast performance which results in a remarkable difference in speeds (CPU vs GPU) and GPU utilization above 0% if the metrics are accurate and if they are not, I'm still experiencing the same speed when run on CPU or GPU.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import wandb
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

import gym
import argparse
import numpy as np
from collections import deque
import random

tf.keras.backend.set_floatx('float64')
wandb.init(name='DQN', project=""deep-rl-tf2"")

parser = argparse.ArgumentParser()
parser.add_argument('--gamma', type=float, default=0.95)
parser.add_argument('--lr', type=float, default=0.005)
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--eps', type=float, default=1.0)
parser.add_argument('--eps_decay', type=float, default=0.995)
parser.add_argument('--eps_min', type=float, default=0.01)

args = parser.parse_args()

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def put(self, state, action, reward, next_state, done):
        self.buffer.append([state, action, reward, next_state, done])
    
    def sample(self):
        sample = random.sample(self.buffer, args.batch_size)
        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))
        states = np.array(states).reshape(args.batch_size, -1)
        next_states = np.array(next_states).reshape(args.batch_size, -1)
        return states, actions, rewards, next_states, done
    
    def size(self):
        return len(self.buffer)

class ActionStateModel:
    def __init__(self, state_dim, aciton_dim):
        self.state_dim  = state_dim
        self.action_dim = aciton_dim
        self.epsilon = args.eps
        
        self.model = self.create_model()
    
    def create_model(self):
        model = tf.keras.Sequential([
            Input((self.state_dim,)),
            Dense(32, activation='relu'),
            Dense(16, activation='relu'),
            Dense(self.action_dim)
        ])
        model.compile(loss='mse', optimizer=Adam(args.lr))
        return model
    
    def predict(self, state):
        return self.model.predict(state)
    
    def get_action(self, state):
        state = np.reshape(state, [1, self.state_dim])
        self.epsilon *= args.eps_decay
        self.epsilon = max(self.epsilon, args.eps_min)
        q_value = self.predict(state)[0]
        if np.random.random() < self.epsilon:
            return random.randint(0, self.action_dim-1)
        return np.argmax(q_value)

    def train(self, states, targets):
        self.model.fit(states, targets, epochs=1, verbose=0)
    

class Agent:
    def __init__(self, env):
        self.env = env
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n

        self.model = ActionStateModel(self.state_dim, self.action_dim)
        self.target_model = ActionStateModel(self.state_dim, self.action_dim)
        self.target_update()

        self.buffer = ReplayBuffer()

    def target_update(self):
        weights = self.model.model.get_weights()
        self.target_model.model.set_weights(weights)
    
    def replay(self):
        for _ in range(10):
            states, actions, rewards, next_states, done = self.buffer.sample()
            targets = self.target_model.predict(states)
            next_q_values = self.target_model.predict(next_states).max(axis=1)
            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma
            self.model.train(states, targets)
    
    def train(self, max_episodes=1000):
        for ep in range(max_episodes):
            done, total_reward = False, 0
            state = self.env.reset()
            while not done:
                action = self.model.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                self.buffer.put(state, action, reward*0.01, next_state, done)
                total_reward += reward
                state = next_state
            if self.buffer.size() >= args.batch_size:
                self.replay()
            self.target_update()
            print('EP{} EpisodeReward={}'.format(ep, total_reward))
            wandb.log({'Reward': total_reward})


def main():
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    if len(physical_devices) > 0:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)
    env = gym.make('CartPole-v1')
    agent = Agent(env)
    agent.train(max_episodes=1000)

if __name__ == ""__main__"":
    main()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45949,TFLiteTransferConverter (tfltransfer) for model personalization cannot convert a model on 'keras_model_head.py': AttributeError: 'list' object has no attribute 'values' line 51,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory: 750 ti 2GB

**Describe the current behavior**
I am following the model personalization post available on the TensorFlow GitHub repository (https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/README.md) to build a custom trainable model on a mobile device. I expect to have the custom (or even the provided example) model for the 'head' can be converted via the following code:
`converter = TFLiteTransferConverter(4,
                                    base,
                                    heads.KerasModelHead(head),
                                    optimizers.SGD(3e-2),
                                    train_batch_size=20)`

But the library gets stuck on `heads.KerasModelHead(head)` line and produces the following error:

```
/tfltransfer/heads/keras_model_head.py in __init__(self, keras_model)
     49     self._predict_signature = loaded_model.signatures['serving_default']
     50 
---> 51     input_def = next(self._predict_signature.inputs.values().__iter__())
     52     self._input_shape = tuple(
     53         dim.size for dim in input_def.tensor_shape.dim[1:])

AttributeError: 'list' object has no attribute 'values'
```

I have tried both the provided example and a custom model on different systems including Google Colab. The issue cannot be resolved.

**Describe the expected behavior**

The converter should produce the converted model files to be used on the device. The converter should produce the following file based on a custom base and head models.
```
bottleneck.tflite
inference.tflite
initialize.tflite
optimizer.tflite
train_head.tflite
```

**Standalone code to reproduce the issue**
The following is the example code obtained from the TensorFlow Github tested on Google Colab.
```
import tensorflow as tf
from google.colab import drive
drive.mount('/content/drive')
nb_path = '/content/drive/MyDrive/ML/converter/'
sys.path.insert(0,nb_path)
from tfltransfer import bases
from tensorflow.keras import layers
from tensorflow.keras.regularizers import l2
from tfltransfer import bases
from tfltransfer import heads
from tfltransfer import optimizers
from tfltransfer.tflite_transfer_converter import TFLiteTransferConverter
base = bases.MobileNetV2Base(image_size=224)

head = tf.keras.Sequential([
layers.Flatten(input_shape=(7, 7, 1280)),
layers.Dense(
units=32,
activation='relu',
kernel_regularizer=l2(0.01),
bias_regularizer=l2(0.01)),
layers.Dense(
units=4,
activation='softmax',
kernel_regularizer=l2(0.01),
bias_regularizer=l2(0.01)),
])
head.compile(loss='categorical_crossentropy', optimizer='sgd')
converter = TFLiteTransferConverter(4,
base,
heads.KerasModelHead(head),
optimizers.SGD(3e-2),
train_batch_size=20)
converter.convert_and_save('custom_keras_model')

```

**Other info / logs** Include any logs or source code that would be helpful to
```
tfltransfer/heads/keras_model_head.py"", line 51, in __init__
    input_def = next(self._predict_signature.inputs.values().__iter__())
AttributeError: 'list' object has no attribute 'values'
```

"
45948,https://youtu.be/IGpHWo0ySBU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45947,https://youtu.be/IGpHWo0ySBU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45946,Tensorflow operations : Invalid data type according to Tensorflow Profiler,"Hi,

I use Tensorflow 2.5 installed from source with Cuda 11.1 and Cudnn 8 on Ubuntu 20.04. My GPU is a Nvidia Quadro RTX 6000. 

I have got Out of Memory problems with the GPU and the training of a CycleGAN model. In order to track memory leaks in my code, I have run it with [profiler trace](https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/Trace) and I can now see with Tensorboard strange results in the memory breakdown table (memory profile tab). Indeed, some Tensorflow operations have ""INVALID"" data type, no region type and no shape. This suggests there are bugs with some Tensorflow operations.

Above I put a minimal code sample which reproduces this kind of error.

```
import tensorflow as tf
import keras

IMAGE_SHAPE = [256,256,3]

def Discriminator():
    return keras.Sequential([
        keras.layers.Flatten(input_shape=IMAGE_SHAPE),
        keras.layers.Dense(1, activation=""sigmoid"")
    ])

def Generator():
    return keras.Sequential([
        keras.layers.Conv2D(filters=IMAGE_SHAPE[-1], kernel_size=3, strides=1, padding=""same"", use_bias=False,
                           input_shape=IMAGE_SHAPE)
    ])

generator_BtoA = Generator()
discriminator_A = Discriminator()

loss_obj = keras.losses.MeanSquaredError()

discriminator_A_optimizer = keras.optimizers.Adam(0.0002)

BATCH_SIZE = 32

@tf.function
def train_step():
    # training discriminator
    imagesA = tf.random.uniform([BATCH_SIZE]+IMAGE_SHAPE)
    imagesB = tf.random.uniform([BATCH_SIZE]+IMAGE_SHAPE)
    fakesA = generator_BtoA(imagesB, training=False)
    with tf.GradientTape(persistent=True) as tape:
        disc_fakesA = discriminator_A(fakesA, training=True)
        discA_loss = loss_obj(tf.zeros_like(disc_fakesA), disc_fakesA)
    gradients_discA = tape.gradient(discA_loss, discriminator_A.trainable_variables)
    discriminator_A_optimizer.apply_gradients(zip(gradients_discA, discriminator_A.trainable_variables))


from tensorflow.profiler.experimental import Trace as Trace_profiler, start as start_profiler, stop as stop_profiler

start_profiler(""my_logdir/"")
with Trace_profiler(""train"", step_num=1, _r=-1):
    train_step()
stop_profiler()
```

With this code, I get the following results in the memory profile tab :

Op Name | Allocation Size (GiBs) | Requested Size (GiBs) | Occurrences | Region type | Data type | Shape
-- | -- | -- | -- | -- | -- | --
sequential/conv2d/Conv2D | 0.227 | 0.227 | 1 | Â  | INVALID
sequential/conv2d/Conv2D | 0.039 | 0.039 | 1 | Â  | INVALID
sequential/conv2d/Conv2D | 0.023 | 0.023 | 1 | output | float | [32,3,256,256]
sequential/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer | 0.023 | 0.023 | 1 | output | float | [32,3,256,256]


How do you interprete these results ?
"
45945,SavedModel: KeyError on concrete_functions ,"
**System information**
-Google Colab
-Tensorflow version 2.4.0

I wrote encoder_network, decoder_network using Keras Functional API and saved in SavedModel Format.
But when I use 'saved_model_cli show --dir model  --all', it shows those errors

2020-12-23 17:23:49.847954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input_2'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 110, 470, 3)
        name: serving_default_input_2:0
    inputs['input_3'] tensor_info:
        dtype: DT_INT32
        shape: (-1, -1)
        name: serving_default_input_3:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['decoder_network'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1, 37)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict
Traceback (most recent call last):
  File ""/usr/local/bin/saved_model_cli"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 1185, in main
    args.func(args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 715, in show
    _show_all(args.dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 307, in _show_all
    _show_defined_functions(saved_model_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 187, in _show_defined_functions
    trackable_object = load.load(saved_model_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 603, in load
    return load_internal(export_dir, tags, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 633, in load_internal
    ckpt_options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 130, in __init__
    self._load_all()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 141, in _load_all
    self._load_nodes()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 283, in _load_nodes
    node, setter = self._recreate(proto, node_id)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 393, in _recreate
    return factory[kind]()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 382, in <lambda>
    ""function"": lambda: self._recreate_function(proto.function),
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 421, in _recreate_function
    proto, self._concrete_functions), setattr
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 261, in recreate_function
    concrete_function_objects.append(concrete_functions[concrete_function_name])
KeyError: '__inference_functional_3_layer_call_fn_718866'


Do you have any hint how to fix this ? What am I doing wrong here ? is it because I am using Keras Functional API?"
45944,Tensorflow 1.14 is not recognizing custom operator: Posenet_Decoder_Op,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.7
- Installed using virtualenv? pip? conda?: using virtualenv and pip
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source):  gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-11)
- CUDA/cuDNN version: --
- GPU model and memory: --


**Describe the problem**
I have been trying to run this model: [posenet](https://github.com/google-coral/project-posenet/blob/master/models/mobilenet/posenet_mobilenet_v1_075_481_641_quant_decoder.tflite) but it retrieves an error in the decoder part indicating you need a custom operator called PosenetDecoderOp. I investigated what to do and I approached this problem installing Tensorflow from source adding the custom operators. After compilation and instalation of Tensorflow I tried to run my code again:

              _``import numpy as np
              import tensorflow as tf
              
              #Load the TFLite model and allocate tensors.
              interpreter = tf.lite.Interpreter(model_path=""posenet_mobilenet_v1_075_481_641_quant_decoder.tflite"")
              interpreter.allocate_tensors()
              
              #Get input and output tensors.
              input_details = interpreter.get_input_details()
              output_details = interpreter.get_output_details()
              
              #Test the model on random input data.
              input_shape = input_details[0]['shape']
              input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
              interpreter.set_tensor(input_details[0]['index'], input_data)
              
              interpreter.invoke()
              output_data = interpreter.get_tensor(output_details[0]['index'])
              print(output_data)``_

And I got the following error:
`  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/ec2-user/decoder/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Traceback (most recent call last):
  File ""tf_run_model.py"", line 5, in <module>
    interpreter = tf.lite.Interpreter(model_path=""posenet_mobilenet_v1_075_481_641_quant_decoder.tflite"")
  File ""/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 76, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/local/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
  File ""/usr/local/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 696, in _load
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /home/ec2-user/decoder/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite3ops6custom27Register_POSENET_DECODER_OPEv`

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45943,[C++][FATAL]Calling FreezeSavedModel in cc/tools/saved_model.h causes protobuf::FatalException,"**Please fix this one ASAP. A machine learning API that can not properly export the trained model is fatal.**
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc-10
- CUDA/cuDNN version: cuda 11 installed but not used
- GPU model and memory: mx150
- protobuf version: 3.9.2 / 3.10.1 (tried both, former one is the designated version), 3.14.0 simply does not compile
- abseil version: abseil-cpp-20200225 with `absl/bas/option.h` configured as the one attached at the end of the code.

**Describe the current behavior**
Whenever FreezeSavedModel function is called in the code, tensorflow::ClientSession cannot execute properly.
Things work absolutely fine if FreezeSavedModel function is commented out.
Error message shown.
```
2020-12-23 10:10:29.483878: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-23 10:10:29.502660: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/map.h:1060] CHECK failed: it != end(): key not found: Reciprocal
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: it != end(): key not found: Reciprocal
Aborted (core dumped)
```

**Describe the expected behavior**
Execute without error.
```
2020-12-23 10:01:51.025954: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-23 10:01:51.046594: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz
Tensor<type: float shape: [1,150,150,3] values: [[[0.34117648 0.313725501 0.24313727]]]...>
Tensor<type: float shape: [150,150,3] values: [[0.34117648 0.313725501 0.24313727]]...>
```

**Standalone code to reproduce the issue**
Header file for CatDogCnn
```
/**
 * @file CatDogCnn.hpp
 * @author Hung-Tien Huang (paperbus72@gmail.com)
 * @brief A simplified AlexNet for classifying cats and dogs.
 * @version 0.1
 * @date 2020-12-16
 *
 * @copyright Copyright (c) 2020
 *
 */
#ifndef INCLUDE_TUTORIAL_02_CAT_DOG_CNN_CAT_DOG_CNN_H_
#define INCLUDE_TUTORIAL_02_CAT_DOG_CNN_CAT_DOG_CNN_H_

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/framework/gradients.h>
#include <tensorflow/cc/ops/image_ops.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/cc/ops/state_ops.h>
#include <tensorflow/cc/tools/freeze_saved_model.h>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/lib/io/path.h>
#include <tensorflow/core/platform/types.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/summary/summary_file_writer.h>

#include <filesystem>
#include <fstream>
#include <iostream>
#include <map>
#include <memory>
#include <string>
#include <tuple>
#include <unordered_set>
#include <variant>
#include <vector>

namespace tensorflow_tutorial {
class CatDogCnn {
 public:
  /**
   * @brief Construct a new Cat Dog Cnn object. Assume images are square.
   *
   * @param image_size the size of image
   * @param num_of_channels number of channels
   */
  CatDogCnn(const int image_size, const int num_of_channels);
  /**
   * @brief Create a Graph for Input Image. If unstack is false, the
   * output_image_tensor_node_ will have shape of [height, width, channel]. If
   * unstack is true, the ouput_image_tensor_node_ will have shape of [batch,
   * height, width, channel]. In addition, only the first element will be stored
   * to output_image_tensor_node_
   *
   * @param unstack unstack or not (set to true if batch mode, false otherwise)
   * @return tensorflow::Status of the current scope
   */
  tensorflow::Status CreateGraphForInputImage(bool unstack);

  /**
   * @brief Convert an image to a tensor
   *
   * @param path_to_image path to the jpeg file to be converted
   * @param out_tensor pointer to output tensor
   * @return tensorflow::Status of the current scope
   */
  tensorflow::Status ConvertImageToTensor(
      const std::filesystem::path& path_to_image,
      tensorflow::Tensor* out_tensor);

  /**
   * @brief Load all the image in a directory represented by dataset_root_path.
   * std::vector<std::pair<std::string, float>> folder_label is a vector of
   * [""string representation of class"", and float representation of the class].
   * std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>
   * image_tensors is [""tensor representation of an image"", type of class in
   * float, and the path to the actual image file]; its content will get cleared
   * prior to loading
   *
   * @param dataset_root_path path to the dataset's root folder
   * @param folder_label reference to the manually assigned type and each type's
   * numerical representation; label string MUST match the actual folder name
   * @param image_tensors pointer to the vector which images are to be stored
   * @return tensorflow::Status of the current scope
   */
  tensorflow::Status LoadDataset(
      const std::filesystem::path& dataset_root_path,
      const std::vector<std::pair<std::string, float>>& folder_label,
      std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>*
          image_tensors);

  /**
   * @brief Load entire dataset into memory and chunk them into batches.
   *
   * @param dataset_root_path path to the dataset's root folder
   * @param folder_label reference to the manually assigned type and each type's
   * numerical representation; label string MUST match the actual folder name
   * @param batch_size number of elements per batch
   * @param image_batch pointer to a vector of all image batches
   * @param label_batch pointer to a vector of all label batches
   * @return tensorflow::Status of the current scope
   */
  tensorflow::Status LoadBatach(
      const std::filesystem::path& dataset_root_path,
      const std::vector<std::pair<std::string, float>>& folder_labels,
      const size_t batch_size, std::vector<tensorflow::Tensor>* image_batch,
      std::vector<tensorflow::Tensor>* label_batch);

  /**
   * @brief returns an input object that performs Gloron/Xavier normal
   * initialization
   *
   * @param scope
   * @param in_chan
   * @param out_chan
   * @param filter_side
   * @return tensorflow::Input  an input node that returns initialized
   * parameters
   */
  tensorflow::Input GlorotUniformInitializer(const tensorflow::Scope& scope,
                                             const int input_channels,
                                             const int output_channels,
                                             const int filter_side = 0);

  /**
   * @brief Add convolution layer with relu and max pool. Weights are
   * initialized with Glorot/Xavier univorm initialization.
   *
   * @param layer_index
   * @param scope
   * @param input_channels
   * @param output_channels
   * @param filter_side
   * @param input
   * @return tensorflow::Input
   */
  tensorflow::Output AddConvLayer(const std::string& layer_index,
                                  const tensorflow::Scope& scope,
                                  const int input_channels,
                                  const int output_channels,
                                  const int filter_side,
                                  const tensorflow::Input& input);

  /**
   * @brief Add fully connected layer with or withot activation based on bool
   * need_activation variable.
   *
   * @param layer_index
   * @param scope
   * @param input_channels
   * @param output_channels
   * @param need_activation whether to append additional Relu at the end
   * @param input
   * @return tensorflow::Input
   */
  tensorflow::Output AddDenseLayer(const std::string& layer_index,
                                   const tensorflow::Scope& scope,
                                   const int input_channels,
                                   const int output_channels,
                                   const bool need_activation,
                                   const tensorflow::Input& input);

  /**
   * @brief Create a CNN Graph
   *
   * @param filter_side
   * @return tensorflow::Status
   */
  tensorflow::Status CreateGraphForCnn(const int filter_side);

  /**
   * @brief Create an Optimization(backpropagation) graph.
   *
   * @param learning_rate
   * @return tensorflow::Status
   */
  tensorflow::Status CreateGraphForOptimization(const float learning_rate);

  /**
   * @brief Use the image_batch and label_batch to train the neural network.
   *
   * @param image_batch
   * @param label_batch
   * @param results
   * @param loss
   * @return tensorflow::Status
   */
  tensorflow::Status Train(const tensorflow::Tensor& image_batch,
                           const tensorflow::Tensor& label_batch,
                           std::vector<float>* results, float* loss);

  /**
   * @brief Use the image_batch and label_batch provided to validate the neural
   * network.
   *
   * @param image_batch
   * @param label_batch
   * @param results
   * @return tensorflow::Status
   */
  tensorflow::Status Validate(const tensorflow::Tensor& image_batch,
                              const tensorflow::Tensor& label_batch,
                              std::vector<float>* results);

  /**
   * @brief Executes all the Assgin Variable nodes
   *
   * @return tensorflow::Status
   */
  tensorflow::Status Initialize();

  /**
   * @brief Freeze the model
   *
   * @param path
   * @return tensorflow::Status
   */
  tensorflow::Status FreezeModel(const std::filesystem::path& path);

  /**
   * @brief Load a frozen model from file.
   *
   * @param path
   * @return tensorflow::Status
   */
  tensorflow::Status LoadFrozenModel(const std::filesystem::path& path);

  /**
   * @brief Predict from frozen model
   *
   * @param image
   * @param result
   * @return tensorflow::Status
   */
  tensorflow::Status PredictFromFrozen(const tensorflow::Tensor& image,
                                       int* result);

 private:
  const int kImageSize;      // assume squre picture
  const int kNumOfChannels;  // RGB
  // load image
  tensorflow::Scope load_image_scope_;
  tensorflow::Output input_image_filename_node_;
  tensorflow::Output output_image_tensor_node_;
  // networks
  tensorflow::Scope network_root_scope_;
  std::unique_ptr<tensorflow::ClientSession> train_session_ptr_;
  std::unique_ptr<tensorflow::Session> frozen_session_ptr_;
  std::map<std::string, tensorflow::Output> variables_;
  std::map<std::string, tensorflow::TensorShape> variable_shapes_;
  std::map<std::string, tensorflow::Output> variable_assign_nodes_;
  // cnn variables
  tensorflow::Output input_batch_tensor_node_;
  tensorflow::Output input_labels_tensor_node_;
  tensorflow::Output drop_rate_node_;
  tensorflow::Output skip_drop_node_;
  tensorflow::Output output_classification_;
  tensorflow::Output squared_difference_node_;
  tensorflow::Output output_loss_logits_node_;
  const std::string kInputNodeName;
  const std::string kInputDropRateNodeName;
  const std::string kInputSkipDropNodeName;
  const std::string kOutputClassificationNodeName;
  // back propagation
  std::vector<tensorflow::Operation> adam_operations;
  std::vector<tensorflow::Output> weights_biases_;

  /**
   * @brief Shuffle the dataset randomly.
   *
   * @param image_tensors
   */
  static void ShuffleSet(
      std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>*
          image_tensors);
};
}  // namespace tensorflow_tutorial

#endif  // INCLUDE_TUTORIAL_02_CAT_DOG_CNN_CAT_DOG_CNN_H_
```
CatDogCnn.cpp
```
/**
 * @file CatDogCnn.cpp
 * @author Hung-Tien Huang (paperbus72@gmail.com)
 * @brief
 * @version 0.1
 * @date 2020-12-16
 *
 * @copyright Copyright (c) 2020
 *
 */

#include <tensorflow_tutorial/tutorial_02/CatDogCnn/CatDogCnn.hpp>

namespace tensorflow_tutorial {

CatDogCnn::CatDogCnn(const int image_size, const int num_of_channels)
    : kImageSize{image_size},
      kNumOfChannels{num_of_channels},
      load_image_scope_{tensorflow::Scope::NewRootScope()},
      network_root_scope_{tensorflow::Scope::NewRootScope()},
      kInputNodeName{""input_batch""},
      kInputDropRateNodeName{""drop_rate""},
      kInputSkipDropNodeName{""skip_drop""},
      kOutputClassificationNodeName{""output_classification""} { /*empty*/
}

tensorflow::Status CatDogCnn::CreateGraphForInputImage(bool unstack) {
  input_image_filename_node_ = tensorflow::ops::Placeholder{
      load_image_scope_.NewSubScope(""input_file_name""),
      tensorflow::DataType::DT_STRING};
  tensorflow::ops::ReadFile read_file_node{
      load_image_scope_.NewSubScope(""read_file""), input_image_filename_node_};
  tensorflow::ops::DecodeJpeg decode_image_node{
      load_image_scope_.NewSubScope(""decode_image""), read_file_node};
  // convert each pixel to float
  tensorflow::ops::Cast cast_float_node{
      load_image_scope_.NewSubScope(""cast_float""), decode_image_node,
      tensorflow::DataType::DT_FLOAT};
  // [height, width channel] -> [batch, height, width, channel]
  tensorflow::ops::ExpandDims expand_batch_dim_node{
      load_image_scope_.NewSubScope(""exapnd_batch_dim""), cast_float_node, 0};
  // resize image to square
  tensorflow::ops::ResizeBilinear resize_image_node(
      load_image_scope_.NewSubScope(""resize""), expand_batch_dim_node,
      {kImageSize, kImageSize});
  // divide each pixel by 255 so that each pixel is [0, 1]
  tensorflow::ops::Div normalize_image_node{
      load_image_scope_.NewSubScope(""normalize_image""),
      resize_image_node,
      {255.f}};
  if (unstack) {
    /**
     * @todo Unstack has documentation mismach. According to documentation, it
     * should be unstack along 0th axis. However, doing so cause segmentation
     * fault. When unstack along 1st axis, it is the expected behaviour when
     * unstack along 0th axis
     *
     */
    // unstack along batch axis
    // array of [height, width, channel]
    tensorflow::ops::Unstack unstack_image_node{
        load_image_scope_.NewSubScope(""unstack_image""), normalize_image_node,
        1};
    output_image_tensor_node_ = unstack_image_node[0];
  } else {
    output_image_tensor_node_ = normalize_image_node;
  }
  return load_image_scope_.status();
}

tensorflow::Status CatDogCnn::ConvertImageToTensor(
    const std::filesystem::path& path_to_image,
    tensorflow::Tensor* out_tensor) {
  if (load_image_scope_.ok() == false) {
    return load_image_scope_.status();
  }
  if (path_to_image.extension().string() != "".jpg"" &&
      path_to_image.extension().string() != "".jpeg"") {
    std::cerr << path_to_image.string() << "" is NOT *.jpeg"" << std::endl;
    /**
     * @todo cannot create status with error message. change back to
     * tensorflow::errors::InvalidArgument when tensorflow fix the bug
     *
     */
    // return tensorflow::errors::InvalidArgument(""Image must be jpeg encoded"");
    // return tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT,
    //                           err_msg,};
    return tensorflow::Status::OK();
  }
  std::vector<tensorflow::Tensor> out_tensors;
  tensorflow::ClientSession client_session{load_image_scope_};
  TF_CHECK_OK(
      client_session.Run({{input_image_filename_node_, path_to_image.string()}},
                         {output_image_tensor_node_}, &out_tensors));
  (*out_tensor) = std::move(out_tensors[0]);
  return load_image_scope_.status();
}

tensorflow::Status CatDogCnn::LoadDataset(
    const std::filesystem::path& dataset_root_path,
    const std::vector<std::pair<std::string, float>>& folder_labels,
    std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>*
        image_tensors) {
  image_tensors->clear();
  // for each class
  for (const std::pair<std::string, float>& current_class : folder_labels) {
    // create path to the curren class directory
    const std::filesystem::path current_class_path =
        dataset_root_path / current_class.first;
    // if the current class directory exists
    if (std::filesystem::exists(current_class_path)) {
      // for each file inside the current class directory
      for (const std::filesystem::directory_entry& p :
           std::filesystem::directory_iterator{current_class_path}) {
        const std::filesystem::path& current_image_path = p.path();
        // continue if not a jpeg file
        if (current_image_path.extension().string() != "".jpeg"" &&
            current_image_path.extension().string() != "".jpg"") {
          std::cerr << current_image_path << "" is NOT a jpeg"" << std::endl;
          continue;
        }
        tensorflow::Tensor current_image_tensor;
        TF_RETURN_IF_ERROR(
            ConvertImageToTensor(current_image_path, &current_image_tensor));
        image_tensors->emplace_back(
            std::tuple<tensorflow::Tensor, float, std::filesystem::path>{
                std::move(current_image_tensor), current_class.second,
                std::move(current_image_path)});
      }
      ShuffleSet(image_tensors);
      image_tensors->shrink_to_fit();
    } else {
      std::cerr << current_class_path << "" does NOT exists"" << std::endl;
      /**
       * @todo status is not ok!!
       *
       */
      return tensorflow::Status::OK();
    }
  }
  return load_image_scope_.status();
}
void CatDogCnn::ShuffleSet(
    std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>*
        image_tensors) {
  uint_fast32_t seed = static_cast<uint_fast32_t>(
      std::chrono::system_clock::now().time_since_epoch().count());
  std::shuffle(image_tensors->begin(), image_tensors->end(),
               std::mt19937{seed});
}

tensorflow::Status CatDogCnn::LoadBatach(
    const std::filesystem::path& dataset_root_path,
    const std::vector<std::pair<std::string, float>>& folder_labels,
    const size_t batch_size, std::vector<tensorflow::Tensor>* image_batch,
    std::vector<tensorflow::Tensor>* label_batch) {
  // clear batch prior to begin
  image_batch->clear();
  label_batch->clear();
  // load entire dataset in
  std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>
      entire_dataset;
  TF_RETURN_IF_ERROR(
      LoadDataset(dataset_root_path, folder_labels, &entire_dataset));
  // start to chunk batch
  auto begin = entire_dataset.begin();
  auto end = entire_dataset.begin() + batch_size;
  size_t num_of_batches = entire_dataset.size() / batch_size;
  if (num_of_batches * batch_size < entire_dataset.size()) {
    num_of_batches = num_of_batches + 1;
  }
  for (size_t i = 0; i < num_of_batches; ++i) {
    // if end exceeds the entire_dataset.end()
    if (end > entire_dataset.end()) {
      end = entire_dataset.end();
    }
    // break Tensor and label into two std::vector
    std::vector<tensorflow::Input> current_batch_image_v{};
    std::vector<tensorflow::Input> current_batch_label_v{};
    for (auto curr = begin; curr < end; ++curr) {
      current_batch_image_v.emplace_back(std::move(std::get<0>(*curr)));
      tensorflow::Tensor tmp{tensorflow::DataType::DT_FLOAT, {1}};
      tmp.scalar<float>()(0) = std::get<1>(*curr);
      current_batch_label_v.emplace_back(std::move(tmp));
    }
    // convert std::vector to tensorflow::InputList
    tensorflow::InputList current_batch_image{current_batch_image_v};
    tensorflow::InputList current_batch_label{current_batch_label_v};
    // stack all the image in current batch into one big tensor
    tensorflow::Scope root = tensorflow::Scope::NewRootScope();
    tensorflow::ops::Stack stack_current_image_batch_node{root,
                                                          current_batch_image};
    tensorflow::ops::Stack stack_current_label_batch_node{root,
                                                          current_batch_label};
    TF_CHECK_OK(root.status());
    tensorflow::ClientSession client_session{root};
    std::vector<tensorflow::Tensor> current_batch;
    TF_CHECK_OK(client_session.Run(
        {stack_current_image_batch_node, stack_current_label_batch_node},
        &current_batch));
    image_batch->emplace_back(std::move(current_batch[0]));
    label_batch->emplace_back(std::move(current_batch[1]));
    // increment the entire_dataset being processed
    begin = end;
    // break if begin reaches the end
    if (begin == entire_dataset.end()) {
      break;
    }
    end = end + batch_size;
  }
  return tensorflow::Status::OK();
}

tensorflow::Input CatDogCnn::GlorotUniformInitializer(
    const tensorflow::Scope& scope, const int input_channels,
    const int output_channels, const int filter_side) {
  float std = 0.0f;
  tensorflow::Tensor tensor_shape;
  if (filter_side == 0) {  // dense
    std = std::sqrt(6.0f / (input_channels + output_channels));
    tensor_shape = tensorflow::Tensor{tensorflow::DT_INT64, {2}};
    auto v = tensor_shape.vec<tensorflow::int64>();
    v(0) = input_channels;
    v(1) = output_channels;
  } else {  // conv
    std = std::sqrt(6.0f / (filter_side * filter_side *
                            (input_channels + output_channels)));
    tensor_shape = tensorflow::Tensor{tensorflow::DT_INT64, {4}};
    auto v = tensor_shape.vec<tensorflow::int64>();
    v(0) = filter_side;
    v(1) = filter_side;
    v(2) = input_channels;
    v(3) = output_channels;
  }
  // std::cout << tensor_shape.DebugString() << std::endl;
  // rand_node returns tensor with value [0,1]
  tensorflow::ops::RandomUniform rand_node{scope, tensor_shape,
                                           tensorflow::DT_FLOAT};
  // ([0, 1] - [.5, .5]) * 2.0f = [-1, 1]
  return tensorflow::ops::Multiply{
      scope, tensorflow::ops::Sub{scope, rand_node, 0.5f}, std * 2.0f};
}

tensorflow::Output CatDogCnn::AddConvLayer(const std::string& layer_index,
                                           const tensorflow::Scope& scope,
                                           const int input_channels,
                                           const int output_channels,
                                           const int filter_side,
                                           const tensorflow::Input& input) {
  const std::string kCurrWeightStr{""weight_"" + layer_index};
  const std::string kCurrBiasStr{""bias_"" + layer_index};
  // conv2d weights
  tensorflow::TensorShape shape{filter_side, filter_side, input_channels,
                                output_channels};
  variables_[kCurrWeightStr] =
      tensorflow::ops::Variable{scope.NewSubScope(kCurrWeightStr + ""_var""),
                                shape, tensorflow::DataType::DT_FLOAT};
  variable_shapes_[kCurrWeightStr] = shape;
  variable_assign_nodes_[kCurrWeightStr] = tensorflow::ops::Assign{
      scope.NewSubScope(kCurrWeightStr + ""_assign""), variables_[kCurrWeightStr],
      GlorotUniformInitializer(scope, input_channels, output_channels,
                               filter_side)};
  // bias after conv2d
  shape = {output_channels};
  variables_[kCurrBiasStr] =
      tensorflow::ops::Variable{scope.NewSubScope(kCurrBiasStr + ""_var""), shape,
                                tensorflow::DataType::DT_FLOAT};
  variable_shapes_[kCurrBiasStr] = shape;
  variable_assign_nodes_[kCurrBiasStr] = tensorflow::ops::Assign{
      scope.NewSubScope(kCurrBiasStr + ""_assign""), variables_[kCurrBiasStr],
      tensorflow::Input::Initializer{0.0f, shape}};
  /**
   * @todo tensorflow::StringPiece is using absl::string_view, which is being
   * redirected to std::string_view and causes error.
   *
   */
  // here have to use assign node as input, otherwise the variables will be
  // uninitialized
  tensorflow::ops::Conv2D conv_2d_node{scope.WithOpName(""Conv""),
                                       input,
                                       variables_[kCurrWeightStr],
                                       {1, 1, 1, 1},
                                       ""SAME""};
  tensorflow::ops::BiasAdd bias_add_node{
      scope.WithOpName(""bias_add""), conv_2d_node, variables_[kCurrBiasStr]};
  tensorflow::ops::Relu relu_node{scope.WithOpName(""relu""), bias_add_node};
  return tensorflow::ops::MaxPoolV2{scope.WithOpName(""max_pool""),
                                    relu_node,
                                    {1, 2, 2, 1},
                                    {1, 2, 2, 1},
                                    ""SAME""};
}

tensorflow::Output CatDogCnn::AddDenseLayer(const std::string& layer_index,
                                            const tensorflow::Scope& scope,
                                            const int input_channels,
                                            const int output_channels,
                                            const bool need_activation,
                                            const tensorflow::Input& input) {
  const std::string kCurrWeightStr{""weight_"" + layer_index};
  const std::string kCurrBiasStr{""bias_"" + layer_index};
  tensorflow::TensorShape shape{input_channels, output_channels};
  variables_[kCurrWeightStr] =
      tensorflow::ops::Variable{scope.NewSubScope(kCurrWeightStr + ""_var""),
                                shape, tensorflow::DataType::DT_FLOAT};
  variable_shapes_[kCurrWeightStr] = shape;
  variable_assign_nodes_[kCurrWeightStr] = tensorflow::ops::Assign{
      scope.NewSubScope(kCurrWeightStr + ""_assign""), variables_[kCurrWeightStr],
      GlorotUniformInitializer(scope, input_channels, output_channels)};
  shape = {output_channels};
  variables_[kCurrBiasStr] =
      tensorflow::ops::Variable{scope.NewSubScope(kCurrBiasStr + ""_var""), shape,
                                tensorflow::DataType::DT_FLOAT};
  variable_shapes_[kCurrBiasStr] = shape;
  variable_assign_nodes_[kCurrBiasStr] = tensorflow::ops::Assign{
      scope.NewSubScope(kCurrBiasStr + ""_assign""), variables_[kCurrBiasStr],
      tensorflow::Input::Initializer{0.0f, shape}};
  // here have to use variable assign node as input, otherwise variables will be
  // uninitialized
  tensorflow::ops::MatMul multiply_weight_node{
      scope.WithOpName(""multiply_weight""), input, variables_[kCurrWeightStr]};
  tensorflow::ops::Add add_bias_node{scope.WithOpName(""add_bais""),
                                     multiply_weight_node,
                                     variables_[kCurrBiasStr]};
  if (need_activation) {
    return tensorflow::ops::Relu{scope.WithOpName(""relu""), add_bias_node};
  }
  return add_bias_node;
}

tensorflow::Status CatDogCnn::CreateGraphForCnn(const int filter_side) {
  input_batch_tensor_node_ = tensorflow::ops::Placeholder(
      network_root_scope_.WithOpName(kInputNodeName),
      tensorflow::DataType::DT_FLOAT);
  drop_rate_node_ = tensorflow::ops::Placeholder{
      network_root_scope_.WithOpName(kInputDropRateNodeName),
      tensorflow::DataType::DT_FLOAT};
  skip_drop_node_ = tensorflow::ops::Placeholder{
      network_root_scope_.WithOpName(kInputSkipDropNodeName),
      tensorflow::DataType::DT_FLOAT};
  // conv 1
  tensorflow::Scope scope_conv1 = network_root_scope_.NewSubScope(""conv1"");
  int input_channels = kNumOfChannels;
  int output_channels = 32;
  tensorflow::Output pool1_node =
      AddConvLayer(""1"", scope_conv1, input_channels, output_channels,
                   filter_side, input_batch_tensor_node_);
  int new_side = std::ceil(kImageSize / 2.0f);

  // conv 2
  tensorflow::Scope scope_conv2 = network_root_scope_.NewSubScope(""conv2"");
  input_channels = output_channels;
  output_channels = 64;
  tensorflow::Output pool2_node =
      AddConvLayer(""2"", scope_conv2, input_channels, output_channels,
                   filter_side, pool1_node);
  new_side = std::ceil(new_side / 2.0f);

  // conv_3
  tensorflow::Scope scope_conv3 = network_root_scope_.NewSubScope(""conv3"");
  input_channels = output_channels;
  output_channels = 128;
  tensorflow::Output pool3_node =
      AddConvLayer(""3"", scope_conv3, input_channels, output_channels,
                   filter_side, pool2_node);
  new_side = std::ceil(new_side / 2.0f);

  // conv_4
  tensorflow::Scope scope_conv4 = network_root_scope_.NewSubScope(""conv4"");
  input_channels = output_channels;
  output_channels = 128;
  tensorflow::Output pool4_node =
      AddConvLayer(""4"", scope_conv4, input_channels, output_channels,
                   filter_side, pool3_node);
  new_side = std::ceil(new_side / 2.0f);

  // flatten
  tensorflow::Scope scope_flatten = network_root_scope_.NewSubScope(""flatten"");
  int flat_length = new_side * new_side * output_channels;
  tensorflow::ops::Reshape flat_node{
      scope_flatten, pool4_node, {-1, flat_length}};

  // dropout
  tensorflow::Scope scope_dropout = network_root_scope_.NewSubScope(""dropout"");
  // [0,1)
  tensorflow::ops::RandomUniform rand_node{
      scope_dropout, tensorflow::ops::Shape{scope_dropout, flat_node},
      tensorflow::DataType::DT_FLOAT};
  // binary = floor(rand + (1 - drop_rate) + skip_drop)
  tensorflow::ops::Floor drop_out_binary_node{
      scope_dropout,
      tensorflow::ops::Add{
          scope_dropout, rand_node,
          tensorflow::ops::Add{
              scope_dropout,
              tensorflow::ops::Sub{scope_dropout, 1.0f, drop_rate_node_},
              skip_drop_node_}}};
  // multiply elementwise
  tensorflow::ops::Multiply apply_drop_out_node{
      scope_dropout.WithOpName(""apply_drop_out""),
      tensorflow::ops::Div{scope_dropout, flat_node, drop_rate_node_},
      drop_out_binary_node};

  // dense1
  input_channels = flat_length;
  output_channels = 512;
  tensorflow::Scope scope_dense1 = network_root_scope_.NewSubScope(""dense1"");
  tensorflow::Output relu5 =
      AddDenseLayer(""dense1"", scope_dense1, input_channels, output_channels,
                    false, apply_drop_out_node);
  // dense2
  input_channels = output_channels;
  output_channels = 256;
  tensorflow::Scope scope_dense2 = network_root_scope_.NewSubScope(""dense2"");
  tensorflow::Output relu6 = AddDenseLayer(
      ""dense2"", scope_dense2, input_channels, output_channels, false, relu5);
  // desne3
  input_channels = output_channels;
  output_channels = 1;
  tensorflow::Scope scope_dense3 = network_root_scope_.NewSubScope(""dense3"");
  tensorflow::Output relu7 = AddDenseLayer(
      ""dense3"", scope_dense3, input_channels, output_channels, false, relu6);

  output_classification_ = tensorflow::ops::Sigmoid{
      network_root_scope_.WithOpName(kOutputClassificationNodeName), relu7};

  return network_root_scope_.status();
}

tensorflow::Status CatDogCnn::CreateGraphForOptimization(
    const float learning_rate) {
  input_labels_tensor_node_ = tensorflow::ops::Placeholder{
      network_root_scope_.WithOpName(""input_labels_tensor""),
      tensorflow::DataType::DT_FLOAT};
  tensorflow::Scope scope_loss = network_root_scope_.NewSubScope(""loss"");
  TF_CHECK_OK(scope_loss.status());
  // calculate loos
  squared_difference_node_ = tensorflow::ops::SquaredDifference{
      scope_loss, output_classification_, input_labels_tensor_node_};
  output_loss_logits_node_ =
      tensorflow::ops::Mean{scope_loss.WithOpName(""output_loss_logits""),
                            squared_difference_node_,
                            {0}};
  TF_CHECK_OK(scope_loss.status());
  for (std::pair<std::string, tensorflow::Output> v : variables_) {
    weights_biases_.push_back(v.second);
  }
  std::vector<tensorflow::Output> grad_outputs{};
  TF_CHECK_OK(tensorflow::AddSymbolicGradients(network_root_scope_,
                                               {output_loss_logits_node_},
                                               weights_biases_, &grad_outputs));
  int index = 0;
  for (std::pair<std::string, tensorflow::Output> v : variables_) {
    std::string index_str = std::to_string(index);
    // m and v are described in API documentation
    tensorflow::ops::Variable m_var{network_root_scope_,
                                    variable_shapes_[v.first],
                                    tensorflow::DataType::DT_FLOAT};
    tensorflow::ops::Variable v_var{network_root_scope_,
                                    variable_shapes_[v.first],
                                    tensorflow::DataType::DT_FLOAT};
    variable_assign_nodes_[""m_"" + index_str] = tensorflow::ops::Assign{
        network_root_scope_, m_var,
        tensorflow::Input::Initializer{0.f, variable_shapes_[v.first]}};
    variable_assign_nodes_[""v_"" + index_str] = tensorflow::ops::Assign{
        network_root_scope_, v_var,
        tensorflow::Input::Initializer{0.0f, variable_shapes_[v.first]}};
    tensorflow::ops::ApplyAdam apply_adam{network_root_scope_,
                                          v.second,
                                          m_var,
                                          v_var,
                                          0.0f,
                                          0.0f,
                                          learning_rate,
                                          0.9f,
                                          0.999f,
                                          0.00000001f,
                                          {grad_outputs[index]}};
    adam_operations.emplace_back(std::move(apply_adam.operation));
    ++index;
  }
  return network_root_scope_.status();
}

tensorflow::Status CatDogCnn::Initialize() {
  if (network_root_scope_.ok() == false) {
    return network_root_scope_.status();
  }
  std::vector<tensorflow::Output> assign_operations{};
  for (std::pair<std::string, tensorflow::Output> i : variable_assign_nodes_) {
    assign_operations.emplace_back(i.second);
  }
  train_session_ptr_.reset(new tensorflow::ClientSession{network_root_scope_});
  TF_CHECK_OK(train_session_ptr_->Run(assign_operations, nullptr));
  return network_root_scope_.status();
}

tensorflow::Status CatDogCnn::Train(const tensorflow::Tensor& image_batch,
                                    const tensorflow::Tensor& label_batch,
                                    std::vector<float>* results, float* loss) {
  if (network_root_scope_.ok() == false) {
    return network_root_scope_.status();
  }
  std::vector<tensorflow::Tensor> output_tensors{};
  TF_CHECK_OK(train_session_ptr_->Run(
      {
          {input_batch_tensor_node_, image_batch},
          {input_labels_tensor_node_, label_batch},
          {drop_rate_node_, 0.5f},
          {skip_drop_node_, 0.0f},
      },
      {output_loss_logits_node_, output_classification_}, adam_operations,
      &output_tensors));

  *loss = output_tensors[0].scalar<float>()(0);
  auto label_mat = label_batch.matrix<float>();
  auto classification_mat = output_tensors[1].matrix<float>();
  for (size_t i = 0; i < label_mat.dimension(0); ++i) {
    results->push_back(
        (std::fabs(classification_mat(i, 0) - label_mat(i, 0)) > 0.5f) ? 0 : 1);
  }
  return network_root_scope_.status();
}

tensorflow::Status CatDogCnn::Validate(const tensorflow::Tensor& image_batch,
                                       const tensorflow::Tensor& label_batch,
                                       std::vector<float>* results) {
  if (network_root_scope_.ok() == false) {
    return network_root_scope_.status();
  }
  std::vector<tensorflow::Tensor> output_tensors;
  TF_CHECK_OK(train_session_ptr_->Run(
      {
          {input_batch_tensor_node_, image_batch},
          {input_labels_tensor_node_, label_batch},
          {drop_rate_node_, 1.0f},
          {skip_drop_node_, 1.0f},
      },
      {output_classification_}, &output_tensors));
  auto label_mat = label_batch.matrix<float>();
  auto classification_mat = output_tensors[0].matrix<float>();
  for (size_t i = 0; i < label_mat.dimension(0); ++i) {
    results->push_back(
        (std::fabs(classification_mat(i, 0) - label_mat(i, 0)) > 0.5f) ? 0 : 1);
  }
  return network_root_scope_.status();
}

tensorflow::Status CatDogCnn::FreezeModel(const std::filesystem::path& path) {
  std::vector<tensorflow::Tensor> output_tensors{};
  TF_CHECK_OK(train_session_ptr_->Run(weights_biases_, &output_tensors));
  std::unordered_map<std::string, tensorflow::Tensor> name_variable_map;
  for (size_t i = 0; i < weights_biases_.size(); ++i) {
    name_variable_map[weights_biases_[i].node()->name()] = output_tensors[i];
  }
  tensorflow::GraphDef graph_def{};
  TF_CHECK_OK(network_root_scope_.ToGraphDef(&graph_def));
  tensorflow::SavedModelBundle saved_model_bundle{};
  tensorflow::SignatureDef signature_def{};
  (*(signature_def.mutable_inputs()))[input_batch_tensor_node_.name()].set_name(
      input_batch_tensor_node_.name());
  (*(signature_def.mutable_inputs()))[output_classification_.name()].set_name(
      output_classification_.name());
  tensorflow::MetaGraphDef* meta_graph_def =
      &(saved_model_bundle.meta_graph_def);
  (*(meta_graph_def->mutable_signature_def()))[""signature_def""] = signature_def;
  *(meta_graph_def->mutable_graph_def()) = graph_def;
  tensorflow::SessionOptions options{};
  saved_model_bundle.session.reset(tensorflow::NewSession(options));
  tensorflow::GraphDef frozen_graph_def{};
  std::unordered_set<std::string> inputs;
  std::unordered_set<std::string> outputs;
  // <---------------- uncomment following causes runtime exception start ---------------->
  TF_CHECK_OK(tensorflow::FreezeSavedModel(saved_model_bundle, &frozen_graph_def, &inputs, &outputs));
  // <--------------- uncomment following causes runtime exception end ---------------->
  return tensorflow::WriteBinaryProto(tensorflow::Env::Default(), path.string(),
                                      frozen_graph_def);
}

tensorflow::Status CatDogCnn::LoadFrozenModel(
    const std::filesystem::path& path) {
  // std::unique_ptr<tensorflow::GraphDef> graph_def_ptr;
  // tensorflow::SessionOptions options{};
  // frozen_session_ptr_.reset(tensorflow::NewSession(options));
  // graph_def_ptr.reset(new tensorflow::GraphDef{});
  // TF_CHECK_OK(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),
  //                                         path.string(),
  //                                         graph_def_ptr.get()));
  // return frozen_session_ptr_->Create(*graph_def_ptr.get());
}

tensorflow::Status CatDogCnn::PredictFromFrozen(const tensorflow::Tensor& image,
                                                int* result) {
  // std::vector<tensorflow::Tensor> output_tensors{};
  // tensorflow::Tensor t{tensorflow::DataType::DT_FLOAT,
  //                      tensorflow::TensorShape{{1}}};
  // t.scalar<float>()(0) = 1.0f;
  // // drop rate and skip drop rate both set to 1
  // TF_RETURN_IF_ERROR(frozen_session_ptr_->Run(
  //     {
  //         {kInputNodeName, image},
  //         {kInputDropRateNodeName, t},
  //         {kInputSkipDropNodeName, t},
  //     },
  //     {kOutputClassificationNodeName}, {}, &output_tensors));
  // auto mat = output_tensors[0].matrix<float>();
  // *result = (mat(0, 0) > 0.5f) ? 1 : 0;
  // return tensorflow::Status::OK();
}

}  // namespace tensorflow_tutorial

```
hearder file for entry point (main.hpp)
```
/**
 * @file tutorial_02.hpp
 * @author your name (you@domain.com)
 * @brief
 * @version 0.1
 * @date 2020-12-16
 *
 * @copyright Copyright (c) 2020
 *
 */
#ifndef INCLUDE_TENSORFLOW_TUTORIAL_TUTORIAL_02_HPP_
#define INCLUDE_TENSORFLOW_TUTORIAL_TUTORIAL_02_HPP_

#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/summary/summary_file_writer.h>

#include <chrono>
#include <iostream>
#include <map>
#include <tensorflow_tutorial/tutorial_02/CatDogCnn/CatDogCnn.hpp>
#include <tuple>
#include <vector>

void TestCnn(const int argc, char** argv);

void TestConvertImageToTensor(const int argc, char** argv);
void TestLoadDataset(const int argc, char** argv);
void TestLoadBatch(const int argc, char** argv);
void TestAddConvLayer(const int argc, char** argv);
void TestAddDenseLayer(const int argc, char** argv);

void AssignVariable(const tensorflow::ClientSession& session,
                    const tensorflow::ops::Assign& variable_assign_node);
void ProperAssignVariableExample(const int argc, char** argv);

#endif  // INCLUDE_TENSORFLOW_TUTORIAL_TUTORIAL_02_HPP_

```

entry point to cause the exception (main.cpp)
```
/**
 * @file tutorial_02.cpp
 * @author Hung-Tien Huang (paperbus72@gmail.com)
 * @brief
 * @version 0.1
 * @date 2020-12-16
 *
 * @copyright Copyright (c) 2020
 *
 */

#include <tensorflow_tutorial/tutorial_02/tutorial_02.hpp>

int main(int argc, char** argv) {
  // TestCnn(argc, argv);

  TestConvertImageToTensor(argc, argv);
  // TestLoadDataset(argc, argv);
  // TestLoadBatch(argc, argv);
  // TestAddConvLayer(argc, argv);
  // TestAddDenseLayer(argc, argv);
  // ProperAssignVariableExample(argc, argv);
  return 0;
}

void TestCnn(const int argc, char** argv) {
  if (argc != 3) {
    std::cerr << ""./tutorial_2 /path/to/base/folder ""
                 ""/path/to/export/filename/without/extension""
              << std::endl;
    exit(EXIT_FAILURE);
  }
  const std::filesystem::path kBaseFolder{argv[1]};
  const std::filesystem::path kTrainFolder = kBaseFolder / ""train"";
  const std::filesystem::path kValidateFolder = kBaseFolder / ""validation"";
  const std::filesystem::path kExportPath =
      std::filesystem::path{argv[2]}.replace_extension("".pb"");
  std::filesystem::create_directories(kExportPath.parent_path());
  const size_t kImageSide = 150;
  const size_t kImageChannels = 3;
  const size_t kBatchSize = 20;
  const size_t kFilterSide = 3;

  tensorflow_tutorial::CatDogCnn model{150, 3};
  // load dataset
  TF_CHECK_OK(model.CreateGraphForInputImage(true));
  std::vector<tensorflow::Tensor> train_image_batch;
  std::vector<tensorflow::Tensor> train_label_batch;
  std::vector<tensorflow::Tensor> validate_image_batch;
  std::vector<tensorflow::Tensor> validate_label_batch;
  TF_CHECK_OK(model.LoadBatach(
      kTrainFolder, {std::make_pair(""cats"", 0), std::make_pair(""dogs"", 1)},
      kBatchSize, &train_image_batch, &train_label_batch));
  TF_CHECK_OK(model.LoadBatach(
      kValidateFolder, {std::make_pair(""cats"", 0), std::make_pair(""dogs"", 1)},
      kBatchSize, &validate_image_batch, &validate_label_batch));
  // check data length
  assert(train_image_batch.size() == train_label_batch.size());
  assert(validate_image_batch.size() == validate_label_batch.size());
  const size_t num_epochs = 30;
  const size_t kNumTrainBatches = train_label_batch.size();
  const size_t kNumValidateBatches = validate_label_batch.size();

  // std::cout << kNumTrainBatches << "" "" << kNumValidateBatches << std::endl;
  // std::cout << train_image_batch[0].DebugString() << std::endl;
  // std::cout << train_image_batch[20].DebugString() << std::endl;
  // std::cout << train_label_batch[0].DebugString() << std::endl;
  // std::cout << train_label_batch[20].DebugString() << std::endl;

  // build cnn graph
  TF_CHECK_OK(model.CreateGraphForCnn(kFilterSide));
  TF_CHECK_OK(model.CreateGraphForOptimization(0.0002f));
  TF_CHECK_OK(model.Initialize());
  // Epoch / Step loops
  for (int epoch = 0; epoch < num_epochs; epoch++) {
    std::cout << ""Epoch "" << epoch + 1 << ""/"" << num_epochs << "":"";
    auto t1 = std::chrono::high_resolution_clock::now();
    float loss_sum = 0;
    float accuracy_sum = 0;
    for (int b = 0; b < kNumTrainBatches; b++) {
      std::vector<float> results;
      float loss;
      TF_CHECK_OK(model.Train(train_image_batch[b], train_label_batch[b],
                              &results, &loss));
      loss_sum += loss;
      accuracy_sum +=
          accumulate(results.begin(), results.end(), 0.f) / results.size();
      std::cout << ""."";
    }
    std::cout << std::endl << ""Validation:"";
    float validation_sum = 0;
    for (int c = 0; c < kNumValidateBatches; c++) {
      std::vector<float> results;
      TF_CHECK_OK(model.Validate(validate_image_batch[c],
                                 validate_label_batch[c], &results));
      validation_sum +=
          accumulate(results.begin(), results.end(), 0.f) / results.size();
      std::cout << ""."";
    }
    std::cout << std::endl;
    auto t2 = std::chrono::high_resolution_clock::now();
    std::cout
        << ""Time: ""
        << std::chrono::duration_cast<std::chrono::seconds>(t2 - t1).count()
        << "" seconds "";
    std::cout << ""Loss: "" << loss_sum / kNumTrainBatches
              << "" Results accuracy: "" << accuracy_sum / kNumTrainBatches
              << "" Validation accuracy: ""
              << validation_sum / kNumValidateBatches << std::endl;
    // TF_CHECK_OK(model.FreezeModel(kExportPath.string()));
  }
}

void TestConvertImageToTensor(const int argc, char** argv) {
  if (argc != 2) {
    std::cerr << ""./tutorial_02 /path/to/image"" << std::endl;
    exit(EXIT_FAILURE);
  }
  const std::filesystem::path path_to_image{argv[1]};
  tensorflow_tutorial::CatDogCnn cat_dog_model{150, 3};
  tensorflow::Tensor image_tensor;
  cat_dog_model.CreateGraphForInputImage(false);
  cat_dog_model.ConvertImageToTensor(path_to_image, &image_tensor);
  std::cout << image_tensor.DebugString() << std::endl;
  cat_dog_model.CreateGraphForInputImage(true);
  cat_dog_model.ConvertImageToTensor(path_to_image, &image_tensor);
  std::cout << image_tensor.DebugString() << std::endl;
}

void TestLoadDataset(const int argc, char** argv) {
  if (argc != 2) {
    std::cerr << ""./tutorial_02 /path/to/root/folder"" << std::endl;
    exit(EXIT_FAILURE);
  }
  const std::filesystem::path path_to_root_dir{argv[1]};
  tensorflow_tutorial::CatDogCnn cat_dog_model{150, 3};
  std::vector<std::pair<std::string, float>> label = {std::pair{""cats"", 0},
                                                      std::pair{""dogs"", 1}};
  std::cout << ""Remain Stacked:"" << std::endl;
  cat_dog_model.CreateGraphForInputImage(false);
  std::vector<std::tuple<tensorflow::Tensor, float, std::filesystem::path>>
      image_tensors;
  cat_dog_model.LoadDataset(path_to_root_dir, label, &image_tensors);
  std::cout << ""Number of Samples: "" << image_tensors.size() << std::endl;
  std::cout << ""Shape of 0th element: ""
            << std::get<0>(image_tensors[0]).DebugString()
            << ""\nclass of 0th element (float): ""
            << std::get<1>(image_tensors[0])
            << ""\npath to 0th element: "" << std::get<2>(image_tensors[0])
            << std::endl;
  std::cout << ""dataset size: "" << image_tensors.size() << std::endl;
  std::cout << ""Unstacked:"" << std::endl;
  image_tensors.clear();
  cat_dog_model.CreateGraphForInputImage(true);
  cat_dog_model.LoadDataset(path_to_root_dir, label, &image_tensors);
  std::cout << ""Number of Samples: "" << image_tensors.size() << std::endl;
  std::cout << ""Shape of 0th element: ""
            << std::get<0>(image_tensors[0]).DebugString()
            << ""\nclass of 0th element (float): ""
            << std::get<1>(image_tensors[0])
            << ""\npath to 0th element: "" << std::get<2>(image_tensors[0])
            << std::endl;
  std::cout << ""dataset size: "" << image_tensors.size() << std::endl;
}

void TestLoadBatch(const int argc, char** argv) {
  if (argc != 3) {
    std::cerr << ""./tutorial_02 /path/to/root/folder number_of_batches""
              << std::endl;
    exit(EXIT_FAILURE);
  }
  const std::filesystem::path path_to_root_dir{argv[1]};
  const size_t kNumOfBatches = std::stoul(argv[2]);
  std::vector<std::pair<std::string, float>> label = {std::pair{""cats"", 0},
                                                      std::pair{""dogs"", 1}};
  tensorflow_tutorial::CatDogCnn cat_dog_model{150, 3};
  cat_dog_model.CreateGraphForInputImage(true);
  std::vector<tensorflow::Tensor> image_batches;
  std::vector<tensorflow::Tensor> label_batches;
  cat_dog_model.LoadBatach(path_to_root_dir, label, kNumOfBatches,
                           &image_batches, &label_batches);
  std::cout << ""image_batches: \nLength : "" << image_batches.size()
            << std::endl;
  std::cout << image_batches[0].DebugString() << std::endl;
  std::cout << ""label_batches: \nLength : "" << label_batches.size()
            << std::endl;
  std::cout << label_batches[0].DebugString() << std::endl;
}

void TestAddConvLayer(const int argc, char** argv) {
  std::cout << ""test add conv layer"" << std::endl;
  std::vector<tensorflow::Tensor> output_tensors{};
  tensorflow_tutorial::CatDogCnn cat_dog_model{150, 3};
  tensorflow::Scope scope = tensorflow::Scope::NewRootScope();
  tensorflow::TensorShape shape{1, 150, 150, 3};
  tensorflow::Output input_variable = tensorflow::ops::Variable{
      scope.NewSubScope(""input""), shape, tensorflow::DataType::DT_FLOAT};
  tensorflow::ops::Assign assign_input_var{
      scope.NewSubScope(""assign_input""), input_variable,
      tensorflow::Input::Initializer{0.3f, shape}};
  tensorflow::Output conv = cat_dog_model.AddConvLayer(
      ""conv_test"", scope.NewSubScope(""conv_test""), 3, 32, 3, assign_input_var);
  tensorflow::ClientSession client{scope};

  TF_CHECK_OK(client.Run({conv}, &output_tensors));
  std::cout << output_tensors.size() << std::endl;

  tensorflow::GraphDef graph;
  TF_CHECK_OK(scope.ToGraphDef(&graph));
  tensorflow::SummaryWriterInterface* w;
  TF_CHECK_OK(tensorflow::CreateSummaryFileWriter(
      1, 0, ""."", "".img-graph"", tensorflow::Env::Default(), &w));
  TF_CHECK_OK(w->WriteGraph(0, std::make_unique<tensorflow::GraphDef>(graph)));

  if (!scope.ok()) {
    std::cerr << ""ERROR"" << scope.status().error_message() << std::endl;
    exit(EXIT_FAILURE);
  }
  std::cout << output_tensors[0].DebugString() << std::endl;
}

void TestAddDenseLayer(const int argc, char** argv) {
  std::cout << ""test add dense layer"" << std::endl;

  // initialize input variables
  tensorflow::Scope scope = tensorflow::Scope::NewRootScope();
  tensorflow::TensorShape shape{1, 30};
  tensorflow::ops::Variable input_variable{scope.NewSubScope(""input""), shape,
                                           tensorflow::DataType::DT_FLOAT};
  tensorflow::ops::Assign assign_input_var{
      scope.NewSubScope(""assign_input""), input_variable,
      tensorflow::Input::Initializer{0.3f, shape}};
  tensorflow::ClientSession client{scope};
  AssignVariable(client, assign_input_var);

  // add dense layer
  tensorflow_tutorial::CatDogCnn cat_dog_model{10, 3};
  tensorflow::Output dense = cat_dog_model.AddDenseLayer(
      ""dense_layer"", scope.NewSubScope(""dense_layer""), 30, 5, false,
      input_variable);

  // execute the graph
  std::vector<tensorflow::Tensor> output_tensors{};
  TF_CHECK_OK(client.Run({dense}, &output_tensors));
  std::cout << output_tensors.size() << std::endl;

  // output graph
  tensorflow::GraphDef graph;
  TF_CHECK_OK(scope.ToGraphDef(&graph));
  tensorflow::SummaryWriterInterface* w;
  TF_CHECK_OK(tensorflow::CreateSummaryFileWriter(
      1, 0, ""."", "".img-graph"", tensorflow::Env::Default(), &w));
  TF_CHECK_OK(w->WriteGraph(0, std::make_unique<tensorflow::GraphDef>(graph)));

  // print string
  std::cout << output_tensors[0].DebugString() << std::endl;
}

void AssignVariable(const tensorflow::ClientSession& session,
                    const tensorflow::ops::Assign& variable_assign_node) {
  std::vector<tensorflow::Tensor> output{};
  TF_CHECK_OK(session.Run(
      {}, {}, {tensorflow::Operation{variable_assign_node.node()}}, &output));
}

void ProperAssignVariableExample(const int argc, char** argv) {
  std::vector<tensorflow::Tensor> output_tensors{};
  tensorflow::Scope scope = tensorflow::Scope::NewRootScope();
  tensorflow::TensorShape shape{1, 150, 150, 3};
  tensorflow::ops::Variable input_variable{scope.NewSubScope(""input""), shape,
                                           tensorflow::DataType::DT_FLOAT};
  tensorflow::ops::Assign assign_input_var{
      scope.NewSubScope(""assign_input""), input_variable,
      tensorflow::Input::Initializer{0.3f, shape}};
  // have to execute the assign input node first prior to using the variable
  tensorflow::ClientSession client{scope};
  TF_CHECK_OK(client.Run({}, {},
                         {tensorflow::Operation{assign_input_var.node()}},
                         &output_tensors));
  std::cout << output_tensors.size() << std::endl;
  TF_CHECK_OK(client.Run({input_variable}, &output_tensors));
  std::cout << output_tensors.size() << std::endl;
  std::cout << output_tensors[0].DebugString() << std::endl;
}

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
2020-12-23 09:32:00.026726: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/map.h:1060] CHECK failed: it != end(): key not found: Reciprocal
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: it != end(): key not found: Reciprocal
--Type <RET> for more, q to quit, c to continue without paging--

Thread 1 ""main"" received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
50      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace 
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007fffe8603859 in __GI_abort () at abort.c:79
#2  0x00007fffe89d8951 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007fffe89e447c in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007fffe89e44e7 in std::terminate() () from /lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007fffe89e4799 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007fffeb44aed5 in google::protobuf::internal::LogMessage::Finish() [clone .cold] () from /usr/local/lib/libtensorflow_cc.so.2
#7  0x00007ffff50353cd in tensorflow::grappler::NumOutputs(tensorflow::NodeDef const&, tensorflow::GraphDef*) () from /usr/local/lib/libtensorflow_cc.so.2
#8  0x00007ffff4bd25f6 in tensorflow::grappler::ConstantFolding::RunOptimizationPass(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#9  0x00007ffff4bd3175 in tensorflow::grappler::ConstantFolding::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#10 0x00007ffff4aa40fa in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /usr/local/lib/libtensorflow_cc.so.2
--Type <RET> for more, q to quit, c to continue without paging--
#11 0x00007ffff4aa556d in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#12 0x00007ffff4aa69a9 in tensorflow::grappler::MetaOptimizer::OptimizeConsumeItem(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#13 0x00007ffff4aa88eb in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem&&, tensorflow::ConfigProto const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*) () from /usr/local/lib/libtensorflow_cc.so.2
#14 0x00007ffff4a95b11 in tensorflow::GraphExecutionState::OptimizeGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*) () from /usr/local/lib/libtensorflow_cc.so.2
#15 0x00007ffff4a9675b in tensorflow::GraphExecutionState::BuildGraph(tensorflow::BuildGraphOptions const&, std::unique_ptr<tensorflow::ClientGraph, std::default_delete<tensorflow::ClientGraph> >*) () from /usr/local/lib/libtensorflow_cc.so.2
#16 0x00007ffff4a41250 in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*,--Type <RET> for more, q to quit, c to continue without paging--
 tensorflow::DirectSession::RunStateArgs*, absl::lts_2020_02_25::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::lts_2020_02_25::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long long*) () from /usr/local/lib/libtensorflow_cc.so.2
#17 0x00007ffff4a42b53 in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /usr/local/lib/libtensorflow_cc.so.2
#18 0x00007ffff4a45418 in tensorflow::DirectSession::GetOrCreateExecutors(absl::lts_2020_02_25::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_2020_02_25::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, absl::lts_2020_02_25::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) () from /usr/local/lib/libtensorflow_cc.so.2
#19 0x00007ffff4a496d6 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&) () from /usr/local/lib/libtensorflow_cc.so.2
#20 0x00007ffff4a33d08 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tenso--Type <RET> for more, q to quit, c to continue without paging--
rflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /usr/local/lib/libtensorflow_cc.so.2
#21 0x00007fffeb81640a in tensorflow::ClientSession::Run(tensorflow::RunOptions const&, std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer> > > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) const ()
   from /usr/local/lib/libtensorflow_cc.so.2
#22 0x00007fffeb816547 in tensorflow::ClientSession::Run(std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer> > > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const () from /usr/local/lib/libtensorflow_cc.so.2
#23 0x00007fffeb816788 in tensorflow::ClientSession::Run(std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer> > > const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const () from /usr/local/lib/libtensorflow_cc.so.2
--Type <RET> for more, q to quit, c to continue without paging--
#24 0x000055555556df6a in CatDogCNN::ReadTensorFromImageFile (this=0x7fffffffd390, file_name=""../../data/cats_and_dogs_small/train/cats/cat.470.jpg"", outTensor=...) at ../CatDogCNN.cpp:38
#25 0x000055555556e565 in CatDogCNN::ReadFileTensors (this=0x7fffffffd390, base_folder_name=""../../data/cats_and_dogs_small/train"", v_folder_label=std::vector of length 2, capacity 2 = {...}, 
    file_tensors=std::vector of length 0, capacity 0) at ../CatDogCNN.cpp:60
#26 0x000055555556e972 in CatDogCNN::ReadBatches (this=0x7fffffffd390, base_folder_name=""../../data/cats_and_dogs_small/train"", v_folder_label=std::vector of length 2, capacity 2 = {...}, 
    batch_size=20, image_batches=std::vector of length 0, capacity 0, label_batches=std::vector of length 0, capacity 0) at ../CatDogCNN.cpp:79
#27 0x000055555556307a in main (argc=1, argv=0x7fffffffd7e8) at ../main.cpp:39

```
absl/base/option.h has to be configured this way, otherwise there will be linker error. Absolutely not a good idea to append version number before class name. Slight version change will cause endless error.
```
#ifndef ABSL_BASE_OPTIONS_H_
#define ABSL_BASE_OPTIONS_H_

// Copyright 2019 The Abseil Authors.
//
// Licensed under the Apache License, Version 2.0 (the ""License"");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an ""AS IS"" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// -----------------------------------------------------------------------------
// File: options.h
// -----------------------------------------------------------------------------
//
// This file contains Abseil configuration options for setting specific
// implementations instead of letting Abseil determine which implementation to
// use at compile-time. Setting these options may be useful for package or build
// managers who wish to guarantee ABI stability within binary builds (which are
// otherwise difficult to enforce).
//
// *** IMPORTANT NOTICE FOR PACKAGE MANAGERS:  It is important that
// maintainers of package managers who wish to package Abseil read and
// understand this file! ***
//
// Abseil contains a number of possible configuration endpoints, based on
// parameters such as the detected platform, language version, or command-line
// flags used to invoke the underlying binary. As is the case with all
// libraries, binaries which contain Abseil code must ensure that separate
// packages use the same compiled copy of Abseil to avoid a diamond dependency
// problem, which can occur if two packages built with different Abseil
// configuration settings are linked together. Diamond dependency problems in
// C++ may manifest as violations to the One Definition Rule (ODR) (resulting in
// linker errors), or undefined behavior (resulting in crashes).
//
// Diamond dependency problems can be avoided if all packages utilize the same
// exact version of Abseil. Building from source code with the same compilation
// parameters is the easiest way to avoid such dependency problems. However, for
// package managers who cannot control such compilation parameters, we are
// providing the file to allow you to inject ABI (Application Binary Interface)
// stability across builds. Settings options in this file will neither change
// API nor ABI, providing a stable copy of Abseil between packages.
//
// Care must be taken to keep options within these configurations isolated
// from any other dynamic settings, such as command-line flags which could alter
// these options. This file is provided specifically to help build and package
// managers provide a stable copy of Abseil within their libraries and binaries;
// other developers should not have need to alter the contents of this file.
//
// -----------------------------------------------------------------------------
// Usage
// -----------------------------------------------------------------------------
//
// For any particular package release, set the appropriate definitions within
// this file to whatever value makes the most sense for your package(s). Note
// that, by default, most of these options, at the moment, affect the
// implementation of types; future options may affect other implementation
// details.
//
// NOTE: the defaults within this file all assume that Abseil can select the
// proper Abseil implementation at compile-time, which will not be sufficient
// to guarantee ABI stability to package managers.

// Include a standard library header to allow configuration based on the
// standard library in use.
#ifdef __cplusplus
#include <ciso646>
#endif

// -----------------------------------------------------------------------------
// Type Compatibility Options
// -----------------------------------------------------------------------------
//
// ABSL_OPTION_USE_STD_ANY
//
// This option controls whether absl::any is implemented as an alias to
// std::any, or as an independent implementation.
//
// A value of 0 means to use Abseil's implementation.  This requires only C++11
// support, and is expected to work on every toolchain we support.
//
// A value of 1 means to use an alias to std::any.  This requires that all code
// using Abseil is built in C++17 mode or later.
//
// A value of 2 means to detect the C++ version being used to compile Abseil,
// and use an alias only if a working std::any is available.  This option is
// useful when you are building your entire program, including all of its
// dependencies, from source.  It should not be used otherwise -- for example,
// if you are distributing Abseil in a binary package manager -- since in
// mode 2, absl::any will name a different type, with a different mangled name
// and binary layout, depending on the compiler flags passed by the end user.
// For more info, see https://abseil.io/about/design/dropin-types.
//
// User code should not inspect this macro.  To check in the preprocessor if
// absl::any is a typedef of std::any, use the feature macro ABSL_USES_STD_ANY.

#define ABSL_OPTION_USE_STD_ANY 0


// ABSL_OPTION_USE_STD_OPTIONAL
//
// This option controls whether absl::optional is implemented as an alias to
// std::optional, or as an independent implementation.
//
// A value of 0 means to use Abseil's implementation.  This requires only C++11
// support, and is expected to work on every toolchain we support.
//
// A value of 1 means to use an alias to std::optional.  This requires that all
// code using Abseil is built in C++17 mode or later.
//
// A value of 2 means to detect the C++ version being used to compile Abseil,
// and use an alias only if a working std::optional is available.  This option
// is useful when you are building your program from source.  It should not be
// used otherwise -- for example, if you are distributing Abseil in a binary
// package manager -- since in mode 2, absl::optional will name a different
// type, with a different mangled name and binary layout, depending on the
// compiler flags passed by the end user.  For more info, see
// https://abseil.io/about/design/dropin-types.

// User code should not inspect this macro.  To check in the preprocessor if
// absl::optional is a typedef of std::optional, use the feature macro
// ABSL_USES_STD_OPTIONAL.

#define ABSL_OPTION_USE_STD_OPTIONAL 0


// ABSL_OPTION_USE_STD_STRING_VIEW
//
// This option controls whether absl::string_view is implemented as an alias to
// std::string_view, or as an independent implementation.
//
// A value of 0 means to use Abseil's implementation.  This requires only C++11
// support, and is expected to work on every toolchain we support.
//
// A value of 1 means to use an alias to std::string_view.  This requires that
// all code using Abseil is built in C++17 mode or later.
//
// A value of 2 means to detect the C++ version being used to compile Abseil,
// and use an alias only if a working std::string_view is available.  This
// option is useful when you are building your program from source.  It should
// not be used otherwise -- for example, if you are distributing Abseil in a
// binary package manager -- since in mode 2, absl::string_view will name a
// different type, with a different mangled name and binary layout, depending on
// the compiler flags passed by the end user.  For more info, see
// https://abseil.io/about/design/dropin-types.
//
// User code should not inspect this macro.  To check in the preprocessor if
// absl::string_view is a typedef of std::string_view, use the feature macro
// ABSL_USES_STD_STRING_VIEW.

#define ABSL_OPTION_USE_STD_STRING_VIEW 0

// ABSL_OPTION_USE_STD_VARIANT
//
// This option controls whether absl::variant is implemented as an alias to
// std::variant, or as an independent implementation.
//
// A value of 0 means to use Abseil's implementation.  This requires only C++11
// support, and is expected to work on every toolchain we support.
//
// A value of 1 means to use an alias to std::variant.  This requires that all
// code using Abseil is built in C++17 mode or later.
//
// A value of 2 means to detect the C++ version being used to compile Abseil,
// and use an alias only if a working std::variant is available.  This option
// is useful when you are building your program from source.  It should not be
// used otherwise -- for example, if you are distributing Abseil in a binary
// package manager -- since in mode 2, absl::variant will name a different
// type, with a different mangled name and binary layout, depending on the
// compiler flags passed by the end user.  For more info, see
// https://abseil.io/about/design/dropin-types.
//
// User code should not inspect this macro.  To check in the preprocessor if
// absl::variant is a typedef of std::variant, use the feature macro
// ABSL_USES_STD_VARIANT.

#define ABSL_OPTION_USE_STD_VARIANT 0


// ABSL_OPTION_USE_INLINE_NAMESPACE
// ABSL_OPTION_INLINE_NAMESPACE_NAME
//
// These options controls whether all entities in the absl namespace are
// contained within an inner inline namespace.  This does not affect the
// user-visible API of Abseil, but it changes the mangled names of all symbols.
//
// This can be useful as a version tag if you are distributing Abseil in
// precompiled form.  This will prevent a binary library build of Abseil with
// one inline namespace being used with headers configured with a different
// inline namespace name.  Binary packagers are reminded that Abseil does not
// guarantee any ABI stability in Abseil, so any update of Abseil or
// configuration change in such a binary package should be combined with a
// new, unique value for the inline namespace name.
//
// A value of 0 means not to use inline namespaces.
//
// A value of 1 means to use an inline namespace with the given name inside
// namespace absl.  If this is set, ABSL_OPTION_INLINE_NAMESPACE_NAME must also
// be changed to a new, unique identifier name.  In particular ""head"" is not
// allowed.

#define ABSL_OPTION_USE_INLINE_NAMESPACE 1
#define ABSL_OPTION_INLINE_NAMESPACE_NAME lts_2020_02_25

#endif  // ABSL_BASE_OPTIONS_H_
```"
45942,standalone pip package for tf.io.gfile.GFile,"
Is there a chance to somehow split the `io.gfile` into a standalone pip package? This would be great!

The current `tf.io.gfile.GFile` API is very handy for transparently accessing local or GCS bucket files.
It is so great, I wish I could use it even in projects where I don't have tensorflow. As far as I'm aware the only/main alternative is to use the `google-cloud-storage` pip package (`google.cloud.storage.Client`), but it does not provide a python ideomatic API and cannot transparently handle local files.

I'm not sure where and what would be the best way to handle this, therefore I posted a related issue at googleapis/python-storage#354."
45941,Index used for OOV 'UNK' token from Tokenizer has a breaking change from 2.1.4 to 2.3.0.,"In Keras 2.1.4, the OOV token added to [Tokenizer](https://faroit.com/keras-docs/2.1.4/preprocessing/text/#tokenizer) will create a new vocabularly index that is 1 unit larger than the largest actual word index. Overall, in 2.1.4, you have index 0 as a reserved token index, index 1 through N as word indexes for actual words in a vocabulary, and index N + 1 as the 'UNK' OOV index.

In Keras 2.3.0, the OOV token uses index 1 instead of N + 1. This affects any pass through use of the tokenizer as well, such as when using `texts_to_sequences`.

This is a breaking change that makes Keras code written for 2.1.4 incompatible with 2.3.0 (for example, tests validating the existence of the 'UNK' token will fail, and re-training some models will leads to wrong word indices in applications that take e.g. an embedding vector learned for the UNK token and use it for other types of processing. Such code will now incorrectly be taking an embedding vector for a real word at index N + 1. Fine-tuning or transfer learning of a model with an Embedding layer that relies on this OOV behavior will also fail, since any new usage of `keras.preprocessing.text.Tokenizer` (such as `texts_to_sequences`) to process inputs into the embedding layer will have swapped the meaning of index N + 1 (previously learned weights for OOV) into index position 1 - and any auxiliary code users have written to extract word embeddings or to do feature importance studies at the level of individual word vectors will be wrong). 

There's currently no API support to always access into the embedding layer via some other mechanism that abstracts away the position of the OOV index, especially since users can provide their own custom token string to use - so unfortunately the actual index position itself is an exposed part of the API of Tokenizer.

I'm curious why this breaking change didn't require updating Keras to 3.0.0 at the time, given that it is breaking the version contracts of semver.

Are there any configuration options to restore the old OOV index behavior (N + 1 instead of 1) when using keras 2.3.0?

Tracking this down through code history, you can see the original version of the preprocessing handling here (using N + 1 as the index):

- https://github.com/keras-team/keras-preprocessing/blame/ea4b8e16d48e2522e6b497d7df9c04aedc63fc7b/keras_preprocessing/text.py#L238

and then you can see the new implementation here, specifically setting the OOV index to 1 instead of N + 1:

- https://github.com/keras-team/keras-preprocessing/commit/878a605a6418c8105ac4c9226945a18ce9e3bf57#diff-45a6438ee723ad37ca89a89ab8d76a4b3e645c3f70d7ec16a5a35fd80a6379c5R230

The way it manifests as a breaking change seems to affect both `keras_preprocessing` as a separate package and transitively `keras` according to the way `keras` sets version pins on `keras_preprocessing`.
"
45940,Problem about distributed training with XLA compiling.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  - custom layer and custom training step
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  - I have tested on Windows 10, Ubuntu 16.04, and Ubuntu 18.04.
- TensorFlow installed from (source or binary):
  - both
- TensorFlow version (use command below):
  - I have tried TF 2.4, 2.5 distributed version and source installed 2.4
- Python version:
  - 3.7
- Bazel version (if compiling from source):
  - 3.5.0
- GCC/Compiler version (if compiling from source):
  - 7.5
- CUDA/cuDNN version:
  - 10.1 and 11.0
- GPU model and memory:
  - 1080ti x4

**Describe the current behavior**
When I train my model on multi-gpu with XLA compiling below error is occurred.
```
Training starts
Traceback (most recent call last):
  File ""FFP_/train_w_pruning.py"", line 76, in <module>
    train_step(*data)
  File ""/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 787, in __call__
    result = self._call(*args, **kwds)
  File ""/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 854, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File ""/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1920, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 561, in call
    ctx=ctx)
  File ""/home/cvip/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource ResNet/conv/kernel/replica_1_879 located in device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_88943]
```

**Describe the expected behavior**
I want to compile my multi-gpu code but it seems unavailable.

**Standalone code to reproduce the issue**
https://github.com/sseung0703/TF2-multi-gpu-training
"
45938,TensorFlow 1.5 API : Error : 'DatasetV1Adapter' object does not support indexing,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4
- TensorFlow installed from : Binary (from docker image)
- TensorFlow version : 1.15
- Python version: 2.7.17
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Error emitted by TensorFlow API:
TypeError: 'DatasetV1Adapter' object does not support indexing

**Describe the expected behavior**
No error

**Standalone code to reproduce the issue**
images, labels = read_list ( data_dir, data_list ) <= here data_dir is full path of dir. containing image & label files. data_list is a text file, 2 column containing names of image file and label file.
The output is an array consisting of full path of images & corresponding labels.

queue = tf.data.Dataset.from_tensor_slices([images, labels])
img_contents = tf.io.read_file(queue[0]) <= Error location
label_contents = tf.io.read_file(queue[1])

TypeError: 'DatasetV1Adapter' object does not support indexing


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here is the traceback of the real code - 

Traceback (most recent call last):
  File ""wasr_train_noimu.py"", line 462, in <module>
    main()
  File ""wasr_train_noimu.py"", line 315, in main
    coord)
  File ""/wasr/wasr_models/image_reader.py"", line 249, in __init__
    self.image, self.label, self.imu = read_images_from_disk(self.queue, self.input_size, random_scale, random_mirror, ignore_label, img_mean)
  File ""/wasr/wasr_models/image_reader.py"", line 180, in read_images_from_disk
    img_contents = tf.io.read_file(input_queue[0])
TypeError: 'DatasetV1Adapter' object does not support indexing

The code in 'Standalone code' section is a gist of the actual code. 

I have looked at various posts including [this](https://github.com/tensorflow/tensorflow/issues/28995) one where it is mentioned that the issue is fixed in version 1.5 but I am still encountering it. I have also gone through the TensorFlow 1.5 documentation but could not understand why I am seeing this issue or what is the correct way to use the API if my code is not correct.
Any help will be highly appreciated.
Thanks,
-Shailesh
"
45937,Name argument ignored in operations,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb, 2.4.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I try to create a named operation is just ignored and the default value is generated. 
`
x = tf.keras.Input(shape=4, name=""X"", dtype=""float32"")
y = tf.keras.Input(shape=4, name=""Y"", dtype=""float32"")
ex = tf.math.exp(x, name=""exponential_X"")
lx = tf.math.log(x, name=""log_X"")
xy = tf.multiply(x, y, name=""XY"")
nx = tf.norm(x, name=""norm_X"")
rmx =  tf.reduce_mean(x, name=""reduce_mean_X"")
print(x.name)
print(ex.name)
print(lx.name)
print(xy.name)
print(nx.name)
print(rmx.name)

z = tf.Variable(1.0, name=""Z"")
ez = tf.math.exp(x, name=""exponential_Z"")
print(ez.name)
`
X
tf.math.exp_11/Exp:0
tf.math.log_6/Log:0
tf.math.multiply_5/Mul:0
tf.compat.v1.norm_3/norm/Squeeze:0
tf.math.reduce_mean_2/Mean:0
tf.math.exp_12/Exp:0

**Describe the expected behavior**
Output should be the names defined on the operation.

**Standalone code to reproduce the issue**
[Above example in Google Colab.](https://colab.research.google.com/drive/1Yc8pQoXK1MiGIPEGH-Ls9TKHpUMsrVqz)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45936,Modular filesystem inconsistent rename_file behaviour,"This is not a contribution.

The new modular filesystem includes a [rename_file](https://github.com/tensorflow/tensorflow/blob/67b4154b7dff78b1a2f1d0cd1ae6bbf6cea90673/tensorflow/c/experimental/filesystem/filesystem_interface.h#L485) operation. The docstring states that the implementation must throw a TF_FAILED_PRECONDITION error if either of the src or dst paths are directories. This is also part of the [test suite](https://github.com/tensorflow/tensorflow/blob/67b4154b7dff78b1a2f1d0cd1ae6bbf6cea90673/tensorflow/c/experimental/filesystem/modular_filesystem_test.cc#L859-L882). However the [gfile rename api description](https://github.com/tensorflow/tensorflow/blob/2800f688ffcce464054e41dbb09a83815dc3810d/tensorflow/python/lib/io/file_io.py#L537) says 'Rename or move a file / __directory__.'. Current implementations of the filesystem interface follow the latter description and will happily rename a directory, eg. [s3](https://github.com/tensorflow/tensorflow/blob/67b4154b7dff78b1a2f1d0cd1ae6bbf6cea90673/tensorflow/c/experimental/filesystem/plugins/s3/s3_filesystem.cc#L1109-L1151), [gcs](https://github.com/tensorflow/tensorflow/blob/67b4154b7dff78b1a2f1d0cd1ae6bbf6cea90673/tensorflow/c/experimental/filesystem/plugins/gcs/gcs_filesystem.cc#L1011-L1024).

Should we follow the existing implementations or the interface documentation?
"
45935,saved_model.save write .pb with read only permissions that can't be removed even with admin permissions,"### System information

* OS: Windows 10
* ONNX 1.8.0
* ONNX_TF 1.7.0
* TENSORFLOW 2.4.0
* Desktop
* Python 3.7.3
* CUDA 460.89
* running from anaconda3:
_anaconda_depends         2019.03                  py37_0
anaconda                          custom                   py37_1
anaconda-client                1.7.2                       py37_0
anaconda-navigator         1.9.7                        py37_0
anaconda-project              0.8.3                       py_0

I'm trying to convert onnx model to tensorflow and create pb file.
The conversion is successful, but the save .pb directory has ""read only"" attribute that can't be removed even when I'm running as admin (after removing it returns back)...

CODE:

`    onnx_model = onnx.load(onnx_model_path)
    
    tf_rep = prepare(onnx_model)
    
    tensorflow.saved_model.save(
        tf_rep.tf_module,
        tensorflow_model_path)
`

How it looks like:
![image](https://user-images.githubusercontent.com/2767433/102983532-e5ae4d00-4514-11eb-9ca7-6e2ed910772e.png)

NOTE: I also used a built in function "" export_graph"" - same issue

"
45933,"Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 5 (FILL) failed to invoke.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.5.0-dev20201222
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I have been able to successfully convert the [Boundless model](https://arxiv.org/pdf/1908.07007.pdf) with full integer quantization but it fails during inference. I have made sure to scale the inputs w.r.t. quantization i.e. the following - 

```python
if quantization==""integer"":
    input_img = preprocess_image(""test_image.jpg"")
    input_scale, input_zero_point = input_details[0][""quantization""]
    input_img = input_img / input_scale + input_zero_point
    input_img = input_img.astype(input_details[0][""dtype""])
```

I am running into - 

```python
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-35-1e81bd6daa76> in <module>()
      2 interpreter.set_tensor(input_details[0]['index'], input_img)
      3 start = time.time()
----> 4 interpreter.invoke()
      5 print(f""Inference took: {time.time()-start} seconds"")

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)
    758     """"""
    759     self._ensure_safe()
--> 760     self._interpreter.Invoke()
    761 
    762   def reset_all_variables(self):

RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 5 (FILL) failed to invoke.
```

**Describe the expected behavior**

Currently, it's not clear to me why it is failing. Essentially, I would expect it to work. 

**Standalone code to reproduce the issue**
Colab Notebook: https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45932,Float model size is lesser than dynamic-range model size,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.5.0-dev20201222
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am converting the [Boundless model](https://arxiv.org/pdf/1908.07007.pdf) to TensorFlow Lite. While the model conversion is successful and the inference works as expected there's an inconsistency in the model sizes. Currently, the dynamic-range quantized model size is ~14 MB whereas the float16 quantized model size is ~7MB. 

**Describe the expected behavior**

Float16 model size should be higher, technically twice the size of the dynamic-range quantized model. 

**Standalone code to reproduce the issue**
Colab Notebook - https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45931,Graidents for weights in tf.keras.layers.GRUCell not being computed when using tf.estamtor,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Official docker image (tensorflow/tensorflow:2.4.0-gpu)
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb
- Python version: 3.6, as is in the official image
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: as is in the official image
- GPU model and memory: NVIDIA GTX1050

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The weights in GRUCell are not computed when using tf.estimator, `tape.gradient(loss_val, nn.trainable_variables)` returns `None` for weights in `GRUCell`
**Describe the expected behavior**
Computation of gradients with respect to weights in `GRUCell` should be working as in ""keras mode"", please look into the following code for details.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Please clear `./test_output/` before re-running the code to replicated the issue.
```
import tensorflow as tf
import numpy as np
import functools
from typing import Optional


class TokenWeightingLayer(tf.keras.layers.Layer):
    def __init__(self, mask_constant=-50000, **kwargs):
        super(TokenWeightingLayer, self).__init__(**kwargs)
        self.fc = tf.keras.layers.Dense(1)
        self.softmax = tf.keras.layers.Softmax(axis=-1)
        self._mask_constant = mask_constant

    def call(self, inputs, mask, **kwargs):
        activation = tf.squeeze(self.fc(tf.tanh(inputs)), axis=-1)
        masked_activation = tf.where(mask, activation,
                                     (tf.ones_like(activation) * self._mask_constant))  # TODO: fix dtype
        alpha = self.softmax(masked_activation)
        return alpha


class RNN_Attention_Clf(tf.keras.Model):
    def __init__(self,
                 word_embedding_dim: int,
                 pos1_embedding_dim: int,
                 pos2_embedding_dim: int,
                 vocab_size: int,
                 pos_embedding_num: int,
                 rnn_dim: int,
                 dropout_rate: float = 0.,
                 pre_trained_word_embedding: np.ndarray = None,
                 fine_tune_word_embedding: bool = False,
                 variational_recurrent: bool = False,
                 num_cls: Optional[int] = None,
                 **kwargs):
        super(RNN_Attention_Clf, self).__init__()
        self._word_embedding_dim = word_embedding_dim
        self._entity_1_embedding_dim = pos1_embedding_dim
        self._entity_2_embedding_dim = pos2_embedding_dim
        self.cell_type = tf.keras.layers.GRUCell
        self._vocab_size = vocab_size  # int, without unk and pad
        self._pos_embedding_num = pos_embedding_num
        self._pre_trained_word_embedding = pre_trained_word_embedding  # numpy.ndarray object
        self._fine_tune_word_embedding = (self._pre_trained_word_embedding is None) or kwargs.get(
            ""fine_tune_word_embedding"", False)
        self._variational_recurrent = variational_recurrent
        self._rnn_dim = rnn_dim  # int
        self._num_cls = num_cls
        self._dropout_rate = dropout_rate
        if pre_trained_word_embedding is None:
            self.token_embedding = tf.keras.layers.Embedding(self._vocab_size, self._word_embedding_dim, mask_zero=True)
        else:
            raise NotImplementedError(""WIP"")
        self._pos_embedding_1 = tf.keras.layers.Embedding(self._pos_embedding_num, self._word_embedding_dim,
                                                          mask_zero=True)
        self._pos_embedding_2 = self._pos_embedding_1
        cell = tf.keras.layers.GRUCell(rnn_dim)
        rnn_layer = tf.keras.layers.RNN(cell, return_sequences=True, return_state=True)
        self.rnn_encoder = tf.keras.layers.Bidirectional(rnn_layer, merge_mode=""sum"")
        self.token_weighting_layer = TokenWeightingLayer()
        self.fc_layer = tf.keras.layers.Dense(num_cls, use_bias=True)

    def call(self, token_ids, pos_ids_1, pos_ids_2, sequence_length, training=None, mask=None):
        token_embedding = self.token_embedding(token_ids)
        # token_dropout
        pos_embedding_1 = self._pos_embedding_1(pos_ids_1)
        pos_embedding_2 = self._pos_embedding_2(pos_ids_2)
        encoded_input = tf.concat([token_embedding, pos_embedding_1, pos_embedding_2], axis=-1, )
        # Compute mask
        mask = tf.sequence_mask(sequence_length, maxlen=token_ids.get_shape()[1])
        # BiRNN
        encoder_outputs, last_state_fw, last_state_bw = self.rnn_encoder(encoded_input, mask=mask)

        # The original authors of this network call this ""attention"", well...
        alpha = self.token_weighting_layer(encoder_outputs, mask)
        weighted_outputs = tf.squeeze(tf.matmul(tf.expand_dims(alpha, axis=1), encoder_outputs), axis=1)
        out = self.fc_layer(weighted_outputs)
        return out, alpha

    def model_fn(self, features, labels, mode, params):
        # dropout_rate = params[""dropout_rate""]
        if labels is not None:
            sparse_label_ids = labels
        # TODO: Set configure RNN dropout for RNN input, state, output
        input_text, inputs, pos1_inputs, pos2_inputs, sequence_length = features
        training = (mode == tf.estimator.ModeKeys.TRAIN)
        if not training:
            self.trainable = False  # Do I have to?
        logits, alpha = self(inputs, pos1_inputs, pos2_inputs, sequence_length)
        print(self.summary())

        top1_prediction = tf.argmax(logits, axis=-1, name=""top1_cls"")

        confidence = tf.nn.softmax(logits, axis=-1, name=""confidence"")
        top1_confidence = tf.reduce_max(confidence, axis=-1)
        if mode != tf.estimator.ModeKeys.PREDICT:
            def loss_fn():  # For training stage only.
                logits, alpha = self(inputs, pos1_inputs, pos2_inputs, sequence_length,
                                     training=True)  # Apply dropout, batch-normalization etc. when calculating loss.
                loss = tf.keras.losses.sparse_categorical_crossentropy(sparse_label_ids, logits, from_logits=True)
                loss = tf.reduce_mean(loss)
                return loss

            global_step = tf.compat.v1.train.get_or_create_global_step()
            y_true = tf.one_hot(sparse_label_ids, depth=self._num_cls)
            metrics = {}
            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=sparse_label_ids, y_pred=confidence,
                                                                   from_logits=False)
            loss = tf.reduce_mean(loss)
            acc = tf.metrics.SparseTopKCategoricalAccuracy(k=1)
            acc.update_state(y_true=sparse_label_ids, y_pred=confidence)
            metrics[""acc""] = acc
            for cls_id in range(self._num_cls):
                p = tf.metrics.Precision(top_k=1, class_id=cls_id)
                p.update_state(y_true=y_true, y_pred=confidence)
                metrics[""precision_cls_{}"".format(cls_id)] = p
                r = tf.metrics.Recall(top_k=1, class_id=cls_id)
                r.update_state(y_true=y_true, y_pred=confidence)
                metrics[""recall_cls_{}"".format(cls_id)] = r

        if mode == tf.estimator.ModeKeys.PREDICT:
            predictions = {
                ""cls_id"": top1_prediction,
                ""confidence"": confidence,
                ""att_weight"": alpha,
                ""input_text"": input_text
            }
            return tf.estimator.EstimatorSpec(mode, predictions=predictions)
        elif mode == tf.estimator.ModeKeys.TRAIN:
            init_lr = params[""init_lr""]
            decay_step = params[""decay_step""]
            decay_rate = params[""decay_rate""]
            learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(init_lr, decay_steps=decay_step,
                                                                           decay_rate=decay_rate)

            if params[""optimizer""] == 'Adam':
                optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            elif params[""optimizer""] == 'Adadelta':
                optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)
            elif params[""optimizer""] == 'Adagrad':
                optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)
            elif params[""optimizer""] == 'RMSProp':
                optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
            elif params[""optimizer""] == 'Momentum':
                optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
            elif params[""optimizer""] == 'SGD':
                optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
            else:
                optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
            trainable_variables = self.trainable_variables
            optimizer.iterations = global_step
            tf.summary.scalar(""learning_rate"", learning_rate(optimizer.iterations), global_step)
            clip_val = params.get(""clip_value"")
            if clip_val is not None:
                # Tampering gradients manually instead of setting optimizer.clipvalue
                with tf.GradientTape() as tape:
                    loss_val = loss_fn()
                gradients = tape.gradient(loss_val, trainable_variables)
                gradients = [(tf.clip_by_value(grad, -clip_val, clip_val)) if grad is not None else grad for grad in
                             gradients]
                assert len(trainable_variables) == len(gradients)
                for var, grad in zip(trainable_variables, gradients):
                    print(""****variable:{} Gradient: {}"".format(var, grad))
                train_op = optimizer.apply_gradients(zip(gradients, trainable_variables))
            else:
                train_op = optimizer.minimize(loss_fn, var_list=trainable_variables)
            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, eval_metric_ops=metrics)
        elif mode == tf.estimator.ModeKeys.EVAL:
            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)
        else:
            raise ValueError(""Unknown mode: {}"".format(mode))



def test_non_estimator_approach():
    nn = RNN_Attention_Clf(word_embedding_dim=64, pos1_embedding_dim=5, pos2_embedding_dim=5, pos_embedding_num=6,
                           vocab_size=100, rnn_dim=20, num_cls=10)
    inputs = tf.constant([[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]])
    pos_1 = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0]])
    pos_2 = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0]])
    seq_length = tf.constant([5, 4, 8])
    sparse_label_ids = tf.constant([0, 1, 2])
    def loss_fn():  # For training stage only.
        logits, alpha = nn(inputs, pos_1, pos_1, seq_length,
                           training=True)
        loss = tf.keras.losses.sparse_categorical_crossentropy(sparse_label_ids, logits, from_logits=True)
        loss = tf.reduce_mean(loss)
        return loss

    out = nn(inputs, pos_1, pos_2, seq_length)
    print(nn.summary())  # Variables in `directional layers are trainable
    clip_val = 5
    if clip_val is not None:
        with tf.GradientTape() as tape:
            loss_val = loss_fn()

    grads = tape.gradient(loss_val, nn.trainable_variables)
    assert len(nn.trainable_variables) == len(grads)
    for var, grad in zip(nn.trainable_variables, grads):
        print(""****variable:{} Gradient: {}"".format(var, grad))


def test_estimator_approach():
    nn = RNN_Attention_Clf(word_embedding_dim=64, pos1_embedding_dim=5, pos2_embedding_dim=5, pos_embedding_num=6,
                           vocab_size=100, rnn_dim=20, num_cls=10)
    params = {""clip_value"": 5, ""init_lr"": 0.1, ""clip_val"": 5, ""decay_step"": 1, ""decay_rate"": 0.99, ""optimizer"": ""Adam""}
    output_dir = ""./test_output""
    estimator = tf.estimator.Estimator(nn.model_fn,
                                       config=tf.estimator.RunConfig(save_checkpoints_secs=20, save_summary_steps=1),
                                       model_dir=output_dir,
                                       params=params)
    def input_fn():
        def gen():
            dummy_text = [""text1"", ""text2"", ""text3""]
            dummy_labels = [0, 1, 2]
            all_inputs = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
            all_pos1 = [[1, 1, 1, 1, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0]]
            all_pos2 = [[1, 1, 1, 1, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 0, 0]]
            seq_length = [5, 4, 8]
            for text, inputs, pos1, pos2, sequence_length, y in zip(dummy_text, all_inputs, all_pos1, all_pos2, seq_length, dummy_labels):
                out = ((text, inputs, pos1, pos2, sequence_length), y)
                yield out
        d = tf.data.Dataset.from_generator(gen,
                                           output_types=((tf.string, tf.int32, tf.int32, tf.int32, tf.int32), tf.int32),
                                           output_shapes=(((), (None,), (None,), (None,), ()), ()))
        d = d.batch(batch_size=3)
        return d
    estimator.train(input_fn=input_fn, max_steps=100)


if __name__ == ""__main__"":
    # To replicate the issue, clear `./test_output` before re-running

    test_non_estimator_approach()  # No problem with this approach
    test_estimator_approach()  # No gradient computed for weights in GRUCell

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The most notable part from the logs:
```
WARNING:tensorflow:Gradients do not exist for variables ['rnn__attention__clf_1/bidirectional_1/forward_rnn_1/gru_cell_1/kernel:0', 'rnn__attention__clf_1/bidirectional_1/forward_rnn_1/gru_cell_1/recurrent_kernel:0', 'rnn__attention__clf_1/bidirectional_1/forward_rnn_1/gru_cell_1/bias:0', 'rnn__attention__clf_1/bidirectional_1/backward_rnn_1/gru_cell_1/kernel:0', 'rnn__attention__clf_1/bidirectional_1/backward_rnn_1/gru_cell_1/recurrent_kernel:0', 'rnn__attention__clf_1/bidirectional_1/backward_rnn_1/gru_cell_1/bias:0'] when minimizing the loss.
```
I have to skip these gradients when applying gradient clipping as they all return `None`"
45930,Could not load dynamic library 'libcudart.so.11.0',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.0
- Python version: 3.7.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: libcudnn8_8.0.5.39-1+cuda11.0_amd64.deb (I also installed the dev version)
- NVIDIA Driver Version: 450.80.02
- GPU model and memory: GeForce RTX 2080 Super with Max-Q Design



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I tried following the instructions as described in: https://www.tensorflow.org/install/gpu
- I installed Ubuntu 20.04 (which installed the NVIDIA driver)
- Installed python using pyenv
- sudo apt install nvidia-cuda-toolkit
- pip install tensorflow
- Installed: libcudnn8_8.0.5.39-1+cuda11.0_amd64.deb
- (saw that it failed to find 'libcudart.so.11.0')
- Installed: libcudnn8-dev_8.0.5.39-1+cuda11.0_amd64.deb
- (still failed to find 'libcudart.so.11.0')

Is there a way for me to check which part of the installation broke?
Any ideas on what I can do to fix this?
Thanks!

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
PyDev console: starting.
Python 3.7.9 (default, Dec 22 2020, 21:13:51) 
[GCC 9.3.0] on linux
>>> import tensorflow as tf
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2020-12-22 22:56:35.044676: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2020-12-22 22:56:35.044691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/ido/TeraResearch/venv/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.
  warnings.warn(msg)
2020-12-22 22:56:35.938373: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-22 22:56:35.938801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-12-22 22:56:37.494184: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2020-12-22 22:56:37.494273: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ido-ml
2020-12-22 22:56:37.494291: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ido-ml
2020-12-22 22:56:37.494480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.80.2
2020-12-22 22:56:37.494544: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2
2020-12-22 22:56:37.494561: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.80.2
Num GPUs Available:  0
```"
45929,TanhExp activation function,"Please add the new TanhExp activation function, itâ€™s faster to compute and converge better than Mish and Swish.

Link for the paper: https://arxiv.org/pdf/2003.09855.pdf"
45927,Linking issue: undefined reference to `tflite::micro::GetTensorShape SparkfunEdge,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2
- Python version: many versions including latest
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): gcc 7.5
- CUDA/cuDNN version: N/A
- GPU model and memory: intel onboard graphics


**Describe the current behavior**
Currently trying to run the magic wand examples from another project that is written in C (one of the FreeRTOS examples on SparkFun Edge).
I compiled the magic wand examples, moved all the .o files to the other project and tried compiling the other project together with linking the TFLite .o files. The linking step generated the following errors:

```
../gcc/edge/bin/fully_connected.o: In function `EvalFloat':
/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:192: undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'
/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:194: undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'
/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:196: undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'
/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:198: undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'
../gcc/edge/bin/fully_connected.o: In function `EvalQuantizedInt8':
/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:124: undefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'
../gcc/edge/bin/fully_connected.o:/home/fadel/Desktop/Projects/ml/tensorflow/tensorflow/lite/micro/kernels/fully_connected.cc:126: more undefined references to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)' follow
collect2: error: ld returned 1 exit status
```
This is the exact same problem as this: https://github.com/tensorflow/tensorflow/issues/41821 except that I am not using the ""CMSIS-NN"" tag anywhere.

**Describe the expected behavior**
Successful linking of the code.

**Standalone code to reproduce the issue**
*Download any FreeRTOS example from SparkFun's repository, ex: https://github.com/sparkfun/SparkFun_Apollo3_AmbiqSuite_BSPs/tree/6398086a1a87ddea78274521683ba3ad817bee82/common/examples/ble_freertos_tag
*Compile the magic wand example
*Copy all the created .o files to the /src/gcc directory of the FreeRTOS
*Use an ""extern C"" to allow magic wand's main() function to be called from a C interface (FreeRTOS project is in C).
*Call the main() function in the FreeRTOS project.
*Edit the Makefile of the RTOS project to link all the .o files from the magic wand example
*Compile the FreeRTOS example, the error will show at the linking step.

I know that it's time-consuming to replicate this, I can provide my repositories that contains the ready made Makefiles, etc. However, perhaps you know what the issue is just from reading the above.
"
45926,No algorithm Worked,"Getting this error when using the code in collab

Found 14400 images belonging to 2 classes.
Found 400 images belonging to 2 classes.
Epoch 1/50
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-28-86598d638a85> in <module>()
     15         epochs=50,
     16         validation_data=test_generator,
---> 17         validation_steps=800)

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    853       # In this case we have created variables on the first call, so we run the
    854       # defunned version which is guaranteed to never create variables.
--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    856     elif self._stateful_fn is not None:
    857       # Release the lock early so that multiple threads can perform the call

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   2942     return graph_function._call_flat(
-> 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 
   2945   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1917       # No tape is watching; skip to running the function.
   1918       return self._build_call_outputs(self._inference_function.call(
-> 1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(
   1921         args,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    558               inputs=args,
    559               attrs=attrs,
--> 560               ctx=ctx)
    561         else:
    562           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

NotFoundError:  No algorithm worked!
	 [[node sequential_2/conv2d_6/Conv2D (defined at <ipython-input-26-b3f11172c6da>:17) ]] [Op:__inference_train_function_3219]

Function call stack:
train_function




My code on colab i was just testing a random CNN architecture on my dataset
https://colab.research.google.com/drive/1mb7c3Dg6CDwcjYitp85uNpa_A6il1J6f?usp=sharing"
45925,label_image build fails for stm32f1,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution : ubuntu 18.04
- TensorFlow installed from : Source
- Tensorflow commit: bba12d0401800fbf873ea35f34517a8c47a54272 (Branch:Master)
- Target platform : Arm

I am trying to build target 'stm32f1' at location tensorflow/lite/tools/make/targets in file stm32f1_makefile.inc 
for this I am using toolchain provided at https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm for x86_64 which is Pre-built GNU bare-metal toolchain for 32-bit Arm processors.
while building label_image I am getting error saying that it was not able to find #include <dlfcn.h>

Following are the steps to produce this issue: (in tensorflow dir)
$./tensorflow/lite/tools/make/download_dependencies.sh
export the tools from toolchain downloaded from above link.
And modified the build_rpi_build.sh where I changed TARGET_ARCH to TARGET and passes TARGET=stm32f1."
45924,"Feature clolumns  cannot be used in the keras model, it may be a bug","## URL(s) with the issue:
https://tensorflow.google.cn/api_docs/python/tf/keras/layers/DenseFeatures

## Description of issue (what needs changing):

### Clear description

I have made a keras function API model, which uses' fit_generator 'for multiple input. If you don't use tensorflow feature columns, the model can work normally, but if you use feature columns to input to densefeatures layer, the model will not work. It looks like a bug, because the same code has been tried before, and the model works
For example, why should someone use this method? How is it useful?

### Correct links
### Usage example

my code:
set use_feature_columns=False can work but,set use_feature_columns=True,it can't work.


https://colab.research.google.com/drive/18e-T5UYi__uzsMmavXTj86pM9GPhqe7I?usp=sharing

"
45923,Loss reported by fit vs custom_loop,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/distribute/custom_training

## Description of issue (what needs changing):

I wanted to compare the `fit` performance vs a ""custom loop"" with a `MultiWorker` strategy according to the tutorials at https://www.tensorflow.org/tutorials/distribute/keras and https://www.tensorflow.org/tutorials/distribute/custom_training 

However I'm seeing differences in the loss reported. Examples for an MNIST dataset with a Lenet:
- fit: Eval loss: 32.09, Eval Accuracy: 0.9307
- custom loop: Eval loss: 1.742, Eval Accuracy: 0.965

Also the loss reported in the `logs` member of the callbacks seems to be higher by some factor compared to the custom_loop.

### Clear description

At https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function I found the following:

> - Using tf.reduce_mean is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step.
> - This reduction and scaling is done automatically in keras `model.compile` and `model.fit`
> - ... SUM_OVER_BATCH_SIZE is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So instead we ask the user do the reduction themselves explicitly.

So question here which isn't clear to me from reading the docs/tutorials: How is the loss calculated by `model.compile` and `model.fit` and is that expected to be the same as the one shown in the tutorial for custom_loops? 

What I recon is that it should be the average loss per sample over the whole batch (on all replicas). So the sum of all `per_example_loss` divided by the number of examples.
The custom_loop tutorial divides by the global batch size, i.e. the local batch size times the number of replicas. This would be wrong for the mentioned case where the ""per replica batch size [...] may vary step to step."" So this sounds like `tf.reduce_mean` would be useful with an additional step of averaging the `per_replica_losses` instead of summing them after the `strategy.run` part. Why is that not recommended?"
45922,AutoGraph could not transform custom Train_Step function,"**System information**
- Have I written custom code
- OS Platform: Windows 10 64-bit, Desktop
- TensorFlow installed from binary:
- TensorFlow version: 2.3.1
- Python version: 3.7.9
- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243
- GPU model and memory: two system linked Nvidia GeForce 2070 RX.  8gb ram each (16gb total).

**Describe the current behavior**
I get the following warning message when using tf.function to convert my function:
""
WARNING:tensorflow:AutoGraph could not transform <function Train_Step at 0x000002935E41C048> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
""

**Describe the expected behavior**
I expected that when I used a @tf.function decorator the function would compile properly not throw out a warning.

**Standalone code to reproduce the issue**
""
import tensorflow as tf
import numpy as np

batch_size = 100
feature_length = 45
length_target = 1
array_sample_data = np.asarray(np.random.random((batch_size, feature_length)), dtype = np.float32)
array_target_data = np.asarray(np.random.random((batch_size, length_target)), dtype = np.float32)
array_std_data = np.std(array_sample_data, axis = 0)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(50, activation = 'relu', input_shape = [feature_length]))
model.add(tf.keras.layers.Dense(length_target, activation = 'sigmoid'))

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.MSE
model.compile(optimizer = optimizer, loss = loss)

tensor_sample_data = tf.constant(array_sample_data, dtype = 'float32')
tensor_target_data = tf.constant(array_target_data, dtype = 'float32')
tensor_weight = None

clip_norm = 1
@tf.function	
def Train_Step(tensor_sample_data, tensor_target_data, tensor_weight):
	with tf.GradientTape() as tape:
		y_pred = model.call(tensor_sample_data, training = True)
		loss = model.compiled_loss(y_true = tensor_target_data, y_pred = y_pred, \
sample_weight = tensor_weight, regularization_losses = model.losses) 

	per_replica_gradients = tape.gradient(loss, model.trainable_variables)
	gradients = tf.distribute.get_replica_context().all_reduce('sum', per_replica_gradients) #summed all grads across devices
	loss = tf.distribute.get_replica_context().all_reduce('sum', loss)
	y_pred = tf.distribute.get_replica_context().all_reduce('sum', y_pred)
	(gradients, global_norm_value) = tf.clip_by_global_norm(t_list = gradients, clip_norm = clip_norm)
	model.optimizer.apply_gradients(zip(gradients, model.trainable_variables), experimental_aggregate_gradients = False)

	return (y_pred, loss)

epochs = 3
for index_epoch in range(epochs):
	(y_pred, loss) = Train_Step(tensor_sample_data = tensor_sample_data, tensor_target_data = tensor_target_data, tensor_weight = tensor_weight)
	print('Training loss is {} on epoch {}.'.format(loss, index_epoch + 1))

""
Thanks for your time and consideration."
45921,FullyConnected reference_integer_ops miscalculate output data due to endianness issue on s390x,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The Test case `//tensorflow/lite/micro/examples/hello_world:hello_world_test` failed on s390x with error:

```
Testing LoadModelAndPerformInference
0. (1.0*2^-127) near value (-1.1018541*2^0) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:103
0.841 (1.6819992*2^-1) near value (-1.1018541*2^0) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:111
0.141 (1.1279996*2^-3) near value (-1.1018541*2^0) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:118
-0.959 (-1.9179993*2^-1) near value (-1.1018541*2^0) failed at tensorflow/lite/micro/examples/hello_world/hello_world_test.cc:125
0/1 tests passed
~~~SOME TESTS FAILED~~~
```

On debugging, I found that `GetTensorData<int32_t>(bias)` returns tensor data in LE format, where bias is an Optional Input Tensor being populated in `Eval` as ```const TfLiteTensor* bias = GetOptionalInputTensor(context, node, kBiasTensor);```

This tensor data of bias is used in reference_integer_ops FullyConnected in calculation of `acc`. 
I tried a solution of byte-swapping this data while populating `acc` which fixed the test case. The solution looked like this:
```
if (bias_data) {
#if FLATBUFFERS_LITTLEENDIAN
        acc += bias_data[out_c];
#else
        acc += __bswap_32(bias_data[out_c]);
#endif
``` 
in [fully_connected.h](https://github.com/tensorflow/tensorflow/blob/bba12d0401800fbf873ea35f34517a8c47a54272/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h#L54)

Since, I do not have a view of the reach and use of this optional tensor, can you please suggest if there is another way that you would like me to try or if I should PR this solution?

**Describe the expected behavior**
The tensor data on BE systems must be in correct format and the test case should pass.

**Standalone code to reproduce the issue**
To reproduce the issue, please run this test case:
```
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --host_javabase=""@local_jdk//:jdk"" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial -k --test_timeout 300,450,1200,3600 --build_tests_only -c dbg --copt=-O -c opt --copt=-g --strip never --color=yes --curses=yes --test_output=errors --verbose_failures  -- //tensorflow/lite/micro/examples/hello_world:hello_world_test
```

**Other info / logs**"
45920,Could not find device for node: {{node FloorMod}} = FloorMod[T=DT_HALF],"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0
- Python version: 3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
Since TF 2.4.0 i get an error during FloorMod operation with float16.
In TF 2.3.1 it worked as expected.

**Describe the expected behavior**
FloorMod should be available for float16 dtype.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/19kRHgXjuf9R9UuBYmEmZiFQPpodnbsOM?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```python
import tensorflow as tf

print(tf.version.GIT_VERSION, tf.version.VERSION)
# v2.4.0-0-g582c8d236cb 2.4.0

x = tf.constant([1, 2, 3], 'float16')
x % 4

---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-6-86e5a110f54d> in <module>()
----> 1 x % 4

3 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: Could not find device for node: {{node FloorMod}} = FloorMod[T=DT_HALF]
All kernels registered for op FloorMod:
  device='GPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_UINT64]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:FloorMod]
```"
45919,Error using custom activation function while mixed precision enable,"Config
```
TensorFlow 2.3
```

I already posted in [SO](https://stackoverflow.com/questions/65403976/typeerror-using-custom-activation-function-while-mixed-precision-enabled), a few hours ago without any response until now. But I need a quick pointer. 

----

### Problem

I was trying to use a **custom activation** in **mixed-precision** enabled training pipelines but faced the following error:

```
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
```

### Reproduce 

Enabling Mixed precision...

```
import tensorflow as tf 

policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
tf.keras.mixed_precision.experimental.set_policy(policy)
print('Mixed precision enabled')
```

Custom activation... 

```
def ARelu(x, alpha=0.90, beta=2.0):
    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)
    beta  = 1 + tf.math.sigmoid(beta)
    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha
```

Training...

```
import tensorflow as tf

(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.mnist.load_data()

def pre_process(inputs, targets):
    inputs  = tf.expand_dims(inputs, -1)
    targets = tf.one_hot(targets, depth=10)
    return tf.divide(inputs, 255), targets

train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).\
    take(10_000).shuffle(10_000).batch(8).map(pre_process)
test_data = tf.data.Dataset.from_tensor_slices((xtest, ytest)).\
    take(1_000).shuffle(1_000).batch(8).map(pre_process)

model = tf.keras.Sequential([
                             
            tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1),
                                   input_shape=(28, 28, 1), activation=ARelu),
            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),

            tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), 
                                   activation=ARelu),
            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),

            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(64, activation=ARelu), 
            tf.keras.layers.Dense(10, activation='softmax', dtype=tf.float32)]) 

opt = tf.keras.optimizers.Adam()

model.compile(loss='categorical_crossentropy', optimizer=opt)
history = model.fit(train_data, validation_data=test_data, epochs=10)

# ------------------

TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
```

However, without mixed-precision, it works. I understand the problem simply types miss match but where I should look into it? 

Additionally, while trying to solve it, I've found that using `tf.keras.mixed_precision.LossScaleOptimizer` is safe to avoid numeric underflow. Is it something that we should use for mixed-precision training? "
45918,TF 2.4 met 'TypeError: can't pickle _thread.lock objects' where 2.3 did not.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v.2.4
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am working on distributed learning in tensorflow through estimators API using below simple code template:

```
def multi_worker_strategy_estimator():
    mirrored_strategy = tf.distribute.MultiWorkerMirroredStrategy()
    config = tf.estimator.RunConfig(train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)
    regressor = tf.estimator.LinearRegressor(
        feature_columns=[tf.feature_column.numeric_column('feats')],
        optimizer='SGD',
        config=config)

    def input_fn():
        dataset = tf.data.Dataset.from_tensors(({""feats"": [1.]}, [1.]))
        return dataset.repeat(1000).batch(10)

    train_spec = tf.estimator.TrainSpec(
        input_fn=input_fn,
        max_steps=FLAGS.train_steps,
        hooks=[]
    )
    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)
    tf.estimator.train_and_evaluate(regressor, train_spec, eval_spec)
    print(""---training finished---"")
```

Then I encountered the errors as follow:
```
Traceback (most recent call last):
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/test-mutiworker-strategy.py"", line 318, in <module>
    test()
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/test-mutiworker-strategy.py"", line 66, in test
    config=config)
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/linear.py"", line 1369, in __init__
    warm_start_from=warm_start_from)
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 183, in __init__
    config, model_dir)
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1832, in maybe_overwrite_model_dir_and_session_config
    config = run_config.RunConfig.replace(config, session_config=session_config)
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/run_config.py"", line 923, in replace
    copy.deepcopy(self),
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 161, in deepcopy
    y = copier(memo)
  File ""/root/PycharmProjects/urmsone-multi-gpu-tf-demo/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1542, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.6/copy.py"", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle _thread.lock objects
```

The same code runs on Tensorflow v2.3  without this problem. 

Is this a bug in the  v2.4ï¼Ÿ

"
45917,"SavedModelBundle.java do not has serializeï¼Œso in spark do not broadcast the load model, only run local or one executor ","https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/SavedModelBundle.java

"
45916,Survival/Drop Connect Rate (aka Stochastic Depth) is not implemented in tf.keras.applications.efficientnet,"In the original implementation, [the `drop_connect` function](https://github.com/tensorflow/tpu/blob/341c3b628409a03b74d2d66d33fd09532690e04e/models/official/efficientnet/utils.py#L276-L291) is implemented in the following way:

```python
def drop_connect(inputs, is_training, survival_prob):
  """"""Drop the entire conv with given survival probability.""""""
  # ""Deep Networks with Stochastic Depth"", https://arxiv.org/pdf/1603.09382.pdf
  if not is_training:
    return inputs

  # Compute tensor.
  batch_size = tf.shape(inputs)[0]
  random_tensor = survival_prob
  random_tensor += tf.random_uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)
  binary_tensor = tf.floor(random_tensor)
  # Unlike conventional way that multiply survival_prob at test time, here we
  # divide survival_prob at training time, such that no addition compute is
  # needed at test time.
  output = tf.div(inputs, survival_prob) * binary_tensor
  return output
```

This function is used to decide whether to keep a block or not with a given probability `p_l`; this described in depth in [the original paper](https://arxiv.org/abs/1603.09382). 

Looking at the `tf.keras.applications.efficientnet` implementation, we can notice that the value is used as a dropout rate instead:

https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L349-L353

https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L413-L423

https://github.com/tensorflow/tensorflow/blob/5855c1c8ae2d2b36bf9d6f906b717046bb4afb36/tensorflow/python/keras/applications/efficientnet.py#L514-L517


The difference in implementation would be that the connection dropped is at a module level in the original implementation and at a unit level in the `tf.keras.applications` implementation."
45915,tf.train.Saver and saver.restore do not work when data_augmentation_options rgb_to_gray and random_vertical_flip are configured in the pipeline config file,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (but added a workaround to overcome first issue which will be explained)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.2 (also with 1.14.0)
- Python version: 3.6.4
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I am using object detection api and training my model using faster_rcnn_resnet152. Training and evaluation worked fine until I added below data augmentation options in the pipeline config file. The evaluation script (I am using old eval.py on purpose) fails with below error. The error seems to be due to rgb_to_gray option. Pipeline config file has also subtract_channel_mean  and random_horizontal_flip options, however they work fine until the other two configurations are added. To overcome the error I added ""reshape=True"" argument to saver = tf.train.Saver(variables_to_restore) line in the legcy/evaluator.py line 270 and it started to work if only saver.restore(sess, latest_checkpoint) is commented out in the line 274. When the line is enabled this time it throws the second exception given below. Could you propose a solution/workaround without having to upgrade tensorflow/python etc versions?

  data_augmentation_options {
    rgb_to_gray {
    }
  }
  data_augmentation_options {
    random_vertical_flip {
    }

ERROR:1
------------------------------
Assign requires shapes of both tensors to match. lhs shape= [7,7,3,64] rhs shape= [7,7,1,64]
         [[node save/Assign_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]

Caused by op 'save/Assign_780', defined at:
  File ""legacy/eval.py"", line 149, in <module>
    tf.app.run()
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""legacy/eval.py"", line 145, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py"", line 270, in evaluate
    saver = tf.train.Saver(variables_to_restore)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 832, in __init__
    self.build()
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 844, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 881, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 513, in _build_internal
    restore_sequentially, reshape)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 354, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 73, in restore
    self.op.get_shape().is_fully_defined())
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 223, in assign
    validate_shape=validate_shape)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 64, in assign
    use_locking=use_locking, name=name)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Assign requires shapes of both tensors to match. lhs shape= [7,7,3,64] rhs shape= [7,7,1,64]
         [[node save/Assign_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]
------------------------------------------

ERROR:2
----------------------------------------------------------------------------------------------------------

Traceback (most recent call last):
  File ""legacy/eval.py"", line 154, in <module>
    tf.app.run()
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""legacy/eval.py"", line 150, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py"", line 302, in evaluate
    eval_export_path=eval_config.export_path)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/eval_util.py"", line 519, in repeated_checkpoint_run
    process_metrics_fn=process_metrics_fn)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/eval_util.py"", line 314, in _run_checkpoint_once
    restore_fn(sess)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py"", line 280, in _restore_latest_checkpoint
    saver.restore(sess, latest_checkpoint)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1312, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Input to reshape is a tensor with 3136 values, but the requested shape has 9408
         [[node save/Reshape_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]

Caused by op 'save/Reshape_780', defined at:
  File ""legacy/eval.py"", line 154, in <module>
    tf.app.run()
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""legacy/eval.py"", line 150, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py"", line 270, in evaluate
    saver = tf.train.Saver(var_list=variables_to_restore,reshape=True)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 832, in __init__
    self.build()
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 844, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 881, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 513, in _build_internal
    restore_sequentially, reshape)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 354, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 68, in restore
    restored_tensor = array_ops.reshape(restored_tensor, restored_shapes[0])
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7179, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/truba/home/iuzun/anaconda3/envs/tensorflow_cpu1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Input to reshape is a tensor with 3136 values, but the requested shape has 9408
         [[node save/Reshape_780 (defined at /truba/home/iuzun/Tensorflow/models/research/object_detection/legacy/evaluator.py:270) ]]
----------------------------------------------------------------------------------------------------------

Executed command:
python legacy/eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet152_coco.config --checkpoint_dir=training/ --eval_dir=eval/

Thanks in advance"
45914,TF 2.4 shows shape mismatch where 2.3.1 did not,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.4
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

i_model = Model([obs1.input, obs2.input], x, name='icm_inverse_model') returns a shape mismatch under the current version of tensorflow.


No mismatch should be given as was the case in version 2.3.1 unless I am missing somthing...

```from keras.models import Model, Sequential
from keras.layers import Input, Concatenate, GRU, Dense, Reshape
from keras.optimizers import Adam
from keras.backend import clear_session
from pathlib import Path
from subprocess import Popen, PIPE, STDOUT, TimeoutExpired
import numpy as np
import tensorflow as tf
from rl.agents import SARSAAgent
from rl.policy import BoltzmannQPolicy
import os

cmd = 'echo Hello World!'
env_reward = 0
length_penalty = .25
learning_reward = 10

hidden_layers = 4
layer_neurons = 128
learning_rate = 0.001
nb_actions = 96

tf.get_logger().setLevel('ERROR')

done = False
cmd_in = True
obs_last = None
initialize = True

while True:
    if cmd_in:
        proc = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)
        try:
            stdout = proc.communicate(timeout=1)[0].decode()
            exitcode = proc.returncode
        except TimeoutExpired:
            proc.kill()
            stdout = proc.communicate()[0].decode()
            exitcode = proc.returncode
        nnin = ''.join(char for char in stdout if char.isprintable())
        filename = Path('mem.txt')
        filename.touch(exist_ok=True)
        if not nnin:
            nnin = 'Done!'
            stdout = nnin
        if exitcode == 0:
            done = True
            with open('mem.txt', 'r+') as mem:
                for line in stdout.splitlines():
                    if line + '\n' not in mem:
                        mem.write(line + '\n')
                        env_reward += learning_reward
        cmd = ''
        print('\n')
        print(stdout)
        print('# ', end='', flush=True)
    else:
        nnin = cmd
        print(nnin[-1], end='', flush=True)
        env_reward -= length_penalty
    idxs = (np.frombuffer(nnin.encode(), dtype=np.uint8) - 32) / 100 
    env = tf.reshape(idxs, idxs.shape + (1,))
    shape = env.shape


    def build_actor_model(shape, nb_actions):
        model = Sequential()
        model.add(Reshape(shape[1::], input_shape=shape))
        for layer in range(hidden_layers):
            model.add(GRU(layer_neurons, name='GRU' + str(layer), return_sequences=True))
        model.add(GRU(layer_neurons, name='GRU' + str(hidden_layers)))
        model.add(Dense(nb_actions, name='output', activation='softmax'))
        return model


    def build_main(shape, name_prefix='main.'):
        inputs = Input(shape=shape)
        x = inputs
        for layer in range(hidden_layers):
            x = GRU(layer_neurons, name=name_prefix + ('GRU' + str(layer)), return_sequences=True)(x)
        x = GRU(layer_neurons, name=name_prefix + ('GRU' + str(hidden_layers)))(x)
        model = Model(inputs, x, name=name_prefix + 'main')
        return model


    def build_inverse_model(obs1, obs2, nb_actions):
        x = Concatenate()([obs1.output, obs2.output])
        x = Dense(nb_actions, name='icm_i.output', activation='sigmoid')(x)
        i_model = Model([obs1.input, obs2.input], x, name='icm_inverse_model')
        return i_model


    def build_forward_model(obs1, nb_actions):
        act1 = Input(shape=nb_actions)
        x = Concatenate()([obs1.output, act1])
        output_shape = obs1.output_shape[1]
        x = Dense(output_shape, name='icm_f.output', activation='linear')(x)
        f_model = Model([obs1.input, act1], x, name='icm_forward_model')
        return f_model


    inv_weights_fname = '{}_inv_weights.h5f'.format(""SMB"")
    fwd_weights_fname = '{}_fwd_weights.h5f'.format(""SMB"")
    agent_weights_fname = '{}_agent_weights.h5f'.format(""SMB"")

    main = build_main(shape)
    main2 = build_main(shape, name_prefix='main2.')
    inverse_model = build_inverse_model(main, main2, nb_actions)
    inverse_model.compile(Adam(learning_rate), loss='mse', metrics=['mse'])
    forward_model = build_forward_model(main, nb_actions)
    forward_model.compile(Adam(learning_rate), loss='mse', metrics=['mse'])
    model = build_actor_model((1,) + shape, nb_actions)
    policy = BoltzmannQPolicy()
    agent = SARSAAgent(model=model, nb_actions=nb_actions, policy=policy)
    agent.compile(Adam(learning_rate), metrics=['mae'])
    agent.reset_states()

    if initialize:
        if os.path.isfile(inv_weights_fname):
            inverse_model.load_weights(inv_weights_fname)
        if os.path.isfile(fwd_weights_fname):
            forward_model.load_weights(fwd_weights_fname)
        if os.path.isfile(agent_weights_fname):
            agent.load_weights(agent_weights_fname)
        initialize = False
    agent.training = True

    obs_now = env
    if obs_last is None:
        obs_last = obs_now
    action = agent.forward(obs_now)
    icm_action = np.zeros(nb_actions)
    icm_action[action] = 1
    inv_loss = inverse_model.train_on_batch([np.expand_dims(obs_last, 0), np.expand_dims(obs_now, 0)],
                                            [np.expand_dims(icm_action, 0)])
    features_now = main.predict(np.expand_dims(obs_now, 0))
    fwd_loss = forward_model.train_on_batch([np.expand_dims(obs_last, 0), np.expand_dims(icm_action, 0)],
                                            [features_now])
    obs_last = obs_now
    r_intr = (fwd_loss[0] ** 0.5) / 100
    reward = r_intr + env_reward
    agent.backward(reward, done)
    clear_session()
    done = False

    enc_ascii = action + 32
    if enc_ascii != 127:
        cmd += chr(enc_ascii)
        cmd_in = False
        continue
    cmd_in = True
    inverse_model.save_weights(inv_weights_fname, overwrite=True)
    forward_model.save_weights(fwd_weights_fname, overwrite=True)
    agent.save_weights(agent_weights_fname, overwrite=True)```"
45913,ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\programdata\\anaconda3\\lib\\site-packages\\cloudpickle-1.6.0.dist-info\\direct_url.json' Consider using the `--user` option or check the permissions,i am facing this issue while intalling rasa
45911,Important performance difference between training and evaluation mode,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7

Training a model using .fit method, there is an important difference of performance between training and validation even if the train split is also used for validation. The difference is the training and testing mode for the batchnorm but this should not give a difference in performance.

```python
import tensorflow as tf
import tensorflow_datasets as tfds


class Residual3x3Unit(tf.keras.layers.Layer):
    def __init__(self, channels_in, channels_out, stride, droprate=0., activate_before_residual=False):
        super(Residual3x3Unit, self).__init__()
        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)
        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)
        self.conv_0 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=stride, padding='same', use_bias=False)
        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.999)
        self.relu_1 = tf.keras.layers.LeakyReLU(alpha=0.1)
        self.conv_1 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=1, padding='same', use_bias=False)
        self.downsample = channels_in != channels_out
        self.shortcut = tf.keras.layers.Conv2D(channels_out, kernel_size=1, strides=stride, use_bias=False)
        self.activate_before_residual = activate_before_residual
        self.dropout = tf.keras.layers.Dropout(rate=droprate)
        self.droprate = droprate

    def call(self, x, training=True):
        if self.downsample and self.activate_before_residual:
            x = self.relu_0(self.bn_0(x, training=training))
        elif not self.downsample:
            out = self.relu_0(self.bn_0(x, training=training))
        out = self.relu_1(self.bn_1(self.conv_0(x if self.downsample else out), training=training))
        if self.droprate > 0.:
            out = self.dropout(out)
        out = self.conv_1(out)
        return out + (self.shortcut(x) if self.downsample else x)


class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, n_units, channels_in, channels_out, unit, stride, droprate=0., activate_before_residual=False):
        super(ResidualBlock, self).__init__()
        self.units = self._build_unit(n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual)

    def _build_unit(self, n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual):
        units = []
        for i in range(n_units):
            units.append(unit(channels_in if i == 0 else channels_out, channels_out, stride if i == 0 else 1, droprate, activate_before_residual))
        return units

    def call(self, x, training=True):
        for unit in self.units:
            x = unit(x, training=training)
        return x


class WideResNet(tf.keras.Model):
    def __init__(self, num_classes, depth=28, width=2, droprate=0., input_shape=(None, 32, 32, 3), **kwargs):
        super(WideResNet, self).__init__(input_shape, **kwargs)
        assert (depth - 4) % 6 == 0
        N = int((depth - 4) / 6)
        channels = [16, 16 * width, 32 * width, 64 * width]

        self.conv_0 = tf.keras.layers.Conv2D(channels[0], kernel_size=3, strides=1, padding='same', use_bias=False)
        self.block_0 = ResidualBlock(N, channels[0], channels[1], Residual3x3Unit, 1, droprate, True)
        self.block_1 = ResidualBlock(N, channels[1], channels[2], Residual3x3Unit, 2, droprate)
        self.block_2 = ResidualBlock(N, channels[2], channels[3], Residual3x3Unit, 2, droprate)
        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)
        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)
        self.avg_pool = tf.keras.layers.AveragePooling2D((8, 8), (1, 1))
        self.flatten = tf.keras.layers.Flatten()
        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs, training=True):
        x = inputs
        x = self.conv_0(x)
        x = self.block_0(x, training=training)
        x = self.block_1(x, training=training)
        x = self.block_2(x, training=training)
        x = self.relu_0(self.bn_0(x, training=training))
        x = self.avg_pool(x)
        x = self.flatten(x)
        x = self.dense(x)
        return x

train_ds = tfds.load('cifar10', split=""train"")

test_ds = tfds.load('cifar10', split=""train"")

def preprocess(ex):

    image = tf.cast(ex['image'], tf.float32) / 255.
    return image, ex['label']

train_ds = train_ds.map(preprocess).shuffle(1024).repeat().batch(64)
test_ds = test_ds.map(preprocess).repeat().batch(64)

model = WideResNet(10)

optimizer = tf.keras.optimizers.SGD(0.03, momentum=0.9)
model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

model.fit(train_ds, epochs=10, steps_per_epoch=100, validation_data=test_ds, validation_steps=100)
```

The output is the following:

```
Epoch 1/10
100/100 [==============================] - 15s 58ms/step - loss: 1.9948 - accuracy: 0.2491 - val_loss: 3.6047 - val_accuracy: 0.1061
Epoch 2/10
100/100 [==============================] - 5s 51ms/step - loss: 1.6504 - accuracy: 0.3899 - val_loss: 2.1616 - val_accuracy: 0.2086
Epoch 3/10
100/100 [==============================] - 5s 51ms/step - loss: 1.4418 - accuracy: 0.4759 - val_loss: 5.3945 - val_accuracy: 0.1839
Epoch 4/10
100/100 [==============================] - 5s 51ms/step - loss: 1.3975 - accuracy: 0.4871 - val_loss: 2.5811 - val_accuracy: 0.2809
Epoch 5/10
100/100 [==============================] - 5s 52ms/step - loss: 1.2669 - accuracy: 0.5371 - val_loss: 2.1341 - val_accuracy: 0.3431
Epoch 6/10
100/100 [==============================] - 5s 51ms/step - loss: 1.1513 - accuracy: 0.5834 - val_loss: 3.0617 - val_accuracy: 0.3303
Epoch 7/10
100/100 [==============================] - 5s 51ms/step - loss: 1.1181 - accuracy: 0.5982 - val_loss: 2.0735 - val_accuracy: 0.4008
Epoch 8/10
100/100 [==============================] - 5s 51ms/step - loss: 1.0284 - accuracy: 0.6285 - val_loss: 3.5450 - val_accuracy: 0.3250
Epoch 9/10
100/100 [==============================] - 5s 49ms/step - loss: 1.0219 - accuracy: 0.6352 - val_loss: 1.9222 - val_accuracy: 0.4300
Epoch 10/10
100/100 [==============================] - 5s 49ms/step - loss: 0.9897 - accuracy: 0.6519 - val_loss: 2.6546 - val_accuracy: 0.4075
```"
45910,TF-TRT conversion failing (maybe because of 2GB limit on GraphDef),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 16.04
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 2080

**Describe the current behavior**
While trying to convert a RoBERTa model to TRT, it throws an error: `KeyError: ""The name 'input_word_ids:0' refers to a Tensor which does not exist. The operation, 'input_word_ids', does not exist in the graph.""`

But after checking with `saved_model_cli`, it is indeed part of the graph. Investigating a bit further, this might happen because of another (previous) error:

```
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/message_lite.cc:406] tensorflow.GraphDef exceeded maximum protobuf size of 2GB: 2222908841
```

**Describe the expected behavior**

It should convert it to TRT, as it successfully happens when using a smaller model (e.g. `DistilRoBERTa`).

**Standalone code to reproduce the issue**
```
from tensorflow.python.compiler.tensorrt import trt_convert as trt

converter = trt.TrtGraphConverterV2(
    input_saved_model_dir=<saved model directory>, 
    conversion_params=trt.TrtConversionParams()
)

converter.convert()
converter.save(<output dir>)
```

Additional info:
Related to https://github.com/tensorflow/tensorrt/issues/229"
45909,asdasdas,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if youâ€™re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45906,TF 2.4.0 Python dependency issues,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : OSX, Centos
- TensorFlow installed from (source or binary): package installed with pipenv from pypi
- TensorFlow version: 2.4.0
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: pipenv

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```pipenv install``` from the following Pipfile, which now picks up the latest TF 2.4.0:
```
[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[dev-packages]
pylint = ""*""
pytest-cov = ""*""
grpcio-tools = ""*""

[packages]
scikit-learn = ""*""
pytest = ""*""
pytest-docker = ""*""
numpy = ""*""
tensorflow-serving-api = ""*""
tensorflow.python.framework = ""*""
grpcio = ""*""
grpcio-reflection = ""*""
spacy = ""*""
py-grpc-prometheus = ""*""
protobuf = ""*""
dynaconf = ""*""
google-cloud-storage = ""*""

[requires]
python_version = ""3.8""
```

**Any other info / logs**
Here is the log from ```pipenv check``` after installation:
```
Checking PEP 508 requirements...
Passed!
Checking installed package safety...
38932: cryptography <=3.2 resolved (3.1.1 installed)!
Cryptography 3.2 was released with the warning that its maintainers became aware of a Bleichenbacher vulnerability that they were only partly able to mitigate. See: CVE-2020-25659.
```
However, this was from an existing ```Pipfile.lock```. Deleting ```Pipfile.lock``` and re-installing in a new venv resulted in the following dependency mismatches:
```
[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.
  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.
 Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.
  Hint: try $ pipenv lock --pre if it is a pre-release dependency.
ERROR: Could not find a version that matches grpcio>=1.0<2,>=1.10.0,>=1.34.0,~=1.32.0 (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))
Tried: 0.13.0, 0.13.1, 0.14.0, 0.15.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.3.5, 1.4.0, 1.6.0, 1.6.3, 1.7.0, 1.7.3, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 1.8.6, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.11.0, 1.11.1, 1.12.0, 1.12.1, 1.13.0, 1.14.0, 1.14.1, 1.14.2, 1.15.0, 1.16.0, 1.16.1, 1.17.0, 1.17.1, 1.18.0, 1.19.0, 1.20.0, 1.20.1, 1.21.1, 1.22.0, 1.22.1, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.3, 1.25.0, 1.26.0, 1.26.0, 1.27.1, 1.27.1, 1.27.2, 1.27.2, 1.28.1, 1.28.1, 1.29.0, 1.29.0, 1.30.0, 1.30.0, 1.31.0, 1.31.0, 1.32.0, 1.32.0, 1.33.1, 1.33.1, 1.33.2, 1.33.2, 1.34.0, 1.34.0
Skipped pre-versions: 0.4.0a0, 0.4.0a1, 0.4.0a2, 0.4.0a3, 0.4.0a4, 0.4.0a5, 0.4.0a6, 0.4.0a7, 0.4.0a8, 0.4.0a13, 0.4.0a14, 0.5.0a0, 0.5.0a1, 0.5.0a2, 0.9.0a0, 0.9.0a1, 0.10.0a0, 0.11.0b0, 0.11.0b1, 0.12.0b0, 0.13.1rc1, 0.14.0rc1, 1.0.0rc1, 1.0.0rc2, 1.0.1rc1, 1.0.2rc0, 1.1.0rc1, 1.2.0rc1, 1.2.0rc2, 1.4.0rc1, 1.6.0rc1, 1.7.0rc1, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.10.0rc1, 1.10.0rc2, 1.10.1rc1, 1.10.1rc2, 1.11.0rc1, 1.11.0rc2, 1.11.1rc1, 1.12.0rc1, 1.13.0rc1, 1.13.0rc2, 1.13.0rc3, 1.14.0rc1, 1.14.0rc2, 1.14.2rc1, 1.15.0rc1, 1.16.0rc1, 1.16.1rc1, 1.17.0rc1, 1.17.1rc1, 1.18.0rc1, 1.19.0rc1, 1.20.0rc1, 1.20.0rc2, 1.20.0rc3, 1.21.0rc1, 1.21.1rc1, 1.22.0rc1, 1.23.0rc1, 1.24.0rc1, 1.25.0rc1, 1.26.0rc1, 1.26.0rc1, 1.27.0rc1, 1.27.0rc1, 1.27.0rc2, 1.27.0rc2, 1.28.0.dev0, 1.28.0.dev0, 1.28.0rc1, 1.28.0rc1, 1.28.0rc2, 1.28.0rc2, 1.28.0rc3, 1.28.0rc3, 1.30.0rc1, 1.30.0rc1, 1.31.0rc1, 1.31.0rc1, 1.31.0rc2, 1.31.0rc2, 1.32.0rc1, 1.32.0rc1, 1.33.0rc1, 1.33.0rc1, 1.33.0rc2, 1.33.0rc2, 1.34.0rc1, 1.34.0rc1
There are incompatible versions in the resolved dependencies:
  grpcio (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))
  grpcio>=1.0<2 (from tensorflow-serving-api==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 13))
  grpcio>=1.10.0 (from py-grpc-prometheus==0.2.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 12))
  grpcio>=1.34.0 (from grpcio-reflection==1.34.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 10))
  grpcio~=1.32.0 (from tensorflow==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 3))
```
The problem here is TF 2.4.0 depends on ```grpcio``` 1.32.0, which does not exist.

Fixing version to TF 2.3.1 and tensorflow-serving-api to 2.3.0 resolves the pipenv install and the check failure. Pipfile as follows:
```
[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[dev-packages]
pylint = ""*""
pytest-cov = ""*""
grpcio-tools = ""*""

[packages]
scikit-learn = ""*""
pytest = ""*""
pytest-docker = ""*""
numpy = ""*""
tensorflow = ""==2.3.1""
tensorflow-serving-api = ""==2.3.0""
grpcio = ""*""
grpcio-reflection = ""*""
spacy = ""*""
py-grpc-prometheus = ""*""
protobuf = ""*""
dynaconf = ""*""
google-cloud-storage = ""*""

[requires]
python_version = ""3.8""
```"
45905,Entmax-alpha in addons,"I would like to implement the entmax-alpha feature as is described in https://arxiv.org/abs/1905.05702 in the addons. For alpha=1, it is softmax, and for alpha=2, it is sparsemax. 

This feature is a key to realize sparse seq-to-seq model. It will not change the current API. 

I have almost finished implementing the feature. Therefore, I would like to know if this has any chance of being merged before continuing. 
"
45904,regression: dropped Support of CUDAÂ® architecture 5.0?,"I'm using trying to use tensorflow 2.4 with nvidia 960m (compute compatibility 5.0)

According to the following change: https://github.com/tensorflow/docs/commit/cb886cfdd16d66ff7f8d1430676ff395b02910e6

Support of my card and many others have been dropped (My card at least definitely worked fine with tensorflow 2.2)

This is a major regression for many who are trying to upgrade, and currently, it is not even clear to me if it's intended and what is the rational behind it. If it is intended, please document it prominently in the 2.4 release. (and please consider not dropping the support of this architecture)"
45903,Model with custom metrics broken if saved and reloaded,"There is a new problem in r2.4 (not present in 2.3.1). After saving and reloading a model with custom metric the model is broken. The next training will not work. Here is my minimum code to easily reproduce: 
```
import numpy as np
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam

def cmetrics(y_true, y_pred):
	return(0)

model = Sequential()
model.add(Dense(10,activation=""relu"", input_shape=(331, 331, 3)))
model.add(Flatten())
model.add(Dense(10, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
	optimizer=Adam(),
	metrics=[cmetrics])
model.summary()
xdata = np.random.rand(100,331,331,3)
ydata = np.random.rand(100,10)
history = model.fit(x=xdata, y=ydata)
model.save('test.h5', save_format='h5')
model = load_model('test.h5', custom_objects={'cmetrics': cmetrics,})
history = model.fit(x=xdata,y=ydata)
```
When running with tensorflow 2.3.1, I get the expected result:
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 331, 331, 10)      40        
_________________________________________________________________
flatten (Flatten)            (None, 1095610)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                10956110  
=================================================================
Total params: 10,956,150
Trainable params: 10,956,150
Non-trainable params: 0
_________________________________________________________________
4/4 [==============================] - 1s 149ms/step - loss: 52.2964 - cmetrics: 0.0000e+00
4/4 [==============================] - 1s 142ms/step - loss: 46.6724 - cmetrics: 0.0000e+00
```
When running with tensorflow 2.4.0, I get this:
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 331, 331, 10)      40        
_________________________________________________________________
flatten (Flatten)            (None, 1095610)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                10956110  
=================================================================
Total params: 10,956,150
Trainable params: 10,956,150
Non-trainable params: 0
_________________________________________________________________
2020-12-21 13:30:33.688730: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-21 13:30:33.708412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2593735000 Hz
4/4 [==============================] - 1s 190ms/step - loss: 18.9856 - cmetrics: 0.0000e+00
Traceback (most recent call last):
  File ""modeltest.py"", line 22, in <module>
    history = model.fit(x=xdata,y=ydata)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 726, in _initialize
    *args, **kwds))
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:790 run_step  **
        with ops.control_dependencies(_minimum_control_deps(outputs)):
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2793 _minimum_control_deps
        outputs = nest.flatten(outputs, expand_composites=True)
    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/util/nest.py:341 flatten
        return _pywrap_utils.Flatten(structure, expand_composites)

    TypeError: '<' not supported between instances of 'function' and 'str'

```
**Additional Information:**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Debian 10**
- TensorFlow installed from (source or binary):
**pip install tensorflow**
- TensorFlow version (use command below):
**2.4.0**
- Python version:
**3.7**
"
45902,Number of Delegated Nodes,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Ubunto 18
- TensorFlow installed from (source or binary):
- source 
- Tensorflow version (commit SHA if source):
- 2.4 nigthly
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
When I run my NN tflite quantized model on mobile device's DSP. I get the following message:

INFO: Hexagon delegate: 44 nodes delegated out of 67 nodes with 7 partitions.

When visualizing the tflite model, I see that the whole model was successfully quantized (FFQ or IOQ - w/ or w/o inputs/outputs) 
So, why only part of the model was delegated?
Is there a way to control or optimize this? how to make all the NN to go through the DSP?

(Model Attached)
[myModel2.zip](https://github.com/tensorflow/tensorflow/files/5724126/myModel2.zip)

"
45901,How to catch internal TensorFlow error: 'GPU sync failed' in python?,"TF version 1.x and 2.x.

Is there a way to catch this error with try-expect in python?

`tensorflow.python.framework.errors_impl.InternalError: GPU sync failed`"
45900,How to catch/expect the error: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1,"TF version 1.x and 2.x.

Is there a way to catch this error with try-expect in python?

`F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1`"
45899,Training a keras model on TPU pods?,"Hi!

Are there any examples on how to run a Keras model on TPU pods? I have a model which runs fine on a V3-8. However, when trying to run the same code on V3-32, it fails with the following error:

`Failed copying input tensor from /job:worker/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:1/device:CPU:0 in order to run DatasetFromGraph: FetchOutputs node : not found [Op:DatasetFromGraph]`

The model is created and compiled within the TPU scope as recommended:

```
with strategy.scope():
    keras_model = create_model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)
    keras_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

```
I am running Tensorflow 2.3.1 whith the same version on the TPU.

I have already asked on StackOverflow without getting any good pointers / answers, see link here:

https://stackoverflow.com/questions/65331321/training-a-keras-model-using-tpu-pods?noredirect=1#comment115577267_65331321

Thanks!"
45897,Missing Documentation for compiling binaries in c++,"Not sure if this is the right place to post this, but anyways, does anyone have a link for documentation on building TensorFlow for C++ to be used with VS 2019? I've looked everywhere and can't seem to find any up to date information on how to do so. The build from source page on TensorFlow's website seemed to look like it was just for building a pip installer for python. 

I followed along with 

https://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8

but after linking the header files and .lib & .dll I got spewed a bunch of missing dependencies/syntax errors, so I'm guessing that that the bugs mentioned in the article didnt follow through to v2.4.0 (which is what I compiled it for)

If anyone could point to up to date documentation, or a pre-compiled library that works with c++ VS19 I'd appreciate it."
45895,keras Callbacks without _supports_tf_logs update separate `logs` ,"## System information
- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 18.04:
- TensorFlow installed from: binary
- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219
- Python version: 3.7.7

## Current Behaviour
Callbacks in `CallbackList` update different `logs` dicts based on private `_supports_tf_logs` attribute (see e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L431)). This attribute is undocumented and it is unclear exactly what it is intended to signify. If a callback adds an entry to `logs`, the effect this has depends on this value.

## Expected behavior
Callbacks should be affected to previous callbacks' mutations of logs regardless of `_supports_tf_logs` values.

## Standalone code
[This colab](https://colab.research.google.com/drive/1Gd6JaZZk9fKmZSFdX7yd9Q_Oui-YzqDv?usp=sharing) shows the result of manually changing the property value of `LearningRateScheduler` and it's affect on `ProgbarLogger`'s behaviour.

Code reproduced below for convenience
```python
import tensorflow as tf
inp = tf.keras.Input((1,))
out = tf.keras.layers.Dense(1)(inp)
model = tf.keras.Model(inp, out)
model.compile(loss='mse', optimizer='sgd')

x = tf.random.uniform((10, 1))
y = 2 * x + 3
dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)

sched = tf.keras.callbacks.LearningRateScheduler(lambda i: 1. / (i+1))
sched._supports_tf_logs = True  # makes ProgbarLogger display lr
# same issue with ReduceLROnPlateau
callbacks = [sched]

# add logger at end, otherwise it's inserted at front and won't print lr
callbacks.append(tf.keras.callbacks.ProgbarLogger())

model.fit(dataset, epochs=10, callbacks=callbacks)
```
Output
```txt
Epoch 1/10
5/5 [==============================] - 1s 109ms/sample - loss: 58.7811 - lr: 1.0000
Epoch 2/10
5/5 [==============================] - 0s 2ms/sample - loss: 94.1334 - lr: 0.5000
Epoch 3/10
5/5 [==============================] - 0s 2ms/sample - loss: 22.3592 - lr: 0.3333
Epoch 4/10
5/5 [==============================] - 0s 1ms/sample - loss: 3.3667 - lr: 0.2500
Epoch 5/10
5/5 [==============================] - 0s 1ms/sample - loss: 1.4383 - lr: 0.2000
Epoch 6/10
5/5 [==============================] - 0s 1ms/sample - loss: 0.8452 - lr: 0.1667
Epoch 7/10
5/5 [==============================] - 0s 1ms/sample - loss: 0.5714 - lr: 0.1429
Epoch 8/10
5/5 [==============================] - 0s 1ms/sample - loss: 0.4179 - lr: 0.1250
Epoch 9/10
5/5 [==============================] - 0s 1ms/sample - loss: 0.3216 - lr: 0.1111
Epoch 10/10
5/5 [==============================] - 0s 1ms/sample - loss: 0.2565 - lr: 0.1000
```"
45894,tf.data.experimental.assert_cardinality incompatible with INFINITE_CARDINALITY,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219
- Python version: 3.7.7

**Describe the current behavior**: `tf.data.experimental.assert_cardinality`  can be used to fix `Dataset.cardinality` in instances where it cannot be inferred. In situations where the cardinality is infinite, this raises an error where it shouldn't.

**Describe the expected behavior** if `ds = ds_base.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY)`, then an error should be raised if `ds_base` stops producing elements, as opposed to when the first element is produced.

**Standalone code to reproduce the issue**
[Notebook](https://colab.research.google.com/drive/13s9geZzR4xUtYJ6ImINl0kuisL-n8H_6?usp=sharing)

Code (same as notebook):
```python
import tensorflow as tf

ds = tf.data.Dataset.range(5).repeat().flat_map(lambda i: tf.data.Dataset.range(i))
print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # False
ds = ds.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY))
print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # True

for example in ds.take(1):
    pass
# tensorflow.python.framework.errors_impl.FailedPreconditionError:
# Input dataset was expected to contain -1 elements but contained at least 1 element.
```"
45892,Can't use contrib.slim in tf 1.14,"I am running on google colab. 
`import tensorflow
print(tensorflow.__version__)`
after running this cell the output is 1.14.0 so I assume the above command `!pip install tensorflow==1.14` worked
but i have the following error: 
`import tensorflow.contrib.slim as slim`
---> ImportError: No module named contrib.slim
I've tried changing the import to `import tf_slim as slim` but i get a similar error: ImportError: No module named tf_slim"
45890,"Need to install tensorflow 1.12.0 for gpt-2, not working","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20 Ulyana
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 12
- Python version: 3.8.3

**Describe the current behavior**

I am trying `pip3 install tensorflow==1.12.0`

Which returns:
`ERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)
ERROR: No matching distribution found for tensorflow==1.12.0`

**Describe the expected behavior**

1.12.0 installing

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45888,TF 2.4.0 with disable_eager_execution twice slowlier than 2.3.1 with disable_eager_execution,"I just upgraded from TF 2.3.1 to TF 2.4.0 and my tf.keras model is at least twice slowlier than it was. Is anyone encountering the same problem ?

I am on Windows 10 x64 2004
Python 3.8.6
GPU RTX 2070 8 Gb RAM
CPU i9 9900 K
64 Gb RAM, 1Tb SSD
CUDA 11.0
CuDNN 8.0.2
Numpy 1.19.3
and I disabled Keras eager execution (disabled since I use TF 2.0) with the statement: tensorflow.compat.v1.disable_eager_execution()

The execution console gives:
--------------------------------------------------------
2020-12-20 17:42:12.138275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-20 17:49:15.949788: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-20 17:49:15.951856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-20 17:49:15.991681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-20 17:49:15.991885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-20 17:49:16.099671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-20 17:49:16.099749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-20 17:49:16.146047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-20 17:49:16.181370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-20 17:49:16.369302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-20 17:49:16.412993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-20 17:49:16.426034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-20 17:49:16.426310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
Model: ""model""
Layer (type)                    Output Shape         Param #     Connected to        
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 8)]          0         
__________________________________________________________________________________________________                      
input_2 (InputLayer)            [(None, 22)]         0                               
__________________________________________________________________________________________________
embtri10 (Embedding)            (None, 8, 16)        235584      input_1[0][0]       
__________________________________________________________________________________________________
fcpos (Dense)                   (None, 19)           437         input_2[0][0]       
__________________________________________________________________________________________________
fctri (FC_SYMETRIQUE)           (None, 46)           5888        embtri10[0][0]      
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 65)           0           fcpos[0][0]              fctri[0][0]         
__________________________________________________________________________________________________
line (Dense)                    (None, 1)            65          concatenate[0][0]   
__________________________________________________________________________________________________
Total params: 241,974
Trainable params: 241,974
Non-trainable params: 0

2020-12-20 17:49:16.988484: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-20 17:49:16.989498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-20 17:49:16.989582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-20 17:49:16.991351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-20 17:49:16.991616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-20 17:49:16.991879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-20 17:49:16.992133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-20 17:49:16.992402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-20 17:49:16.992655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-20 17:49:16.992919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-20 17:49:16.993210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-20 17:49:17.759695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-20 17:49:17.759774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-12-20 17:49:17.760494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-12-20 17:49:17.763105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6637 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-12-20 17:49:17.763980: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-20 17:49:17.776001: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2020-12-20 17:49:17.940776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-20 17:49:18.573803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
__________________________________________________________________________________________________
Epoch 1/24
10165/10165 - 95s - loss: 0.7623 - mae: 0.0554 - mse: 0.0053
Epoch 2/24
10165/10165 - 73s - loss: 0.7172 - mae: 0.0518 - mse: 0.0045
etc.
__________________________________________________________________________________________________
whereas with TF 2.31 I use to have, for exactly the same model and data:
Epoch 1/24
10165/10165 - 34s - loss: 0.7643 - mae: 0.0555 - mse: 0.0053
Epoch 2/24
10165/10165 - 34s - loss: 0.7183 - mae: 0.0519 - mse: 0.0046
etc.

My model is compiled with:
OPTIM = Adam(lr=LR_ADAM)
MODELE.compile(optimizer=OPTIM, loss=MSE_BCE_LOSS, metrics=['mae', 'mse'])

And the Fit statement:
MODELE.fit(x=BATCH(T_JEUX, T_PARAMS, T_SCORES, BATCH_SIZE),
               epochs=NB_EPOCHS_ADAM, shuffle=False, verbose=2)

With BATCH being a Sequence

Olivier"
45887,Cannot quantize only part of a QAT model for running on both Android CPU/DSP,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0-dev20201208
- Python version: 3.7

**Standalone code to reproduce the issue**
What I'm trying to do is to quantize part of a model, running quantize-aware training, and then deploy it to run on both Android CPU/DSP for inference. The code is below:

```
def apply_quantization(layer):
    if isinstance(layer, keras.layers.Conv2D):
        return tfmot.quantization.keras.quantize_annotate_layer(layer)
    return layer

annotated_model = keras.models.clone_model(
    base_model, # base_model is a mobilenetv2 or resnet50
    clone_function=apply_quantization
)
q_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)
```
Then I try to convert it to a quantized tflite model (with only weights)

```
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.convert()
```
The tflite model works, but can only run on CPU but no op is delegated to DSP because the activations are not quantized (am I right?)

So I tried to quantize both weights and activations:

```
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.allow_custom_ops = True
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model = converter.convert()
```
The tflite model is generated, but I cannot initialize an interpreter with this model with following error:
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-13-3d7cc3476638> in <module>
      1 interpreter = tf.lite.Interpreter(model_content=m7)
----> 2 interpreter.allocate_tensors()
      3 input_index = interpreter.get_input_details()[0][""index""]
      4 output_index = interpreter.get_output_details()[0][""index""]
      5 print (input_index, output_index)

~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    333   def allocate_tensors(self):
    334     self._ensure_safe()
--> 335     return self._interpreter.AllocateTensors()
    336 
    337   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 5 (DEQUANTIZE) failed to prepare.
```
I've checked the generated tflite model, it seems there're two consecutive DEQUANTIZE op inserted. Any one knows how to fix it?"
45886,Problem getting python include path,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Cygwin
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.4.0-rc2
- Python version: 3.6.10
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.2.0

**Describe the problem**

```
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl"", line 267, column 40, in _python_autoconf_impl
                _create_local_python_repository(repository_ctx)
        File ""C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl"", line 212, column 41, in _create_local_python_repository
                python_include = _get_python_include(repository_ctx, python_bin)
        File ""C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl"", line 152, column 21, in _get_python_include
                result = execute(
        File ""C:/cygwin64/home/username/tensorflow/third_party/remote_config/common.bzl"", line 217, column 13, in execute
                fail(
Error in fail: Problem getting python include path.
java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\cygwin64\home\username\_bazel_username\5ud3wb25\external\local_config_python\usr\bin\python3"" -c ""from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())""): The system cannot find the file specified.
 (error: 2)
Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?
INFO: Repository llvm-project instantiated at:
  C:/cygwin64/home/username/tensorflow/WORKSPACE:19:16: in <toplevel>
  C:/cygwin64/home/username/tensorflow/tensorflow/workspace.bzl:690:20: in tf_repositories
Repository rule tf_http_archive defined at:
  C:/cygwin64/home/username/tensorflow/third_party/repo.bzl:131:34: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Problem getting python include path.
java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""C:\cygwin64\home\username\_bazel_username\5ud3wb25\external\local_config_python\usr\bin\python3"" -c ""from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())""): The system cannot find the file specified.
 (error: 2)
Is the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?
INFO: Elapsed time: 2.198s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)
    Fetching @local_execution_config_python; fetching
```

`C:\cygwin64\home\username\_bazel_username\5ud3wb25\external\local_config_python\` is an empty directory...


**Provide the exact sequence of commands / steps that you executed before running into the problem**

any of the following
```
$ bazel build '//tensorflow/tools/pip_package:build_pip_package'
$ env PYTHON_BIN_PATH='/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'
$ env PYTHON_BIN_PATH='C:/cygwin64/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'
```

**Any other info / logs**

I would of assumed it has something to do with bazel getting confused by cygwin paths (e.g. it can not handle any paths that have a space character in their own build script), but that would not explain why it is using such a bizarre path in the first place.

bazel can not be compiled from source, so that is not an option.
tensorflow can not be installed via pip, so that is not an option.
"
45885,Colab session getting restarted during conversion of the Boundless model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.4.0


## Colab Notebook for reproducing the issue
https://colab.research.google.com/gist/sayakpaul/baf963933cbc3e627e66f6330d66c519/boundless_tflite.ipynb

## Issue

When trying to convert the [Boundless model](https://tfhub.dev/google/boundless/quarter/1) Colab Notebook session is getting restarted and the converter is unable to generate the TensorFlow Lite model. 

## Code

```python
model_handle = https://tfhub.dev/google/boundless/quarter/1
model = hub.load(model_handle)
concrete_function = model.signatures['default']

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

**Note** that I did try to run the conversion without `converter.optimizations = [tf.lite.Optimize.DEFAULT]` and it does not help. 

## Logs

```
WARNING:tensorflow:Issue encountered when serializing model_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
WARNING:tensorflow:Issue encountered when serializing model_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
WARNING:tensorflow:Issue encountered when serializing variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
WARNING:tensorflow:Issue encountered when serializing variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
WARNING:tensorflow:Issue encountered when serializing trainable_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
WARNING:tensorflow:Issue encountered when serializing trainable_variables.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
to_proto not supported in EAGER mode.
```

## Useful references

The pre-trained Boundless model on TensorFlow Hub comes with [this tutorial](https://www.tensorflow.org/hub/tutorials/boundless). 

Anything I am missing out on during the conversion process? "
45883,Model.test_on_batch reset_metrics incorrect,"URL with issue: [Model.test_on_batch](https://www.tensorflow.org/api_docs/python/tf/keras/Model#test_on_batch)

## Description of issue (what needs changing):
`reset_metrics` states: 

> IfÂ True, the metrics returned will be only for this batch. IfÂ False, the metrics will be statefully accumulated across batches.

This is incorrect, as metric states are only reset AFTER values are computed. See example below.

This could be considered a bug, but it might be considered a breaking change to fix it, and addressing it as a documentation issue may be preferable.

```python
import tensorflow as tf

class Counter(tf.keras.metrics.Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.count = self.add_weight('count', dtype=tf.int64, initializer='zeros')

    def update_state(self, *args, **kwargs):
        self.count.assign_add(1)

    def result(self):
        return self.count


tf.random.set_seed(0)
inp = tf.keras.Input((1,))
out = tf.keras.layers.Dense(2, activation='softmax')(inp)
model = tf.keras.Model(inp, out)

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[Counter()])
batch_size = 1
x = tf.zeros((batch_size, 1,), dtype=tf.float32)
y = tf.zeros((batch_size,), dtype=tf.int64)
model.train_on_batch(x, y, reset_metrics=False)
logs = model.test_on_batch(x, y, reset_metrics=True, return_dict=True)
#  logs contains metrics for both both steps
print(f""counter = {logs['counter']}"")  # counter = 2
```"
45882,"Ubuntu18.04 running on WSL2: ""libcuda.so.1"" does not exist. How to create that?","------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:

```
def test_sum():
    assert sum([1, 2, 3]) == 6, ""Should be 6""

if __name__ == ""__main__"":
    test_sum()
    print(""Everything passed"")

    # import tensorflow as tf
    import tensorflow.compat.v1 as tf       # To get TF 1.x like behaviour in TF 2.0 one can run
    tf.disable_v2_behavior()

    print(tf.__version__)
    print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?:: installed using conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA11.0/ cuDNN8
- GPU model and memory: 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4.0
- Python version:
- Installed using virtualenv? pip? conda?:: installed using conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA11.0/ cuDNN8
- GPU model and memory: Quadro RTX 4000, 8GB




-   **Exact command to reproduce**:
print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))


### Describe the problem
The number of GPU available is being shown to be zero. 

I am getting following output:
 
```
(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ python3 /home/$USER/test.py
Everything passed
2020-12-19 12:47:55.757762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:From /home/dushyant/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2.4.0
2020-12-19 12:47:56.642478: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-19 12:47:56.645707: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:
2020-12-19 12:47:56.645745: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2020-12-19 12:47:56.645767: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-U96RKFC): /proc/driver/nvidia/version does not exist
Num GPUs Available:  0
```
Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:

I searched for libcuda.so.1 in my directory: ""LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:"" and elsewhere. It does not seem to exist.

I also search for 'libcuda.so*' and found the following:
```
(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ find /usr/ -name 'libcuda.so*'
/usr/local/cuda-11.0/doc/man/man7/libcuda.so.7
/usr/local/cuda-11.0/targets/x86_64-linux/lib/stubs/libcuda.so
```

I am under impression that I can create ""libcuda.so.1"". However, I have no idea how to create that. Any help would be appreciated.
"
45881,"When tried to Make Project or Build the app got an error ""Task :lib_task_api:downloadModelFile FAILED""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Emulator Pixel 3 API 29
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.3 

**Describe the current behavior**
When try to Debug or Run, Android Studio gives the following error (Sometimes it happen when I do Make Project too):
```
> Task :lib_interpreter:downloadModelFile
Download https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite

> Task :lib_interpreter:downloadModelFile FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':lib_interpreter:downloadModelFile'.
> javax.net.ssl.SSLException: Connection reset

```
There was no issue with the internet connection at that time. However, this work without errors previously, suddenly started getting this error.

**Describe the expected behavior**
It should build without errors.

**Standalone code to reproduce the issue**
Download text_classification example from https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android
After downloading, try to Make Project or Run/Debug. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
WARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.
Android SDK Build Tools 29.0.2 will be used.
To suppress this warning, remove ""buildToolsVersion '29.0.0'"" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.
WARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.
Android SDK Build Tools 29.0.2 will be used.
To suppress this warning, remove ""buildToolsVersion '29.0.0'"" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.
> Task :app:preBuild UP-TO-DATE
> Task :app:preInterpreterDebugBuild UP-TO-DATE

> Task :lib_interpreter:downloadModelFile
Download https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite

> Task :lib_interpreter:downloadModelFile FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':lib_interpreter:downloadModelFile'.
> javax.net.ssl.SSLException: Connection reset

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 22s
1 actionable task: 1 executed
```
"
45880,"When tried to Make Project or Build the app got an error ""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45879,Feature request - testing_split in ImageDataGenerator,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): `2.3.1`
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently `ImageDataGenerator` API has a parameter `validation_split` to split data-set _training_ and _validation_ split purpose.

**Will this change the current api? How?**

What about _testing_ ?

**Who will benefit with this feature?**

Anyone, I guess.

**Any Other info.**

After implementing, code may look something like this, way more simpler :
```
image_datagen = ImageDataGenerator(
    validation_split = 0.2
    testing_split = 0.1
)

train_generator = image_datagen.flow_from_directory(
        subset=""training"",)

valid_generator = image_datagen.flow_from_directory(
        subset=""validation"",)

test_generator = image_datagen.flow_from_directory(
        subset=""testing"",)
```"
45877,SavedModel FROM OFFICAL DOCS always returns the same class( use OFFICAL DOCS' CODE TO PREDICT),"Hello every1!
I'm learning tensorflow to predict multi classes images. And I first use **OFFCIAL DOCS** to learn.
url's here( a transfer learning ex): 
tensorflow.google.cn/tutorials/images/transfer_learning

and I add only two lines of code to export my model, like this:
```
# keras_model_path = './keras_save'
# model.save(keras_model_path)
```
After that, i tried to use this model to make a simple prediction of dogs and cats, like this:
```
from keras.models import load_model
from keras.preprocessing import image
import tensorflow as tf
import numpy as np

model = load_model('keras_save')

img = image.load_img('imgs_to_predict/1.jpg', target_size=(160,160))
img_array = tf.expand_dims(image.img_to_array(img), 0)
predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

class_names = ['cats','dogs'] # right names, use a code to print when generated the model

print(
    ""This image most likely belongs to {} with a {:.2f} percent confidence.""
    .format(class_names[np.argmax(score)], 100 * np.max(score)))
```
and, it's basically from this **offical doc**, here:
tensorflow.google.cn/tutorials/images/classification

the last part of it, the only thing is the image which the doc used is from Internet.

When trying to make a classification, the output always :
`This image most likely belongs to cats with a 100 percent confidence.`
even it's a **cat,** a **dog** ,or a **sunflower** from the offical doc.

I'm wondering what's wrong with it, and thanks for all guys who answering my question. Appreciate! :)"
45876,tf.compat.v1.get_variable does not reuse variables within a scope and same variable name,"1-  Tensorflow version = **2.4.0**

2- **Google Colab Notebook** 

The following code from the documentation of [tf.compat.v1.variable_scope](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) throws **assertionError**

```
def foo():
  with tf.compat.v1.variable_scope(""foo"", reuse=tf.compat.v1.AUTO_REUSE):
    v = tf.compat.v1.get_variable(""v"", [1])
  return v

v1 = foo()  # Creates v.
v2 = foo()  # Gets the same, existing v.
assert v1 == v2
```

Here is a screenshot of the error:
![Screenshot from 2020-12-19 19-52-16](https://user-images.githubusercontent.com/35839837/102691633-e6549400-4233-11eb-86a7-58a860acb263.png)
"
45875,How to create Flex-free model in TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Installed via PIP
- TensorFlow version (or github SHA if from source): Tensorflow 2.3.1

**Command used to run the converter or code if youâ€™re using the Python API**
```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

```
NOTE: Model is successfully converted.

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- TFlite model is working fine at CPU Linux 16.04 and RaspberryPi. But it gives BUS ERROR at edge device. [Details](https://github.com/tensorflow/tensorflow/issues/45504).

The original model is in PyTorch. I have used PyTorch-ONNX-Tensorflow-TFLite approach to convert the model in TFLite.
I am sharing graphs of ONNX and TFLite model. [Link](https://drive.google.com/drive/folders/1jzLZKus-9Dey2vgcMWjspBZfCRJbLCWW?usp=sharing)

Geeks says 
> Using Flex op requires to build TF op kernels which is difficult to support various targets.
If the required Flex op is only FlexMul, I think you might be able to create Flex-free model with some refactoring.

How can do this ""create Flex-free ""? 
"
45874,"TPU guide doesn't work, but used to - `Op type not registered 'DecodeImage' in binary`","## URL(s) with the issue:

https://www.tensorflow.org/guide/tpu
https://github.com/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb

## Description of issue (what needs changing):

Last time I tried it worked: 
  - [gcloud_scripts/test.bash @ `851b306`](https://github.com/SamuelMarks/ml-glaucoma/blob/851b306/gcloud_scripts/test.bash) using [tensorflow/docs/site/en/guide/tpu.ipynb @ `7931afd`](https://github.com/tensorflow/docs/blob/7931afd/site/en/guide/tpu.ipynb)

But now it does not work:
  - [gcloud_scripts/test.bash @ `a1dffb1`](https://github.com/SamuelMarks/ml-glaucoma/blob/a1dffb1/gcloud_scripts/test.bash) using [tensorflow/docs/site/en/guide/tpu.ipynb @ `945a448`](https://github.com/tensorflow/docs/blob/945a448/site/en/guide/tpu.ipynb)

### Clear description

My scripts are pretty simple, I made them from my [**Nothing to TPU in 5 Google Cloud commands** gist](https://gist.github.com/SamuelMarks/d5e4e4233c0a1401635f783994366daa), they just use the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud), and it's as easy as:

```sh
$ git clone https://github.com/SamuelMarks/ml-glaucoma
# Set env vars (you'll want to update the various IDs hereâ€¦):
$ . ./ml-glaucoma/gcloud_scripts/.env.sh
# Create network, firewall, compute, and tpu:
$ . ./ml-glaucoma/gcloud_scripts/init.sh
# Install Python deps, convert the Jupyter Notebook to a Python script, then run it:
$ . ./ml-glaucoma/gcloud_scripts/test.sh
# Destroy network, firewall, compute, and tpu
$ . ./ml-glaucoma/gcloud_scripts/teardown.sh
```

Unfortunately it fails in the `test.sh` phase, here's the relevant snippet:
```
2020-12-19 08:38:55.052102: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.0.2:8470}
2020-12-19 08:38:55.052147: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31480}
2020-12-19 08:38:55.068584: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.0.2:8470}
2020-12-19 08:38:55.068633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31480}
2020-12-19 08:38:55.069112: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31480
All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]
c device:  /job:worker/replica:0/task:0/device:TPU:0
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
PerReplica:{
  0: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  1: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  2: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  3: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  4: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  5: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  6: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  7: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
}
Traceback (most recent call last):
  File ""tpu-tester.py"", line 85, in <module>
    train_dataset = get_dataset(batch_size, is_training=True)
  File ""tpu-tester.py"", line 52, in get_dataset
    dataset, info = tfds.load(name='mnist', split=split, with_info=True,
  File ""tensorflow_datasets/core/load.py"", line 356, in load
    ds = dbuilder.as_dataset(**as_dataset_kwargs)
  File ""tensorflow_datasets/core/dataset_builder.py"", line 552, in as_dataset
    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)
  File ""tensorflow_datasets/core/utils/py_utils.py"", line 183, in map_nested
    return function(data_struct)
  File ""tensorflow_datasets/core/dataset_builder.py"", line 582, in _build_single_dataset
    ds = ds.cache()
  File ""tensorflow/python/data/ops/dataset_ops.py"", line 1400, in cache
    return CacheDataset(self, filename)
  File ""tensorflow/python/data/ops/dataset_ops.py"", line 3779, in __init__
    variant_tensor = gen_dataset_ops.cache_dataset_v2(
  File ""tensorflow/python/ops/gen_dataset_ops.py"", line 782, in cache_dataset_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. [Op:CacheDatasetV2]
2020-12-19 08:39:12.439461: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""tensorflow/python/distribute/tpu_strategy.py"", line 738, in async_wait
    context.async_wait()
  File ""tensorflow/python/eager/context.py"", line 2330, in async_wait
    context().sync_executors()
  File ""tensorflow/python/eager/context.py"", line 645, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DecodeImage' in binary running on n-73dd04c3-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
2020-12-19 08:39:12.516206: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 399, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1608367152.516107804"",""description"":""Error received from peer ipv4:192.168.0.2:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 399, Output num: 0"",""grpc_status"":3}
```"
45872,Illegal instruction  CPUs under version 2.4.0,"System information

OS: Ubuntu 18.04
TensorFlow binary installed using pip
TensorFlow version 2.4.0rc0
Python version 3.8.6
CUDA/cuDNN version: 11.0/8.0.4
GPU model: GTX 1080 ti
Describe the current behavior

Attempting to import tensorflow produces an ""Illegal instruction"" error.

Describe the expected behavior

Import tensorflow without error. (Illegal instruction)

python -c ""import tensorflow as tf""


"
45871,Documentation for tf.keras.backend.max is missing,"## URL(s) with the issue:

Please provide a link to the documentation entry:
https://www.tensorflow.org/api_docs/python/tf/keras/backend/max?hl=FA

## Description of issue (what needs changing): 
Go to https://www.tensorflow.org/s/results?q=keras.backend.max and click on the first link (in fact, any link related to that API).

The link to the API, **`tf.keras.backend.max`** is not available. Is it intentional, because, in the [Source Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272), it is set **not to generate the Docs**.

If it is intentional, can you please explain why? Is there a better alternative to this API, or you have plans to Deprecate it, etc..

### Correct links

Is the link to the source code correct? : No (Link for the API itself is missing)

### Parameters defined

Are all parameters defined and formatted correctly? : No

### Returns defined

Are return values defined? : No

### Raises listed and defined : No

### Usage example

Is there a usage example? : No

### Submit a pull request? : 
I'm ready to submit a PR to remove this [line of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2272) if it is fine with you."
45866,Illegal instruction on older CPUs under version 2.4.0 ,"
**System information**
- Ubuntu 18.04 and 20.04, Scientific Linux 7
- binary installed via pip
- version 2.4.0
- Python 3.8
- installed via pip (either inside or not inside a Conda environment)
- various CPU-only and GPU-hosting machines

**Describe the problem**

`import tensorflow` produces ""Illegal instruction (core dumped)"" on older machines (seemingly those that do not support AVX2 instructions). There is no problem on new machines (seemingly those that support AVX2 instructions). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
pip install tensorflow
python -c ""import tensorflow""
```

**Any other info / logs**

The core dump occurs on various machines with various types of CPUs. The common thread seems to be that it occurs on machines that don't support AVX2 instructions. Most of the machines on which this occurs do support AVX instructions. 

The issue does not occur with Tensorflow 2.3.1 nor with Tensorflow 2.5.0 installed via tf-nightly. 

Any chance Tensorflow 2.4.0 was built in a way (perhaps unintentionally) that requires AVX2 instructions or some other requirement that causes it to fail on somewhat older (but not really old) machines? Based on what I am seeing, it seems that using 2.4.0 on many machines will fail.

The same issue occurs when running in the official Tensorflow Docker container.

This seems related to issue #44668.
"
45864,Converting speech_embedding hub module to tflite results in `Encountered unresolved custom op: TensorArrayV3.Node`,"**System information**
- Colab default settings
- TF version: 2.4.0



**Describe the current behavior**
Converting speech_embedding module ''https://tfhub.dev/google/speech_embedding/1'' to tflite results in:

`RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.`

during inference.


**Standalone code to reproduce the issue**

```
HUB_URL = 'https://tfhub.dev/google/speech_embedding/1'
TEST_PATH = '.'

embedding_layer = hub.KerasLayer(HUB_URL, input_shape=(16000,), trainable=False)

model = tf.keras.Sequential([
    embedding_layer
])

model.save(TEST_PATH)
converter = tf.lite.TFLiteConverter.from_saved_model(TEST_PATH)


converter.allow_custom_ops = True
tflite_model = converter.convert()
tflite_model_file = 'converted_model.tflite'

with open(tflite_model_file, ""wb"") as f:
  f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=tflite_model_file)
interpreter.allocate_tensors()
```

**Output**

```---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-16-5f54d02787e4> in <module>()
      1 interpreter = tf.lite.Interpreter(model_path=tflite_model_file)
----> 2 interpreter.allocate_tensors()
      3 
      4 input_index = interpreter.get_input_details()[0][""index""]
      5 output_index = interpreter.get_output_details()[0][""index""]

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    257   def allocate_tensors(self):
    258     self._ensure_safe()
--> 259     return self._interpreter.AllocateTensors()
    260 
    261   def _safe_to_run(self):

RuntimeError: Encountered unresolved custom op: TensorArrayV3.Node number 2 (TensorArrayV3) failed to prepare.```



I am aware that STFT may not be supported by tflite yet. If this is the issue, is there a quick workaround?

Thanks,
Bryan"
45861,"Tensorflow's hwloc build force-enables use of sys/sysctl.h, which breaks on recent Linux/glibc","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, openSUSE Tumbleweed, tested on various snapshots up to 20201216 .
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested.
- TensorFlow installed from (source or binary): Source.
- TensorFlow version: 1.15.2
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 9.3
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

Tensorflow vendors the hwloc library and heavily customizes the way in which this library is built. I am not familiar enough with Bazel to fully understand the details of what you are doing here and the reasons why you are doing it, but unfortunately, what I do know is that on my machine, the net result is a broken hwloc build...

The immediate symptom is that some hwloc source files do not compile because they are configured to include the `<sys/sysctl.h>` header, which [has been removed from glibc >=2.32](https://sourceware.org/pipermail/libc-announce/2020/000029.html) because the underlying system call has been removed from the Linux kernel since release 5.5.

This is not a hwloc bug/incompatibility however, as it would intuitively seem, because the hwloc build system is perfectly able to figure out that this header does not exist and the hwloc source code knows how to avoid using it when that happens.

The actual problem is this line of the tensorflow build system: https://github.com/tensorflow/tensorflow/blob/1cd185160a061a1213e8e8d05eb078e880ac9e46/third_party/hwloc/BUILD.bazel#L113

For some reason that I do not know, it is pretty clear that you force-set the `HAVE_SYS_SYSCTL_H` define, which would normally be unset by the hwloc build system after it correctly detects that there is no `sysctl.h` header...

Removing this line of the `BUILD.bazel` file fixes the build on my machine, but I can only assume that you added it for some reason (most likely to make the build work on an operating system that does use the `sysctl.h` header, but on which the hwloc build system does not correctly detect said header ?), which means that the actual tensorflow patch will need to be more nuanced and only perform this patch on the OS configurations where it is necessary.

I am not able to easily share the build instructions that I followed because they are inside a mildly complicated build system within a closed-source project. But from my understanding of the problem, detailed reproducer instructions should not be necessary here, you should be able to easily replicate this issue just by trying to build tensorflow from source, through any method of your choosing, on any Linux distribution that uses glibc >=2.32. Although I personally observed this problem on openSUSE Tumbleweed, I would also expect it to reproduce identically on Gentoo, Arch, or Fedora 34..."
45860,TF2.4 doc missing keras.backend methods,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/backend

## Description of issue (what needs changing):

Missing a lot of `keras.backend` method documentation

### Clear description

For example, why should someone use this method? How is it useful?

In TF2.4 doc, a lot of methods in keras.backend are missing. E.g., Fig1 is the screenshot of v2.4, while Fig2 is the screenshot of v2.3.

![image](https://user-images.githubusercontent.com/5104719/102656451-bec4d500-4128-11eb-913e-cc4a3e27e58e.png)
![image](https://user-images.githubusercontent.com/5104719/102656515-d8661c80-4128-11eb-95eb-d7ba74c5d1fc.png)


### Correct links

Is the link to the source code correct? N/A

### Parameters defined

Are all parameters defined and formatted correctly? N/A

### Returns defined

Are return values defined? N/A

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

N/A

### Usage example

Is there a usage example? N/A

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
45858,Need to remove CorrectTensorEndianness in AllocateTensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.3.1
- Python version:3.6.9
- Bazel version (if compiling from source):3.4.1
- GCC/Compiler version (if compiling from source):Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the current behavior**
On testing  ```//tensorflow/lite/micro:memory_arena_threshold_test``` on s390x, the TC was failing with a segmentation fault. 
On debugging, it was found the `CorrectTensorDataEndianness` function was byte swapping a tensor which made it cross its data type limit. It looks like the `FlatBufferVectorToTfLiteTypeArray` already converts flatbuffer tensor data from little endian to big endian during `StartModelAllocation` function call within `AllocateTensors` function and ```CorrectTensorDataEndianness``` is not required anymore.
To make the test case pass, Allocation size values were also changed in `memory_arena_threshold_test.cc` file. Although as suggested in my last PR request #45790, I am raising this bug with a separate PR with removal of  `CorrectTensorDataEndianness` function call. 

**Describe the expected behavior**
`CorrectTensorDataEndianness` is not required as the tensor data is already converted to BE format.

**Standalone code to reproduce the issue**
Code to reproduce the issue:
```bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --host_javabase=""@local_jdk//:jdk"" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/lite/micro:memory_arena_threshold_test```

**Other info / logs**
I am creating a new PR request with only this change as suggested by @advaitjain. "
45857,Changing kBufferAlignment to 8 from 16 for SIMD extensions fixes TfLite micro_allocator_test,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
While testing ```//tensorflow/lite/micro:micro_allocator_test``` on s390x, a segmentation fault was observed.
On debugging, I found an issue with SSE alignment. There were multiple warnings in the logs: ```8 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.``` The TC was failing further when assigning the buffers but it seemed that on working on the warnings and changing `kBufferAlignment` from `16` to `8` fixed the issue.

To debug, I compared the results of this TC on Intel and s390x.
In ```MicroAllocator::Create```, with `kBufferAlignment = 16` ,  and everything else same on Intel and s390x ```uint8_t* aligned_result = reinterpret_cast<uint8_t*>(((data_as_uintptr_t + (alignment - 1)) / alignment) * alignment);``` returns `aligned_result` as ```(uint8_t *) 0x3ffffffe1e0``` where tensor_arena was ```(uint8_t *) 0x3ffffffe1d8```  while on Intel, same returns ```(uint8_t *) 0x7fffffffd2b0``` which is equal to address of tensor_arena or data_as_uintptr_t . If the value of ```kBufferAlignment``` is 8 , then aligned_arena is same as tensor_arena.
Although, I could not find if for s390x tensor buffers should be aligned to 8 bytes or 16 bytes for SIMD extensions, but changing the value to 8 helped in passing the TCs. This value effects a multiple TfLite Micro related test cases.

To our understanding,  s390x can handle any byte alignment. It would be great if we can identify the real cause of this issue and someone can look into this issue from SIMD perspective. 

**Describe the expected behavior**
The tensor arena should not lose any bytes of data due to alignment.

**Standalone code to reproduce the issue**
To reproduce this issue, you can simply run the test case using:
```bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --host_javabase=""@local_jdk//:jdk"" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/lite/micro:micro_allocator_test```

**Other info / logs** 
A PR was raised with this change and other arena size related fixes. That PR is rejected and closed and I was asked to raise a github issue to discuss this further. PR reference: #45790 . It would be great if we can find out the real cause behind this problem. @advaitjain Can you please help tag someone who would be able to help here. Thanks!"
45854,MultiHeadAttention masking mechanism,"Hello!

I wonder how we should apply masks (both padding and look-ahead) to the MultiHeadAttention layer, described in:

[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/multi_head_attention.py)

I have been trying to adapt the tutorial [https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer) with this layer but it seems like I'm having some masking problems (loss strangely ~100 times lower than usual and worse empirical results). The tutorial implementation works fine, but replacing the MultiHeadAttention with the one in tf.keras.layers just breaks it.

I'm pretty sure I'm missing something, but I can't figure it out by reading the implementation. What about including an example for both padding and look-ahead masks? I think it would be easy and useful.

Another option would be updating the tutorial (as it happened with LayerNormalization) though I'm afraid that would have more impact.

cc @tanzhenyu Sorry for the spam but I believe you are the one on charge about this.

EDIT: I don't know if on charge, but I see you every time I come around issues/commits pages."
45853,Tensorflow 2.3.0 MKL Intel AVX Binary Issue,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tensorflow-mkl 2.3.0
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

2.3.0

**Describe the current behavior**
Here's the code:

```

import tensorflow as tf
import numpy as np
a = np.array([2 , 4, 5])
ap=tf.constant(a)

```

Here's the warning message: 

> "" I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.""

Moreover, if I run system check mentioned on Intel's website (https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html),

Code:

```
import tensorflow as tf
major_version = int(tf.__version__.split(""."")[0])
if major_version >= 2:
   from tensorflow.python import _pywrap_util_port
   print(""MKL enabled:"", _pywrap_util_port.IsMklEnabled())
else:
   print(""MKL enabled:"", tf.pywrap_tensorflow.IsMklEnabled()) 
```
I get 

> MKL enabled: False


**Describe the expected behavior**

I shouldn't get the warning about AVX because I am using Intel's MKL version. Moreover, surprisingly, if I downgrade tensorflow to 2.1, warning changes to the issue described https://github.com/tensorflow/tensorflow/issues/45632. 


**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
a = np.array([2 , 4, 5])
ap=tf.constant(a)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

CPU Version: Intel i7-10610U CPU @1.8GHz
I have installed Tensorflow in a new environment to ensure that there is no issue with dependencies.


"
45852,TPU compile fail using the new 2.4.0 version,"After Colab update its tensorflow version to 2.4.0 recently, I have found that the code which I used to run the example https://colab.research.google.com/drive/1DvLiYBaolddDFok1FttwEkxHNhsRwK_R?usp=sharing has led to very peculiar behavior. In general, the reported bug is 
```
Compilation failure: Expected element type in shape to be arithmetic type for operation subtract; got PRED.
```

I have not been able to pinpoint the root cause of the bug. However, it would seem that all the bug is caused by a substraction operation. When I remove the substraction operation in the code, another substraction issue pops up, albiet in a different location. It is impossible to remove all the substraction in the code. Therefore it would really be helpful if one can pinpoint the problem. 

PS: The code runs perfectly fine under CPU/GPU environment. "
45851,Unexpected behavior when a function which involves tf.reshape is run using strategy.run on a TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow version (use command below): 2.4.0

**Describe the current behavior**
When computing a function which involves `tf.reshape` using `strategy.run` and a TPU, there is an unexpected behavior and errors are thrown. The function works fine if called directly.
This is the same error I get when I use that function as a loss function in a model.

**Describe the expected behavior**
I would expect the same behavior when the function is called directly and using strategy.run. Is there something I am missing out?

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/18yHVyEwbbfYXJ_r3XfP_NaStiVHqwkzN?usp=sharing

I am not completely sure this is a bug of Tensorflow and not a bug in my code, if you could help me I would really appreciate it.
Thank you very much for your help
"
45849,Inexact numeric jacobian causes test failures,"Using TensorFlow 2.4 with GPUs

**Describe the current behavior**

//tensorflow/python/keras/integration_test:gradients_test fails when run on a node with GPUs. I traced the problem to inexact computation in `_compute_numeric_jacobian`

Output is:
```
FAIL: testLSTMBatchJacobian (__main__.GradientsTest)
GradientsTest.testLSTMBatchJacobian
...
AssertionError: 
Not equal to tolerance rtol=0.01, atol=1e-06
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([2]))
not close lhs = [0.00074506]
not close rhs = [0.00076706]
not close dif = [2.20043e-05]
not close tol = [8.670623e-06]
dtype = float32, shape = (1, 1, 6)
Mismatch: 16.7%
Max absolute difference: 2.20043e-05
Max relative difference: 0.02868646
 x: array([[[-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461,
         -0.004366]]], dtype=float32)
 y: array([[[-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446,
         -0.004386]]], dtype=float32)
```

The numeric result is `array([-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461, -0.004366], dtype=float32)`
while the eager_result, function_result and backprop_result all are `y: array([-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446, -0.004386], dtype=float32)`


**Standalone code to reproduce the issue**
Reduced extracted test code:

```
import numpy as np
import tensorflow as tf

class GradientsTest(tf.test.TestCase):
  def testLSTMBatchJacobian(self):
    class HasLSTM(tf.keras.Model):

      def __init__(self):
        super(HasLSTM, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=5)
        self.dense = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)

      def call(self, x):
        return self.dense(self.lstm(x))

    m = HasLSTM()

    def jacobian(x):
      with tf.GradientTape() as tape:
        tape.watch(x)
        y = m(x)  # pylint: disable=not-callable
      return tape.batch_jacobian(y, x)

    inp = tf.nn.l2_normalize(tf.ones([1, 2, 3]), axis=[1, 2])
    eager_result = jacobian(inp)
    #function_result = tf.function(jacobian)(inp)
    #self.assertAllClose(eager_result, function_result)
    backprop_result, numeric_result = tf.test.compute_gradient(
        m, [inp], delta=1e-3)

    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)
    self.assertAllClose(tf.reshape(numeric_result, [-1]),
                        tf.reshape(eager_result, [-1]), rtol=1e-2)

if __name__ == ""__main__"":
  tf.test.main()
```


**Other info / logs**
Increasing the delta to `1e-2` makes the test pass.

Side note: Uncommenting the `tf.function` line yields a couple wrong-looking errors/warnings:
```
WARNING:tensorflow:Using a while_loop for converting CudnnRNNBackprop
pfor.py:1052] Using a while_loop for converting CudnnRNNBackprop
I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
 W tensorflow/core/common_runtime/process_function_library_runtime.cc:805] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
```


"
45848,"Could not load dynamic library 'libcusolver.so.10' with official TF 2.4, even though it's installed","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS
- TensorFlow installed from (source or binary): official 2.4 binary, installed via Python `pip`
- TensorFlow version (use command below): 2.4.0 (git version v2.4.0-rc4-71-g582c8d236cb)
- Python version: 3.8.5
- CUDA/cuDNN version: 11.2.0-1 / 8.0.5.39-1 (both installed from Nvidia repo)
- GPU model and memory: Tesla P100 (16 GB)

**Describe the current behavior**
After installing latest CUDA and cuDNN via the Nvidia repo I installed latest TF 2.4 via Python `pip`. Then I run the following:

```python
In [1]: import tensorflow as tf
2020-12-18 14:38:28.563109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0

In [2]: tf.config.list_physical_devices('GPU')
2020-12-18 14:38:30.711190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-18 14:38:30.711736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-12-18 14:38:30.738163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-18 14:38:30.738690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2020-12-18 14:38:30.738710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-18 14:38:30.741099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-12-18 14:38:30.741136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-12-18 14:38:30.741931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-18 14:38:30.742126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-18 14:38:30.742245: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-12-18 14:38:30.742805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-12-18 14:38:30.742917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-12-18 14:38:30.742930: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```

So apparently the library `libcusolver.so.10` cannot be found.
However, this library file is installed on my machine:

```shell
$ locate libcusolver.so.10
/usr/lib/x86_64-linux-gnu/libcusolver.so.10
/usr/lib/x86_64-linux-gnu/libcusolver.so.10.2.0.243
```

Any ideas what I am missing here?"
45847,TensorFlow Lite NNAPI with Quantisation: Invalid Zero Point,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10 5G
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0-dev20201210
- Python version: 3.7.5
- CUDA/cuDNN version: 11.1

**Describe the current behavior**
TensorFlow Lite raises an error when doing inference with NNAPI on a mobile device, with a custom model quantised to int8. Issue happens both when I use int8 quantisation with float fallback and strict int8 quantisation. If I take PReLU out of the model the error goes away.

Terminal output below:
```

2020-12-18 12:01:37.452 6713-7171/org.tensorflow.benchmarking I/tflite: Created TensorFlow Lite delegate for NNAPI.
2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/tflite: Initialized TensorFlow Lite runtime.
2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: DeviceManager::DeviceManager
2020-12-18 12:01:37.454 6713-7171/org.tensorflow.benchmarking I/Manager: findAvailableDevices
2020-12-18 12:01:37.455 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface armnn
2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.900000, .powerUsage = 0.000000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.000000, .powerUsage = 0.900000}, .operandPerformance = [16]{{.type = FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = UINT32, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 0.400000, .powerUsage = 0.400000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = BOOL, .info = {.execTime = 340282346638528859811704183484516925440.000000, .powerUsage = 340282346638528859811704183484516925440.000000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 0.600000, .powerUsage = 0.600000}}, {.type = TENSOR_FLOAT16
2020-12-18 12:01:37.458 6713-7171/org.tensorflow.benchmarking I/Manager: Found interface liteadaptor
2020-12-18 12:01:37.459 6713-6713/org.tensorflow.benchmarking D/RtgSchedIpcFile: setCommandByIoctl failed ret:-1, cmdid:32, errno:13
2020-12-18 12:01:37.453 6713-6713/org.tensorflow.benchmarking W/ow.benchmarking: type=1400 audit(0.0:22623): avc: denied { ioctl } for pid=6713 path=""/proc/6713/rtg"" dev=""proc"" ino=478805 ioctlcmd=0xab20 scontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tcontext=u:r:untrusted_app:s0:c155,c256,c512,c768 tclass=file permissive=0
2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/Manager: Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.100000, .powerUsage = 0.100000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.100000, .powerUsage = 0.100000}, .operandPerformance = [14]{{.type = FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = UINT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT8_ASYMM, .info = {.execTime = 0.200000, .powerUsage = 0.200000}}, {.type = BOOL, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = TENSOR_QUANT16_SYMM, .info = {.execTime = 1.000000, .powerUsage = 1.000000}}, {.type = TENSOR_FLOAT16, .info = {.execTime = 0.100000, .powerUsage = 0.100000}}, {.type = TENSOR_BOOL8, .info = {.execTime = 0.500000, .powerUsage = 0.500000}}, {.type = FLOA
2020-12-18 12:01:37.460 6713-7171/org.tensorflow.benchmarking I/TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.
2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:396): type.zeroPoint == 0 (type.zeroPoint = -128, 0 = 0) ANeuralNetworksModel_addOperand invalid zeroPoint: -128
2020-12-18 12:01:37.461 6713-7171/org.tensorflow.benchmarking E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/Utils.cpp:463): validateQuant8SymmParams(type, tag) 
2020-12-18 12:01:37.462 6713-7171/org.tensorflow.benchmarking E/org.tensorflow.benchmarking.MainActivity: Error initialising tflite interpreter
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1380 while adding operand for tensor 'model_tf_nlhd_nld/p_re_lu/add;model_tf_nlhd_nld/p_re_lu/Relu;model_tf_nlhd_nld/p_re_lu/Neg_1;model_tf_nlhd_nld/p_re_lu/Relu_1;model_tf_nlhd_nld/p_re_lu/mul'.
    
```

I've also added the model file below.

[model.zip](https://github.com/tensorflow/tensorflow/files/5715611/model.zip)

"
45846, tf.keras.experimental.WideDeepModel saved model fails with 2 optimizers,"tensorflow 2.4

    
```
    wide_deep_model = tf.keras.experimental.WideDeepModel(linear_model, dnn_model, activation='sigmoid')
    wide_deep_model.compile(optimizer= [linear_optimizer,dnn_optimizer]
                            loss=tf.keras.losses.BinaryCrossentropy(),
                            metrics=tf.keras.metrics.BinaryAccuracy())

    ...
    model.fit(dataset.batch(100).shuffle(100), epochs=2000, callbacks=[tensorboard_callback])
    tf.saved_model.save(model, 'model/{}'.format(int(time.time())))
```

saved_mode will fail if given 2 optimizer to  tf.keras.experimental.WideDeepModel, the error is straight forward(self.optimizer is a list, so has no get_config method):

Lib\site-packages\tensorflow\python\keras\saving\saving_utils.py

```
      else:
        optimizer_config = {
            'class_name':
                generic_utils.get_registered_name(model.optimizer.__class__),
            'config':
                model.optimizer.get_config()
        }
      metadata['training_config']['optimizer_config'] = optimizer_config
  return metadata
```

"
45845,Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 98x8 (Android),"**System information**
- OS Platform and Distribution: Android 9, 10 , 11
- Mobile device : Pixel 3a, Nokia 6.1 
- TensorFlow installed from:
  https://bintray.com/google/tensorflow/tensorflow-lite-gpu
  https://bintray.com/google/tensorflow/tensorflow-lite
- TensorFlow version : 2.4.0

**Describe the current behavior**
I upgraded tensorflow-lite and tensorflow-lite-gpu from 2.3.0 to 2.4.0 
And getting this error on initialization interpeteur 

` java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 98x8
    TfLiteGpuDelegate Prepare: delegate is not initialized
Node number 329 (TfLiteGpuDelegateV2) failed to prepare.`

`  val tfliteOptions = Interpreter
                .Options()
                .setNumThreads(THREADS_COUNT)
                .addDelegate(GpuDelegate())

  Interpreter(loadModelFile(context), tfliteOptions)`

The problem is somewhere in tensorflow-lite-gpu 2.4.0

If i'm using such configuration with older version all works well 

implementation(""org.tensorflow:tensorflow-lite:2.4.0"") 
implementation(""org.tensorflow:tensorflow-lite-gpu:2.3.0"")

"
45842,"""InternalError: cudaGetDevice() failed. Status: initialization error""  while using Sequential() and other","**System information**
- I wrote a very simple code.
- OS Platform and Distribution: ManjaroLinux 20.2
- CPU: ryzen7 3750H 
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA : V11.1.105 / cuDNN : cudnn-8.0.5.39-1   
- GPU model and memory: GTX 1660ti Max-Q 6 GB

```

== check python ===================================================
python version: 3.8.6
python branch: 
python build version: ('default', 'Sep 30 2020 04:00:38')
python compiler version: GCC 10.2.0
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 10.2.0
Copyright (C) 2020 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                        1.19.2
protobuf                     3.14.0
tensorflow-estimator         2.4.0
tensorflow-gpu               2.4.0

== check for virtualenv =========================================
False



== env ==========================================================
LD_LIBRARY_PATH :/opt/cuda/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Dec 18 17:15:55 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 166...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   56C    P8    10W /  N/A |     27MiB /  5944MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       936      G   /usr/lib/Xorg                                 14MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 6, 'final', 0)

== bazel version  ===============================================
```
"" tensorflow import"" part is too long so I uploaded here [https://justpaste.it/9olm4](https://justpaste.it/9olm4)


This is my code
```
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
gpus = tf.config.experimental.list_physical_devices('GPU')
print(gpus)
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)
model = Sequential()
```
Output
```
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-3-65dd765202ab> in <module>
     12     except RuntimeError as e:
     13         print(e)
---> 14 model = Sequential()

~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    115     """"""
    116     # Skip the init in FunctionalModel since model doesn't have input/output yet
--> 117     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call
    118         name=name, autocast=False)
    119     base_layer.keras_api_gauge.get_cell('Sequential').set(True)

~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
    291     self._steps_per_execution = None
    292 
--> 293     self._init_batch_counters()
    294     self._base_model_initialized = True
    295 

~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    515     self._self_setattr_tracking = False  # pylint: disable=protected-access
    516     try:
--> 517       result = method(self, *args, **kwargs)
    518     finally:
    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)
    299     # `evaluate`, and `predict`.
    300     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA
--> 301     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    302     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    303     self._predict_counter = variables.Variable(

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v1_call(*args, **kwargs)
    261     elif cls is Variable:
--> 262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    242     if aggregation is None:
    243       aggregation = VariableAggregation.NONE
--> 244     return previous_getter(
    245         initial_value=initial_value,
    246         trainable=trainable,

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
    235                         shape=None):
    236     """"""Call on Variable class. Useful to force the signature.""""""
--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    239       previous_getter = _make_getter(getter, previous_getter)

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
   2652   shape = kwargs.get(""shape"", None)
   2653 
-> 2654   return resource_variable_ops.ResourceVariable(
   2655       initial_value=initial_value,
   2656       trainable=trainable,

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    265 
    266 

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1572       self._init_from_proto(variable_def, import_scope=import_scope)
   1573     else:
-> 1574       self._init_from_args(
   1575           initial_value=initial_value,
   1576           trainable=trainable,

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1715               self._update_uid = initial_value.checkpoint_position.restore_uid
   1716               initial_value = initial_value.wrapped_value
-> 1717             initial_value = ops.convert_to_tensor(initial_value,
   1718                                                   name=""initial_value"",
   1719                                                   dtype=dtype)

~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)
    161         with Trace(trace_name, **trace_kwargs):
    162           return func(*args, **kwargs)
--> 163       return func(*args, **kwargs)
    164 
    165     return wrapped

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1538 
   1539     if ret is None:
-> 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1541 
   1542     if ret is NotImplemented:

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)
     50 def _default_conversion_function(value, dtype, name, as_ref):
     51   del as_ref  # Unused.
---> 52   return constant_op.constant(value, dtype, name=name)
     53 
     54 

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    262     ValueError: if called on a symbolic tensor.
    263   """"""
--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,
    265                         allow_broadcast=True)
    266 

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    274       with trace.Trace(""tf.constant""):
    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    277 
    278   g = ops.get_default_graph()

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):
    300   """"""Implementation of eager constant.""""""
--> 301   t = convert_to_eager_tensor(value, ctx, dtype)
    302   if shape is None:
    303     return t

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     95     except AttributeError:
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
---> 97   ctx.ensure_initialized()
     98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py in ensure_initialized(self)
    524         if self._use_tfrt is not None:
    525           pywrap_tfe.TFE_ContextOptionsSetTfrt(opts, self._use_tfrt)
--> 526         context_handle = pywrap_tfe.TFE_NewContext(opts)
    527       finally:
    528         pywrap_tfe.TFE_DeleteContextOptions(opts)

InternalError: cudaGetDevice() failed. Status: initialization error
```

Jupyter Terminal Output
```
2020-12-18 17:52:13.476509: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-18 17:52:13.476810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-18 17:52:13.478279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5
coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s
2020-12-18 17:52:13.478341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-18 17:52:13.478429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-12-18 17:52:13.478465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-12-18 17:52:13.478496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-18 17:52:13.478525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-18 17:52:13.478553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-12-18 17:52:13.478582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-12-18 17:52:13.478610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-12-18 17:52:13.478836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-18 17:52:13.480513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-18 17:52:13.481882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0

```"
45839,tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.15.4
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am working on distributed learning in tensorflow through estimators API using below simple code template:
```
def main(argv):

    # Init, set model dir| export dir | log dir.
    model_dir, export_dir = init()
    # Loading dataset
    download_dataset()

    # Select distribute strategy, such as sync, async, allReduce, etc.
    # CollectiveAllReduceStrategy for allReduce
    dist_strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.num_gpus_per_worker)
    # Set run config, including checkpoint saving strategy, maximum number of checkpoints saved, etc.
    run_config = tf.estimator.RunConfig(train_distribute=dist_strategy,
                                        eval_distribute=dist_strategy,
                                        # save_checkpoints_secs=10,
                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps,
                                        keep_checkpoint_max=FLAGS.keep_checkpoint_max)

    # Feature columns describe how to use the input.
    my_feature_columns = get_feature_columns()

    # Model
    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = Net(model_dir, my_feature_columns, run_config).net

    # TrainSpec for training
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),
        max_steps=FLAGS.train_steps,
        hooks=[])
    # EvalSpec for test
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))
    print(""---training and testing---"")
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
    print(""---training finished---"")

    # All role are workers, pick the task_index with 0 to save model
    if FLAGS.task_index == 0:
        classifier.export_saved_model(export_dir, serving_input_receiver_fn)
        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)
    print(""finish..."")
```
Firstly, I train for 1000 rounds(train_stpes=1000) and save the checkpoint, it works normally.

Then I set the train_steps to 2000, only the is_chief role can restore the model from the checkpoint without any error.

Chief output as follow:
```
INFO:tensorflow:Graph was finalized.
I1218 15:24:28.721170 139824828032832 monitored_session.py:240] Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000
I1218 15:24:28.722573 139824828032832 saver.py:1284] Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000
WARNING:tensorflow:From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
W1218 15:24:28.753480 139824828032832 deprecation.py:323] From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
I1218 15:24:28.782264 139824828032832 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1218 15:24:28.938534 139824828032832 session_manager.py:502] Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.
I1218 15:24:29.215120 139824828032832 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.
```

Worker output as follow:
```
INFO:tensorflow:Graph was finalized.
I1218 15:24:59.388847 140629246773056 monitored_session.py:240] Graph was finalized.
```

**Describe the expected behavior**

I expect that when I increase train_stepsï¼ˆfrom 1000 to 2000ï¼‰, no matter which roles should be able to restore the model from 

checkpoint and continue training. Because I think each role only saves a part of the parameters of the model on allReduce mode, 

so each role should participate in restoring the model from checkpoint.

But, I analyzed and debug the source code and found only roles with chief or worker index=0 can restore model parameters 

from checkpoint
```
/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py
line 93: is_chief
def is_chief(cluster_spec=None, task_type=None, task_id=None):
  ...
  if task_type == ""chief"" or task_type == ""evaluator"":
    return True

  if (""chief"" not in cluster_spec and task_type == ""worker"" and task_id == 0):
    return True
  return False
  ...
```
"
45838,Why model can't converge when batch size > 1?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but only demo.
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-2.5.0
- Python version: 3.7.4
- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8.0.4
- GPU model and memory: RTX3090 24GB
- RAM: 32GB

This question seems lkie not related to the environment.This is behaviour:

I wrote a code to build the Faster RCNN model. I train it by set batch size = 1. Model can converge, and predict exactly.
But when I set batch size = 2 or greater, The faster_rcnn_regr_loss(smooth L1)  can't converge.

At first I thought it was my something wrong with the program. However I checked my code, I didn't find any error. So I try to  set batch size = 8 but still backpropagation one sample losses in one training step, like this:

**Batch size = 8, backpropagation 8 sample average loss in one training step**
**Mode can not converge**
```
loss_class = classifier_train(model_classifier,
                              [image[valid_roi], x_roi],
                              [y_class_label, y_classifier])
```

**Batch size = 8, backpropagation 1 sample losses in one training step**
**Mode can converge**
```
loss_class = [0, 0]
for j in range(len(valid_roi)):
    loss = classifier_train(model_classifier,
                            [np.expand_dims(image[valid_roi[j]], axis=0),
                             np.expand_dims(x_roi[j], axis=0)],
                            [np.expand_dims(y_class_label[j], axis=0),
                             np.expand_dims(y_classifier[j], axis=0)])
    loss_class[0] += loss[0]
    loss_class[1] += loss[1]

loss_class[0] /= len(valid_roi)
loss_class[1] /= len(valid_roi)
```

**Batch size = 8, backpropagation 1 sample losses 1 times in one training step**
**Mode can converge**
```
loss_class = classifier_train(model_classifier,
                              [np.expand_dims(image[valid_roi[0]], axis=0),
                               np.expand_dims(x_roi[0], axis=0)],
                              [np.expand_dims(y_class_label[0], axis=0),
                               np.expand_dims(y_classifier[0], axis=0)])
```
I think this is a bug.

This is my faster_rcnn_regr_loss fuction:
```
def class_loss_regr(num_classes):
    epsilon = 1e-4

    def class_loss_regr_fixed_num(y_true, y_pred):
        """"""

        :param y_true:  [batch_size, num_rois, num_classes * 8]
                        [:, :, :num_classes * 4] is label index, [:, :, num_classes * 4:] is ground true boxes coordinate.
        :param y_pred: [batch_size, num_rois, num_classes * 4]
        :return: classifier regr_loss
        """"""
        regr_loss = 0
        batch_size = len(y_true)
        for i in range(batch_size):
            x = y_true[i, :, 4 * num_classes:] - y_pred[i, :, :]                  
            x_abs = backend.abs(x)                                                  
            x_bool = backend.cast(backend.less_equal(x_abs, 1.0), 'float32')       

            loss = 4 * backend.sum(
                y_true[i, :, :4 * num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / backend.sum(
                epsilon + y_true[i, :, :4 * num_classes])
            regr_loss += loss

        return regr_loss / backend.constant(batch_size)

    return class_loss_regr_fixed_num
```

This is my training code:
```
def main():
    global rpn_optimizer, classifier_optimizer
    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

    gpus = tf.config.experimental.list_physical_devices(""GPU"")
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    img_input = Input(shape=(None, None, 3))
    roi_input = Input(shape=(None, 4))

    share_layer = ResNet50(img_input)
    rpn = frcnn.rpn(share_layer, num_anchors=len(cfg.anchor_box_ratios) * len(cfg.anchor_box_scales))
    classifier = frcnn.classifier(share_layer, roi_input, cfg.num_rois, nb_classes=cfg.num_classes)

    model_rpn = models.Model(img_input, rpn)
    model_classifier = models.Model([img_input, roi_input], classifier)
    model_all = models.Model([img_input, roi_input], rpn + classifier)

    anchors = get_anchors(cfg.share_layer_shape, cfg.input_shape)

    box_parse = BoundingBox(anchors, max_threshold=cfg.rpn_max_overlap, min_threshold=cfg.rpn_min_overlap)

    reader = DataReader(cfg.annotation_path, box_parse, cfg.batch_size)
    train_data = reader.generate()
    train_step = len(reader.train_lines) // cfg.batch_size

    losses = np.zeros((train_step, 4))
    best_loss = np.Inf

    rpn_lr = CosineAnnealSchedule(cfg.epoch, train_step, cfg.rpn_lr_max, cfg.rpn_lr_min)
    cls_lr = CosineAnnealSchedule(cfg.epoch, train_step, cfg.cls_lr_max, cfg.cls_lr_min)

    rpn_optimizer = optimizers.Adam(rpn_lr)
    classifier_optimizer = optimizers.Adam(cls_lr)

    for e in range(cfg.epoch):
        invalid_data = 0        
        print(""Learning rate adjustment, rpn_lr: {}, cls_lr: {}"".
              format(rpn_optimizer._decayed_lr(""float32"").numpy(),
                     classifier_optimizer._decayed_lr(""float32"").numpy()))


        progbar = utils.Progbar(train_step)
        print('Epoch {}/{}'.format(e+1, cfg.epoch))
        for i in range(train_step):

            image, rpn_y, bbox = next(train_data)
            loss_rpn = rpn_train(model_rpn, image, rpn_y)
            predict_rpn = model_rpn(image)

            predict_boxes = box_parse.detection_out(predict_rpn, confidence_threshold=0)
            height, width = np.shape(image[0])[:2]
            x_roi, y_class_label, y_classifier, valid_roi = get_classifier_train_data(predict_boxes,
                                                                                      bbox,
                                                                                      width,
                                                                                      height,
                                                                                      cfg.batch_size,
                                                                                      cfg.num_classes)

            invalid_data += (cfg.batch_size - len(valid_roi))
            if len(x_roi) == 0:
                progbar.update(i+1, [('rpn_cls', np.mean(losses[:i+1, 0])),
                                     ('rpn_regr', np.mean(losses[:i+1, 1])),
                                     ('detector_cls', np.mean(losses[:i+1, 2])),
                                     ('detector_regr', np.mean(losses[:i+1, 3]))])
                continue
            
            #######you can test code here#########
            loss_class = classifier_train(model_classifier,
                                          [image[valid_roi], x_roi],
                                          [y_class_label, y_classifier])
            #################################

            losses[i, 0] = loss_rpn[0].numpy()
            losses[i, 1] = loss_rpn[1].numpy()
            losses[i, 2] = loss_class[0].numpy()
            losses[i, 3] = loss_class[1].numpy()

            progbar.update(i+1, [('rpn_cls', np.mean(losses[:i+1, 0])),
                                 ('rpn_regr', np.mean(losses[:i+1, 1])),
                                 ('detector_cls', np.mean(losses[:i+1, 2])),
                                 ('detector_regr', np.mean(losses[:i+1, 3]))])

        else:
            loss_rpn_cls = np.mean(losses[:, 0])
            loss_rpn_regr = np.mean(losses[:, 1])
            loss_class_cls = np.mean(losses[:, 2])
            loss_class_regr = np.mean(losses[:, 3])

            curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr

            print('\nLoss RPN classifier: {:.4f}'.format(loss_rpn_cls))
            print('Loss RPN regression: {:.4f}'.format(loss_rpn_regr))
            print('Loss Detector classifier: {:.4f}'.format(loss_class_cls))
            print('Loss Detector regression: {:.4f}'.format(loss_class_regr))
            print(""{} picture can't detect any roi."".format(invalid_data))

            print('The best loss is {:.4f}. The current loss is {:.4f}.'.format(best_loss, curr_loss))
            if curr_loss < best_loss:
                best_loss = curr_loss

            print('Saving weights.\n')
            model_all.save_weights(""./logs/model/voc_{:.4f}.h5"".format(curr_loss))
```
I try to show it colab,but I can't download the VOC dataset.And it only occured in my project.I can't write demo to show it because faster_rcnn_regr need rpn predict data.
So maybe you should download my [code](https://github.com/Runist/Faster_RCNN).And you should do this:

1. Download the code.
2. Run this code to get voc annotation 'train.txt' file:
```
iimport xml.etree.ElementTree as ET
import os

def convert_annotation(image_id, list_file, class_names):
    in_file = open('/content/VOCdevkit/VOC2012/Annotations/%s.xml' % (image_id))
    tree = ET.parse(in_file)
    root = tree.getroot()

    for obj in root.iter('object'):

        difficult = obj.find('difficult').text
        cls = obj.find('name').text

        cls_id = class_names.index(cls)
        xmlbox = obj.find('bndbox')

        b = (int(xmlbox.find('xmin').text),
            int(xmlbox.find('ymin').text),
            int(xmlbox.find('xmax').text),
            int(xmlbox.find('ymax').text))

        list_file.write("" "" + "","".join([str(a) for a in b]) + ',' + str(cls_id))


if __name__ == '__main__':
    label = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
         'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']
    
    xmlfilepath = '/content/VOCdevkit/VOC2012/Annotations'
    temp_xml = os.listdir(xmlfilepath)
    total_xml = []

    for xml in temp_xml:
        if xml.endswith("".xml""):
            total_xml.append(xml[:-4])

    files = open('train.txt', 'w')
    for image_id in total_xml:
        files.write('/content/VOCdevkit/VOC2012/JPEGImages/{}.jpg'.format(image_id))
        convert_annotation(image_id, files, label)
        files.write('\n')
    files.close()
```
3. Move the train.txt to './config/' directory.
4. Edit the './config/config.py' some training paramters.
5. Run 'python train.py'.

"
45837,Op type not registered 'StatefulPartitionedCall' in binary running,"ä½¿ç”¨spark-2.3ï¼Œç„¶åŽé€šè¿‡mavenä¸‹è½½ä¾èµ–åŒ…ï¼Œlibtensorflow-1.9.0.jar,libtensorflow_jni-1.9.0.jarï¼›
åœ¨spark-submitæäº¤ï¼Œè¯»å–æ¨¡åž‹ï¼ŒæŠ¥å¦‚ä¸‹é”™ï¼š

org.tensorflow.TensorFlowException: Op type not registered 'StatefulPartitionedCall' in binary running on [æœºå™¨å]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
"
45835,v1.15.4 NoneType' object has no attribute 'UnimplementedError,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.4
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory: N/A


**Describe the current behavior**
I am working on distributed learning in tensorflow through estimators API using below simple code template:

```
# TrainSpec for training
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),
        max_steps=FLAGS.train_steps,
        hooks=[])
    # EvalSpec for test
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))
    print(""---training and testing---"")
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
    print(""---training finished---"")

    # All role are workers, pick the task_index with 0 to save model
    if FLAGS.task_index == 0:
        classifier.export_saved_model(export_dir, serving_input_receiver_fn)
        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)
    print(""finish..."")
```

I encountered the following error during training.

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/server_lib.py"", line 158, in __del__
AttributeError: 'NoneType' object has no attribute 'UnimplementedError'
```

I analyzed the source code and found that the errors object may be garbage collected.

I will submit a PR later, please pass it. Because I will use v1.15.4(v1.15.4 version on our production environment) 

for distributed training on kubeflow, 

I donâ€™t want to modify the code there every time.

Thanks!!!
"
45834,Tensorflow 2.4 Throwing Errors ,"**System information**
- OS Platform and Distribution: Windows 10 v20H2 
- TensorFlow installed from (source or binary): Through pip command
- TensorFlow version (use command below): v2.4.0
- Python version: v3.8.5
- CUDA/cuDNN version: v11.0 (update 1)/v8.0.4
- GPU model and memory: Nvidia GTX 1650 4GB GDDR5

-tf.version.GIT_VERSION: library cudart64_110.dll
-tf.version.VERSION: v2.4.0-rc4-71-g582c8d236cb 2.4.0

**Describe the current behavior**
UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-f922f88cbe61>:19) ]] [Op:__inference_train_function_753]
Function call stack:
train_function

**Describe the expected behavior**
It should be training the network with no issues. The same code worked on tensorflow v2.3.0.

**Standalone code to reproduce the issue**
Notebook at:
https://colab.research.google.com/drive/14vdfsSkqGn375E54vd11--yXy0WKemWm?usp=sharing
It was working fine on v2.3.0 at my end but after upgrading to 2.4.0 its throwing error.  Also I tried one more CNN network with different architecture which is working fine with no errors. I have also renamed a dll file in order to make tensorflow open CUPTI library.

**Other info / logs** Logs are included.
[Expected_log.txt](https://github.com/tensorflow/tensorflow/files/5713830/Expected_log.txt)
[Current_log.txt](https://github.com/tensorflow/tensorflow/files/5713831/Current_log.txt)


"
45824,micro: port op SPACE_TO_DEPTH from lite,"@tensorflow/micro

This issue tracks my work porting operator SPACE_TO_DEPTH from lite to micro. The port will be submitted in a number of PRs. Here's a rough flight plan in the style of #45306:

* PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
* PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
* PR 3: Copy operator from lite to micro without making any changes or including in the build
* PR 4: Delete extra code from the micro copy of the operator
* PR 5: Port micro copy of operator as necessary and add a corresponding test
"
45823,Official Sample C++ Test Custom Op Build Fails With TF 2.4 Using VS2019,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Both
- TensorFlow version (use command below): 2.4
- Python version: 3.8
- GCC/Compiler version (if compiling from source): VS2019 16.8.3
- CUDA/cuDNN version: 11.0 / 8.0.5
- GPU model and memory: 2070 MaxQ

**Describe the current behavior**

When attempting to build the test custom c++ op as described on https://www.tensorflow.org/guide/create_op , fails to build with the errors listed below. This same code / VS project builds perfectly fine with TF 2.3 and before.

```
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
.Input(""to_zero: int32"")
.Output(""zeroed: int32"")
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
    c->set_output(0, c->input(0));
    return Status::OK();
});


using namespace tensorflow;

class ZeroOutOp : public OpKernel {
public:
    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(OpKernelContext* context) override {
        // Grab the input tensor
        const Tensor& input_tensor = context->input(0);
        auto input = input_tensor.flat<int32>();

        // Create an output tensor
        Tensor* output_tensor = NULL;
        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
            &output_tensor));
        auto output_flat = output_tensor->flat<int32>();

        // Set all but the first element of the output tensor to 0.
        const int N = input.size();
        for (int i = 1; i < N; i++) {
            output_flat(i) = 0;
        }

        // Preserve the first input value if possible.
        if (N > 0) output_flat(0) = input(0);
    }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

Error Message:

```
1>zero_test.cpp
1>C:\sdks\tensorflow\bazel-tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src\Tensor\Tensor.h(76,1): warning C4554: '&': check operator precedence for possible error; use parentheses to clarify precedence
1>C:\sdks\tensorflow\bazel-tensorflow\external\eigen_archive\unsupported\Eigen\CXX11\src\Tensor\TensorMap.h(33): message : see reference to class template instantiation 'Eigen::Tensor<T,1,1,int>' being compiled
1>        with
1>        [
1>            T=float
1>        ]
1>C:\sdks\tensorflow\tensorflow\core\framework\tensor_types.h(105): message : see reference to class template instantiation 'Eigen::TensorMap<Eigen::Tensor<T,1,1,int>,16,Eigen::MakePointer>' being compiled
1>        with
1>        [
1>            T=float
1>        ]
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\attr_value.pb.h(687,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\node_def.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(300,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(332,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(587,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(590,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(621,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\function.pb.h(624,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\util\tensor_format.h(502,79): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\util\tensor_format.h(524,71): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\util\tensor_format.h(558,77): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\node_def_util.h(147,20): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\framework\step_stats.pb.h(1180,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\cluster.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\rewriter_config.pb.h(543,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\config.pb.h(1949,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\config.pb.h(3784,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\config.pb.h(3787,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\config.pb.h(3818,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\bazel-bin\tensorflow\core\protobuf\config.pb.h(3821,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(158,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(165,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(306,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(314,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(722,56): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\op_kernel.h(727,49): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\shape_inference.h(265,26): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\shape_inference.h(320,27): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>C:\sdks\tensorflow\tensorflow\core\framework\shape_inference.h(785,39): warning C4267: 'initializing': conversion from 'size_t' to 'const tensorflow::int32', possible loss of data
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_OP_IMPL'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): warning C4002: too many arguments for function-like macro invocation 'SHOULD_REGISTER_OP'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2059: syntax error: '||'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2039: 'value': is not a member of '`global namespace''
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2146: syntax error: missing ')' before identifier 'value'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2065: 'value': undeclared identifier
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2059: syntax error: ')'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): error C2440: '<function-style-cast>': cannot convert from 'initializer list' to 'tensorflow::register_op::OpDefBuilderWrapper'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(7,1): message : No constructor could take the source type, or constructor overload resolution was ambiguous
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(34,33): warning C4244: 'initializing': conversion from '__int64' to 'int', possible loss of data
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(34,21): warning C4244: 'initializing': conversion from '__int64' to 'const int', possible loss of data
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_KERNEL_BUILDER_IMPL_2'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_KERNEL_BUILDER_IMPL_3'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'SHOULD_REGISTER_OP_KERNEL'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): warning C4002: too many arguments for function-like macro invocation 'SHOULD_REGISTER_OP'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2059: syntax error: '||'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2039: 'value': is not a member of '`global namespace''
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2146: syntax error: missing ')' before identifier 'value'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2065: 'value': undeclared identifier
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2059: syntax error: ')'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2059: syntax error: ','
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2062: type 'void' unexpected
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2059: syntax error: 'return'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2059: syntax error: '}'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2143: syntax error: missing ')' before '.'
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): error C2064: term does not evaluate to a function taking 0 arguments
1>C:\Users\Adam\Documents\Visual Studio 2019\Projects\OSD_Custom_Op\ZeroTest\zero_test.cpp(44,1): message : class does not define an 'operator()' or a user defined conversion operator to a pointer-to-function or reference-to-function that takes appropriate number of arguments
```
"
45821,I think I only have cudart64_110.dll,"I think I only have cudart64_110.dll
(The one that I highlighted)
![image](https://user-images.githubusercontent.com/68514251/93034064-d0b54a00-f606-11ea-991e-429bb7713d58.png)

_Originally posted by @CalendulaED in https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-691760016_

please pass me this file cudart64_110.dll its misssing here, i dont know what else i can do
"
45820,MHLO/LHLO Fusion lower,"It seems mhlo-fusion op is unable to legalize to lhloï¼Œ and lhlo-fusion op is also unable to lower to linalg  dialect. Is there any plan to support the fusion op lower? Linalg fusion on tensor is not perfect for multi-output elemwise ops. Following is a  example
```
// CHECK-LABEL: func @multi_outputs_same_2
func @multi_outputs_same_2(%arg0: tensor<?x?xf32>, %arg1: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>) {
  %0 = ""mhlo.abs""(%arg0) : (tensor<?x?xf32>) -> tensor<?x?xf32>
  %1 = ""mhlo.abs""(%arg1) : (tensor<?x?xf32>) -> tensor<?x?xf32>
  %2 = ""mhlo.add""(%0, %1) : (tensor<?x?xf32>, tensor<?x?xf32>) -> tensor<?x?xf32>
  %3 = ""mhlo.abs""(%0) : (tensor<?x?xf32>) -> tensor<?x?xf32>
  %4 = ""mhlo.abs""(%1) : (tensor<?x?xf32>) -> tensor<?x?xf32>
  // CHECK: %[[RET:.*]]:3 = ""mhlo.fusion""
  // CHECK-NEXT: mhlo.abs
  // CHECK-NEXT: mhlo.abs
  // CHECK-NEXT: mhlo.add
  // CHECK-NEXT: mhlo.abs
  // CHECK-NEXT: mhlo.abs
  // CHECK-NEXT: mhlo.return
  return %2, %3, %4 : tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>
}
```
with command`mlir-hlo-opt ./lhlo-test.mlir -hlo-legalize-to-linalg -linalg-bufferize -convert-linalg-to-affine-loops`ï¼Œget the following result
```
#map0 = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<() -> (0)>
#map2 = affine_map<()[s0] -> (s0)>
module {
  func @multi_outputs_same_2(%arg0: tensor<?x?xf32>, %arg1: tensor<?x?xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>) {
    %c0 = constant 0 : index
    %c1 = constant 1 : index
    %0 = tensor_to_memref %arg0 : memref<?x?xf32>
    %1 = dim %arg0, %c0 : tensor<?x?xf32>
    %2 = dim %arg0, %c1 : tensor<?x?xf32>
    %3 = alloc(%1, %2) : memref<?x?xf32>
    %4 = dim %0, %c0 : memref<?x?xf32>
    %5 = dim %0, %c1 : memref<?x?xf32>
    affine.for %arg2 = 0 to %4 {
      affine.for %arg3 = 0 to %5 {
        %18 = affine.load %0[%arg2, %arg3] : memref<?x?xf32>
        %19 = absf %18 : f32
        affine.store %19, %3[%arg2, %arg3] : memref<?x?xf32>
      }
    }
    %6 = tensor_to_memref %arg1 : memref<?x?xf32>
    %7 = dim %arg1, %c0 : tensor<?x?xf32>
    %8 = dim %arg1, %c1 : tensor<?x?xf32>
    %9 = alloc(%7, %8) : memref<?x?xf32>
    %10 = dim %6, %c0 : memref<?x?xf32>
    %11 = dim %6, %c1 : memref<?x?xf32>
    affine.for %arg2 = 0 to %10 {
      affine.for %arg3 = 0 to %11 {
        %18 = affine.load %6[%arg2, %arg3] : memref<?x?xf32>
        %19 = absf %18 : f32
        affine.store %19, %9[%arg2, %arg3] : memref<?x?xf32>
      }
    }
    %12 = alloc(%1, %2) : memref<?x?xf32>
    affine.for %arg2 = 0 to %1 {
      affine.for %arg3 = 0 to %2 {
        %18 = affine.load %3[%arg2, %arg3] : memref<?x?xf32>
        %19 = affine.load %9[%arg2, %arg3] : memref<?x?xf32>
        %20 = addf %18, %19 : f32
        affine.store %20, %12[%arg2, %arg3] : memref<?x?xf32>
      }
    }
    %13 = tensor_load %12 : memref<?x?xf32>
    %14 = alloc(%1, %2) : memref<?x?xf32>
    affine.for %arg2 = 0 to %1 {
      affine.for %arg3 = 0 to %2 {
        %18 = affine.load %3[%arg2, %arg3] : memref<?x?xf32>
        %19 = absf %18 : f32
        affine.store %19, %14[%arg2, %arg3] : memref<?x?xf32>
      }
    }
    %15 = tensor_load %14 : memref<?x?xf32>
    %16 = alloc(%7, %8) : memref<?x?xf32>
    affine.for %arg2 = 0 to %7 {
      affine.for %arg3 = 0 to %8 {
        %18 = affine.load %9[%arg2, %arg3] : memref<?x?xf32>
        %19 = absf %18 : f32
        affine.store %19, %16[%arg2, %arg3] : memref<?x?xf32>
      }
    }
    %17 = tensor_load %16 : memref<?x?xf32>
    return %13, %15, %17 : tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>
  }
}

```

I expect one affine nest,  but it get three affine nests. Will mhlo-fusion lowering solve this problem, or is there any other way to fuse the three for loops into one?

"
45813,Shape of model output can differ from target data shape,"**System information**

Python 3.6, Tensorflow 2.3, Windows 10/ Ubuntu 16.04

**Describe the current behavior**

It is possible to train a model with an output shape other than the shape of the target data.
I donÂ´t know what itÂ´s doing there (and hope I made no mistake when thinking this trough), but shouldnÂ´t this cause and error ?
See Section of how to reproduce.

**Describe the expected behavior**

Throw an Error if model outputshape  != target data shape.

**Standalone code to reproduce the issue**


Can be reproduced using Colab of

https://www.tensorflow.org/tutorials/keras/regression

as a basis (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb,

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb?short_path=38f0bfa)

and change Line 574
` ""    layers.Dense(units=1)\n"",`
to
` ""    layers.Dense(units=3)\n"",`


The output-shape of the Neural Network is now (batchsize, 3) however it successfully fits target data of of shape (batchsize, 1).


"
45801,Said in the docs to add the Softmax Activation function but never did in code.,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/images/cnn#add_dense_layers_on_top

## Description of issue (what needs changing):
In the last sentence ""CIFAR has 10 output classes, so you use a final Dense layer **with 10 outputs and a softmax activation.""** 
""and a softmax activation"" **but I guess some one forgot to add the softmax activation at the end.**
**OLD CODE:**
```
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))
```

**NEW CODE:**
```
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))
model.add(layers.Activation('softmax'))
```

"
45797,saved_model does not support RaggedTensors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

inp = layers.Input(shape=(None, 4), ragged=True)
x = layers.Lambda(lambda x: x)(inp)
model = keras.Model(inp, x)
model.compile(loss=""mse"")

model.save(""test"", save_traces=True)
```

https://colab.research.google.com/drive/1hGrYVPrTy6vDbZPAHAPn2XGmpQMXQmQX?usp=sharing

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Colab

- TensorFlow version (use command below):
tf-nightly (~=2.5.0.a)

**Describe the current behavior**
```keras.Model.save()``` raises an exception when a layer returns a RaggedTensor.

The exception message states that the object returned from tracing a function must be a tf.Tensor. RaggedTensors are not instances of tf.Tensor.

**Describe the expected behavior**
It should be possible to save and load models that take RaggedTensors as inputs and return RaggedTensors as outputs.

**Workaround**
The model can be saved when the option ```save_traces=False``` is used.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1hGrYVPrTy6vDbZPAHAPn2XGmpQMXQmQX?usp=sharing

Issue #41034 Reports a similar issue with loading a model that uses a RaggedTensor as input. This issue is for the specific problem of a RaggedTensor not being accepted as the return value of a keras node / tf.function as far as the saved_model package is concerned.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py:158 signature_wrapper  *
        structured_outputs, signature_function.name, signature_key)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py:219 _normalize_outputs  **
        .format(value, key, compat.as_str_any(function_name), signature_key))

    ValueError: Got a non-Tensor value tf.RaggedTensor(values=Tensor(""PartitionedCall:0"", shape=(None, 4), dtype=float32), row_splits=Tensor(""PartitionedCall:1"", shape=(None,), dtype=int64)) for key 'lambda' in the output of the function __inference__wrapped_model_173 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor.
```

As far as I can tell this relates to the following function in python/saved_model/signature_serializatioon.py

 ```python
def _normalize_outputs(outputs, function_name, signature_key):
  """"""Construct an output dictionary from unnormalized function outputs.""""""
  # Convert `outputs` to a dictionary (if it's not one already).
  if not isinstance(outputs, collections_abc.Mapping):
    if not isinstance(outputs, collections_abc.Sequence):
      outputs = [outputs]
    outputs = {(""output_{}"".format(output_index)): output
               for output_index, output
               in enumerate(outputs)}

  # Check that the keys of `outputs` are strings and the values are Tensors.
  for key, value in outputs.items():
    if not isinstance(key, compat.bytes_or_text_types):
      raise ValueError(
          (""Got a dictionary with a non-string key {!r} in the output of the ""
           ""function {} used to generate the SavedModel signature {!r}."")
          .format(key, compat.as_str_any(function_name), signature_key))
    if not isinstance(value, ops.Tensor):
      raise ValueError(
          (""Got a non-Tensor value {!r} for key {!r} in the output of the ""
           ""function {} used to generate the SavedModel signature {!r}. ""
           ""Outputs for functions used as signatures must be a single Tensor, ""
           ""a sequence of Tensors, or a dictionary from string to Tensor."")
          .format(value, key, compat.as_str_any(function_name), signature_key))

  return outputs
```

The test that requires the return value to be an instance of tf.Tensor is too restrictive."
45796,Keras compile(loss=) missing gradient for captured tensors,"**System information**
- OS Platform and Distribution : Ubuntu 18.04.5 LTS (Dockerized)
- TensorFlow installed from : Docker tensorflow/tensorflow:latest-gpu-jupyter
- TensorFlow version : 2.3.1
- Python version: 3.6.9
- GPU model and memory: No GPU

**Describe the current behavior**

A gradient is not being recorded for tensors captured by a loss function passed to .compile() 

This results in a ValueError: No gradients provided for any variable ...

**Describe the expected behavior**

.compile(loss=) should correctly integrate the gradients for all tensors used in the loss computation.

**Standalone code to reproduce the issue**

https://gist.github.com/TheBeaNerd/78844dacf2d236eb50188062036a1082
"
45795,Keras add_loss()/kernel_regularizer incompatability,"**System information**
- OS Platform and Distribution : Ubuntu 18.04.5 LTS (Dockerized)
- TensorFlow installed from : Docker tensorflow/tensorflow:latest-gpu-jupyter
- TensorFlow version : 2.3.1
- Python version: 3.6.9
- GPU model and memory: No GPU

**Describe the current behavior**

Given a Model with a Dense layer that includes a kernel_regularizer, using add_loss() and then compiling results in the following error during .fit():  ValueError: Shapes must be equal rank, but are 0 and 1.

Note: this issue does not manifest when the loss is passed directly to .compile().

**Describe the expected behavior**

The Keras framework should seamlessly combine various loss functions.

**Standalone code to reproduce the issue**

https://gist.github.com/TheBeaNerd/e123c0e4a6245c9069bf645da5501fd7"
45794,[C++] tensorflow::ops::Unstack Documentation and Implementation Mismatch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc-10
- CUDA/cuDNN version: 11.2 (installed but not used)
- GPU model and memory: MX150

**Describe the current behavior**
the input tensor has shape [1,150,150,3]. According to the documentation in the array_ops.h, unstack the input along 0th axis should produce [150,150,3] tensor. However, putting 0 as input cause segmentation fault.
`Segmentation fault (core dumped)`
If unstack along 1st axis, the output tensor has shape of [150,150,3]

**Describe the expected behavior**
unstack along 0th axis of a tensor with shape [a,b,c,d] should output a tensor with shape [b,c,d].

**Standalone code to reproduce the issue**
```
tensorflow::ops::Unstack unstack_image_node{load_image_scope_.NewSubScope(""unstack_image""), normalize_image_node, 0};
output_image_tensor_node_ = unstack_image_node[0]; // and then std::cout<<out_tensor.DebugString(); to examine the shape
```

**Other info / logs**
Start from line 6958 to line 6982 in arrays_ops.h describes the expected behavior of class Unstack, which is basically as same as the documentation of its [python counterpart](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/stack)
```
/// Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.
///
/// Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.
/// For example, given a tensor of shape `(A, B, C, D)`;
///
/// If `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`
///   and each tensor in `output` will have shape `(B, C, D)`. (Note that the
///   dimension unpacked along is gone, unlike `split`).
///
/// If `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`
///   and each tensor in `output` will have shape `(A, C, D)`.
/// Etc.
///
/// This is the opposite of `pack`.
///
/// Arguments:
/// * scope: A Scope object
/// * value: 1-D or higher, with `axis` dimension size equal to `num`.
///
/// Optional attributes (see `Attrs`):
/// * axis: Dimension along which to unpack.  Negative values wrap around, so the
/// valid range is `[-R, R)`.
///
/// Returns:
/// * `OutputList`: The list of tensors unpacked from `value`.
class Unstack {
```
however, using the following code with axis = 1
```
tensorflow::ops::Unstack unstack_image_node{
  load_image_scope_.NewSubScope(""unstack_image""), normalize_image_node, 1};
output_image_tensor_node_ = unstack_image_node[0];
```
produce the expected behavior when axis = 0, which is
```
2020-12-17 11:45:05.233760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-17 11:45:05.252351: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz
Tensor<type: float shape: [1,150,150,3] values: [[[0.0705882385 0.0941176564 0.298039228]]]...>
Tensor<type: float shape: [150,150,3] values: [[0.0705882385 0.0941176564 0.298039228]]...>
```
If axis = 0
```
tensorflow::ops::Unstack unstack_image_node{
  load_image_scope_.NewSubScope(""unstack_image""), normalize_image_node, 0};
output_image_tensor_node_ = unstack_image_node[0];
```
segmentation fault occurs.
```
2020-12-17 11:38:37.604552: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-17 11:38:37.624902: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz
Tensor<type: float shape: [1,150,150,3] values: [[[0.0705882385 0.0941176564 0.298039228]]]...>
Segmentation fault (core dumped)
```"
45793,[C++] Cannot Construct tensorflow::Status with std::string,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc-10
- CUDA/cuDNN version: 11.2 (installed but NOT used)
- GPU model and memory: MX150

**Describe the current behavior**
All the below cannot successfully construct a tensorflow::Status instance
1. ```
    tensorflow::StringPiece err_msg = ""NOT jpg"";
    tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT, err_msg};
    ```
2. `tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT, ""string""};`

3. `tensorflow::errors::InvalidArgument(""Image must be jpeg encoded"");`
Only this work
`tensorflow::Status::OK();`

All of them causes the following error
```
[build] /usr/bin/ld: src/tutorial_02/CatDogCnn/libCatDogCnn.a(CatDogCnn.cpp.o): in function `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >)':
[build] /usr/local/include/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/platform/status.h:54: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'
```

**Describe the expected behavior**
Should compile without error

**Standalone code to reproduce the issue**
class definition 
```
#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/framework/gradients.h>
#include <tensorflow/cc/ops/image_ops.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/cc/tools/freeze_saved_model.h>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/lib/io/path.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/summary/summary_file_writer.h>

#include <filesystem>
#include <fstream>
#include <iostream>
#include <string>
#include <vector>
class CatDogCnn {
 public:
  /**
   * @brief Construct a new Cat Dog Cnn object. Assume images are square.
   *
   * @param image_size the size of image
   * @param num_of_channels number of channels
   */
  CatDogCnn(const int image_size, const int num_of_channels);
  /**
   * @brief Create a Graph for Input Image. If unstack is false, the
   * output_image_tensor_node_ will have shape of [batch, width, channel]. If
   * unstack is true, the ouput_image_tensor_node_ will have shape of [batch,
   * height, width, channel]. In addition, only the first element will be stored
   * to output_image_tensor_node_
   *
   * @param unstack ouput tensor or not
   * @return tensorflow::Status
   */
  tensorflow::Status CreateGraphForInputImage(bool unstack);

  /**
   * @brief Convert an image to a tensor
   *
   * @param path_to_image path to the jpeg file to be converted
   * @param out_tensor pointer to output tensor
   * @return tensorflow::Status
   */
  tensorflow::Status ConvertImageToTensor(
      const std::filesystem::path& path_to_image,
      tensorflow::Tensor* out_tensor);

 private:
  const int kImageSize;      // assume squre picture
  const int kNumOfChannels;  // RGB
  // load image
  tensorflow::Scope load_image_scope_;
  tensorflow::Output input_image_filename_node_;
  tensorflow::Output output_image_tensor_node_;
};
```

methods implementation
```
CatDogCnn::CatDogCnn(const int image_size, const int num_of_channels)
    : kImageSize{image_size},
      kNumOfChannels{num_of_channels},
      load_image_scope_{tensorflow::Scope::NewRootScope()} { /*empty*/
}

tensorflow::Status CatDogCnn::CreateGraphForInputImage(bool unstack) {
  input_image_filename_node_ = tensorflow::ops::Placeholder{
      load_image_scope_.NewSubScope(""input_file_name""),
      tensorflow::DataType::DT_STRING};
  tensorflow::ops::ReadFile read_file_node{
      load_image_scope_.NewSubScope(""read_file""), input_image_filename_node_};
  tensorflow::ops::DecodeJpeg decode_image_node{
      load_image_scope_.NewSubScope(""decode_image""), read_file_node};
  // convert each pixel to float
  tensorflow::ops::Cast cast_float_node{
      load_image_scope_.NewSubScope(""cast_float""), decode_image_node,
      tensorflow::DataType::DT_FLOAT};
  // [height, width channel] -> [batch, height, width, channel]
  tensorflow::ops::ExpandDims expand_batch_dim_node{
      load_image_scope_.NewSubScope(""exapnd_batch_dim""), cast_float_node, 0};
  // resize image to square
  tensorflow::ops::ResizeBilinear resize_image_node(
      load_image_scope_.NewSubScope(""resize""), expand_batch_dim_node,
      {kImageSize, kImageSize});
  // divide each pixel by 255 so that each pixel is [0, 1]
  tensorflow::ops::Div normalize_image_node{
      load_image_scope_.NewSubScope(""normalize_image""),
      resize_image_node,
      {255.f}};
  if (unstack) {
    // unstack along height axis
    // array of [batch, width, channel]
    tensorflow::ops::Unstack unstack_image_node{
        load_image_scope_.NewSubScope(""unstack_image""), normalize_image_node,
        1};
    output_image_tensor_node_ = unstack_image_node[0];
  } else {
    output_image_tensor_node_ = normalize_image_node;
  }
  return load_image_scope_.status();
}

tensorflow::Status CatDogCnn::ConvertImageToTensor(
    const std::filesystem::path& path_to_image,
    tensorflow::Tensor* out_tensor) {
  if (load_image_scope_.ok() == false) {
    return load_image_scope_.status();
  }
  if (path_to_image.extension().string() != "".jpg"" ||
      path_to_image.extension().string() != "".jpeg"") {
    tensorflow::StringPiece err_msg = ""NOT jpg"";
    return tensorflow::errors::InvalidArgument(""Image must be jpeg encoded"");
    // return tensorflow::Status{tensorflow::errors::Code::INVALID_ARGUMENT,
    //                           err_msg,};
    // return tensorflow::Status::OK();
  }
  std::vector<tensorflow::Tensor> out_tensors;
  tensorflow::ClientSession client_session{load_image_scope_};
  TF_CHECK_OK(
      client_session.Run({{input_image_filename_node_, path_to_image.string()}},
                         {output_image_tensor_node_}, &out_tensors));
  (*out_tensor) = std::move(out_tensors[0]);
  return load_image_scope_.status();
}
```
entry point
```
#include <iostream>
#include <tensorflow_tutorial/tutorial_02/CatDogCnn/CatDogCnn.hpp>
int main(int argc, char** argv) {
  if (argc != 2) {
    std::cerr << ""./tutorial_02 /path/to/image"" << std::endl;
    exit(EXIT_FAILURE);
  }
  CatDogCnn cat_dog_model{150, 3};
  tensorflow::Tensor image_tensor;
  cat_dog_model.CreateGraphForInputImage(false);
  cat_dog_model.ConvertImageToTensor(std::filesystem::path{argv[1]},
                                     &image_tensor);
  std::cout << image_tensor.DebugString() << std::endl;
  cat_dog_model.CreateGraphForInputImage(true);
  cat_dog_model.ConvertImageToTensor(std::filesystem::path{argv[1]},
                                     &image_tensor);
  std::cout << image_tensor.DebugString() << std::endl;
  return 0;
}
```
CMakeLists.txt for CatDogCnn
```
add_library(CatDogCnn STATIC CatDogCnn.cpp)

target_link_libraries(CatDogCnn PUBLIC TensorflowCC::TensorflowCC)

if(CUDA_FOUND)
  target_link_libraries(CatDogCnn PUBLIC ${CUDA_LIBRARIES})
endif(CUDA_FOUND)
```
CMakeLists.txt for entry point
```
add_subdirectory(CatDogCnn)

add_executable(tutorial_02 tutorial_02.cpp)

target_link_libraries(tutorial_02 PRIVATE CatDogCnn)

target_link_libraries(tutorial_02 PRIVATE TensorflowCC::TensorflowCC)

# if(CUDA_FOUND)
#   target_link_libraries(tutorial_02 ${CUDA_LIBRARIES})
# endif(CUDA_FOUND)

set_target_properties(tutorial_02 PROPERTIES
  RUNTIME_OUTPUT_DIRECTORY ""${CMAKE_SOURCE_DIR}/bin/tutorial_02""
)
```
library TensorflowCC::TensorflowCC is generated by [FloopCZ/tensorflow_cc](https://github.com/FloopCZ/tensorflow_cc/tree/tf-v2.4.0), which compiles and installs Tensorflow with CMake support.
I don't think TensorflowCC is the smoking gun here since tensorflow::Status{} is working. I think there is a missing pieces that needs to be added to create Status instance with std::string

**Other info / logs** Include any logs or source code that would be helpful to
[main] Building folder: tensorflow_cpp_tutorial 
[build] Starting build
[proc] Executing command: /usr/bin/cmake --build /home/fred/Documents/research/tensorflow_cpp_tutorial/build --config Debug --target all -- -j 10
[build] [1/3  33% :: 11.613] Building CXX object src/tutorial_02/CatDogCnn/CMakeFiles/CatDogCnn.dir/CatDogCnn.cpp.o
[build] [2/3  66% :: 11.648] Linking CXX static library src/tutorial_02/CatDogCnn/libCatDogCnn.a
[build] [3/3 100% :: 14.248] Linking CXX executable ../bin/tutorial_02/tutorial_02
[build] FAILED: ../bin/tutorial_02/tutorial_02 
[build] : && /bin/g++-10  -g   src/tutorial_02/CMakeFiles/tutorial_02.dir/tutorial_02.cpp.o  -o ../bin/tutorial_02/tutorial_02  -Wl,-rpath,/usr/local/lib  src/tutorial_02/CatDogCnn/libCatDogCnn.a  /usr/local/lib/libtensorflow_cc.so.2  -lpthread  /usr/local/cuda/lib64/libcudart_static.a  -lpthread  -ldl  /usr/lib/x86_64-linux-gnu/librt.so && :
[build] /usr/bin/ld: src/tutorial_02/CatDogCnn/libCatDogCnn.a(CatDogCnn.cpp.o): in function `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >)':
[build] /usr/local/include/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/platform/status.h:54: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, std::basic_string_view<char, std::char_traits<char> >, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'
[build] collect2: error: ld returned 1 exit status
[build] ninja: build stopped: subcommand failed.
[build] Build finished with exit code 1"
45792,The problem with the application performing speech command recognition on the mobile phone,"Good afternoon. I ran your example on android (https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android) and it works great for me. And I decided to change the buffer type for AudioRecord to Byte, but the application stopped recognizing. If I change the type to short, it works fine. Please tell me what I need to change to make everything work.
```
class MainActivity : AppCompatActivity() {

    companion object {

        private const val TAG = ""MainActivity""
        private const val SAMPLE_RATE = 16_000
        private const val BUFFER_SIZE_SECONDS = 0.3F
        private const val DETECTION_THRESHOLD = 0.50F
        private const val SUPPRESSION_MS = 1500
        private const val MINIMUM_COUNT = 3
        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L
        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L
        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200
    }

    private val labels = listOf(""_silence_"", ""_unknown_"", ""yes"", ""no"", ""up"", ""down"", ""left"", ""right"", ""on"", ""off"", ""stop"", ""go"")
    private val tfLiteOptions = Interpreter.Options()
    private val recordingBufferLock = ReentrantLock()

    private var recordingOffset = 0
    private var shouldContinue = true
    private var recordingThread: Thread? = null
    private var shouldContinueRecognition = true
    private var recognitionThread: Thread? = null
    private var recordingBuffer = ByteArray(SAMPLE_RATE)

    private lateinit var tfLite: Interpreter
    private lateinit var tfLiteModel: MappedByteBuffer
    private lateinit var recognizeCommands: RecognizeCommands

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        recognizeCommands = RecognizeCommands(
            labels,
            AVERAGE_WINDOW_DURATION_MS,
            DETECTION_THRESHOLD,
            SUPPRESSION_MS,
            MINIMUM_COUNT,
            MINIMUM_TIME_BETWEEN_SAMPLES_MS
        )

        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {
            initTfLite()
        } else {
            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)
        }
    }

    private fun initTfLite() {
        try {
            tfLiteModel = loadModelFile()
            tfLite = Interpreter(tfLiteModel, tfLiteOptions)

            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))
            tfLite.resizeInput(1, intArrayOf(1))

            startRecording()
            startRecognition()
        } catch (exc: IOException) {
            Log.e(TAG, ""Error: ${exc.message}"")
        }
    }

    private fun startRecording() {
        if (recordingThread == null) {
            shouldContinue = true
            recordingThread = Thread { record() }
            recordingThread?.start()
        }
    }

    private fun record() {
        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)

        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2
        val record = AudioRecord(
            MediaRecorder.AudioSource.DEFAULT,
            SAMPLE_RATE,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            bufferSize
        )

        if (record.state != AudioRecord.STATE_INITIALIZED) {
            Log.e(TAG,""Audio Record can't initialize!"")
            return
        }

        record.startRecording()

        while (shouldContinue) {
            val audioBuffer = ByteArray(bufferSize)
            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)
            val newRecordingOffset = recordingOffset + numberRead
            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)
            val firstCopyLength = numberRead - secondCopyLength

            recordingBufferLock.lock()
            try {
                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)
                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)
                recordingOffset = newRecordingOffset % recordingBuffer.size
            } finally {
                recordingBufferLock.unlock()
            }
        }

        record.stop()
        record.release()
    }

    private fun startRecognition() {
        if (recognitionThread == null) {
            shouldContinueRecognition = true
            recognitionThread = Thread { recognize() }
            recognitionThread?.start()
        }
    }

    private fun recognize() {
        val inputBuffer = ByteArray(SAMPLE_RATE)
        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }
        val outputScores = Array(1) { FloatArray(labels.size) }
        val sampleRateList = intArrayOf(SAMPLE_RATE)

        while (shouldContinueRecognition) {
            recordingBufferLock.lock()

            try {
                val maxLength = recordingBuffer.size
                val firstCopyLength = maxLength - recordingOffset
                val secondCopyLength = recordingOffset
                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)
                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)
            } finally {
                recordingBufferLock.unlock()
            }

            for (i in 0 until SAMPLE_RATE) {
                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()
            }

            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)
            val outputMap: MutableMap<Int, Any> = HashMap()
            outputMap[0] = outputScores

            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)

            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())

            if (!result.foundCommand.startsWith(""_"") && result.isNewCommand) {
                Log.d(TAG, ""Command: ${result.foundCommand} (${result.score})"")
            }

            try {
                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)
            } catch (exc: InterruptedException) {
                Log.d(TAG, ""Error: ${exc.message}"")
            }
        }
    }

    @Throws(IOException::class)
    private fun loadModelFile(): MappedByteBuffer {
        val fileDescriptor = assets.openFd(""conv_actions_frozen.tflite"")
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    override fun onRequestPermissionsResult(
        requestCode: Int,
        permissions: Array<out String>,
        grantResults: IntArray
    ) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults)
        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {
            initTfLite()
        } else {
            Toast.makeText(this, ""ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ"", Toast.LENGTH_LONG).show()
            finish()
        }
    }
}
```"
45791,Official Example For TensorFlow Lite Model Maker doesn't work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Mac Book Pro 2017(2.3 GHz Dual-Core Intel Core i5, 8 GB 2133 MHz LPDDR3), MacOS Big Sur 11.0.1,
Browser: Version 87.0.4280.88 (Official Build) (x86_64),
but I guess the above info does not matter as I run the example in Google Colab [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
**OnePlus 6 OxygenOS 10.3.7, Android 10**
- TensorFlow installed from (source or binary):
**binary**
- TensorFlow version (use command below):
**In Colab 2.5.0-dev20201217, in Android 0.0.0-nightly**
- Python version:
**Python 3.6.9**
- Bazel version (if compiling from source):
**N/A**
- GCC/Compiler version (if compiling from source):
**N/A**
- CUDA/cuDNN version:
**nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243**
- GPU model and memory:
I cannot check this as I run the Colab provided in official Tensorflow page on the provided Google Colab Environment, which you can see in the following url: [https://www.tensorflow.org/lite/tutorials/model_maker_question_answer](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)


**Describe the current behavior**
In order to have the full context of the issue, please see [the following issue](https://github.com/tensorflow/tensorflow/issues/45541)


The specific problem described in #45541  is resolved on nightly build, but overall the official example is not working:

here are the issues:

1. When I do `!pip install tflite-model-maker-nightly` I see the following error in the log:
```
ERROR: tensorflow 2.3.0 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.1.0 which is incompatible.
ERROR: tf-nightly 2.5.0.dev20201215 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.
```
Though this does not stop the execution and I can continue.

2. When i try to use the created model with TensorFlow Lite Android v 2.3.0 I get the following error when I try to create interpreter with this model:
https://gist.github.com/ando0689/1874b6881dc7af3a31519e1bb2813386

3. And when I increase the version of TensorFlow Lite on Android to `0.0.0-nightly` I get the following error when I try to do inference:
https://gist.github.com/ando0689/b25f00443249da833af08cb8a6235a95

4. Just a note: The provided dataset with 8000 entities seems to be too much for this example, as running on the free Colab, it takes about 6 hours, and usually fails with some error.. I was only able to train it by reducing the dataset to be 1000 entities, I guess it's enough for example.

**Describe the expected behavior**
The example project provided in Official website of TensorFlow should run without any problem on the officially provided Colab Environment and also the produced model should work on client side as well.

**Standalone code to reproduce the issue**
1. Please try to make TFLite model using Official Colab [here](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)
2. Following the resolution of #45541  issue described [here](https://github.com/tensorflow/tensorflow/issues/45541) install tflit_model_maker from nightly channel  using `!pip install tflite-model-maker-nightly`
3. On client side use the official TensorFlow Lite Android example app for BERT QA, find it [here](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android)

**Other info / logs** 
Provided in issue description above
"
45789,Invalid index list in batch_scatter_ops_test.py ,"**Describe the current behavior**

For current master the tests in batch_scatter_ops_test.py produce an invalid index list/tensor.

Check: https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/python/kernel_tests/batch_scatter_ops_test.py#L59-L62

The comment says, non-duplicate values are required but `randint` is used which does produce duplicates. Hence the test fails.

**Standalone code to reproduce the issue**
A reduced test code which reproduces this on my machine:

```
import numpy as np
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import variables
from tensorflow.python.framework import ops

def _NumpyUpdate(ref, indices, updates):
  for i, indx in np.ndenumerate(indices):
    indx = i[:-1] + (indx,)
    ref[indx] = updates[i]

def _VariableRankTest(vtype, itype):
  np.random.seed(8)
  indices_shape = (2,)
  for extra_shape in (), (5,):
    # Generate random indices with no duplicates for easy numpy comparison
    sparse_dim = len(indices_shape) - 1
    indices = np.random.randint(indices_shape[sparse_dim], size=indices_shape, dtype=itype)
    updates = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)

    old = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)
    print(""indices: %s"" % indices)
    if not extra_shape:
      continue

    # Scatter via numpy
    new = old.copy()
    _NumpyUpdate(new, indices, updates)
    # Scatter via tensorflow
    ref = variables.Variable(old)
    variables.variables_initializer([ref])

    #state_ops.batch_scatter_update(ref, indices, updates)
    ref.batch_scatter_update(ops.IndexedSlices(indices=indices, values=updates))
    ref = ref.numpy()
    assert np.allclose(ref, new, rtol=1e-6, atol=1e-6), ""Failed:\nlhs: %s\nrhs: %s"" % (ref, new)

_VariableRankTest(np.float32, np.int32)
```

I see an output of `indices: [1 1]` followed by:
```
AssertionError: Failed:
lhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]
 [-2.2964916   2.4098344   1.7278361   2.2045562   0.79482764]]
rhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]
 [ 0.9764211  -1.1834271   1.9163636  -1.1233268  -0.6640355 ]]
```

**Other info / logs**
tensorflow/python/kernel_tests/scatter_ops_test.py contains a valid implementation using `arange` and `shuffle` which would fix the issue

Side note: the code raises a warning:
> tensorflow/python/ops/resource_variable_ops.py:1124: batch_scatter_update (from tensorflow.python.ops.state_ops) is deprecated and will be removed after 2018-11-29.
Instructions for updating:
Use the batch_scatter_update method of Variable instead.

Note how the date is well passed but there is not even a way to avoid this as I'm already doing what is suggested."
45788,Allow inverse-normalize data using tf.normalize() function,"**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): No

I am working a Regression(seq-2-seq) problem using a custom model, and while building my Data Pipeline I used `tf.keras.utilities.normalize` which automatically normalizes a numpy array in a feauture range not specified by the user, it works very well in normalizing the data.

However, after training a model on the normalized data (Where the `labels` were normalized using this utility), I am getting predictions as ""normalized"" labels. Apparently, when I run the `.evaluate()` function to test out my model, I am getting an `RMSE: 0.00089` which indicates that the model is performing well and is **de-normalizing the labels during evaluation**. However when I predict on 1 sample, the predictions are not normalized.

So, in short, can a feature be introduced that allows the function to de-normalize a numpy array manually (Like a Boolean argument) perhaps inheriting the required metadata from the `model` object? I think it would be very helpful for everyone using this function and would aid in quick&clean normalization.



"
45786,Keras docs wrongly advise not to pass tf.keras.layers activations to a layer creation,"## URL(s) with the issue:

https://keras.io/api/layers/activations/

## Description of issue (what needs changing):

In tf.keras, there are 2** ways of adding activation functions to your models. In the first instance, objects from `tf.keras.activations` can be passed as the `activation` argument in the creation of a layer as in: 
```python
model.add(layers.Dense(64, activation=tf.nn.tanh))
```

The 2nd API for activations is to explicitly add them as a layer, as in:
```python
x = layers.Dense(10)(x)
x = layers.LeakyReLU()(x)
```

The problem with the docs is that they explicitly say (at https://keras.io/api/layers/activations/) that:
> you should not pass activation layers instances as the activation argument of a layer. They're meant to be used just like regular layers

This runs contrary to current usage in tf2, where can can do exactly this. See, for instance, [this stackoverflow answer](https://stackoverflow.com/a/56869141/7858285) which advises the following:
```python
model.add(Conv2D(..., activation=tf.keras.layers.LeakyReLU(alpha=0.1), ...)
```
in place of the `lambda` function that is otherwise needed if one is to follow the keras docs advice.

**you might say there are 3 ways, as in the '1st' method one can also request an activation by its string identifier. 

### Submit a pull request?
Unless I'm mistaken, the https://keras.io/ docs are not open source. The keras repo currently requests that all issues be opened here in the tf repo. "
45785,GRPCIO v1.32.0 hard requirement results in clashes with other versions,"Tensorflow currently has a hard dependency on `grpcio~=1.32.0`, which results in clashes with packages that depend in newer versions of GRPC. This issue encompasses the requirement to update the GRPCIO library to the latest 1.34.0 version (or later)."
45783,Huge performance degradation on different CPU,"I have a model with ResNet50 which is trained by Keras.
This model can be run on CPU ""Intel(R) Xeon(R) Gold 6138T CPU @ 2.00GHz"" in 1.2 second, but when predict on ""E5 2620"" it need 15s.
Is there any idea to solve the performance degradation? Thanks.
P.S. Both of the 2 cases is in virtual mechine."
45782,Support for AMD Graphics Card,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.x
- Are you willing to contribute it (Yes/No): no



**Describe the feature and the current behavior/state.**
If possible can the support for AMD graphics be provided as this is the issue either way if the system has good features/configuration just because it has an AMD graphics card the TensorFlow won't work as smooth as it works in Nvidia graphics card? 

**Will this change the current API? How?**
I don't know about this.

**Who will benefit from this feature?**
Everyone who uses AMD graphics card.

**Any Other info.**
"
45781,"`Model` subclasses cannot be loaded without ""losing"" their class. ","**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.  I have included a short, self-contained, correct example in a Colab below.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Reproducible on Colab instances.  Originally identified on Debian 10, running on Google Cloud 
Linux jamlong-gpu-4 4.9.0-12-amd64 #1 SMP Debian 4.9.210-1 (2020-01-20) x86_64 GNU/Linux

- TensorFlow installed from (source or binary):
Binary (pip install tensorflow==2.4.0)

- TensorFlow version (use command below):
v2.4.0-rc4-71-g582c8d236cb 2.4.0

- Python version:
Python 3.7.6

- CUDA/cuDNN version:
N/A - reproducible on Colab without GPU

- GPU model and memory:
N/A - reproducible on Colab without GPU

**Describe the current behavior**
Loading a custom `MySubClass` model (subclass of `Model`) from a SavedModel yields a `Functional` instead of a `MySubClass`.


**Describe the expected behavior**
I'd expect when I define a `MySubClass` class as a subclass of Model, and then reload an instance of the model, I would get a `MySubClass`

**Standalone code to reproduce the issue**

Colab demonstrating an SSCCE:

https://colab.research.google.com/drive/12ShtOJ7oBkQsGsn_N801qHeMv3__POv4?usp=sharing


**Other info / logs** 

The issue appears to be that inheritance is effectively ""broken"" for `Model.from_config` as a result of the introduction of the `Functional` class, which `Model` is proxying to, in somewhat baffling (to an outsider, at least) fashion. 

If a user creates `MySubClass` as subclass of `Model`, and doesn't define a `from_config`, the implementation falls back to `Model.from_config`:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L2302
```
 @classmethod
  def from_config(cls, config, custom_objects=None):
    # Since only FunctionalModel produces config, the model can only
    # be constructed for FunctionalModel
    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
    return functional.Functional.from_config(
        config, custom_objects=custom_objects)
```

However, this implementation assumes that everything is really a `Functional` (since any model with a config is a `Functional`), which is what appears to be breaking us in this case. `Model.from_config(MySubClass, config...)` gets called, and it delegates to an explicit `Functional.from_config(config...)`, discarding the fact that cls=MySubClass in the process.  

The problem is, `Model.from_config` (or, more accurately, `Functional.from_config` is doing _all of the work_ in reloading the model, so we're completely reliant on it. However, the fact that the `Model` / `Functional` differentiation exists completely breaks the ability to use it.

* It doesn't work ""out of the box"", which it ideally would without the `Model` / `Functional` divide.
* Normally you'd just do any pre-work, defer to `super().from_config()`, and do any post-processing. However, the super-call would trigger the ""Force it to be `Functional`"" code, which still breaks it.
* Because `Functional` is part of `tf.python.keras` and not exported as part of the public API, we can't simply emulate the logic of `Functional.from_config` (which makes call to `tf.python.keras.engine.functional`,  delegates to the `cls` constructor, and then another call to do some post-processing)
* Of note: if I ignore the fact that it's _also_ completely the wrong thing to do, and have my class subclass  `tf.python.keras.engine.functional.Functional` instead of `Model`, it appears to work. It looks like it may _just_ be the fact that `Model.from_config` discards the `cls`.  But again, not something we can do here since it's not part of the public API.

In short: unless I've missed something obvious, model subclassing is currently broken w.r.t. serialization because of the `Model` vs. `Functional` divide. 

"
45779,Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED,"**System information**
- OS: Windows 10
- TensorFlow installed from (source or binary):  ""pip install tensorflow""
- TensorFlow version: 2.4.0
- Python version: 3.6.8
- Installed using: pip
- CUDA/cuDNN version: CUDA 11.0, cuDNN v8.0.4 (September 28th, 2020) for CUDA 11.0
- GPU model and memory: NVidia RTX2080Ti 11gb

Hi, 

I'm having a problem installing the new version of TF and some help would be greatly appreciated

When I launch TensorFlow, everything works as expected:

`2020-12-17 02:38:06.341560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-17 02:38:08.362868: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-17 02:38:08.363763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-17 02:38:08.393157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-12-17 02:38:08.393327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-17 02:38:08.402693: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-17 02:38:08.402832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-17 02:38:08.408295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-17 02:38:08.410941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-17 02:38:08.420106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-17 02:38:08.424734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-17 02:38:08.426865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-17 02:38:08.427022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
Num GPUs Available:  1
2020-12-17 02:38:08.443738: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-17 02:38:08.444619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-12-17 02:38:08.444716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-17 02:38:08.445101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-17 02:38:08.446152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-17 02:38:08.446937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-17 02:38:08.447301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-17 02:38:08.447647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-17 02:38:08.448059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-17 02:38:08.448403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-17 02:38:08.448795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-17 02:38:08.969074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 02:38:08.969175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-12-17 02:38:08.970017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-12-17 02:38:08.970724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9416 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-12-17 02:38:08.971475: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set`

But then, when I begin training the model:

`2020-12-17 02:38:10.285052: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-17 02:38:10.484223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-17 02:38:10.925209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-17 02:38:10.929739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-17 02:38:11.425989: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2020-12-17 02:38:11.426101: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2020-12-17 02:38:11.428154: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2020-12-17 02:38:11.428256: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2020-12-17 02:38:11.428655: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
Traceback (most recent call last):
  File ""style.py"", line 226, in <module>
    combination_image, base_image, style_reference_image
  File ""C:\Users\Jordan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Jordan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\def_function.py"", line 895, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File ""C:\Users\Jordan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""C:\Users\Jordan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 560, in call
    ctx=ctx)
  File ""C:\Users\Jordan\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node model/block1_conv1/Relu (defined at style.py:159) ]] [Op:__inference_compute_loss_and_grads_1267]

Function call stack:
compute_loss_and_grads`



I used to use older versions of tensorflow-gpu on my GTX980 but had to update for my new RTX2080Ti, I've installed the CUDA and cuDNN versions noted here: https://www.tensorflow.org/install/gpu

Unfortunately using this combination doesn't seem to work, help would be greatly appreciated. The above errors occur when attempting to run the Keras style transfer example https://keras.io/examples/generative/neural_style_transfer/ during the final block of code:
`for i in range(1, iterations + 1):
	loss, grads = compute_loss_and_grads(
		combination_image, base_image, style_reference_image
	)
	optimizer.apply_gradients([(grads, combination_image)])
	print(i)
	if i % 100 == 0:
		print(""Iteration %d: loss=%.2f"" % (i, loss))
		img = deprocess_image(combination_image.numpy())
		fname = result_prefix + ""_at_iteration_%d.png"" % i
		keras.preprocessing.image.save_img(fname, img)`

Thanks,
Jordan"
45778,[Doc bug] tf.sparse.concat doesn't have argument `expand_nonconcat_dim`,"
## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/sparse/concat

## Description of issue (what needs changing):
The signature of API  `tf.sparse.concat ` doesn't have argument `expand_nonconcat_dim`
![image](https://user-images.githubusercontent.com/24580222/102425051-90d17a80-3fda-11eb-88fb-8da8d44f381c.png)

But in the Argument section, it describes argument `expand_nonconcat_dim`
![image](https://user-images.githubusercontent.com/24580222/102425085-a34bb400-3fda-11eb-8d57-5834a8a9f9a7.png)

## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.3.0
- **Python version**: 3.7.6"
45777,[Documentation bug] Format issue in document of `tf.raw_ops.For`,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/raw_ops/For

## Description of issue (what needs changing):
The signature of the function has format issue
![image](https://user-images.githubusercontent.com/24580222/102424794-f1ac8300-3fd9-11eb-81d1-72dec90f96f1.png)

## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.3.0
- **Python version**: 3.7.6"
45771,Floating point exception in tf.truncatemod when x is boundary value,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.1.0
- Python version:3.7.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A


**Describe the current behavior**
Floating point exception in `tf.truncatemod` when `x` is a boundary value of int64

**Describe the expected behavior**
expect no crash 

**Standalone code to reproduce the issue**
~~~python
import tensorflow as tf
tf.truncatemod(x=-9223372036854775808, y=[-1])
~~~


"
45770,Segmentation fault in tf.histogram_fixed_width,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.1.0
- Python version:3.7.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A


**Describe the current behavior**
`tf.histogram_fixed_width` crashes (segmentation fault) when `values` contain nan

**Describe the expected behavior**
Expect no crash 

**Standalone code to reproduce the issue**
~~~python
import tensorflow as tf
import numpy as np
tf.histogram_fixed_width(values=np.nan, value_range=[1,2])
~~~

"
45765,micro: port op SPACE_TO_BATCH_ND from lite,"@tensorflow/micro

This issue tracks my work porting operator SPACE_TO_BATCH_ND from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan in the style of #45306:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro without making any changes or including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test"
45764,Hexagon Delegate of fully connected layer ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Hello, with the attached example tflite model, it runs on our device cpu good, output matches with expected. However if we use hexagon delegate to run, the output do not match, every 0x200 blocks have the same data.
The model is generated with tf1.15
I see that there's a test code at lite/delegates/hexagon/builders/tests/matmul_test.cc, however it asks the weight to be const, while in our model we need both inputs to matmul to be dynamic (output from previous nodes).
The matmul inputs are both at 512x256.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45762,CUDA 10.0 support has been dropped in tf 2.4,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.0/7.6.3
- GPU model and memory: 1080 Ti

**Describe the current behavior**

When building tf 2.4.0 from source on CUDA 10.0 the build fails with

```
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  /tensorflow_src/third_party/gpus/cuda_configure.bzl:1430:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 1400
		_create_local_cuda_repository(<1 more arguments>)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 606, in _find_libs
		_check_cuda_libs(repository_ctx, <2 more arguments>)
	File ""/tensorflow_src/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
		execute(repository_ctx, <1 more arguments>)
	File ""/tensorflow_src/third_party/remote_config/common.bzl"", line 217, in execute
		fail(<1 more arguments>)
Repository command failed
No library found under: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublasLt.so.10.0
```

The dependency on cublasLt was added in 592947d1a6c4123d8aec8362d74b020c8b4e2b1a but cublasLt was added only in CUDA 10.1 and later.

**Describe the expected behavior**

The dependency on cublasLt is modelled such that on CUDA 10.0 it is not required.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45760,Progress bar silence during validation for model.fit(),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

When using model.fit(), even setting `verbose=2` will still disable progress bar in the validation steps. Only the final progress bar with metric shows up when validation steps are finished. After digging the cause, it turns out that an attribute called `_called_in_fit ` is set to `True`, as seen in the following lines. I am not sure what is the reasoning behind disabling progress bar in validation step. But IMO enabling/disabling progress bar option should be left for user to choose.

https://github.com/tensorflow/tensorflow/blob/d3e40668078c96ef46cc3bd59b91e222852d4214/tensorflow/python/keras/callbacks.py#L1001-L1003

**Will this change the current api? How?**

Potentially adding an argument called `val_verbose` in `model.fit()` to enable/disable the progress bar.

**Who will benefit with this feature?**

Anyone whose val dataset is large and requires long time to run.

**Any Other info.**
"
45759,int8 quantization with int16 activations support for TFMicro and Cortex-M,"Hello community,

There is a support of int16 activations in TFLite, which helps a lot in keeping precision of an int8-quantized network, but not in TFMicro.

It should be noted that the TFMicro users are ones who will benefit from this features the most, because the platforms where one can run standard TFLite interpreter are often powerful enough/have enough memory so that things can often be done without any quantization at all, so this may not bring as much added value for the deployment on TFLite-compatible systems in this case(but it definitely does for research).

For TFMicro int16 activations support is absolutely paramount.

Just wondering if there is a plan to put this feature into production soon?


**System information**
- TensorFlow version (you are using):
TF 2.4rc3

- Are you willing to contribute it (Yes/No):
No

**Describe the feature and the current behavior/state.**
When the model with int8 weights and int16 activations is run with TFMicro interpreter the following error is observed: ""Node X failed to prepare""

**Will this change the current api? How?**
No, this shouldn't change APIs. It is already supported by TFLite but not by TFMicro.

**Who will benefit with this feature?**
TFMicro users will benefit from this feature.

**Any Other info.**
"
45758,[TensorFlow 2.x]'s model from OFFICAL DOC cannot freeze in any way( included [frozen_graph.py] in offical doc) [TenseorFlowSharp]. [C#] [Python][frozen model] CSharp Transfer Learning,"Sorry to trouble you guys, but i cost days and problem dosen't gone :(

Description: As a outsider of mechine learning, i use ml.net before. And there's a project need ML on Raspberry Pi, so it won't work( Linux Arm is not sueported).
Instead of ML.net, i plan to use **TensorFlowShar****p**(TF#, 'cause i muna use C# to write GUI and other things), and it **need a FROZEN MODEL**. So as a idot of mechin-learning, i followed officical docs to built a image classifier.(Details: _Anaconda_, _TensorFlow 2.3_, docs here: [https://tensorflow.google.cn/tutorials/images/transfer_learning](url) )

Then just to use a code like this of the end of the codes to get the model file:
keras_model_path = './keras_save'
model.save(keras_model_path)
h5_save_path = 'model.h5'
model.save(h5_save_path)

The difficulity is to make the 'h5' or savedmodel to be a frozen model.
The official code ""frozen_graph"" dosent work, and I used many scripts but failed.
The last code in my latop looks like this( From Lei Mao, thank you).

Follow the code, i put the model firstly to create ConcreteFunction, and then get frozen ConcreteFunction, finally write it on the hard drive.

For this clip of code, question is : **tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.**
In this place
 `frozen_func = convert_variables_to_constants_v2(full_model) # Error`

Freeze code like this:
`import matplotlib.pyplot as plt`
`import numpy as np`
`import os`
`import tensorflow  as tf`
`from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2`
`from tensorflow import keras`

`model = keras.models.load_model('keras_save')`

`# å°† Keras æ¨¡åž‹è½¬æ¢ä¸º ConcreteFunction`
`full_model = tf.function(lambda x: model(x))`
`full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))`

`# èŽ·å– å†»ç»“çš„ ConcreteFunction`
`frozen_func = convert_variables_to_constants_v2(full_model) # Error`
`frozen_func.graph.as_graph_def()`

`layers = [op.name for op in frozen_func.graph.get_operations()]`
`print(""-"" * 50)`
`print(""Frozen model layers: "")`
`for layer in layers:`
  `  print(layer)`

`print(""-"" * 50)`
`print(""Frozen model inputs: "")`
`print(frozen_func.inputs)`
`print(""Frozen model outputs: "")`
`print(frozen_func.outputs)`

`# å°†å†»ç»“çš„å›¾ä»Žå†»ç»“çš„ConcreteFunctionä¿å­˜åˆ°ç¡¬ç›˜`
`tf.io.write_graph(graph_or_graph_def=frozen_func.graph,logdir=""./frozen_models"",name=""frozen_graph.pb"",as_text=False)`

I'm doubt that is there any bugs? I hope you guys could pull me a hand ...

By the way, i also TIRED to use a TensorFlowSharp method called ""FromSavedModel"", it returns a TFSession type object, but icannot use this way to classify image. Only secess is to make image classification from google inception .... I hope you guys could give me a hand, and now the difficultiy is to <1 export a frozen model and then use it in TF# OR
<2 use a SavedModel in TF# .

In C# of TF#ï¼Œ error hereï¼š
TensorFlow.TFException:â€œOp type not registered 'StatefulPartitionedCall' in binary running on LAPTOP-8FU28J1C. Make sure the Op and Kernel are registered in the binary running in this process.â€
code isï¼š

`//load SavedModel`
`            TFGraph _tfGraph = new TFGraph();`
`            TFSession _tfSession;`
`           using (var tmpSess = new TFSession(_tfGraph))`
`            using (var tfSessionOptions = new TFSessionOptions())`
`           using (var metaGraphUnused = new TFBuffer())`
`            {`
`                //for some reason FromSavedModel is not static`
`                _tfSession = tmpSess.FromSavedModel(tfSessionOptions, null, ""tmp/keras_save"", new[] { ""serve"" }, _tfGraph, `metaGraphUnused);`
`            }`
code is from Github also, links here:
[https://github.com/migueldeicaza/TensorFlowSharp/issues/265](url)
migueldeicaza/TensorFlowSharp/issues/265


Last recorded 12.17( Dec,17,2020) Peking Time.
"
45756,SVD Function has (very) different GPU and CPU outputs ,"------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab Notebook 
-   **TensorFlow installed from (source or binary)**: pip install v2.3.0
-   **Python version**: Colab Default 3.6.9
-   **CUDA/cuDNN version**: 10.1, V10.1.243
-   **GPU model and memory**: Colab Default GPU (Persistence-M) ~15GB
-   **Exact command to reproduce**:
```python
import tensorflow as tf 
import numpy as np
tf.debugging.set_log_device_placement(True)
print(tf.__version__)

A= tf.random.uniform((50,9))

with tf.device('cpu:0'):
    _ ,_, v = tf.linalg.svd(A, full_matrices=True, compute_uv=True)
    res_cpu= v[0]
    
with tf.device('gpu:0'):
    _ ,_, v = tf.linalg.svd(A, full_matrices=True, compute_uv=True)
    res_gpu= v[0]
    
np.array_equal(res_cpu,res_gpu) #this will return False but expected should be true- 
np.allclose(res_cpu,res_gpu) #this will also return False but expected should be true- 
```

### Describe the problem
Computing SVD (singular value decomp) operation on gpu matches numpy results more closely and more accurately. The cpu implementation is far off (sometimes gives nans.) I presume there maybe a different solver used but there doesn't seem to be much explanation on that in source. I would like to use the CPU method in my training pipeline because I cannot afford any more GPU cost. 

"
45753,Tensorflow lite C++ library release default sample crashes at runtime on Windows build,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  2.3.1 and 2.4.0
- Python version: 3.6 (not relevant)
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): MSVC++ 14.28  (visual studio 2019 version 16.8.2)
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia RTX 2080 Ti 11Go

**Describe the current behavior**

My build commmands are the following building both tensorflow lite c++ and c libraries

```
git clone https://github.com/tensorflow/tensorflow.git tensorflow
cd tensorflow
git checkout 2.3.1
configure.cmd
   CUDA 7.5 compute capability activated
   all the rest is default
bazel build --config=opt //tensorflow/lite/c:tensorflowlite_c.dll //tensorflow/lite:tensorflowlite.dll
```

The headers are collected with the following commands:
```
xcopy tensorflow\*.h include\tensorflow\ /sy
pushd tensorflow\lite\tools\make
c:\msys64\mingw64.exe ./download_dependencies.sh
popd
xcopy tensorflow\lite\tools\make\downloads\flatbuffers\include\*.h include\ /sy
```

Using the minimal C++ sample [tensorflow\tensorflow\lite\examples\minimal\minimal.cc,](https://github.com/tensorflow/tensorflow/blob/9754a22a8091af2943ad479d5e5a56e84f58a701/tensorflow/lite/examples/minimal/minimal.cc#L67)
just uncommenting the line to get the input tensor pointer as in:
`float* input = interpreter->typed_input_tensor<float>(0);`
there is an exception within the call due to unallocated memory in ..include\vector:
```
Exception thrown: read access violation.
_My_data was 0xFFFFFFFFFFFFFFF7.
```
Call stack was:
```
TestTensorFlow.exe!std::vector<int,std::allocator<int>>::operator[](const unsigned __int64 _Pos) Line 1510	C++
TestTensorFlow.exe!tflite::Interpreter::typed_input_tensor<float>(int index) Line 292	C++
TestTensorFlow.exe!main(int argc, char * * argv) Line 68	C++
```

Testing other models yield the same error. I used pre-trained models downloaded from tensorflow such as this small one: [mobilenet_v1_0.25_128](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.25_128.tgz)
Testing the same models with the c library built with at the same time with the c++ library works fine.

_Important remark: the debug build of the lib works fine_ 
The debug lib built with the below command does not have the issue:
bazel build -c dbg //tensorflow/lite:tensorflowlite

**Describe the expected behavior**

The vector used internally by the tflite::interpreter should be initialized so that I can get the pointer to input tensor.

**Standalone code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Here is the smallest visual studio project I could create - the code minimal.cc is not modifed in any significant way, and the vcproj simply include the headers, add the directory containing the build c++ tensorflowlite.dll and libtensorflowlite.dll.ifso (that i renamed with a .lib extension) and set the file libtensorflowlite.dll.if.lib as an input library.
To use it you need to change the path in the project to the relevant ones for your tensorflow build location.

[TestTensorflowCC.zip](https://github.com/tensorflow/tensorflow/files/5705055/TestTensorflowCC.zip)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45752,"Allowing dict key mismatch among dataloader, keras model output, loss, and metric","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently, when tf.keras.Model.call is called and return type is dictionary, it requires that the keys of the dictionary matches the keys of the Dataloader's sample output, which also need to be sames as the keys of losses and metrics during compile. 

When training a complex model, it might be the case that only certain branches of the model matches the dataloader, losses and metrics. It will be great if only the intersect of all the key sets are used, potentially with warning, instead of throwing exceptions like `The two structures don't have the same sequence length. Input structure has length 11, while shallow structure has length 10.`

**Will this change the current api? How?**

Potentially. During compilation phrase, a flag argument like `allow_key_mismatch` can be passed to suppress the error.

**Who will benefit with this feature?**

Users who train a complex model with a lot of branches (outputs used for different tasks)

**Any Other info.**
"
45751,Unexpected kernel_shape (and trainable parameters) in Keras group convolution,"In Google Colab (tensorflow version 2.3.0) I am getting an unexpected kernel shape (and trainable params in the layer) for keras group convolution. 

```
import tensorflow as tf
print(tensorflow.__version__)

batch = 15
channels = 6
rows = 10
cols = 10
kernel = (3, 3)
groups = 2
filters = 32

conv_layer = tf.keras.layers.Conv2D(filters, kernel, groups)

output = conv_layer(tf.ones([batch, rows, cols, channels]))

print(conv_layer.get_weights()[0].shape)
print(conv_layer.get_weights()[1].shape)
```

gives me the following output:

```
2.3.0
(3, 3, 6, 32)
(32,)
```

Reading the documentation for the groups params in Conv1D/Conv2D/Conv3D, I was expecting a kernel shape of the following dimensions: weights (3, 3, 6, 16) and biases (16,). Is that a bug or I am missing something?

Thanks!
"
45749,micro: port op FLOOR_MOD from lite,"
@tensorflow/micro

This issue tracks my work porting operator FLOOR_MOD from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/floor_div.cc into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro without making any changes or including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test"
45748,Passing tensorflow::ops::DecodeJpeg::Attrs to the tensorflow::ops::DecodeJpeg constructor causes std::logic_error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It's custom code. However, it's based on this [article](https://itnext.io/creating-a-tensorflow-dnn-in-c-part-1-54ce69bbd586).
- OS Platform and Distribution: Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4
- Python version: 3.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc-10
- CUDA/cuDNN version: CUDA 11.2 (installed but not used)
- GPU model and memory: NVIDIA GeForce MX150


**Describe the current behavior**
Passing tensorflow::ops::DecodeJpeg::Attrs to the tensorflow::ops::DecodeJpeg constructor causes std::logic_error

console output
```
terminate called after throwing an instance of 'std::logic_error'
  what():  basic_string::_M_construct null not valid
Aborted (core dumped)
```


**Describe the expected behavior**
Exit without error.

**Standalone code to reproduce the issue**
```
tensorflow::Status ReadTensorFromImageFile(
    const std::string& file_name, const int input_height, const int input_width,
    const float input_mean, const float input_std,
    std::vector<tensorflow::Tensor>* out_tensors, bool writeGraph) {
  // build the graph
  tensorflow::Scope root = tensorflow::Scope::NewRootScope();
  tensorflow::ops::Placeholder input_file_name{root.WithOpName(""input""),
                                               tensorflow::DataType::DT_STRING};
  tensorflow::ops::ReadFile file_reader{root.WithOpName(""file_reader""),
                                        input_file_name};
  constexpr int kNumOfChannels = 3;
  tensorflow::ops::DecodeJpeg decode_file{
      root.WithOpName(""decode_file""), file_reader,
      tensorflow::ops::DecodeJpeg::Channels(kNumOfChannels)};
```
Exit without error iff tensorflow::ops::DecodeJpeg::Attrs is not used.
```
tensorflow::ops::DecodeJpeg decode_file{root.WithOpName(""decode_file""), file_reader};
```

**Other info / logs**
gdb backtrace
```
[New Thread 0x7fffe8839700 (LWP 104836)]
terminate called after throwing an instance of 'std::logic_error'
  what():  basic_string::_M_construct null not valid

Thread 1 ""tutorial_01"" received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
50      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace 
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007fffe8a2b859 in __GI_abort () at abort.c:79
#2  0x00007fffe8cb1951 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007fffe8cbd47c in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007fffe8cbd4e7 in std::terminate() () from /lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007fffe8cbd799 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007fffe8cb425e in std::__throw_logic_error(char const*) () from /lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x00007fffeb6b316c in void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#8  0x00007ffff5617b4c in tensorflow::SetAttrValue(absl::lts_2020_02_25::string_view, tensorflow::AttrValue*) () from /usr/local/lib/libtensorflow_cc.so.2
#9  0x00007ffff551fc13 in tensorflow::NodeDefBuilder::Attr(absl::lts_2020_02_25::string_view, absl::lts_2020_02_25::string_view) () from /usr/local/lib/libtensorflow_cc.so.2
#10 0x00007fffebceff2a in tensorflow::ops::DecodeJpeg::DecodeJpeg(tensorflow::Scope const&, tensorflow::Input, tensorflow::ops::DecodeJpeg::Attrs const&) ()
   from /usr/local/lib/libtensorflow_cc.so.2
#11 0x000055555555a095 in ReadTensorFromImageFile (file_name=""../data/grace_hopper.jpg"", input_height=299, input_width=299, input_mean=0, input_std=255, out_tensors=0x7fffffffd610, 
    writeGraph=true) at ../src/tutorial_01/tutorial_01.cpp:48
#12 0x0000555555559d11 in main (argc=2, argv=0x7fffffffd788) at ../src/tutorial_01/tutorial_01.cpp:22
```"
45746,"Tensorflow compile failure on Power9/PPC64LE, how to pass compile flags to NVCC?","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat 8, PPC64LE
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.3
- Python version: 3.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): gcc 8.3.1
- CUDA/cuDNN version: 10.2/7.6
- GPU model and memory: Tesla V100



**Describe the problem**

I get the following error when  compiling Tensorflow for IBM Power9 architectures (PPC64LE):

```/usr/include/bits/floatn.h(79): error: identifier ""__ieee128"" is undefined```
```/usr/include/bits/floatn.h(82): error: invalid argument to attribute ""__mode__""```

Per this PyTorch github thread that had the same issue (https://github.com/pytorch/pytorch/issues/45073) you apparently need to pass in some compile flags, in particular ""-mno-float128"" to CC and ""-Xcompiler -mno-float128"" to NVCC. From the Bazel documentation you can use ``--copt/--cxxopt`` to pass in compile flags for CC but is there a way to pass in compile flags to NVCC? 

The related issue thread at https://github.com/tensorflow/tensorflow/issues/38912 unfortunately suggests this is still an open problem..

Any help would be appreciated!
"
45745,"TF Lite for MCU porting, why not provide the generating the static library flow.","
Why not provide the generating the static library flow and link with the MCU tools environment. In this way, we only need to focus on the model generation, instead of compiling the source code for examples again and again.

Could anybody tell me the reason? Thanks so much.

"
45744,Illegal instruction (core dumped) in a CPU with AVX support,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Linux Ubuntu 20.10
- TensorFlow installed from (source or binary): pip, tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl
- TensorFlow version (use command below): tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl
- Python version: 3.8.6
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I was using tensorflow 2 without any issue until a few days ago. I installed it using pip in a new virtualenv environment and now it throws illegal instruction error. My CPU is an i5-3230M, which, according to /proc/cpuinfo supports AVX, so it seems not related to #19584 .

My CPU (according to `/proc/cpuinfo`):
model name      : Intel(R) Core(TM) i5-3230M CPU @ 2.60GHz
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d

System:
```
$ uname -a
Linux e431 5.8.0-33-generic #36-Ubuntu SMP Wed Dec 9 09:14:40 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```

**Describe the expected behavior**
The import should work and not give the `Illegal instruction (core dumped)` error.

**Standalone code to reproduce the issue**

```
$ python3 --version
Python 3.8.6
$ python3 -m venv test
$ . test/bin/activate
$ pip install --upgrade pip
Collecting pip
  Using cached pip-20.3.3-py2.py3-none-any.whl (1.5 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.1.1
    Uninstalling pip-20.1.1:
      Successfully uninstalled pip-20.1.1
Successfully installed pip-20.3.3
$ pip install tensorflow
Collecting tensorflow
  Using cached tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl (394.8 MB)
Collecting gast==0.3.3
  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)
Collecting absl-py~=0.10
  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)
Collecting astunparse~=1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers~=1.12.0
  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Collecting google-pasta~=0.2
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting grpcio~=1.32.0
  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)
Collecting h5py~=2.10.0
  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)
Collecting keras-preprocessing~=1.1.2
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Collecting numpy~=1.19.2
  Using cached numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)
Collecting opt-einsum~=3.3.0
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting protobuf>=3.9.2
  Using cached protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)
Collecting six~=1.15.0
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting tensorboard~=2.4
  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)
Requirement already satisfied: setuptools>=41.0.0 in ./test/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (44.0.0)
Collecting google-auth<2,>=1.6.3
  Using cached google_auth-1.24.0-py2.py3-none-any.whl (114 kB)
Collecting cachetools<5.0,>=2.0.0
  Using cached cachetools-4.2.0-py3-none-any.whl (12 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8
  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)
Collecting pyasn1-modules>=0.2.1
  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)
Collecting pyasn1<0.5.0,>=0.4.6
  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)
Collecting requests<3,>=2.21.0
  Using cached requests-2.25.0-py2.py3-none-any.whl (61 kB)
Collecting certifi>=2017.4.17
  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)
Collecting chardet<4,>=3.0.2
  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)
Collecting idna<3,>=2.5
  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)
Collecting requests-oauthlib>=0.7.0
  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Collecting oauthlib>=3.0.0
  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
Collecting rsa<5,>=3.1.4
  Using cached rsa-4.6-py3-none-any.whl (47 kB)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)
Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0
  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)
Collecting termcolor~=1.1.0
  Using cached termcolor-1.1.0.tar.gz (3.9 kB)
Collecting typing-extensions~=3.7.4
  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)
Collecting urllib3<1.27,>=1.21.1
  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)
Collecting werkzeug>=0.11.15
  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Collecting wheel~=0.35
  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)
Collecting wrapt~=1.12.1
  Using cached wrapt-1.12.1.tar.gz (27 kB)
Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.
Using legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.
Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow
    Running setup.py install for wrapt ... done
    Running setup.py install for termcolor ... done
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 certifi-2020.12.5 chardet-3.0.4 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.2 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1
$ python --version
Python 3.8.6
$ python
Python 3.8.6 (default, Sep 25 2020, 09:36:53) 
[GCC 10.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Illegal instruction (core dumped)
```

**Other info / logs**
"
45741,Will tf.reshape() perform extra memcopy in tensorflow gpu backend?,"```py
mediate_op = tf.some_op(inputs)
output_op = tf.reshape(mediate_op, [128, -1])
```
In terms of device memory occupation, is `tf.reshape` performed in place (reuse the data memory of `mediate_op`)?"
45740,tfLite armv7 build Issue,"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make

Is there any option to build for generic armv7 targets other than raspberry pi.
Is this restricted for just raspberry pi targets ? Will the same makefile work for other armv7 targets other than raspi targets."
45739,GET returned 404 Not Found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.9.2009 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0 (release downloaded from github repository)
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: No all dependencies (GCC 6.3.0, Python 3.8.5, Bazel 3.7.1 and  NCCL 2.7.8-1) are installed from source and CUDA 11.0.3 and cuDNN 8.0.5 are manually installed using the installers provided by Nvidia
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: CUDA 11.0.3, cuDNN 8.0.5
- GPU model and memory: Nvidia Tesla V100 (SXM2), 32 GB GPU memory

**Describe the problem**

Some of the sources that need to be downloaded for building tensorflow are not available:

```
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
```

When compiling tensorflow 2.3.0 (with GPU support for CUDA compute capabilities 6.1,7.0,7.5), then there were no issues like this and the total amount of build targets was around 70'000. Now with 2.4.0, some sources seem to be missing and there are only around 30'000 build targets, which seems a bit odd as I set optimization for more compute capabilities than for compinling 2.3.0  (we will soon get some nodes with A100 GPUs, therefore I want to optimize tensorflow already for compute capability 8.0).

Even with those warnings tensorflow builds fine (in the log I attached below, I canceled the build process after seeing the warnings again and noticing again that there are only 30k build targets), but I wouldn't feel comfortable providing the 2.4.0 version to our cluster users before it is not clear if anything went wrong.

There are no network issues on my side. I don't experience any issue downloading any other software or source code.

Any help is appreciated.

Best regards

Samuel Fux

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```

[sfux@eu-a65-001 156113172.tmpdir]$ ls
cache  temp  tensorflow-2.4.0
[sfux@eu-a65-001 156113172.tmpdir]$ cd tensorflow-2.4.0/
[sfux@eu-a65-001 tensorflow-2.4.0]$ module list

Currently Loaded Modules:
  1) StdEnv   2) eth_proxy   3) gcc/6.3.0   4) jdk/8u141-b15   5) bazel/3.7.1   6) openblas/0.2.20   7) python/3.8.5   8) cuda/11.0.3   9) cudnn/8.0.5  10) nccl/2.7.8-1



[sfux@eu-a65-001 tensorflow-2.4.0]$ pwd
/scratch/156113172.tmpdir/tensorflow-2.4.0
[sfux@eu-a65-001 tensorflow-2.4.0]$ ./configure
You have bazel 3.7.1- (@non-git) installed.
Please specify the location of python. [Default is /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python3]: /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python


Found possible Python library paths:
  /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Could not find any cuda.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
        'local/cuda/extras/CUPTI/include'
of:
        '/lib64'
        '/opt/ibutils/lib64'
        '/opt/mellanox/hcoll/lib'
        '/opt/mellanox/mxm/lib'
        '/opt/mellanox/sharp/lib'
        '/usr'
        '/usr/lib64//bind9-export'
        '/usr/lib64/mysql'
        '/usr/lib64/qt-3.3/lib'
        '/usr/lib64/xulrunner'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.0


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 2.7


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g


Found CUDA 11.0 in:
    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/lib
    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/include
Found cuDNN 8 in:
    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64
    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/include
Found NCCL 2 in:
    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib
    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0]: 6.1,7.0,7.5,8.0


Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -ftree-vectorize -march=core-avx2 -mavx2 -Wno-sign-compare


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
[sfux@eu-a65-001 tensorflow-2.4.0]$ bazel --output_base=$TMPDIR/cache build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=213
INFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --action_env PYTHON_LIB_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages --python_path=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --config=xla --action_env TF_CUDA_VERSION=11.0 --action_env TF_CUDNN_VERSION=8.0 --action_env TF_NCCL_VERSION=2.7 --action_env TF_CUDA_PATHS=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g --action_env CUDA_TOOLKIT_PATH=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5,8.0 --action_env LD_LIBRARY_PATH=/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib:/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64:/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/lib64:/cluster/apps/gcc-6.3.0/openblas-0.2.20-cot3cawsqf4pkxjwzjexaykbwn2ch3ii/lib:/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64:/cluster/apps/gcc-6.3.0/jdk-8u141-b15-e47jxtcwbf7n6umt34mef2auq66ktj4k/lib:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib --action_env GCC_HOST_COMPILER_PATH=/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:opt in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc: --copt=-ftree-vectorize --copt=-march=core-avx2 --copt=-mavx2 --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:linux in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  /scratch/156113172.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>
  /scratch/156113172.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories
Repository rule git_repository defined at:
  /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
DEBUG: /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /scratch/156113172.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>
  /scratch/156113172.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (415 packages loaded, 32798 targets configured).
INFO: Found 1 target...
[6,201 / 33,224] 96 actions running
```

"
45738,Why is the object_C library empty,"
"
45737,@Tuxius and others which stuck until this bug is fixed:,"@Tuxius and others which stuck until this bug is fixed: 
I had the same issue  and I found that my validation data set had less samples as the batch_size.  Because I'm working with TFRecords-data-sets which have no meta data about how many records, i.e. samples, are in the data set I check now upfront whether the data set  contains at least batch_size records (samples). 

For that I use the following helper functions:

```
def doesDataSetContainsEnoughDataForBatch(dataset, batch_size):
    return len(list(dataset.take(batch_size).as_numpy_iterator())) == batch_size
  
def doesDataSetFileContainsEnoughDataForBatch(sampleFileName="""", batch_size=100):
    dataset = tf.data.TFRecordDataset(sampleFileName)
    return doesDataSetContainsEnoughDataForBatch(dataset, batch_size=batch_size)

if __name__ == '__main__':
  dataSetFileName=""./Samples/validation_data_123.tfrecord""
  if not doesDataSetFileContainsEnoughDataForBatch(dataSetFileName,batch_size=100):
    raise Exception(f""Data set file {dataSetFileName} doesn't contain enough data"")
   
  # Now open the data set a second time knowing you have enough data 
  # and use it ....
  """"""
  trainDataset = tf.data.TFRecordDataset(dataSetFileName)
  
  model.fit ( trainDataset ... 
  
  """"""

```

Please keep in mind that by using the first helper function directly , 
i.e. doesDataSetContainsEnoughDataForBatch,  you already read batch_size samples from the data set. So you should recreate the data set after the check.

By using the second helper function  you just lose some execution time upfront.

If you don't use TFRecord data sets you might have a similar  issue. But then it is also a good idea to check upfront whether you have enough data for at least one batch.

_Originally posted by @PeterBrummer in https://github.com/tensorflow/tensorflow/issues/38064#issuecomment-614800998_"
45736,"model.predict(x,batch_size=500)  for around half an hour report WebSocketClosedError()","<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.3.0
- Python version:3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11
- GPU model and memory: Tesla V100 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

execute model.predict as below for around half an hour report  StreamClosedError() and WebSocketClosedError()
```python 
model.predict(x,batch_size=500) 
```
 

but when execute smaller part of the same data for prediction, no errors occur.

by the way, as a beginner I wonder if the total number of samples for prediction needs to be integer multiple  of batch_size? what if that cannot be?

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```python
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13499' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13500' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1104, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
Task exception was never retrieved
future: <Task finished name='Task-13501' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/tornado/websocket.py"", line 1102, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed


```"
45735,No tensorflow lite 2.4.0 pre-built libraries,"**System information**
- OS Platform and Distribution: Android
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.4.0

**Describe the problem**
Can not find pre-built libraries for tensorflow lite 2.4.0 at the below location. Latest available version is older 2.3.0

https://bintray.com/google/tensorflow/tensorflow-lite"
45733,[Build] [ROCm] [Bazel] /opt/rocm/.info/version-dev file not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0
- Python version: 3.9.1
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: N/A
- ROCm version: 3.10.0
- GPU model and memory:



**Describe the problem**

When building I am getting the following error:
```
INFO: Repository local_config_rocm instantiated at:
  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/WORKSPACE:19:16: in <toplevel>
  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/tensorflow/workspace.bzl:102:19: in tf_repositories
Repository rule rocm_configure defined at:
  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl:794:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_rocm':
   Traceback (most recent call last):
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl"", line 775, column 38, in _rocm_autoconf_impl
		_create_local_rocm_repository(repository_ctx)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl"", line 534, column 35, in _create_local_rocm_repository
		rocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl"", line 388, column 30, in _get_rocm_config
		config = find_rocm_config(repository_ctx, find_rocm_config_script)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl"", line 366, column 41, in find_rocm_config
		exec_result = _exec_find_rocm_config(repository_ctx, script_path)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl"", line 362, column 19, in _exec_find_rocm_config
		return execute(repository_ctx, [python_bin, ""-c"", decompress_and_execute_cmd])
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/remote_config/common.bzl"", line 217, column 13, in execute
		fail(
Error in fail: Repository command failed
ERROR: ROCm version file ""/opt/rocm/.info/version-dev"" not found
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_rocm//rocm': Repository command failed
ERROR: ROCm version file ""/opt/rocm/.info/version-dev"" not found
ERROR: no such package '@local_config_rocm//rocm': Repository command failed
ERROR: ROCm version file ""/opt/rocm/.info/version-dev"" not found
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. git clone
2. `export TF_NEED_ROCM=1`
3. `./configure`
4. 
```
  bazel \
        build \
          //tensorflow:libtensorflow.so \
          //tensorflow:libtensorflow_cc.so \
          //tensorflow:install_headers \
          //tensorflow/tools/pip_package:build_pip_package
      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu ""${srcdir}""/tmpoptrocm
```

To be exactly precise I am using the following build script (PKGBUILD):
https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Downstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/17

It could be that rocm 3.10 doesn't ship with `/opt/rocm/.info/version-dev` or that the Arch Linux package doesn't not ship with `/opt/rocm/.info/version-dev` files.

Could someone check if `/opt/rocm/.info/version-dev` exists in a ubuntu installation of rocm-3.10 and if so which package it is installed under?
If not then we should remove this check in our bazel code and figure out someother way of obtaining the rocm version: https://github.com/tensorflow/tensorflow/blob/9e7a15398b6e16fdf9b684aaa1437d1d3c76a5ab/third_party/gpus/find_rocm_config.py#L72-L83"
45731,ambiguity risk of crop_size,"https://github.com/tensorflow/tensorflow/blob/14793703c9fde08b643892b1f9d2504488f2d5f4/tensorflow/python/ops/image_ops_impl.py#L4698

Based on the introduction of this operator, crop_height, crop_width are supposed to stand for the size of output, which literally the middle step size of this op"
45730,How to specify `force_gpu_compatible` in TensorFlow 2.x?,"In TF 1.x, we can use these codes to force CPU tensor located on pinned CPU memory, whose bindwidth will be higher than pageable CPU memory.
```python
config = tf.ConfigProto()
config.gpu_options.force_gpu_compatible = True
...
with tf.Session(config = config) as sess:
    ...
```

But in TF 2.x, what is the mechanism to force CPU tensor located on pinned memory?"
45727,Issues about the TRTEngineOp,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.0/8.0.4
- GPU model and memory: GTX 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I tried to convert tensorflow FP32 model to FP16 using tensorRT 7.2.1, when I tried to save the converted model, it complained:

**....: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at trt_engine_resource_ops.cc:195 : Not found: Resource TF-TRT/TRTEngineOp_1_0/N10tensorflow8tensorrt22TRTEngineCacheResourceE does not exist.**

**INFO:tensorflow:Could not find TRTEngineOp_1_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.**

The first message seems like a known issue which can be ignored, but the second message seems not OK.

After that when I tried to infer using the converted model tensorflow complained:

**....: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:587] Running native segment forPartitionedCall/TRTEngineOp_0_0 due to failure in verifying input shapes: Input shapes do not match input partial shapes stored in graph, for PartitionedCall/TRTEngineOp_0_0: [[28,28]] != [[?,28,28]]**

Did I miss some configurations for the TRTEngine?

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
params = tf.experimental.tensorrt.ConversionParams(
    precision_mode='FP16')
converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir=""my_dir"", conversion_params=params)
converter.convert()
converter.save(output_saved_model_dir)
```
The above codes are from https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter

```
print(""The signature keys are: "",list(loaded.signatures.keys())) 
infer = loaded.signatures[""serving_default""]

im_select = 0 # choose train-image you want to classify
labeling = infer(tf.constant(train_images[im_select],dtype=float))['LastLayer']   ## Here, the Image classification happens; we need the name of the last layer we defined in the beginning
```
The above are from https://colab.research.google.com/gist/monkeyman85/e04e240cde74be48750d8160e078d5d7/convert_simplemodel_to_tensorrt_rev1.ipynb#scrollTo=oixTGKOSutMN


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45717,Different fill value for different channel with ImageProjectiveTransformV3,"**System information**
- TensorFlow version (you are using): 2.4
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Currently, `tf.raw_ops.ImageProjectiveTransformV3` accepts a scalar `fill_value` that fill out of bound pixel with `fill_value` when `fill_mode` is ""constant"". However, in some tensorflow repos like tensorflow-addons and tensorflow-models have some additional functionality to suppport different fill value for different channel. It's doable and less burden to do such thing in the C++ ops by checking if fill_value is either a scalar or a vector of #channels. With pure C++ implementation, we do not need compositions of python ops, which is suppposed to be slow.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/image_ops.cc#L88-L96
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/autoaugment_utils.py#L1189-L1224
https://github.com/tensorflow/addons/blob/master/tensorflow_addons/image/utils.py#L112-L151

**Will this change the current api? How?**

No. Only change underlying implementation.

**Who will benefit with this feature?**

Users of Tensorflow KPL layers, addons and models.

**Any Other info.**
"
45716,Model training stalls forever after just a few batches. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Windows 10.0.19041 Build 19041
- TensorFlow installed via pip
- TensorFlow version: 2.4.0-rc4
- TensorFlow Git version: v2.4.0-rc3-20-g97c3fef64ba 
- Python version: 3.8.5
- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.4
- GPU model and memory: Nvidia RTX 3090, 24GB RAM

**Describe the current behavior**
Model training regularly freezes for large models. 
Sometimes the first batch or so works, but then just a few batches later and training seems stuck in a loop. From my activity monitor, I see GPU CUDA use hovering around 100%. This goes on for minutes or more, with no more batches being trained.

I don't see an OOM error, nor does it seem like I'm hitting memory limits in activity monitor or `nvidia-smi`.

**Describe the expected behavior**
I would expect the first batch to take a bit longer, then any subsequent batches to take less than <1s. Never have a random batch take minutes (after previous batches took <1s) or stall forever.

**Standalone code to reproduce the issue**
Run through all the cells in the notebook shared below to initialize the model, then run the final cell just a few times. Eventually it will hang and never finish.

https://github.com/not-Ian/tensorflow-bug-example/blob/main/tensorflow%20error%20example.ipynb

**Other info / logs**
My intuition:
Smaller models train quickly as expected, however I think even then they eventually stall out after training many, many batches.
I had another similar, small VAE like in my example that trained for 5k-10k batches overnight before stalling. Could there be a memory leak somewhere?
"
45709,change LSTM to CuDNNLSTM cause core dumped,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS x86_64 GNU/Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu                     1.15.0, v1.15.0-rc3-22-g590d6ee 1.15.0
- Python version: python3.6
- CUDA/cuDNN version:  Cuda compilation tools, release 10.0, V10.0.130 / libcudnn.so.7
- GPU model and memory: GeForce GTX 1660 Ti with Max-Q Design


**Describe the current behavior**
https://stackoverflow.com/questions/65311478/change-lstm-to-cudnnlstm-cause-core-dumped

I tried a model, and changed LSTM (which is running fine) to CuDNNLSTM, then it core dumped.

```
from tensorflow.keras.layers import CuDNNLSTM as LSTM  # no core dump if just use LSTM

...
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise (GaussianNois (None, 63, 41)            0         
_________________________________________________________________
cu_dnnlstm_1 (CuDNNLSTM)     (None, 20)                5040      
_________________________________________________________________
dense (Dense)                (None, 1)                 21        
_________________________________________________________________
batch_normalization (BatchNo (None, 1)                 4         
_________________________________________________________________
re_lu (ReLU)                 (None, 1)                 0         
_________________________________________________________________
lambda (Lambda)              (None, 1)                 0         
=================================================================
Total params: 5,065
Trainable params: 5,063
Non-trainable params: 2
_________________________________________________________________

Epoch 1/1000
2020-12-15 12:58:38.233733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-12-15 12:58:38.410567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Segmentation fault (core dumped)


package versions:

Keras                              2.2.0
tensorflow-gpu                     1.15.0
```

**Describe the expected behavior**
CuDNNLSTM should running fine just as LSTM.


**Other info / logs** Include any logs or source code that would be helpful to

the stack trace from the core file:
```
$ gdb python3 core 
GNU gdb (Ubuntu 9.1-0ubuntu1) 9.1
...
Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x00007ff4a50edfc0 in tensorflow::Tensor::Tensor(tensorflow::Tensor const&) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
[Current thread is 1 (Thread 0x7ff42cffd700 (LWP 12422))]
(gdb) where
#0  0x00007ff4a50edfc0 in tensorflow::Tensor::Tensor(tensorflow::Tensor const&) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#1  0x00007ff4a5f1c4af in tensorflow::CudnnRNNForwardOpV2<Eigen::GpuDevice, double>::MaybeAutoTune(tensorflow::OpKernelContext*, tensorflow::(anonymous namespace)::CudnnRnnModelShapes const&, stream_executor::dnn::RnnInputMode const&, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, stream_executor::dnn::AlgorithmConfig*) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#2  0x00007ff4a5f15e1f in tensorflow::CudnnRNNForwardOp<Eigen::GpuDevice, double>::ComputeAndReturnAlgorithm(tensorflow::OpKernelContext*, stream_executor::dnn::AlgorithmConfig*, bool, bool, int) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#3  0x00007ff4a5f17400 in tensorflow::CudnnRNNForwardOpV2<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#4  0x00007ff4a1788dc2 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#5  0x00007ff4a17e4b47 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#6  0x00007ff4a17e513f in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#7  0x00007ff4a1895021 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#8  0x00007ff4a1892718 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1
#9  0x00007ff4a1eefb1f in std::execute_native_thread_routine (__p=0x55f81d1f75f0) at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83
#10 0x00007ff50f75f609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#11 0x00007ff50f686103 in clone () from /usr/lib/x86_64-linux-gnu/libc.so.6
(gdb) q
```

"
45708,"Validation Split causing  ResourceExhaustedError: OOM when allocating tensor with shape[54000,128,128,3]","Everything goes fine when I don't give validation_split while fitting the model.  Why is doing so? Does validation split need more memory although the data is being divided between training and validation right after the passing validation_split argument to model.fit() function. 

What could be the reasons of this odd behavior? 


`
--------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-18-bfe8abdce7db> in <module>
----> 1 history = model.fit(train_images,train_labels, batch_size=batch_size, epochs=epochs,
      2                     validation_split=0.1, callbacks=[callback], shuffle=True)

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    793       # `Tensor` and `NumPy` input.
    794       (x, y, sample_weight), validation_data = (
--> 795           data_adapter.train_validation_split((x, y, sample_weight),
    796                                               validation_split=validation_split,
    797                                               shuffle=False))

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in train_validation_split(arrays, validation_split, shuffle)
   1335     return array_ops.gather_v2(t, indices)
   1336 
-> 1337   train_arrays = nest.map_structure(
   1338       functools.partial(_split, indices=train_indices), arrays)
   1339   val_arrays = nest.map_structure(

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    615 
    616   return pack_sequence_as(
--> 617       structure[0], [func(*x) for x in entries],
    618       expand_composites=expand_composites)
    619 

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    615 
    616   return pack_sequence_as(
--> 617       structure[0], [func(*x) for x in entries],
    618       expand_composites=expand_composites)
    619 

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _split(t, indices)
   1333       return t
   1334     t = ops.convert_to_tensor_v2(t)
-> 1335     return array_ops.gather_v2(t, indices)
   1336 
   1337   train_arrays = nest.map_structure(

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in gather_v2(params, indices, validate_indices, axis, batch_dims, name)
   4533               batch_dims=0,
   4534               name=None):
-> 4535   return gather(
   4536       params,
   4537       indices,

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)
   4522     return params.sparse_read(indices, name=name)
   4523   except AttributeError:
-> 4524     return gen_array_ops.gather_v2(params, indices, axis, name=name)
   4525 
   4526 

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, batch_dims, name)
   3753         pass  # Add nodes to the TensorFlow graph.
   3754     except _core._NotOkStatusException as e:
-> 3755       _ops.raise_from_not_ok_status(e, name)
   3756   # Add nodes to the TensorFlow graph.
   3757   if batch_dims is None:

~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6651   message = e.message + ("" name: "" + name if name is not None else """")
   6652   # pylint: disable=protected-access
-> 6653   six.raise_from(core._status_to_exception(e.code, message), None)
   6654   # pylint: enable=protected-access
   6655 

~/.conda/envs/my_env/lib/python3.8/site-packages/six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[54000,128,128,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]
`"
45707,iOS Build TFlite as framework,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: arm64 ios
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest from master - 2.4.0
- Python version: Python 3.7.6
- Installed using virtualenv? pip? conda?:  anaconda3
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source):  12 
- CUDA/cuDNN version: NA
- GPU model and memory: NA 


**Describe the problem**
Running  build with the following command : 
```
bazel build --config=ios_fat -c opt \
  //tensorflow/lite/ios:TensorFlowLiteC_framework

```
This outputs a framework file i would like to add to my pod and distribute later. 
I've done it with previous versions but since moving to Xcode 12 i keep getting various issues.
1. Running dev version works ok but in release build results from tensor running return faulty. 
2.  Building with 
```
bazel build --config=ios_fat --ios_multi_cpus=armv7,arm64,i386,x86_64 -c opt \
  //tensorflow/lite/ios:TensorFlowLiteC_framework
```
Cause build issues where multiple c code is missing for arm64. 



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45706,Dot layer incomplete description,"### URL(s) with the issue: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot)

### Description of issue (what needs changing):
The dot layer accepts (0,0) as an argument for the axes, which produces the same result as (1,1). This behavior is not documented and more confusing than helpful since it suggests that the numbering of the axes starts at zero, but it actually starts at one. Since (0,0) as an argument seems to work, it suggests that also combination like (0,1) or (0,2) should work, but in this cases errors are produced. It might be better to not accept (0,0) as input for the axes, or to specify the motivation behind it in the documentation. 

Moreover, it would great to clarify the requirements for matching axes sizes in the documentation. Right now the documentation only states that the size of the selected axes must match, but e.g., also the first axes must mach otherwise an error is produced.



"
45705,Wrong default value in GRU layer documentaion,"### URL(s) with the issue: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)

### Description of issue (what needs changing):
The default value for the recurrent_activation of the GRU layer does not seem to be sigmoid as it is specified in the documentation.

### Usage example
The following two code examples illustrate the issue. The first one specifies the recurrent_activation argument by setting it to 'sigmoid' and the second does not do that, but should have it as a default. However, the examples produce a slightly different result which suggests that there is another default value. (Tensorflow version  'v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')
```
model = keras.Sequential([
keras.layers.LSTM(3,kernel_initializer='ones', recurrent_initializer='zeros',
    bias_initializer='zeros',  input_shape=(2,2),recurrent_activation='sigmoid')])
x= tf.constant([[[1, 2],[2,3]],[[1, 1],[0,1]],[[1, 2],[2,3]]])
print model.predict(x,steps=1)
```

```
model = keras.Sequential([
keras.layers.LSTM(3,kernel_initializer='ones', recurrent_initializer='zeros',
    bias_initializer='zeros',  input_shape=(2,2))])
x= tf.constant([[[1, 2],[2,3]],[[1, 1],[0,1]],[[1, 2],[2,3]]])
print model.predict(x,steps=1)
```
"
45703,Wrong error message for DepthwiseConv2D,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 'v1.14.0-rc1-22-gaf24dc91b5', '1.14.0'
- Python version: python 2.7

**Describe the current behavior**
There seems to be a wrong error message for the DepthwiseConv2D when different kernel dimension are specified. An InvalidArgumentError that states that the strides must be equal length is produced, even though the strides are not specified and are equal per default:""Current implementation only supports equal length strides in the row and column dimensions.""
The error is e.g., produced by the following code.

It works without error when the kernel dimensions are changed from (2,3) to (3,3). Hence, a correct error message that states that unequal kernel dimensions are currently also not supported would be great. It might also be good if this is mentioned in the documentation.

**Standalone code to reproduce the issue**
```
model = keras.Sequential([keras.layers.DepthwiseConv2D(3, (2, 3), depth_multiplier=4, input_shape=(3,3,3))])
x = tf.constant([[[[8,3,5],[5,4,-2],[2,1,-1]],[[8,3,5],[5,4,-2],[2,1,-1]],[[8,3,5],[5,4,-2],[2,1,-1]]]])
print np.array2string(model.predict(x,steps=1), separator=', ')
```


"
45702,Why TPU uses more memory than GPU?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly stock, very little custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 - **Colab PRO**
- TensorFlow installed from (source or binary): Pre-installed
- TensorFlow version (use command below): 2.3.0
- Python version: 3.x
- GPU model and memory: Using TPU

I was trying to train a model with around ~700 Million Parameters, however, when I switch to TPU with the correct strategy and proper initialization (code taken from Official TensorFlow site) I find that it gives me a resource error just before training (when I compile the model):-

```
Epoch 1/3
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.

---------------------------------------------------------------------------

ResourceExhaustedError                    Traceback (most recent call last)

<ipython-input-15-331b28c97632> in <module>()
      8     filepath=checkpoint_prefix, monitor='loss', save_best_only=True)
      9 
---> 10 history = model.fit(X_train, y_train, epochs=3, batch_size=4, verbose=1, callbacks=[checkpoint_callback])

10 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ResourceExhaustedError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:346 run
        return self.extended.tpu_run(fn, args, kwargs, options)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1095 tpu_run
        return func(args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1162 tpu_function
        padding_spec=padding_spec)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:915 replicate
        padding_spec=padding_spec)[1]
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1380 split_compile_and_replicate
        outputs = computation(*computation_inputs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1124 replicated_fn
        result[0] = fn(*replica_args, **replica_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step
        self.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2745 _minimize
        experimental_aggregate_gradients=False)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients
        self._create_all_weights(var_list)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:704 _create_all_weights
        self._create_slots(var_list)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py:129 _create_slots
        self.add_slot(var, 'v')
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:764 add_slot
        initial_value=initial_value)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:262 __call__
        return cls._variable_v2_call(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:256 _variable_v2_call
        shape=shape)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2124 create_colocated_variable
        return next_creator(**kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars
        created = self._create_variable(next_creator, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:870 _create_variable
        **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable
        value_list = real_mirrored_creator(**kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:861 _real_mirrored_creator
        v = next_creator(**kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:685 variable_capturing_scope
        lifted_initializer_graph=lifted_initializer_graph, **kwds)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:264 __call__
        return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:233 __init__
        shape = initial_value.shape
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1167 shape
        self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())

    ResourceExhaustedError: Failed to allocate request for 377.93MiB (396288000B) on device ordinal 0

```     
This is extremely confusing, as I have thought that TPU's have _wayy_ more memory than a GPU (Like at least 64GiB). Does this indicate a bug on my side? Is there any way to check the actual **memory** capacity and memory used of the TPU? please help
. I am using Colab Pro

TPU Initialization code:-
>import tensorflow as tf
>tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
>print(f""Running on TPU: {tpu.cluster_spec().as_dict()['worker']}"")  
>tf.config.experimental_connect_to_cluster(tpu)
>tf.tpu.experimental.initialize_tpu_system(tpu)
>strategy = tf.distribute.TPUStrategy(tpu)
"
45701,To provide the prediction probabilities for Binary classification task using the sigmoid activation in the output layer.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Feature: For binary classification with sigmoid activation in the output layer, the model.predict() should output the probability values for both class 0 and class 1 as it does in the case of multi-class classification.

Current behavior/state: Using the model.predict() method, the output is a numpy array with single probability value. If the probability value>0.5 then it belongs to class 1 else class 0. i.e. It does not provide the probability values for each class separately as in a multiclass classification which uses the softmax activation in the output layer.

**Will this change the current api? How?**
Minor change for the output of model.predict() for binary classification task with 1 neuron and sigmoid activation in the output layer.

**Who will benefit with this feature?**
This feature will be helpful for explaining the model. There are XAI libraries like Lime, Shap which depends on the probability values of each class for explaining the model. They take the model as an input parameter and expect the prediction/output of the model prediction in the form of probabilities for each class to explain the model.

**Any Other info.**
Lime takes a function as an input to its explainer. This function can be user-defined which can implicitly call TF model.predict() and output the probability values for both the classes based on the 0.5 threshold value. 
But in case of Shap, the Shap Deep Explainer (especially designed for TF and Pytorch Neural Netoworks) takes the TF/Pytorch model as input and internally calls the model.predict() and generates the shap values. Since, here in case of binary classification with sigmoid activation only single probability value is generated, so shap also generate single shap values instead of shap values for both the classes.
[Link to Lime](https://github.com/marcotcr/lime)
[Link to Shap Deep Explainer](https://shap.readthedocs.io/en/latest/generated/shap.DeepExplainer.html#shap.DeepExplainer)
"
45700,Request for a tutorial on building C++ API not pure C,"**System information**
- TensorFlow version: 2.4
- Are you willing to contribute it: Yes



**Describe the feature and the current behavior/state.**
The [current documentation](https://www.tensorflow.org/install/lang_c) only tells user how to install TensorFlow for C API. But none of them tells how to install TensorFlow for **C++**.

**Will this change the current api? How?**
This does not change the current api. It just enables user know how to make the existing functionality available on our computer.

**Who will benefit with this feature?**
People who hate python but still wants to do machine learning with tensorflow

**Any Other info.**
"
45698,Failed to run Benchmark ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Latest Master 203f9256a596170151e4ddc93aeda6ed2905e8e7
- Python version:
- Bazel version (if compiling from source): 3.1
- GCC/Compiler version (if compiling from source): 10.2
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
There is no output of benchmark
```
bazel --output_user_root=$build_dir run --copt=-O3 //tensorflow/core/kernels/image:non_max_suppression_op_benchmark_test -- --benchmarks=../
```

**Describe the expected behavior**
It should output the performance data of unit test.

Isolate to this Commit: https://github.com/tensorflow/tensorflow/commit/29bb0deb26db7179eefc87e750fd8755b2c9def3 which enables the new benchmark feature.

"
45695,micro: Port TRANSPOSE from lite to micro,"@tensorflow/micro

This issue tracks my work porting operator TRANSPOSE from lite to micro. @advaitjain 

It will be delivered in a series of PRs.

PR 1 (merged): Refactor flatbuffer_conversions #45439 
PR 2 (merged): Refactor transpose reference op #45438 
PR 3 (merged): Copy of the reference kernel from lite to micro without changes #45843 
PR 4: Modify the micro kernel, port the tests and add the kernel to the micro build (as three separate commits) #47446

"
45693,micro: port ops BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND from lite ,"@tensorflow/micro

Disclaimer: This is my first contribution to a bigger open source project, so please let me know if I'm doing something wrong or if I forget something - I highly appreciate your feedback.

In my project I use causal convolution, implemented with the Keras Layer Conv1D. After the conversion to the TFLite model, the convolution will be performed by the Conv2D op. This requires the ops BATCH_TO_SPACE_ND and SPACE_TO_BATCH_ND and since causal convolution has a huge field of use, I think that it makes sense to add these two ops to tflite micro as well. Since these ops are inverses of each other and (at least sometimes) used together, I thought it is appropriate to create only one issue to add both ops.

I already got a model running on my mcu using these ops and now I would like to share this with the other tensorflow users. I am not sure yet about all the required steps and which Pull Requests I need to make but I am sure that I will figure this out and I will link the pull requests to this issue and document it properly.

Steps
* PR 1: refactor flatbuffer_conversions parsing function #45696 
* PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes. #45699
* PR 3: copy the reference kernel from lite to micro and adjust it to micro by removing optimized ops. Add the File to the build. #45704
* PR 4 (by @njeffrie): Port batch_to_space from TFLite to micro for int8 and float #46681
* PR 5 (by @njeffrie): Port space_to_batch from TFLite to micro for int8 and float #46714
* PR 6: Bugfix in batch_to_space_nd.cc #47304
"
45692,Unable to compile the TFLite for Microcontrollers C++ Library on Raspberry Pi,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Raspbian GNU/Linux 10 (buster) Linux 5.4.72-v7+  armv7l
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.4.0 (582c8d236cb079023657287c318ff26adb239002)
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  Raspberry Pi 3 Model B Rev 1.2

**Describe the problem**
Unable to compile the TFLite for Microcontrollers C++ Library
I have tested from 2.4.0 rc0 to current 2.4.0 release and got the same error

`tensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"" ""7e8191b24853d75de2af87622ad293ba"" tensorflow/lite/micro/tools/make/downloads/gemmlowp
downloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.tar.gz"" ""dfa0ac3073b78ddacdcacf8ca189be91"" tensorflow/lite/micro/tools/make/downloads/flatbuffers
downloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip"" ""c720b1743360259ac45809a321f8f26c"" tensorflow/lite/micro/tools/make/downloads/ruy
downloading https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip"" ""55b85f76e2995153e660391d4a209ef1"" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip"" ""9b5b6d4677dd0a91b1bb992d1c4c0417"" tensorflow/lite/micro/tools/make/downloads/person_model_int8
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2"" ""299ebd3f1c2c90930d28ab82e5d8d6c0"" tensorflow/lite/micro/tools/make/downloads/gcc_embedded
downloading http://mirror.tensorflow.org/developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip"" ""1f4607b05ac45b8a6146fb883dbc2d7b"" tensorflow/lite/micro/tools/make/downloads/image_recognition_model
downloading https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz"" ""c32a1d4ab5d03f1284b67883e8d87530"" tensorflow/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset
downloading http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip"" ""438ba1fef5783cc5f5f201395cc477ca"" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft
downloading http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip
Finished patching kissfft
arm-none-eabi-g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -mcpu=cortex-m7 -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=soft -funsigned-char -mlittle-endian -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M7=1  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7/obj/tensorflow/lite/micro/micro_error_reporter.o
tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++:ELF: not found
tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: Syntax error: Unterminated quoted string
make: *** [tensorflow/lite/micro/tools/make/Makefile:430: tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7/obj/tensorflow/lite/micro/micro_error_reporter.o] Error 2
`

I was able to compile to C++ library on 2.2.0 and 2.3.1 and run a small sample with 2.3.1 on the Raspberry Pi 3 Model B

**Please provide the exact sequence of commands/steps when you ran into the problem**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m7 microlite
"
45688,Problem with installing Tensorflow on Anaconda,"**System information**
- Mac OS 11.0.1 (Big Sur)
- TensorFlow installed from Anaconda
- TensorFlow version: 2.0
- Python version: 3.8.5 and 3.7.9
- Installed using Conda
- GPU model and memory: Intel 3 GHz 6-Core i5 Processor 


Hello I have a problem with anaconda and tensorflow. I tried to install tensorflow on Python 3.8.5 but then I get always an error. So I downgraded Python to 3.7.9 and then I was able to install tensorflow. But when I run a script by using tensorflow I get the warning, that my processor is not optimized for tensorflow (I'm using a intel i5 3GHz 6-core processor on my mac). I tried it with the steps in the following link, but it still don't work.
https://towardsdatascience.com/optimize-your-cpu-for-deep-learning-424a199d7a87
How can I run Tensorflow on my computer?

Thank you for your help."
45687,StringLookup on GPU and disable_eager_mode gives incosistent results,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: release 10.1, V10.1.243
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When using StringLookup and GPU and disable_eager_mode, on each run I get different results - accuracy, (minimal reproducible code).
On training, the trained model don't converge.
On CPU, or when eager mode in enabled, I get consistent results.

**Describe the expected behavior**

consistent accuracy - results among runs (on GPU)

**Standalone code to reproduce the issue**

updated link:
https://colab.research.google.com/gist/IsaacDayan/85c8c7492b6a2f80857ee1795f1d8e45/stringlookupdiscrepancyaccuracybetweeneagerandgraphminimal.ipynb


Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45686,ERROR: flatbuffers::flatbuffer_version_string Multiply defined Global Symbol,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Windows 10
- TensorFlow installed from: source
- Tensorflow version (commit SHA if source): latest
- Target platform: STM32F407IG using Keil uVision with Compiler v6



**Problem**

I have been trying to compile the hello_world example generated with TFLite using Keil uVision, and compiler v6 from them. After compiling for a little while, it stops with an error and I get the following output:


Build started: Project: keil_project
*** Using Compiler 'V6.12', folder: 'C:\Keil_v5\ARM\ARMCLANG\Bin'
Build target 'hello_world'
compiling common.c...
compiling error_reporter.cc...
compiling tensor_utils.cc...
compiling quantization_util.cc...
compiling op_resolver.cc...
compiling flatbuffer_conversions.cc...
compiling keyword_scrambled_model_data.cc...
compiling debug_log.cc...
compiling kernel_util.cc...
compiling constants.cc...
compiling main.cc...
compiling all_ops_resolver.cc...
compiling model.cc...
compiling output_handler.cc...
compiling activations.cc...
compiling main_functions.cc...
compiling arg_min_max.cc...
compiling add.cc...
compiling ceil.cc...
compiling circular_buffer.cc...
compiling comparisons.cc...
compiling conv.cc...
compiling concatenation.cc...
compiling depthwise_conv.cc...
compiling dequantize.cc...
compiling ethosu.cc...
compiling elementwise.cc...
compiling flexbuffers_generated_data.cc...
compiling detection_postprocess.cc...
compiling floor.cc...
compiling fully_connected.cc...
compiling kernel_runner.cc...
compiling hard_swish.cc...
compiling kernel_util.cc...
compiling logical.cc...
compiling l2norm.cc...
compiling logistic.cc...
compiling maximum_minimum.cc...
compiling neg.cc...
compiling pack.cc...
compiling mul.cc...
compiling pad.cc...
compiling pooling.cc...
compiling prelu.cc...
compiling quantize.cc...
compiling reduce.cc...
compiling resize_nearest_neighbor.cc...
compiling round.cc...
compiling reshape.cc...
compiling split.cc...
compiling softmax.cc...
compiling shape.cc...
compiling split_v.cc...
compiling strided_slice.cc...
compiling sub.cc...
compiling svdf.cc...
compiling svdf_common.cc...
compiling unpack.cc...
compiling tanh.cc...
compiling greedy_memory_planner.cc...
compiling linear_memory_planner.cc...
compiling micro_error_reporter.cc...
compiling memory_helpers.cc...
compiling micro_profiler.cc...
compiling micro_string.cc...
compiling micro_allocator.cc...
compiling micro_time.cc...
compiling micro_interpreter.cc...
compiling micro_utils.cc...
compiling recording_simple_memory_allocator.cc...
compiling recording_micro_allocator.cc...
compiling simple_memory_allocator.cc...
compiling test_conv_model.cc...
compiling retarget_io.c...
compiling test_helpers.cc...
compiling schema_utils.cc...
linking...
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(op_resolver.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(all_ops_resolver.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(main_functions.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(add.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(mul.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(reshape.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(shape.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(memory_helpers.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(micro_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(micro_interpreter.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(recording_micro_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(simple_memory_allocator.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(test_helpers.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Warning: L6439W: Multiply defined Global Symbol flatbuffers::flatbuffer_version_string defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(schema_utils.o) rejected in favor of Symbol defined in .data._ZN11flatbuffers25flatbuffer_version_stringE(flatbuffer_conversions.o).
.\Objects\hello_world.axf: Error: L6320W: Ignoring --entry command. Cannot find argument 'Reset_Handler'.
.\Objects\hello_world.axf: Warning: L6320W: Ignoring --first command. Cannot find argument '__Vectors'.
Not enough information to list image symbols.
Not enough information to list load addresses in the image map.
Finished: 2 information, 15 warning and 1 error messages.
"".\Objects\hello_world.axf"" - 1 Error(s), 15 Warning(s).
Target not created.
Build Time Elapsed:  00:02:12



What I could find out, is that flatbuffer_version_string is defined under flatbuffers.h, and initialised there too, so that every file including flatbuffers.h will define flatbuffer_version_string all over again. But I still cannot find a solution.




**Steps**

- Generate hello_world exmaple with MAKE, using the command ""make -f tensorflow/lite/micro/tools/make/Makefile generate_projects hello_world""
- Open keil_project.uvprojx under hello_world/keil.
- Build .
- Wait for build to stop with error.
"
45685,ERROR on running frozen.pb at tf2.x (google.protobuf.message.DecodeError: Error parsing message ),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tensorflow2.3.1
- Python version: python3.6
- CUDA/cuDNN version: CUDA 10.1, cuDNN7.6
- GPU model and memory: 8 GiB


I have my pytorch model and converted it to onnx model.
Nextly, it is converted to tf2.3.1 saved_model.pb (including variables folder with data files) type model using onnx-tf and I checked it ran perfectly. :)

Now, I converted it to frozen.pb type model following here: https://stackoverflow.com/questions/59657166/convert-frozen-model-pb-to-savedmodel
and finally got the frozen_model.pb file.
the converting code is below:
<pre><code>
import tensorflow as tf
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph
from tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config
import numpy as np

OD_SAVED_DIR = ""./od_tf_model""

OD_FROZEN_DIR = ""./frozen_models/od""

def frozen_keras_graph(func_model):
    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(func_model)

    input_tensors = [
        tensor for tensor in frozen_func.inputs
        if tensor.dtype != tf.resource
    ]
    output_tensors = frozen_func.outputs
    graph_def = run_graph_optimizations(
        graph_def,
        input_tensors,
        output_tensors,
        config=get_grappler_config([""constfold"", ""function""]),
        graph=frozen_func.graph)

    return graph_def
    
def convert_saved_model_to_pb(path_to_saved_model, path_to_frozen_model):
    model_dir = path_to_saved_model
    model = tf.saved_model.load(model_dir)
    func_model = model.signatures[""serving_default""]
    graph_def = frozen_keras_graph(func_model)
    tf.io.write_graph(graph_def, path_to_frozen_model, 'frozen_graph.pb')
    
convert_saved_model_to_pb(OD_SAVED_DIR, OD_FROZEN_DIR)

</code></pre>

Now, I am trying to run frozen_model.pb on my tensorflow 2.3.1 but got an error.

<pre><code>
...
graph_def = tf.compat.v1.GraphDef()
loaded = graph_def.ParseFromString(open('path/to/frozen/pb/file/frozen_model.pb','rb').read())
...
</code></pre>

and got following error

<pre><code>
Traceback (most recent call last):
  File ""run_frozen.py"", line 392, in <module>
    loaded = graph_def.ParseFromString(open(PB_MODEL_PATH,'rb').read())
google.protobuf.message.DecodeError: Error parsing message
</code></pre>

is anyone solved this problem?


i.e) running saved_model.pb file on tensorflow 2.3.1 code is below:
<pre><code>
model = tf.saved_model.load('path/to/saved/model/including/folder')
    infer = model.signatures[""serving_default""]
    outputs = infer(images)
</code></pre>"
45684,tensorflow_text.BertTokenizer not giving correct offset values in strings that have special characters,"Is there a bug in tensorflow_text.BertTokenizer()?
------------------------

Please let me know if I am mistaken. I will close the issue.


System information

-  Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow): Yes

-   OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (using a TPU worker through a hosted runtime)

-   Exact command to reproduce: Please see the below code.


Describe the problem:

I am using the SQUAD dataset for a BERT project. I needed offsets for each wordpiece token and thus decided to use the BertTokenizer class from tensorflow_text. A lot of the instances in the dataset have special characters like Greek letters, etc. In each sentence that contains such special characters, I noticed that the tokenizer is messing up the offsets of tokens that come after the special character.


Source code:

```
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

# Get the vocab file
tfhub_handle_encoder = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3""
test_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')
vocab_file = test_encoder.resolved_object.vocab_file.asset_path.numpy()

vocab_file = vocab_file.decode(""utf-8"")
lower_case = test_encoder.resolved_object.do_lower_case.numpy()

# String has a Greek alphabet
mystr = u""The difference in the above factors for the case of Î¸=0 is the reason that most broadcasting uses vertical polarization."".encode('UTF-8')
mystr = tf.constant(mystr)  # mystr is a unicode string tensor

print(mystr.numpy().decode(""utf-8""))           # Î¸ gets printed correct
print(mystr.numpy())                                     # Prints the utf-8 encoding of Î¸
print(tf.strings.substr(mystr, pos=80, len=12, unit='UTF8_CHAR').numpy())    # The token 'broadcasting' starts at offset 80
print(tf.strings.substr(mystr, pos=44, len=4, unit='UTF8_CHAR').numpy())      # The token 'case' starts at offset 44

bert_tokenizer = text.BertTokenizer(vocab_lookup_table=vocab_file, lower_case=lower_case)
(tokens, offset_starts, offset_limits) = bert_tokenizer.tokenize_with_offsets([mystr])
context_offsets = tf.stack([offset_starts.flat_values, offset_limits.flat_values], axis=-1)
context_offsets = context_offsets.numpy()

print(context_offsets)
```

Output:

The BertTokenizer correctly detects the starting offset for 'case' to be 44 but messes up the starting offset for 'broadcasting' (it thinks the starting offset is 81). Please see the below output:

```
The difference in the above factors for the case of Î¸=0 is the reason that most broadcasting uses vertical polarization.
b'The difference in the above factors for the case of \xce\xb8=0 is the reason that most broadcasting uses vertical polarization.'
b'broadcasting'
b'case'
[[  0   3]
 [  4  14]
 [ 15  17]
 [ 18  21]
 [ 22  27]
 [ 28  35]
 [ 36  39]
 [ 40  43]
 **[ 44  48]**    # 'case' starts at offset 44
 [ 49  51]
 [ 52  54]          # The tokenizer reads Î¸ as two tokens. How do we change this behavior?
 [ 54  55]
 [ 55  56]
 [ 57  59]
 [ 60  63]
 [ 64  70]
 [ 71  75]
 [ 76  80]
 **[ 81  93]**   # 'broadcasting' starts at offset 80, not 81
 [ 94  98]
 [ 99 107]
 [108 113]
 [113 120]
 [120 121]]
```"
45681,AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection',"Even though I tried, I couldn't solve this mistake.

**Error Log**
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-138183a2a830> in <module>
      1 # Create model object in inference mode.
----> 2 model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)
      3 
      4 # Load weights trained on MS-COCO
      5 model.load_weights(COCO_MODEL_PATH, by_name=True)

~\**\Mask_RCNN\mrcnn\model.py in __init__(self, mode, config, model_dir)
   1838         self.model_dir = model_dir
   1839         self.set_log_dir()
-> 1840         self.keras_model = self.build(mode=mode, config=config)
   1841 
   1842     def build(self, mode, config):

~\**\Mask_RCNN\mrcnn\model.py in build(self, mode, config)
   2044             # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in
   2045             # normalized coordinates
-> 2046             detections = DetectionLayer(config, name=""mrcnn_detection"")(
   2047                 [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])
   2048 

**\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, *args, **kwargs)
    949     # >> model = tf.keras.Model(inputs, outputs)
    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
--> 951       return self._functional_construction_call(inputs, args, kwargs,
    952                                                 input_list)
    953 

**\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1088           layer=self, inputs=inputs, build_graph=True, training=training_value):
   1089         # Check input assumptions set after layer building, e.g. input shape.
-> 1090         outputs = self._keras_tensor_symbolic_call(
   1091             inputs, input_masks, args, kwargs)
   1092 

**\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)
    821     else:
--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    823 
    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

**\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    861           # TODO(kaftan): do we maybe_build here, or have we already done it?
    862           self._maybe_build(inputs)
--> 863           outputs = call_fn(inputs, *args, **kwargs)
    864 
    865         self._handle_activity_regularization(inputs, outputs)

**\anaconda3\lib\site-packages\tensorflow\python\autograph\impl\api.py in wrapper(*args, **kwargs)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):
--> 670           raise e.ag_error_metadata.to_exception(e)
    671         else:
    672           raise

AttributeError: in user code:

    **\Mask_RCNN\mrcnn\model.py:810 call  *
        detections_batch = utils.batch_slice(
    **\Mask_RCNN\mrcnn\utils.py:820 batch_slice  *
        output_slice = graph_fn(*inputs_slice)
    **\Mask_RCNN\mrcnn\model.py:720 refine_detections_graph  *
        keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),

    AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection'

**model.py** arranged according to here. 

https://github.com/matterport/Mask_RCNN/issues/1070#issuecomment-740430758"
45680,ERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

Bonjour monsieur et madame,

I am struggling to compile tensorflow2.3 from source, and it met the problem which I can not solve, please help.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 professional
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): tensorflow r-2.3
- TensorFlow version: r-2.3
- Python version: Anaconda python 3.8
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.1
- GCC/Compiler version (if compiling from source): MSVC2019
- CUDA/cuDNN version: CUDA10.1, cudnn 7.6
- GPU model and memory: two Geforce GTX1080



**Describe the problem** 
```
ERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
set PATH=%PATH%; //////""include msvc path, bazel vc path, cuda path""
bazel clean
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
INFO: Reading rc options for 'build' from c:\pathtensor\tensorflow-avx2\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/rapid/anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/rapid/anaconda3/lib/site-packages --python_path=C:/Users/rapid/anaconda3/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\pathtensor\tensorflow-avx2\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\pathtensor\tensorflow-avx2\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\pathtensor\tensorflow-avx2\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\pathtensor\tensorflow-avx2\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\pathtensor\tensorflow-avx2\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:opt in file c:\pathtensor\tensorflow-avx2\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\pathtensor\tensorflow-avx2\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\pathtensor\tensorflow-avx2\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\pathtensor\tensorflow-avx2\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\pathtensor\tensorflow-avx2\.bazelrc: --define framework_shared_object=false
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (412 packages loaded, 26776 targets configured).
INFO: Found 1 target...
ERROR: C:/pathtensor/tensorflow-avx2/tensorflow/python/BUILD:5771:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/rapid/_bazel_rapid/kk2upf35/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29333\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
``` 
"
45676,Tensorflow 2.4 takes 3 seconds per epoch during training versus 1 second with TensorFlow 2.3,"Tensorflow 2.4 takes 3 seconds per epoch during training versus 1 second with TensorFlow 2.3

Why is is slower??

When I train, I get this standard log (in case it's helful)

`2020-12-14 18:34:49.552097: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-14 18:34:49.572089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-14 18:34:49.671571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s
2020-12-14 18:34:49.671911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-14 18:34:50.350471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-14 18:34:50.350673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-14 18:34:50.456348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-14 18:34:50.513180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-14 18:34:50.881235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-14 18:34:51.192188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-14 18:34:51.216230: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-14 18:34:51.216525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-14 18:34:51.627424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:83:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s
2020-12-14 18:34:51.627803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-14 18:34:51.627987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-14 18:34:51.628149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-14 18:34:51.628318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-14 18:34:51.628495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-14 18:34:51.628657: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-14 18:34:51.628843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-14 18:34:51.629021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-14 18:34:51.629239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-14 18:34:52.792115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-14 18:34:52.792321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-12-14 18:34:52.792441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-12-14 18:34:52.792890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10243 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:83:00.0, compute capability: 7.0)
2020-12-14 18:34:52.830535: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set`

<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 / 8.02
- GPU model and memory: NVIDIA Titan V 12 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45671,Default Installation is causing errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.4 released 12/14/2020
- Python version: 3.8.6 installed directly from the official site
- Installed using virtualenv? pip? conda?: Nope
- Bazel version (if compiling from source): Nope
- GCC/Compiler version (if compiling from source): Nope
- CUDA/cuDNN version: both 11.1/8.0.5 and 11.0/8.0.2
- GPU model and memory: GEForce GTX1650 with 2 GB RAM



**Describe the problem**
Unable to do a default installation from source
If I use CUDA 11.0 with cuDNN 8.0.2.39 for 11.0, the error is
_E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED_

Alternatively, if I use CUDA 11.1 with cuDNN 8.0.5.39 for 11.1, the error is
_Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices..._

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45670,Unable to read,I'm using Ubuntu
45669,tensorflow 2.4 depends on tensorflow-estimator 2.4 which was not released in tandem,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pipenv 2020.11.15

**Describe the problem**
tensorflow 2.4 depends on `tensorflow-estimator>=2.4,<2.5` but the latest release of tensorflow-estimator on PyPI is 2.3, so resolving dependencies fails.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
$ cat Pipfile
[[source]]
url = ""https://pypi.org/simple""
verify_ssl = true
name = ""pypi""

[packages]
tensorflow = ""==2.4""

[dev-packages]

[requires]
python_version = ""3.7""

$ pip install pipenv==2020.11.15
$ pipenv install
$ pipenv install
Pipfile.lock not found, creating...
Locking [dev-packages] dependencies...
Locking [packages] dependencies...
Building requirements...
Resolving dependencies...
âœ˜ Locking Failed! 
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py"", line 741, in _main
[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages, dev)
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py"", line 709, in resolve_packages
[ResolutionFailure]:       requirements_dir=requirements_dir,
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/resolver.py"", line 692, in resolve
[ResolutionFailure]:       req_dir=requirements_dir
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py"", line 1403, in resolve_deps
[ResolutionFailure]:       req_dir=req_dir,
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py"", line 1108, in actually_resolve_deps
[ResolutionFailure]:       resolver.resolve()
[ResolutionFailure]:   File ""/Users/micahsmith/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pipenv/utils.py"", line 833, in resolve
[ResolutionFailure]:       raise ResolutionFailure(message=str(e))
[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.
  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.
 Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.
  Hint: try $ pipenv lock --pre if it is a pre-release dependency.
ERROR: Could not find a version that matches tensorflow-estimator<2.5.0,>=2.4.0rc0 (from tensorflow==2.4->-r /var/folders/mp/7s96qjnn7tl6nyjk729y16jw0000gn/T/pipenvz7_8pbn3requirements/pipenv-5b66o2rx-constraints.txt (line 2))
Tried: 1.10.6, 1.10.7, 1.10.8, 1.10.9, 1.10.10, 1.10.11, 1.10.12, 1.13.0, 1.14.0, 1.15.0, 1.15.1, 1.15.2, 2.0.0, 2.0.1, 2.1.0, 2.2.0, 2.3.0
Skipped pre-versions: 1.13.0rc0, 1.14.0rc0, 1.14.0rc1, 2.1.0rc0, 2.2.0rc0, 2.3.0rc0, 2.4.0rc0
There are incompatible versions in the resolved dependencies:
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

As of 20:45 UTC (2020-12-14) tensorflow-estimator 2.4 has not been released on PyPI.
<img width=""1236"" alt=""image"" src=""https://user-images.githubusercontent.com/5473606/102133703-815c0100-3e23-11eb-8765-9e6736b098fa.png"">

"
45668,Multi-GPU selection in map_fn and vectorized_map,"**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No



**Current Behaviour**
Currently, both these functions run on a single GPU, even on a multi-GPU machine. This restricts the maximim number of parallel iterations (map_fn) and the amount of memory available for mapping (both).

**Will this change the current api? How?**
This will add another parameter to `tf.map_fn` and `tf.vectorized_map` that specifies the GPUs to use for parallelizing. By default, all visible GPUs will be used.

**Who will benefit with this feature?**
Anyone having a setup with two or more GPUs can benefit from this feature.

**Any Other info.**
For simple parallelism, this may even make it possible to call a model on multiple GPUs without GPU Strategy APIs."
45667,"Build tensorflow from source, issues about CUDA lib and header paths","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.3
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.25.0
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 11.0/8.0.4
- GPU model and memory: GTX 1060 6GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed [this](https://www.tensorflow.org/install/source) tutorial to install tensorflow from source, I would like to link my tensorflow to tensorRT and CUDA/CUDNN, when I ran the configure.py it seems that it couldn't locate the CUDA lib and header files, so I listed all the possible paths for CUDA libs and header files with comma-separated as required in configure, as the following:

/home/myName/Downloads/TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.0/TensorRT-7.2.1.6,
/usr/local/cuda-11.0/targets/x86_64-linux/include,
/usr/local/cuda-11.0/targets/x86_64-linux/lib,
/usr/local/cuda-11.0/bin,
/usr/local/cuda-11.0/nvvm/libdevice,
/usr/local/cuda/nvvm/libdevice

Then it complained that it couldn't find libdevice*.10.bc, but this really doesn't make sense since /usr/local/cuda-11.0/nvvm/libdevice &  /usr/local/cuda/nvvm/libdevice are the paths containing libdevice.10.bc

I have already verified TensorRT's installation by running its sample projects, did I do anything wrong? Thank you in advance!

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
45665,micro: make build warnings cause false-positives while browsing compiler errors via editor,"@tensorflow/micro

make warnings regarding the recipe for person_model_int8 during the micro build are cause annoying, false-positives while browsing complier errors via my editor following a build. E.g.:

    % make -f tensorflow/lite/micro/tools/make/Makefile test
    tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
    tensorflow/lite/micro/tools/make/Makefile:581: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
    tensorflow/lite/micro/tools/make/Makefile:581: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
    g++ -std=c++11 [....]

**System information**
- Host OS Platform and Distribution: Linux Debian bullseye
- GNU Make 4.3
- TensorFlow installed from: source
- Tensorflow version: master at fbcb024d
- Target platform: `make -f tensorflow/lite/micro/tools/make/Makefile test`"
45664,Out of memory in some tests due to GPU memory limit confusion,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**

I have V100 GPUs with ~32GB memory. During startup of the test (many tests show this) I see lines like 
` W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 31614597888 on device 0 within provided limit. [used=0, limit=1073741824]`

Some tests then fail after allocating about 1GB of memory trying to allocate more. The failure message includes the 31GB and shows almost 1GB as used.
E.g. //tensorflow/python/keras/applications:applications_test or //tensorflow/python/keras/layers:convolutional_recurrent_test

**Standalone code to reproduce the issue**
Run bazel test

**Other info / logs**
```
2020-12-14 12:27:32.863559: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:32.863607: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864373: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864404: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864421: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00MiB (rounded to 4194304)requested by op Tanh
Current allocation summary follows.
2020-12-14 12:27:42.864434: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc
...
2020-12-14 12:27:42.866982: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 928.00MiB
2020-12-14 12:27:42.866989: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 976990208 memory_limit_: 31614597888 available bytes: 30637607680 curr_region_allocation_bytes_: 63229195776
2020-12-14 12:27:42.867000: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: 
Limit:                     31614597888
InUse:                       973083648
MaxInUse:                    973083648
NumAllocs:                        2238
MaxAllocSize:                 86081536
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2020-12-14 12:27:42.867013: W tensorflow/core/common_runtime/bfc_allocator.cc:441] ****************************************************************************************************
2020-12-14 12:27:42.867035: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_op_gpu_base.cc:97 : Resource exhausted: OOM when allocating tensor with shape[32,32,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

```"
45663,No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**
A test shows that a GPU implementation for BOOL inputs of ResourceScatterNdUpdate is seemingly missing.
The test is //tensorflow/python/kernel_tests:batch_scatter_ops_test -> ScatterTest.testBooleanScatterUpdate

**Standalone code to reproduce the issue**
Run bazel test

**Other info / logs**
```
ERROR: testBooleanScatterUpdate (__main__.ScatterTest)
ScatterTest.testBooleanScatterUpdate
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/batch_scatter_ops_test.py"", line 91, in testBooleanScatterUpdate
    update0 = state_ops.batch_scatter_update(var, [1], [True])
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 340, in new_func
    return func(*args, **kwargs)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py"", line 915, in batch_scatter_update
    ref, final_indices, updates, use_locking=use_locking)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py"", line 368, in scatter_nd_update
    name=name))
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_state_ops.py"", line 740, in resource_scatter_nd_update
    _ops.raise_from_not_ok_status(e, name)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterNdUpdate}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tindices=DT_INT32, use_locking=true
	.  Registered:  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
 [Op:ResourceScatterNdUpdate]
```

"
45662,Wrong device returned for GPUCompatibleFIFOQueueTests.testEnqueueDequeue test,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**
In the //tensorflow/python/kernel_tests:fifo_queue_test target the GPUCompatibleFIFOQueueTests.testEnqueueDequeue returns the wrong device: ` /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0`

See below log

**Standalone code to reproduce the issue**
Run bazel test

**Other info / logs** 

```
FAIL: testEnqueueDequeue (__main__.GPUCompatibleFIFOQueueTests)
GPUCompatibleFIFOQueueTests.testEnqueueDequeue
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1221, in decorated
    run_eagerly(self, **kwargs)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1205, in run_eagerly
    f(self, *args, **kwargs)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/fifo_queue_test.py"", line 425, in testEnqueueDequeue
    self.assertEqual(elems[0].device, dequeued_tensor.device)
AssertionError: 
- /job:localhost/replica:0/task:0/device:CPU:0
?                                        ^
+ /job:localhost/replica:0/task:0/device:GPU:0
?                                        ^

```"
45661,"Test failures with ""OpError not raised""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**

Some tests fail with ""OpError not raised"": 
- //tensorflow/python/feature_column:feature_column_test  & //tensorflow/python/feature_column:feature_column_v2_test : IdentityCategoricalColumnTest.test_get_sparse_tensors_with_inputs_too_big
- //tensorflow/python/kernel_tests:batch_scatter_ops_test : ScatterTest.testScatterOutOfRange

**Standalone code to reproduce the issue**
Run bazel test
"
45660,Seg faults in various tests,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**

Some tests error with Signal 11 / Segfault: //tensorflow/python:convert_to_constants_test, //tensorflow/python/distribute:parameter_server_strategy_v2_test, //tensorflow/python/keras/tests:convert_to_constants_test, //tensorflow/python/kernel_tests:map_ops_test

**Standalone code to reproduce the issue**
Run bazel test on the above targets

**Other info / logs**
Example log:

```
2020-12-14 13:06:37.124411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize
  function_optimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 0.346ms.
  function_optimizer: Graph size after: 5 nodes (0), 4 edges (0), time = 0.343ms.
Optimization results for grappler item: while_1_body_2134
  function_optimizer: function_optimizer did nothing. time = 0ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
Optimization results for grappler item: while_1_cond_2133
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.

Fatal Python error: Segmentation fault

Thread 0x00002000000484c0 (most recent call first):
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 60 in quick_execute
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 560 in call
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1919 in _call_flat
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1736 in _call_with_flat_signature
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1687 in _call_impl
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/wrap_function.py"", line 247 in _call_impl
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/eager/function.py"", line 1669 in __call__
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py"", line 186 in _testConvertedFunction
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py"", line 466 in testStatelessWhile
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1436 in decorated
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/case.py"", line 628 in run
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/case.py"", line 676 in __call__
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py"", line 122 in run
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py"", line 84 in __call__
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py"", line 122 in run
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/suite.py"", line 84 in __call__
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/runner.py"", line 176 in run
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/main.py"", line 271 in runTests
  File ""/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/unittest/main.py"", line 101 in __init__
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2404 in _run_and_get_tests_result
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2434 in run_tests
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2109 in _run_in_app
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/testing/absltest.py"", line 2002 in main
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 56 in g_main
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/app.py"", line 251 in _run_main
  File ""/tmp/ebinstall/software/TensorFlow/2.4.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/absl/app.py"", line 303 in run
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 40 in run
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 65 in main_wrapper
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 486 in benchmarks_main
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 66 in main
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 58 in main
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants_test.py"", line 1185 in <module>
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xbeae3c)[0x2002f0cbae3c]
[0x2000000504d8]
/scratch/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.7.4-GCCcore-8.3.0/lib/libpython3.7m.so.1.0(+0x9089c)[0x20000010089c]
[0x2000000504d8]
[0x83bce103d9edaebe]
[0x49ad6880]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8BinaryOpIN5Eigen16ThreadPoolDeviceENS_7functor3addIiEEE7ComputeEPNS_15OpKernelContextE+0x560)[0x2002d4b0a9b0]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x298)[0x2002f0b142e8]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xb377d0)[0x2002f0c077d0]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x3a8)[0x2002cf8d00e8]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEEC4EibS5_EUlvE_E9_M_invokeERKSt9_Any_data+0x28)[0x2002cf8d1298]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x84)[0x2002cf8cc1f4]
/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/convert_to_constants_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0xbf8b00)[0x2002f0cc8b00]
/lib64/libpthread.so.0(+0x8b94)[0x200000708b94]
/lib64/libc.so.6(clone+0xe4)[0x2000008985f4]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace[abi:cxx11]()
	
	__kernel_sigtramp_rt64
	
	__kernel_sigtramp_rt64
	
	
	tensorflow::BinaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::add<int> >::Compute(tensorflow::OpKernelContext*)
	tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)
	
	Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)
	std::_Function_handler<void (), Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&)
	std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)
	
	
	clone
*** End stack trace ***
```
"
45659,Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**

The following tests fail with the error ""Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string"": //tensorflow/python/ops/ragged:ragged_dispatch_test, //tensorflow/python/ops/ragged:ragged_map_fn_op_test, //tensorflow/python/ops/ragged:ragged_print_op_test, //tensorflow/python/ops/ragged:ragged_tensor_test

**Standalone code to reproduce the issue**
Running `bazel test` on the above targets

**Other info / logs**

Example log:

```
ERROR: testRaggedToStringescape (__main__.RaggedToStringTest)
RaggedToStringTest.testRaggedToStringescape
testRaggedToStringescape([[""a'b""], ['c\\d']], ""[['a\\'b'], ['c\\\\d']]"")
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1360, in _run_fn
    target_list, run_metadata)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/ops/ragged/ragged_print_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
  (1) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
0 successful operations.
0 derived errors ignored.
	 [[{{node AsString/map/TensorArrayUnstack/TensorListFromTensor}}]]
	 [[Func/AsString/map/while/body/_1/input/_41/_24]]
  (1) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
  (1) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
0 successful operations.
0 derived errors ignored.
	 [[{{node AsString/map/TensorArrayUnstack/TensorListFromTensor}}]]
0 successful operations.
0 derived errors ignored.
```
"
45658, Error during training: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed from conda
- TensorFlow version (use command below): 2.1.0
- Python version: Python 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda64_101, cudnn64_7
- GPU model and memory: GeForce RTX 2080 Super, 8 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

During training neural network on 17th epoch I faced error:

```F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1```

I tried rerun many times and every time failed epoch number of training was different.

**Describe the expected behavior**

I think training should be stable.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

I deployed this repo: https://github.com/arthurflor23/handwritten-text-recognition

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This code:
```python
import tensorflow as tf

sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))
```

gives me:

```
2020-12-14 19:14:24.943891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-14 19:14:26.611932: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-12-14 19:14:26.614457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-12-14 19:14:26.644016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Super computeCapability: 7.5
coreClock: 1.56GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-14 19:14:26.644168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-14 19:14:26.647233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-14 19:14:26.649912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-14 19:14:26.651042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-14 19:14:26.654689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-14 19:14:26.656359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-14 19:14:26.662690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-14 19:14:26.662820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-12-14 19:14:27.098083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-14 19:14:27.098175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-12-14 19:14:27.098252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-12-14 19:14:27.098438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6265 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Super, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-12-14 19:14:27.101774: I tensorflow/core/common_runtime/direct_session.cc:358] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Super, pci bus id: 0000:01:00.0, compute capability: 7.5
```

```
(tf_gpu) D:\repositories\handwritten-text-recognition\src>python main.py --source=bentham --train
Weights are from ..\output\bentham\flor\checkpoint_weights.hdf5
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 1024, 128, 1)]    0
_________________________________________________________________
conv2d (Conv2D)              (None, 1024, 64, 16)      160
_________________________________________________________________
p_re_lu (PReLU)              (None, 1024, 64, 16)      16
_________________________________________________________________
batch_normalization (BatchNo (None, 1024, 64, 16)      112
_________________________________________________________________
full_gated_conv2d (FullGated (None, 1024, 64, 16)      4640
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1024, 64, 32)      4640
_________________________________________________________________
p_re_lu_1 (PReLU)            (None, 1024, 64, 32)      32
_________________________________________________________________
batch_normalization_1 (Batch (None, 1024, 64, 32)      224
_________________________________________________________________
full_gated_conv2d_1 (FullGat (None, 1024, 64, 32)      18496
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 512, 16, 40)       10280
_________________________________________________________________
p_re_lu_2 (PReLU)            (None, 512, 16, 40)       40
_________________________________________________________________
batch_normalization_2 (Batch (None, 512, 16, 40)       280
_________________________________________________________________
full_gated_conv2d_2 (FullGat (None, 512, 16, 40)       28880
_________________________________________________________________
dropout (Dropout)            (None, 512, 16, 40)       0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 512, 16, 48)       17328
_________________________________________________________________
p_re_lu_3 (PReLU)            (None, 512, 16, 48)       48
_________________________________________________________________
batch_normalization_3 (Batch (None, 512, 16, 48)       336
_________________________________________________________________
full_gated_conv2d_3 (FullGat (None, 512, 16, 48)       41568
_________________________________________________________________
dropout_1 (Dropout)          (None, 512, 16, 48)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 256, 4, 56)        21560
_________________________________________________________________
p_re_lu_4 (PReLU)            (None, 256, 4, 56)        56
_________________________________________________________________
batch_normalization_4 (Batch (None, 256, 4, 56)        392
_________________________________________________________________
full_gated_conv2d_4 (FullGat (None, 256, 4, 56)        56560
_________________________________________________________________
dropout_2 (Dropout)          (None, 256, 4, 56)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 256, 4, 64)        32320
_________________________________________________________________
p_re_lu_5 (PReLU)            (None, 256, 4, 64)        64
_________________________________________________________________
batch_normalization_5 (Batch (None, 256, 4, 64)        448
_________________________________________________________________
reshape (Reshape)            (None, 256, 256)          0
_________________________________________________________________
bidirectional (Bidirectional (None, 256, 256)          296448
_________________________________________________________________
dense (Dense)                (None, 256, 256)          65792
_________________________________________________________________
bidirectional_1 (Bidirection (None, 256, 256)          296448
_________________________________________________________________
dense_1 (Dense)              (None, 256, 98)           25186
=================================================================
Total params: 922,354
Trainable params: 921,074
Non-trainable params: 1,280
_________________________________________________________________
Train for 1101 steps, validate for 172 steps
Epoch 1/1000
1100/1101 [============================>.] - ETA: 0s - loss: 19.5034
Epoch 00001: val_loss improved from inf to 18.13556, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 150s 136ms/step - loss: 19.5006 - val_loss: 18.1356
Epoch 2/1000
1100/1101 [============================>.] - ETA: 0s - loss: 18.7811
Epoch 00002: val_loss did not improve from 18.13556
1101/1101 [==============================] - 140s 127ms/step - loss: 18.7732 - val_loss: 18.9815
Epoch 3/1000
1100/1101 [============================>.] - ETA: 0s - loss: 17.4834
Epoch 00003: val_loss did not improve from 18.13556
1101/1101 [==============================] - 140s 127ms/step - loss: 17.4750 - val_loss: 18.3697
Epoch 4/1000
1100/1101 [============================>.] - ETA: 0s - loss: 16.9503
Epoch 00004: val_loss improved from 18.13556 to 17.28087, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 140s 127ms/step - loss: 16.9409 - val_loss: 17.2809
Epoch 5/1000
1100/1101 [============================>.] - ETA: 0s - loss: 16.1360
Epoch 00005: val_loss improved from 17.28087 to 16.63544, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 126ms/step - loss: 16.1276 - val_loss: 16.6354
Epoch 6/1000
1100/1101 [============================>.] - ETA: 0s - loss: 15.7264
Epoch 00006: val_loss improved from 16.63544 to 16.15779, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 140s 128ms/step - loss: 15.7176 - val_loss: 16.1578
Epoch 7/1000
1100/1101 [============================>.] - ETA: 0s - loss: 15.0694
Epoch 00007: val_loss improved from 16.15779 to 15.39602, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 127ms/step - loss: 15.0607 - val_loss: 15.3960
Epoch 8/1000
1100/1101 [============================>.] - ETA: 0s - loss: 14.6364
Epoch 00008: val_loss improved from 15.39602 to 15.06812, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 126ms/step - loss: 14.6277 - val_loss: 15.0681
Epoch 9/1000
1100/1101 [============================>.] - ETA: 0s - loss: 14.4449
Epoch 00009: val_loss improved from 15.06812 to 15.01459, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 127ms/step - loss: 14.4367 - val_loss: 15.0146
Epoch 10/1000
1100/1101 [============================>.] - ETA: 0s - loss: 14.1694
Epoch 00010: val_loss improved from 15.01459 to 14.35110, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 127ms/step - loss: 14.1645 - val_loss: 14.3511
Epoch 11/1000
1100/1101 [============================>.] - ETA: 0s - loss: 13.7056
Epoch 00011: val_loss improved from 14.35110 to 13.85971, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 139s 126ms/step - loss: 13.6979 - val_loss: 13.8597
Epoch 12/1000
1100/1101 [============================>.] - ETA: 0s - loss: 13.3614
Epoch 00012: val_loss did not improve from 13.85971
1101/1101 [==============================] - 140s 127ms/step - loss: 13.3553 - val_loss: 13.9131
Epoch 13/1000
1100/1101 [============================>.] - ETA: 0s - loss: 13.0623
Epoch 00013: val_loss improved from 13.85971 to 13.21627, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 140s 127ms/step - loss: 13.0562 - val_loss: 13.2163
Epoch 14/1000
1100/1101 [============================>.] - ETA: 0s - loss: 12.9299
Epoch 00014: val_loss did not improve from 13.21627
1101/1101 [==============================] - 141s 128ms/step - loss: 12.9227 - val_loss: 13.3021
Epoch 15/1000
1100/1101 [============================>.] - ETA: 0s - loss: 12.6765
Epoch 00015: val_loss improved from 13.21627 to 13.18161, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 141s 128ms/step - loss: 12.6724 - val_loss: 13.1816
Epoch 16/1000
1100/1101 [============================>.] - ETA: 0s - loss: 12.4314
Epoch 00016: val_loss improved from 13.18161 to 13.12220, saving model to ..\output\bentham\flor\checkpoint_weights.hdf5
1101/1101 [==============================] - 142s 129ms/step - loss: 12.4244 - val_loss: 13.1222
Epoch 17/1000
 130/1101 [==>...........................] - ETA: 1:56 - loss: 10.35982020-12-14 17:54:37.809588: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
```
"
45657,micro: port op FLOOR_DIV from lite,"
@tensorflow/micro

This issue tracks my work porting operator FLOOR_DIV from lite to micro.

The port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:

PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver
PR 2: Extract the reference implementation out of tensorflow/lite/kernels/floor_div.cc into its own header which can be included without dragging in reference_ops.h's dependences
PR 3: Copy operator from lite to micro without making any changes or including in the build
PR 4: Delete extra code from the micro copy of the operator
PR 5: Port micro copy of operator as necessary and add a corresponding test"
45656,sum/avg/mean ops of tf.keras.backend do not work correctly over batch axis,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1 (docker)
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.243/7.6.4.38-1
- GPU model and memory: 1080-TI

**Describe the current behavior**
When adding an aggregation function on the batch axis (axis=0), everything is fine, as long as the batchsize is <=32, with batchsize >32 the output shape changes.

Code and test tp reproduce on:
https://stackoverflow.com/questions/64893958/sum-avg-mean-ops-of-tf-keras-backend-do-not-work-correctly-over-batch-axis

**Describe the expected behavior**
output shape is the expected on from model summary and not depending on the batchsize.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
see https://stackoverflow.com/questions/64893958/sum-avg-mean-ops-of-tf-keras-backend-do-not-work-correctly-over-batch-axis

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45653,RTX3070 CUDA 11.1 CuDNN 8.0.5 not GPU available with tensorflow 2.5 (nightly version),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
- TensorFlow installed from (source or binary): binary 
- TensorFlow version: tf-nightly-gpu==2.5.0.dev20201213
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: installed via anaconda (conda 4.9.2) using pip install tf-nightly-gpu
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8.0.5
- GPU model and memory: Geforce RTX 3070 8 Go (Driver 460.79)


**Describe the problem**
I have encountered a problem when I tried to install tensorflow-gpu in my anaconda environment.

When I tested the availability of GPU after import of tensorflow, it seems that cusolver64_10.dll is missing and I cannot use my GPU with Tensorflow. From what I know, Tensorflow is not available for Cuda toolkit 11.1. So, should I try to compile Tensorflow or show I try older version of Cuda toolkit for example in virtual machine ?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
(deeplearning-tf) C:\Users\Utilisateur>python
Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflos as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflos'
>>> import tensorflow as tf
2020-12-14 12:30:59.774387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2020-12-14 12:31:28.599495: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-14 12:31:28.631910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties:
pciBusID: 0000:2b:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-14 12:31:28.631996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-14 12:31:29.003118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-14 12:31:29.003522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-14 12:31:29.266489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-14 12:31:29.287591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-14 12:31:29.288565: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2020-12-14 12:31:29.462695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-14 12:31:29.475294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-14 12:31:29.475391: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Num GPUs Available:  0


Thank you in advance for your help.
Best
Enrico"
45652,Can't load Model with SparseTensor inputs if it calls a Sequential model internally,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7

**Describe the current behavior**

When using model-sublcassing to create a `Model` that: 

1. Has a SparseTensor input
2. Uses a Sequential model as internal layer

the model cannot be loaded using `tf.keras.models.load_model`.

The issue is twofold, and I believe it is related to how the signature of `call` is computed upon saving/loading. 

The first problem is that `SparseTensorSpec` does not have a `name` attribute and thus the call to

https://github.com/tensorflow/tensorflow/blob/dd3499e26c7dceaad5333ea0762903de242150b5/tensorflow/python/keras/saving/saved_model/load.py#L812

fails when it has to deal with sparse tensors. 

The second problem is that the signature of the model cannot be properly restored if it uses a Sequential model internally. This, I can't explain. 
Whatever the cause, removing the Sequential model causes the `load_model` method to take this path instead: 

https://github.com/tensorflow/tensorflow/blob/dd3499e26c7dceaad5333ea0762903de242150b5/tensorflow/python/keras/saving/saved_model/load.py#L814

thus avoiding the crash. 

The title of the issue is a bit weird, I didn't know how to describe the problem concisely :D

**Describe the expected behavior**

A Model's signature should be independent of the sub-modules used in its call function and the saving/loading API should support SparseTensors. 

I am happy to contribute code to solve this issue, but I would need some guidance on how to best proceed. 

**Standalone code to reproduce the issue**
```py
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import save_model, load_model


class Works(Model):
    def build(self, input_shape):
        self.fc = Dense(32)

    def call(self, inputs):
        a, b = inputs
        b = self.fc(b)
        return tf.sparse.sparse_dense_matmul(a, b)


class Crashes(Model):
    def build(self, input_shape):
        self.fc = Sequential([Dense(32)])  # <<<< THIS IS THE ONLY DIFFERENCE

    def call(self, inputs):
        a, b = inputs
        b = self.fc(b)
        return tf.sparse.sparse_dense_matmul(a, b)

# Inputs
a = tf.sparse.from_dense(tf.ones((10, 10)))
b = tf.ones((10, 10))

# This works OK, no Sequential model
works = Works()
works([a, b])
save_model(works, 'works')
load_model('works')

# This crashes, it uses a Sequential model
crashes = Crashes()
crashes([a, b])
save_model(crashes, 'crashes')
load_model('crashes')  # <<<< FAILS

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```py
Traceback (most recent call last):
  File ""~/dev/test.py"", line 38, in <module>
    load_model('crashes')
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 121, in load
    path, options=options, loader_cls=KerasObjectLoader)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 633, in load_internal
    ckpt_options)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 130, in __init__
    self._load_all()
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 221, in _load_all
    self._finalize_objects()
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 526, in _finalize_objects
    _finalize_saved_model_layers(layers_revived_from_saved_model)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 706, in _finalize_saved_model_layers
    inputs = infer_inputs_from_restored_call_function(call_fn)
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 985, in infer_inputs_from_restored_call_function
    spec = nest.map_structure(common_spec, spec, spec2)
  File ~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""~/dev/tf2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 981, in common_spec
    x.dtype, x.name)
AttributeError: 'SparseTensorSpec' object has no attribute 'name'

```
"
45651,Mask RCNN tflite inference on android,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): nightly


**Provide the text output from tflite_convert**

```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: /tmp/tmpjivs_7gq/assets
```

**Standalone code to reproduce the issue** 

int imageTensorIndex = 0;
        int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}
        imageSizeY = imageShape[1];
        imageSizeX = imageShape[2];
        DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();

TensorImage inputImageBuffer = new TensorImage(imageDataType);
 inputImageBuffer.load(bitmap);

int[] probabilityShape = tflite.getOutputTensor(probabilityTensorIndex).shape(); // {1, NUM_CLASSES}
DataType probabilityDataType = tflite.getOutputTensor(probabilityTensorIndex).dataType();
// Creates the output tensor and its processor.
outputProbabilityBuffer = TensorBuffer.createFixedSize(probabilityShape, probabilityDataType);
tflite.run(inputImageBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());

Also, please include a link to a GraphDef or the model if possible.

[{'name': 'input_image', 'index': 0, 'shape': array([   1, 1024, 1024,    3], dtype=int32), 'shape_signature': array([  -1, 1024, 1024,    3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_image_meta', 'index': 1, 'shape': array([ 1, 18], dtype=int32), 'shape_signature': array([-1, 18], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_anchors', 'index': 2, 'shape': array([     1, 261888,      4], dtype=int32), 'shape_signature': array([    -1, 261888,      4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'Identity', 'index': 662, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 740, 'shape': array([   1, 1000,    6,    4], dtype=int32), 'shape_signature': array([   1, 1000,    6,    4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 744, 'shape': array([   1, 1000,    6], dtype=int32), 'shape_signature': array([   1, 1000,    6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_3', 'index': 864, 'shape': array([  1, 100,   6], dtype=int32), 'shape_signature': array([  1, 100,   6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_4', 'index': 936, 'shape': array([  1, 100,  28,  28,   6], dtype=int32), 'shape_signature': array([  1, 100,  28,  28,   6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_5', 'index': 578, 'shape': array([1, 1, 4], dtype=int32), 'shape_signature': array([-1, -1,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_6', 'index': 591, 'shape': array([1, 1, 2], dtype=int32), 'shape_signature': array([-1, -1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

These are the input and output tensors of the model.

--------- beginning of crash
2020-12-11 17:27:42.138 16752-16752/com.objdetector A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 16752 (com.objdetector), pid 16752 (com.objdetector)

This is the segmentation fault error

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

How exactly should I run inference on android for Mask RCNN tflite model? I have added these dependencies:

dependencies {
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    // This dependency adds the necessary TF op support.
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
}

There seem to be 3 inputs and 7 outputs to the model.. what do i need to pass as input, should I pass image, image_meta as well as input anchors? Which of the outputs will provide the result? Do I need to map all the outputs? How can I get the result?

"
45650,  W/native: op_kernel.cc:1401 OP_REQUIRES failed at save_restore_tenProplem restoring the checkpoint in my android app: sor.cc:175 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for [B@66ce18,"I have created a simple tensorflow model and trained it on device using tensorflow java api. I send the weights to a server that I created and there I average them with other phones weights and created a upgrated model so I can do Federated Learning. 

When I download the checkpoint_name.ckpt.meta file so I can restore the checkpoint in the app I got one error that I dont understand. I run this code 


Graph graph = new Graph();
graph.importGraphDef(graphdef);
...
checkpoinPrefix = org.tensorflow.Tensors.create(""checkpoint_name.ckpt.meta""); 
...
...
sess.runner().feed(""save/Const"", checkpointPrefix).addTarget(""save/restore_all"").run();

**error:**
W/native: op_kernel.cc:1401 OP_REQUIRES failed at save_restore_tensor.cc:175 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for [B@66ce18"
45649,Support `class_weight` for TimeSeries models in Keras (`model.fit`),"**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): As much as I can

**Describe the feature and the current behavior/state.**
Currently it is only possible to specify `class_weight` in `tf.keras.engine.training.Model.fit` for y (labels) with at most 2 dimensions `(batch, class)`. Would it be possible to support weights for TimeSeries classification models with three-dimensional labels `(batch, time_step, class)`?

Note that it is currently possible to specify timestep-specific weights for individual input samples using `sample_weights`.

**Will this change the current api? How?**
Maybe. Currently, `class_weight` is accepted in the form of `Dict[int, float]`. It may be possible to change the format to `np.array` with 1 or 2 dimensions. It is also possible to maintain the current argument format and apply the same class weights to each time-step uniformly 

**Who will benefit with this feature?**
People who are training time-series classification models with imbalanced classes.

**Any Other info.**
n/a"
45648, 2.4.0 sess.run freezing in Windows 10 with RTX 3090,"my DeepFaceLab software uses tf 2.3.1 with compat.v1, it works fine.
When I upgrade to 2.4.0rc4, tf sess.run is freezed without any errors.
Seems like it depends on model complexity.
Models with big graph freeze instantly.
Model with simpler graph may be freezed in 30 minutes.
I don't know how to write reproduction code.

OS: Windows 10

Windows 10 setting
Hardware-accelerated GPU scheduling to â€œ default graphics setting â€, solves the problem.

![fwy9iW8 1](https://user-images.githubusercontent.com/8076202/102058534-11408180-3e09-11eb-9bea-839dce2e9870.jpg)
"
45645,Can't Install TF on Apple M1,"**Issue Creation**
On trying to install TF 2.3 using `pip install tensorflow==2.3` from the terminal, I got a success message.
I could see tensorflow 2.3 in the list of installed packages by using either `conda list` or `pip list`

When I run python from the terminal and trying to import tensorflow, it returns this error message:

```
(tfenv) mgd@MGD-m1 ~ % conda list tensorflow
# packages in environment at /Users/mgd/opt/anaconda3/envs/tfenv:
#
# Name                    Version                   Build  Channel
tensorflow                2.3.0                    pypi_0    pypi
tensorflow-datasets       4.1.0                    pypi_0    pypi
tensorflow-estimator      2.3.0                    pypi_0    pypi
tensorflow-metadata       0.26.0                   pypi_0    pypi
(tfenv) mgd@MGD-m1 ~ % python
Python 3.8.5 (default, Sep  4 2020, 02:22:02) 
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
zsh: illegal hardware instruction  python
(tfenv) mgd@MGD-m1 ~ % 
```

on uninstalling it and trying to installing again using:
`pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.3.0-cp38-cp38-macosx_10_14_x86_64.whl`
same issue

Same scenario on trying latest version. i.e. by using just `pip install tensorflow` to get 2.3.1

On trying to import it from Jupyter Lab, no error message is shown and also no interaction is happening. It's as if it's empty cell.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.0.1
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.3.1
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip


"
45644,what are the .pb and .h5  model file?,"```
model = create_model()
model.fit(train_images, train_labels, epochs=5)

!mkdir -p saved_model

model.save('saved_model/my_model') 

model.save('my_model.h5') 
```
what's the .pb  model?
What are the differences and similarities between it and TensorRT's engine file?"
45642,Tensorflow 1.x binary not found with pip install (Python=3.7),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.15
- Python version: 3.7.9
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
Tensorflow 1.x (specifically 1.15) binary cannot be found when installing via `pip` 

```
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)
ERROR: No matching distribution found for tensorflow-gpu==1.15
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
conda create -n tf1 python=3.7
conda activate tf1
pip install tensorflow-gpu==1.15
```

"
45637,Is input_shape on Dense for public use?,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense

https://www.tensorflow.org/tutorials/customization/custom_layers

## Description of issue (what needs changing):

### Clear description

[The API documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) doesn't mention we can pass `input_shape` and `input_dim` to tf.keras.layers.Dense,  but various (official) resources use it, eg. https://www.tensorflow.org/tutorials/customization/custom_layers.

Since the official authoritive API documentation doesn't claim the existance of `input_shape`, readers get very confused why other resources make extensive use of `input_shape`.

I checked the source code, which indeed accesses input_shape.

https://github.com/tensorflow/tensorflow/blob/1987dba1a166d0336ccea8176dd2a16a65ddf19a/tensorflow/python/keras/engine/base_layer.py#L318-L339

Is `input_shape` a private parameter but setting it through user code is an acceptable exploit?

Or is it public and we just forget to add it on the documentation?




"
45636,tf.enable_eager_execution() not working,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- CUDA/cuDNN version: cuDNN 7.6.5
- GPU model and memory: NVIDIA GTX 1050

**Describe the current behavior**
I am trying to use the function tensor.numpy(), which requires eager_excecution to be enabled. I checked online, and it said that Tensorflow 2.0 has eager_execution enabled by default. However, when I run `print(tf.executing_eagerly())` the output is `False`

I have tried the `tf.compat.v1.enable_eager_execution()` function, but it does not seem to change anything


**Standalone code to reproduce the issue**
```
import tensorflow as tf
tf.compat.v1.enable_eager_execution()
print(tf.executing_eagerly())

```
"
45635,"RTX 3060 TI, creating GPU device take about 5 Minutes","Windows 10 Prof.
RTX 3060 TI
TensorFlow 2.1.0
Installer: Anaconda

Note: Same with Ubuntu 20.04 LTS and PIP Installing

Code:

```
import tensorflow as tf
tf.test.is_gpu_available()
```
Output:

WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2020-12-13 10:23:03.545074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-12-13 10:23:03.555731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-12-13 10:23:03.609505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-13 10:23:03.621055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-13 10:23:03.636314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-13 10:23:03.649406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-13 10:23:03.657126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-13 10:23:03.672274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-13 10:23:03.682682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-13 10:23:03.704980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-13 10:23:03.709446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-12-13 10:28:51.751301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 10:28:51.758864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-12-13 10:28:51.763278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-12-13 10:28:51.768347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device 

**<---- about 5 minutes nothing ---->**

(/device:GPU:0 with 6281 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
True
"
45634,Mapping LSTM over arbitrary tensor shapes,"**System information**
- TensorFlow version 2.3.1:

Dense layers map over tensors of up to rank 5, by just being applied to the last dimension:

```
import tensorflow as tf
m = tf.keras.Sequential([tf.keras.layers.Dense(1)])
print(m(tf.ones((10, 9, 8, 6, 5))).shape)
```
So they consume 1 dimension and allow for a freedom of 4 dimensions of additional structure (where the first one usually represents a batch)

Why is this different for LSTM layers, which only support inputs of rank 3:

This works:
 ```
import tensorflow as tf
m = tf.keras.Sequential([tf.keras.layers.LSTM(1)])
print(m(tf.ones((8, 6, 5))).shape)
```

This doesn't work:
 ```
import tensorflow as tf
m = tf.keras.Sequential([tf.keras.layers.LSTM(1)])
print(m(tf.ones((9, 8, 6, 5))).shape)
```
leading to following error: 

```
ValueError: Input 0 of layer sequential_7 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [9, 8, 6, 5]
```

Also LSTM layers consume 2 dimensions, so there is only 1 additional dimension, which is usually required to be the batch.

I am wondering why there are these limitations and if the LSTM layers could at least made equal to the dense layers in that regard. I am aware that through reshaping one can emulate that behaviour but this raises the question if it introduces an unnecessary computational overhead. In addition to that it is convenient to work with `None` dimensions which makes the reshaping problematic in graph mode. 
"
45633,ERROR: An error occurred during the fetch of repository 'com_google_protobuf',"- Windows 10 - fresh installation to reproduce this problem (after experiencing this issue on another Windows 10 machine).
- Following the instructions in https://www.tensorflow.org/install/source_windows (without installing MSYS2).
- TensorFlow version: r2.3 and latest (tried both)
- Python version: 3.8.6
- Bazel version: 3.1.0 and 3.7.1 (both were tried and give the same results)
- Compiler version: according to the instructions as mentioned above (Install Visual C++ Build Tools 2019)

**Describe the problem**
For r2.3, the compilation starts as usual and fail with:
```
...
Repository rule tf_http_archive defined at:
  C:/users/ranip/documents/tensorflow/third_party/repo.bzl:134:19: in <toplevel>
ERROR: An error occurred during the fetch of repository 'com_google_protobuf':
   Traceback (most recent call last):
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 110
                _apply_patch(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 67, in _apply_patch
                _wrap_bash_cmd(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(<1 more arguments>)
BAZEL_SH environment variable is not set
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 110
                _apply_patch(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 67, in _apply_patch
                _wrap_bash_cmd(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(<1 more arguments>)
BAZEL_SH environment variable is not set
INFO: Elapsed time: 180.653s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (8 packages loaded, 12 targets configured)
    currently loading: tensorflow
```

For latest master, it fails with:
```
...
Repository rule third_party_http_archive defined at:
  C:/users/ranip/documents/tensorflow/third_party/repo.bzl:216:28: in <toplevel>
ERROR: An error occurred during the fetch of repository 'flatbuffers':
   Traceback (most recent call last):
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 73, in _apply_delete
                _wrap_bash_cmd(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(<1 more arguments>)
BAZEL_SH environment variable is not set
ERROR: C:/users/ranip/documents/tensorflow/tensorflow/tools/pip_package/BUILD:281:1: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/lite/python:tflite_convert in repository @ which failed to fetch. no such package '@flatbuffers//': Traceback (most recent call last):
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 73, in _apply_delete
                _wrap_bash_cmd(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(<1 more arguments>)
BAZEL_SH environment variable is not set
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@flatbuffers//': Traceback (most recent call last):
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 73, in _apply_delete
                _wrap_bash_cmd(ctx, <1 more arguments>)
        File ""C:/users/ranip/documents/tensorflow/third_party/repo.bzl"", line 28, in _wrap_bash_cmd
                fail(<1 more arguments>)
BAZEL_SH environment variable is not set
INFO: Elapsed time: 174.239s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (50 packages loaded, 14 targets configured)
    currently loading: tensorflow/lite/python
```

I am aware of issues https://github.com/tensorflow/tensorflow/issues/37897 and https://github.com/tensorflow/tensorflow/issues/35414 - no ""Windows Subsystem for Linux"" was installed on this Windows - as said - it is a fresh Windows 10 install, just to reproduce this problem. Did not use gitbash, or any Unix like shell.  Only Powershell was used. 
"
45632,Conda install Tensorflow 2.1 with mkl (avx/avx2 support) not working,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tensorflow 2.1 with mkl binary from Anaconda
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.9 (default, Aug 31 2020, 17:10:11)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: na
- GPU model and memory: na

**Describe the current behavior**
tensorflow complains that ""Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2"".

**Describe the expected behavior**
With Intel's mkl version, we shouldn't get this error message. 

**Standalone code to reproduce the issue**
I have tensorflow2.0 with mkl. 

Here's the code:

```
import tensorflow as tf
from tensorflow import keras

input_A = keras.layers.Input(shape=[5], name=""wide_input"")
input_B = keras.layers.Input(shape=[6], name=""deep_input"")
hidden1 = keras.layers.Dense(30, activation=""relu"")(input_B)
hidden2 = keras.layers.Dense(30, activation=""relu"")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name=""output"")(concat)
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])

```

Here's the output:
```
2020-12-12 17:06:46.037846: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-12-12 17:06:46.040928: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
```


Here's the mkl check from https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html
Code:
```
major_version = int(tf.__version__.split(""."")[0])
if major_version >= 2:
   from tensorflow.python import _pywrap_util_port
   print(""MKL enabled:"", _pywrap_util_port.IsMklEnabled())
else:
   print(""MKL enabled:"", tf.pywrap_tensorflow.IsMklEnabled()) 
```
Output:

`MKL enabled: True
`


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Please let me know how I can assist in debugging this issue."
45631,Is there any support anywhere for Tensorflow on Apple Silicon?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Not sure what this means?



**Describe the feature and the current behavior/state.**

**Will this change the current API? How?**
Not sure. I'm unaware of what changes need to be made.

**Who will benefit with this feature?**
People who own machines running Apple Silicon. 

**Any Other info.**
Currently trying to import TF and Keras and I keep getting an error stating that it's an illegal hardware instruction.
"
45630,Help with object detection project,"Hi all,

I successfully trained my custom ssd_mobilenet_v2_quantized_300x300_coco model with 7 classes to detect.

However, I'm having the next issue when tried to run [this code](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_webcam.py) for object detection:

`RuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:404 ValidateBoxes(decoded_boxes, num_boxes) was not true.Node number 98 (TFLite_Detection_PostProcess) failed to invoke.`

The weirdest thing of all is the fact that if I run the exact same code in my workstation (with tf 1.15.1), the code works flawlessly, so it makes me think that there's something wrong with the raspberry.

The model was trained in Colab with tensorflow 1.15, and compiled with the following sintax:

!tflite_convert --graph_def_file=compiler/tflite_graph.pb --output_file=compiler/detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops

Any help will be so pleased. Thanks in advance!"
45629,The current TensorFlow lite version cannot be built with Visual Studio 2017.,"**System information**
- OS Platform and Distribution: Windows SDK version 10.0.17763.0 to target Windows 10.0.19042
- TensorFlow installed from (source): master, commit 67a9764695429fc5963aeeb905526bfe5af45739
- TensorFlow version: 2.4?
- CMake version: 3.18.5
- Compiler version: Visual Studio 2017 (MSVC 19.16.27045.0)
- CUDA/cuDNN version: 10.2, 11.1
- GPU model and memory: GeForce 1080 Ti

**Describe the problem**
The current TensorFlow lite version cannot be built with Visual Studio 2017.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
D:
mkdir TF
cd TF
git clone https://github.com/tensorflow/tensorflow.git src
mkdir build
cd build
cmake -G ""Visual Studio 15"" -A x64 -D CMAKE_DEBUG_POSTFIX:STRING=d -D CMAKE_INSTALL_PREFIX:PATH=D:\TF\install -D TFLITE_ENABLE_GPU:BOOL=OFF -D GIT:FILEPATH=D:\dev\Tools\GIT\bin\git.exe D:\TF\src\tensorflow\lite
msbuild INSTALL.vcxproj /maxcpucount /nodeReuse:false /nologo /p:Configuration=Debug
```

**Any other info / logs (translated from german)**
```
       ""D:\TF\build\INSTALL.vcxproj"" (default target) (1) ->.
       ""D:\TF\build_BUILD.vcxproj"" (default target) (3) ->.
       ""D:\TF\build\tensorflow-lite.vcxproj"" (default target) (4) ->
       (ClCompile target) ->
         D:\TF\src\tensorflow-lite\kernels\elementwise.cc(299): fatal error C1001: Internal compiler error. [D:\TF\build\tensorflow-lite.vcxproj]
```

Project file built with Visual Studio:
```
42>D:\TF\src\tensorflow\lite\kernels\elementwise.cc(299): fatal error C1001: Internal compiler error.
42>(Compiler file ""msc1.cpp"", line 1518)
42> Simplify or modify the program in the environment of the items listed above. Select
42>Use the ""Technical Support"" command in the ""Help"" menu of Visual C++,
42>or open the technical support help file for more information.
42>INTERNAL COMPILER ERROR in ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX86\x64\CL.exe"".
42> From the Visual C++ Help menu, click Technical Support,
42> or open Technical Support Help for more information.
```
"
45628,RNN Using TensorFlow 2.0,"Please let us know anything wrong in below code, not getting desire result -

```
from numpy import sqrt
from numpy import asarray
from pandas import read_csv
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
import tensorflow as tf
from sklearn import metrics
from sklearn.model_selection import train_test_split
```

Assign the value as 40 to the variabel RANDOM_SEED which will be the seed value.
Set the random seed value using the value stored in the variable RANDOM_SEED.

```
RANDOM_SEED = 40
tf.random.set_seed(RANDOM_SEED)
```

split a univariate sequence into samples

```
def split_sequence(sequence, n_steps):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps
        # check if we are beyond the sequence
        if end_ix > len(sequence)-1:
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return asarray(X), asarray(y)
```


Read the dataset airline-passengers.csv and give parameter index_col as 0 and save it in variable df.

`df = read_csv(""airline-passengers.csv"", index_col=0)
`

Convert the data type of the values dataframe df to float32 and save it in variable values.
Assign the value 5 to the variable n_steps which is the window size.
Split the samples using the function split_sequence and pass the parameters values and n_steps and save it in variables X and y

```
values = df.values.astype('float32')
n_steps = 5
X, y = split_sequence(values, n_steps)
```

Split the data X,y with the train_test_split function of sklearn with parameters test_size=0.33 and random_state=RANDOM_SEED.**

`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED)
`

Construct a fully-connected network structure defined using dense class
Create a sequential model
Add a LSTM layer which has 200 nodes with activation function as relu and input shape as (n_steps,1).
The first hidden layer has 100 nodes and uses the relu activation function.
The second hidden layer has 50 nodes and uses the relu activation function.
The output layer has 1 node.

```
model = Sequential()
model.add(LSTM(200, activation='relu',  input_shape=(n_steps,1)))
model.add(Dense(100, activation='relu'))
model.add(Dense(
50, activation='relu'))
model.add(Dense(1))
```

While comipling the model pass the following parameters -
-optimizer as Adam
-loss as mse
-metrics as mae

`model.compile(optimizer='Adam', loss='mse', metrics=['mae'])`

fit the model with X_train, y_train, epochs=350, batch_size=32,verbose=0.

`model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=0)
`

Perform prediction on the test data (i.e) on X_test and save the predictions in the variable y_pred.

`y_pred = model.predict( X_test)`

Calculate the mean squared error on the variables y_test and y_pred using the mean_squared_error function in sklearn metrics and save it in variable MSE.
Calculate the Root mean squared error on the variables y_test and y_pred by performing square root on the above result and save it in variable RMSE.
Calculate the mean absolute error on the variables y_test and y_pred using the mean_absolute_error function in sklearn metrics and save it in variable MAE.

```
MSE  = metrics.mean_squared_error(y_test,y_pred)
RMSE = sqrt(metrics.mean_squared_error(y_test,y_pred))
MAE  = metrics.mean_absolute_error(y_test,y_pred)
print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (MSE, RMSE,MAE))

```

**MSE: 665.522, RMSE: 25.798, MAE: 17.127 ... this we getting and it is wrong.**

```
with open(""MSE.txt"", ""w"") as text_file:
        MSE=str(MSE)
        text_file.write(MSE)
with open(""RMSE.txt"", ""w"") as text_file:
        RMSE=str(RMSE)
        text_file.write(RMSE)
with open(""MAE.txt"", ""w"") as text_file:
        MAE=str(MAE)
        text_file.write(MAE)
# serialize model to JSON
model_json = model.to_json()
with open(""model.json"", ""w"") as json_file:
    json_file.write(model_json)
```

[airline-passengers.zip](https://github.com/tensorflow/tensorflow/files/5650585/airline-passengers.zip)
[RNN_Question.zip](https://github.com/tensorflow/tensorflow/files/5650599/RNN_Question.zip)

After running code then we execute

```
from hashlib import md5
f = open(""MSE.txt"", ""r"")
s=f.read()
s=float(s)
s=round(s,3)
f1=open(""RMSE.txt"",""r"")
s1=f1.read()
s1=float(s1)
s1=round(s1,3)
f2=open(""MAE.txt"",""r"")
s2=f2.read()
s2=float(s2)
s2=round(s2,3)
if (md5(str(s).encode()).hexdigest() == '51ad543f7ac467cb8b518f1a04cc06af') and (md5(str(s1).encode()).hexdigest() == '6ad48a76bec847ede2ad2c328978bcfa') and (md5(str(s2).encode()).hexdigest() == '64bd1e146726e9f8622756173ab27831'):

	print(""Your MSE,RMSE and MAE Scores matched the expected output"")
else :
	print(""Your MSE,RMSE and MAE Scores does not match the expected output"") 
```

Here our output should be match but coming as unmatched.
"
45627,Potential GPU Memory Leak,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab (couldn't find distro from this)**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.1**
- Python version: **Python 3.8.5 (64-bit)**
- CUDA/cuDNN version: **10.1**
- GPU model and memory: **Nvidia Tesla T4 16GB**

**Describe the current behavior**

Half way through an epoch the network appears to run out of memory despite no new memory requirements i.e. nothing extra is being created that isn't deleted (I think). See ipynb below for a detailed error message. Furthermore, when checking the memory free after the program has finished executing (using `nvidia-smi`) it gives `14599MiB / 15079MiB` used and `No running processes found`.

**Describe the expected behavior**
The network should continue to train as normal.

**Standalone code to reproduce the issue**
https://gist.github.com/IlleQuiProgrammat/cd37e1b0767e0c859fef922387284898

*Note: the full error is shown at the bottom*
"
45626,"WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028717BD2C18> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause:","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45625,How to set the signature of keras.models.save_model,"Maybe this problem is not suitable for this module

# My environment

tf:2.3
system:ubuntu 18

# My question

I updated from tf14 to tf2.3. The model I used is a model of keras type. After viewing the official document, adding signature failed

# My main code

```
model = VGG16(weights = weights_dir)
...
keras.models.save_model(model, model_dir_saved_model)
```
This function has the input of signature, but I don't know how to organize it

# Here's my try

```

def saveKerasModelAsProtobuf(model, outputPath):
    inputs = {'image': utils.build_tensor_info(model.input)}
    outputs = {'scores': utils.build_tensor_info(model.output)}

    signature = tf.saved_model.signature_def_utils.build_signature_def(
        inputs, outputs, 'name')

    builder = tf.saved_model.builder.SavedModelBuilder(outputPath)
    builder.add_meta_graph_and_variables(
        sess=keras.backend.get_session(),
        tags=['serving_default'],
        signature_def_map={'serving_default': signature})
    builder.save()
```

So, what's the right way to keep it

"
45623,NaN predictions/gradients/losses in basic GAN training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- TensorFlow installed from (source or binary):
Conda
- TensorFlow version (use command below):
2.2.0
- Python version:
3.7.9
- CUDA/cuDNN version:
10.1.243
7.6.5
- GPU model and memory:
RTX3080 10GB VRAM

**Describe the current behavior**
When training simple GAN models using the exact code from the documentation ([""DCGAN""](https://www.tensorflow.org/tutorials/generative/dcgan) & [""Writing a training loop from scratch""](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)), the generator predicts ""nan"" after one or two training steps. 

When debugging and looking at the gradients, I see that some layers' gradients have nan values

**Describe the expected behavior**
The model should predict actual numbers and not GANs, or 2 different documentation articles need to be fixed.

**Standalone code to reproduce the issue**
I can't reproduce it on Colab, but here's the code I run on my PC:
https://colab.research.google.com/gist/asaf92/8de8b9fc0e9e34c9079ebbe54f41e0b4/copy-of-gan-bug.ipynb
I get nan losses after the first epoch.

**Other info / logs** Include any logs or source code that would be helpful to

```
Connected to pydev debugger (build 202.7660.27)
2020-12-12 11:52:37.370048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-12 11:52:40.950277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-12-12 11:52:40.987435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:65:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2020-12-12 11:52:40.988331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-12 11:52:40.997389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-12 11:52:41.004564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-12 11:52:41.006971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-12 11:52:41.013938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-12 11:52:41.016784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-12 11:52:41.028209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 11:52:41.028663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-12-12 11:52:41.030082: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-12-12 11:52:41.049835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c04f8b7f70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-12 11:52:41.050215: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-12-12 11:52:41.050521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:65:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s
2020-12-12 11:52:41.050776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-12-12 11:52:41.051112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-12 11:52:41.051189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-12-12 11:52:41.051269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-12-12 11:52:41.051347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-12-12 11:52:41.051767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-12-12 11:52:41.051965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 11:52:41.052387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-12-12 11:52:42.074102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 11:52:42.074197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-12-12 11:52:42.074417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-12-12 11:52:42.075088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8454 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6)
2020-12-12 11:52:42.082271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c004532730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-12-12 11:52:42.082872: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3080, Compute Capability 8.6
Model: ""discriminator""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 14, 14, 64)        640       
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         
_________________________________________________________________
global_max_pooling2d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 74,625
Trainable params: 74,625
Non-trainable params: 0
_________________________________________________________________

Start epoch 0
2020-12-12 11:52:43.175082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-12-12 11:52:43.567573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 11:52:46.072136: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
discriminator loss at step 0: 0.69
adversarial loss at step 0: 0.76

Start epoch 1
discriminator loss at step 0: nan
adversarial loss at step 0: nan

Start epoch 2
discriminator loss at step 0: nan
adversarial loss at step 0: nan

Start epoch 3
discriminator loss at step 0: nan
adversarial loss at step 0: nan

```
"
45622,tf.test.gpu_device_name() breaks tensorflow,"**System information**
- I am working on a vae in tensorflow using probability layers
- Ubuntu 20.04
- tensorflow 2.5 compiled from source
- Python version: 3.8.5
- CUDA 11.1
- RTX 2070 Super with 8gb of ram

I want to reproduce this [example](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE). When running it I get the following exception:
```
Blas xGEMM launch failed : a.shape=[1,300,4096], b.shape=[1,4096,512], m=300, n=512, k=4096
	 [[node encoder/dense/MatMul (defined at <ipython-input-8-24f1e62ee75b>:15) ]] [Op:__inference_train_function_1790]
```
Jupyter also throws a ```CUBLAS_STATUS_NOT_INITIALIZED``` error. NVTOP also show that all of the VRAM is being used. I run ``` tf.config.experimental.set_memory_growth(gpu, True)``` right after tensorflow import.

Removing
```
if tf.test.gpu_device_name() != '/device:GPU:0':
  print('WARNING: GPU device not found.')
else:
  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))
```
fixes the problem. When adding this snippet to any other tensorflow program it breaks as well.

My theory is that ```tf.test.gpu_device_name()``` somehow breaks or resets ```tf.config.experimental.set_memory_growth(gpu, True)```."
45617,UnicodeDecodeError: 'utf-8' codec can't decode byte 0xXX in position YY : invalid continuation byte,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm trying to adapt a code that I found the code [here](https://medium.com/@martin.lees/image-recognition-with-machine-learning-in-python-and-tensorflow-b893cd9014d2). But this code doesn't work (well, I have an error)
- OS Platform and Distribution : Windows 10
- TensorFlow version (use command below):TF 2.0
- Python version: 3.8


**Describe the current behavior**
Run, but got this UnicodeDecodeError at the end so it stops. 

**Describe the expected behavior**
Should run without the error at the end. 


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
ATTENTION RESULTAT  0.8888889 #% it's not a mistake, I just put 5 pic in each folder just to see if it was working
Traceback (most recent call last):

  File ""D:\pokemon\PogoBot\PoGo-Adb\ml_test_data_test.py"", line 108, in <module>
    tf.app.run(main=main)

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)

  File ""C:\Users\pierr\anaconda3\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)

  File ""C:\Users\pierr\anaconda3\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))

  File ""D:\pokemon\PogoBot\PoGo-Adb\ml_test_data_test.py"", line 104, in main
    saver.save(sess, ""./model"")

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1183, in save
    model_checkpoint_path = sess.run(

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 957, in run
    result = self._run(None, fetches, feed_dict, options_ptr,

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1180, in _run
    results = self._do_run(handle, final_targets, final_fetches,

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1358, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1365, in _do_call
    return fn(*args)

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1349, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,

  File ""C:\Users\pierr\anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1441, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 109: invalid continuation byte
```

**Standalone code to reproduce the issue**
````
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import cv2
from os import listdir
from os.path import isfile, join
import numpy as np
import tensorflow as tf2
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import math



class Capchat:
    data_dir = ""data_test//""
    nb_categories = 9
    X_train = None # X is the data array
    Y_train = None # Y is the labels array, you'll see this notation pretty often
    
    train_nb = 0 # number of train images
    X_test = None
    Y_test = None
    test_nb = 0 # number of tests images
    index = 0 # the index of the array we will fill 
    def readimg(self, file, label, train = True):
        im = cv2.imread(file); # read the image to PIL image
        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY).flatten() # put it in black and white and as a vector
        # the train var definies if we fill the training dataset or the test dataset
        if train : 
            self.X_train[self.index] = im
            self.Y_train[self.index][label - 1] = 1
        else :
            self.X_test[self.index] = im
            self.Y_test[self.index][label - 1] = 1
        self.index += 1
    def __init__(self):
        total_size = [f for f in listdir(self.data_dir + ""1/"") if isfile(join(self.data_dir + ""1/"", f))].__len__() # ge the total size of the dataset
        self.train_nb = math.floor(total_size * 0.8) # we get 80% of the data to train
        self.test_nb = math.ceil(total_size *0.2) # 20% to test
        
        # We fill the arrays with zeroes 840 is the number of pixels in an image
        self.X_train = np.zeros((self.train_nb*self.nb_categories, 735), np.int32)
        self.Y_train = np.zeros((self.train_nb*self.nb_categories, 3), np.int32)
        self.X_test = np.zeros((self.test_nb*self.nb_categories, 735), np.int32)
        self.Y_test = np.zeros((self.test_nb*self.nb_categories, 3), np.int32)
        # grab all the files 
        files_1 = [f for f in listdir(self.data_dir+""1/"") if isfile(join(self.data_dir+""1/"", f))]
        files_2 = [f for f in listdir(self.data_dir+""2/"") if isfile(join(self.data_dir+""2/"", f))]
        files_3 = [f for f in listdir(self.data_dir+""3/"") if isfile(join(self.data_dir+""3/"", f))]

        for i in range(self.train_nb):
            # add all the files to training dataset
            self.readimg(self.data_dir+""1/""+files_1[i], 1)
            self.readimg(self.data_dir+""2/""+files_2[i], 2)
            self.readimg(self.data_dir+""3/""+files_3[i], 3)

        self.index = 0
        
        for i  in range (self.train_nb, self.train_nb + self.test_nb):
            self.readimg(self.data_dir+""1/"" + files_1[i], 1, False)
            self.readimg(self.data_dir+""2/"" + files_2[i], 2, False)
            self.readimg(self.data_dir+""3/"" + files_3[i], 3, False)
        print(""donnÃ©e triÃ©e"")


def main(_):
  # Import the data
  cap = Capchat()
  # Create the model
  x = tf.placeholder(tf.float32, [None, 735])
  W = tf.Variable(tf.zeros([735, 3]), name=""weights"")
  b = tf.Variable(tf.zeros([3]), name=""biases"")
  mult = tf.matmul(x, W) # W * X...
  y = tf.add(mult, b, name=""calc"") # + b
  # Define loss and optimizer
  y_ = tf.placeholder(tf.float32, [None, 3])
  # cost function
  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
  # optimizer
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
  # allows to save the model later
  saver = tf.train.Saver()
  # start a session to run the network on
  sess = tf.InteractiveSession()
  # initialize global variables
  tf.global_variables_initializer().run()
  # Train for 1000 steps, notice the cap.X_train and cap.Y_train
  for _ in range(1000):
    sess.run(train_step, feed_dict={x: cap.X_train, y_: cap.Y_train})
  # Extract one hot encoded output via argmax
  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
  # Test for accuraccy on the testset, notice the cap.X_test and cap.Y_test
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  print(""\nATTENTION RESULTAT "",sess.run(accuracy, feed_dict={x: cap.X_test,
                                      y_: cap.Y_test}))
  # save the model learned weights and biases
  saver.save(sess, ""./model"")
  
  
if __name__ == '__main__':
  tf.app.run(main=main)
"
45616,"tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","## My machine have - Cuda Version - 10.1.105, ##cuDNN Version - 7.6 , ##Device - GeForce GTX 1660 SUPER 
##Tensorflow - 2.3.1 , ##Python 3.8.3 , O.S - Windows 10
=====================================================
E:\xtreme_vision_yolo>python train_yolov4.py
2020-12-12 02:49:01.254777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
class path E:/xtreme_vision_yolo/classes.names
train images file E:/xtreme_vision_yolo/train_images_file.txt
train image dir E:/xtreme_vision_yolo/train_img_dir/
2020-12-12 02:49:18.534478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-12-12 02:49:18.585778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1660 SUPER computeCapability: 7.5
coreClock: 1.785GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2020-12-12 02:49:18.594476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-12-12 02:49:19.157955: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-12-12 02:49:19.495738: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-12-12 02:49:19.529529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-12-12 02:49:19.880640: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-12-12 02:49:20.170330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-12-12 02:49:20.477234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 02:49:20.482540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-12-12 02:49:20.487618: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-12 02:49:20.508225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25fa6271050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-12 02:49:20.516217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-12-12 02:49:20.524054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1660 SUPER computeCapability: 7.5
coreClock: 1.785GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2020-12-12 02:49:20.536073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-12-12 02:49:20.543032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-12-12 02:49:20.548638: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-12-12 02:49:20.554466: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-12-12 02:49:20.559750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-12-12 02:49:20.566821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-12-12 02:49:20.572949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 02:49:20.580348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-12-12 02:49:22.507429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 02:49:22.513254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-12-12 02:49:22.517366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-12-12 02:49:22.521455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4618 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-12-12 02:49:22.533883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25fd40d41e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-12-12 02:49:22.542623: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1660 SUPER, Compute Capability 7.5
Downloading weights file...
Please wait...
2020-12-12 02:49:34.825447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-12-12 02:49:34.829432: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2020-12-12 02:49:34.835081: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found
2020-12-12 02:49:34.843967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2020-12-12 02:49:34.850964: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.

Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(9.0483736e-05, shape=(), dtype=float32).
Epoch 1/500
2020-12-12 02:50:13.332541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-12-12 02:50:14.779540: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2020-12-12 02:50:14.789174: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
Traceback (most recent call last):
  File ""train_yolov4.py"", line 18, in <module>
    model.train(epochs=500, lr=1e-4, steps_per_epoch=400)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\xtreme_vision\Detection\Custom\yolo.py"", line 186, in train
    self.model.fit(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\xtreme_vision\Detection\yolov4\tf\__init__.py"", line 260, in fit
    self.model.fit(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 545, in call
    outputs = execute.execute(
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node YOLOv4/CSPDarknet53/yolo_conv2d/sequential/conv2d/Conv2D (defined at C:\ProgramData\Anaconda3\lib\site-packages\xtreme_vision\Detection\yolov4\model\common.py:92) ]] [Op:__inference_train_function_54586]

Errors may have originated from an input operation.
Input Source operations connected to node YOLOv4/CSPDarknet53/yolo_conv2d/sequential/conv2d/Conv2D:
 IteratorGetNext (defined at C:\ProgramData\Anaconda3\lib\site-packages\xtreme_vision\Detection\yolov4\tf\__init__.py:260)

Function call stack:
train_function
## Kindly suggest solution"
45615,metrics  in the Classification on imbalanced data tutorial ,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/tutorials/structured_data/imbalanced_data

## Description of issue (what needs changing):

The example uses the AUC-ROC metric, both for evaluation and as the criterion for early stopping. However, for imbalanced datasets the Precision-Recall curve may be a better choice. In fact, this is the metric that is recommended in the Kaggle dataset (https://www.kaggle.com/mlg-ulb/creditcardfraud), see also:

https://www.biostat.wisc.edu/~page/rocpr.pdf

### Clear description

Instead of using the ROC-AUC, the PRC Area Under the Curve should be used instead. In particular, 

* The `METRICS` array should include:

    `keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve`

* `early_stopping` should be defined as:

    ```
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_prc', 
        verbose=1,
        patience=10,
        mode='max',
        restore_best_weights=True)
   ```

* The first line of `plot_metrics(history)` should become:

    ```
    metrics = ['loss', 'prc', 'precision', 'recall']
    ```

* Instead of having a `plot_auc()` we should have:

    ```
    def plot_prc(name, labels, predictions, **kwargs):
        precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)

        plt.plot(precision, recall, label=name, linewidth=2, **kwargs)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.grid(True)
        ax = plt.gca()
        ax.set_aspect('equal')
    ```

### Submit a pull request?

No, because I have also made some other changes, the most important of which is that oversampling does not seem to give very good results, but a combination of SMOTE oversampling and undersampling does give better results. This is easy to do with imbalanced-learn (I might submit another issue). Also, I am not sure why the adjusted output bias is used when we reweight the classes, as the effect is the same as with oversampling, and we zero out the last bias in oversampling."
45611,Tensorflow crashes python when using Keras Convolution/Max Pooling layers on rtx 3070,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): pip install tensorflow-gpu==2.4.0-rc0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1/8
- GPU model and memory: RTX 3070 (8gb)

**Current Behavior**
Running **""Code 1""** [in standalone code sect.] works fine and engages the GPU, obviously the results are garbage but it is an illustrative example. 

By adding either a convolutional layer, max pooling layer, or both and running **""Code 2""** [in standalone code sect.] causes python to crash and yields the terminal output in the ""other info"" section. The error comes when the ```model.fit(...)``` call is made; the model is able to compile successfully but crashes on the first epoch of training. 

**Expected behavior**
Since the standard sequential ANN trains fine on the GPU i would expect the CNN to be able to train as well. 

**Standalone code to reproduce the issue**

**Code 1** - works fine
```import tensorflow as tf
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

train_images, test_images = train_images / 255.0, test_images / 255.0

print(tf.config.list_physical_devices('GPU'))

model = models.Sequential()
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

history = model.fit(train_images, train_labels, batch_size=1, epochs=100)
```
**Code 2** - crashes when .fit(...) is called on model
```import tensorflow as tf
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

train_images, test_images = train_images / 255.0, test_images / 255.0

print(tf.config.list_physical_devices('GPU'))

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

history = model.fit(train_images, train_labels, batch_size=1, epochs=100)
```

**Other info / logs**
```
2020-12-11 14:28:08.841765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-11 14:28:11.095172: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-11 14:28:11.096076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-12-11 14:28:11.124073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-11 14:28:11.124461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-11 14:28:11.136398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-11 14:28:11.136622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-11 14:28:11.139581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-11 14:28:11.140535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-11 14:28:11.147342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-11 14:28:11.149809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-11 14:28:11.150352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-11 14:28:11.150569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2020-12-11 14:28:11.153569: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-11 14:28:11.154591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-11 14:28:11.154972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-12-11 14:28:11.155203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-11 14:28:11.155383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-11 14:28:11.155567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-12-11 14:28:11.155753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-12-11 14:28:11.155934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-12-11 14:28:11.156112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-12-11 14:28:11.156288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-12-11 14:28:11.156488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-11 14:28:11.595698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-11 14:28:11.595908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-12-11 14:28:11.596028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-12-11 14:28:11.596248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6177 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2020-12-11 14:28:11.597093: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-11 14:28:12.139175: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/100
2020-12-11 14:28:12.386035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-12-11 14:28:13.019914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-12-11 14:28:13.024711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
Process finished with exit code -1073740791 (0xC0000409)
```"
45609,Tensorflow_datsaet api fails to cache my training data.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: V100 (16GB)

**Describe the current behavior**
Here is the error I get:
![Screenshot from 2020-12-11 16-39-07](https://user-images.githubusercontent.com/70702200/101952699-727d1080-3bcf-11eb-8aaf-cb55fe37bec4.png)


The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.



**Standalone code to reproduce the issue**
I run a custom data loader:
```
start_time = time.clock()

data_loader = None

if translation_direction in [Translation_Direction.English_to_Farsi,Translation_Direction.Farsi_to_English]:
    data_loader = Data_loader_persian(translation_direction, fa_vocab_size=12000, en_vocab_size=24000)
    
else:
    raise Exception(""Requested Translation Direction is not supported!"")
    
train_examples, val_examples, test_examples, dataset_size_dict, tokenizer = data_loader.run()

end_time = time.clock()
print(f""Execution time: {end_time - start_time} seconds."")
```

Then I try to cache and prefetch the dataset:
```
train_dataset = train_examples.cache()
train_dataset = train_dataset.padded_batch(global_batch_size)
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)
```

Then when I try to read the dataset:
```
for i,x in enumerate(train_examples):
    if i==5:
        break
```
Here is when the error comes up! No matter how many times I try to read, every times it reads from file (no caching happens!)

Note: Screenshot of the profiling window is attaches. As could be seen, the program is HIGHLY input-bound.
I would appreciate if you have any idea. Thanks :) "
45608,Micro: port op CAST from Lite,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): master
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I am about to port The TF Lite kernel op CAST to TF Lite Micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
PR 1: refactor flatbuffer_conversions parsing function
PR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.
PR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.
"
45607,TFLM: Adding ported and optimized operations for CEVA-BX1,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): nightly
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): CEVA-BX1

I will be adding optimized operations for the CEVA-BX1 platform (later on for other CEVA platforms as well).

Starting with fully connected but many others will follow: see PR: https://github.com/tensorflow/tensorflow/pull/45606

"
45605,No documentation on how to access the ram:// filesystem of TPU's,"According to this comment there is support to access the RAM filesystem of TPU's: https://github.com/tensorflow/tensorflow/issues/32651#issuecomment-534685146

However, the documentation seems to be lacking information on the subject (although it's a bit hard to search, as the search term 'ram' (special characters are stripped) returns a lot of irrelevant results.

Is the documentation indeed missing, or is the feature maybe not implemented yet?


"
45604,No gradients available error,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7

Using a minus sign before the loss in tape.gradient gives a ValueError: No gradients provided for any variable whereas there should be.

```python

import tensorflow as tf
import numpy as np

layers = tf.keras.layers


def _create_mlp(
        hidden_size: int = 1024, output_size: int = 256
):
    return tf.keras.Sequential(
        [
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation(""relu""),
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation(""relu""),
            layers.Dense(output_size, activation=None, use_bias=True)
        ]
    )



class ModelTest(tf.keras.Model):
    def __init__(self):
        super(ModelTest, self).__init__()

        self.generator = _create_mlp(output_size=128)

    def train_step(self, inputs):

        with tf.GradientTape() as gen_tape:

            z = self.generator(inputs)

            final_loss = tf.reduce_mean(z)

        gen_gradients = gen_tape.gradient(-final_loss, self.generator.trainable_variables)

        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))

        return {}


input_data = np.random.uniform(size=(200, 32))

dataset = tf.data.Dataset.from_tensor_slices(input_data)

dataset = dataset.batch(20)

model = ModelTest()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, run_eagerly=True)

model.fit(dataset, epochs=20)
```

This code gives the following error:

```
Traceback (most recent call last):
  File ""tests/model_test.py"", line 57, in <module>
    model.fit(dataset, epochs=20)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 572, in train_function
    self.train_step, args=(data,))
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 951, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2290, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2649, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 282, in wrapper
    return func(*args, **kwargs)
  File ""tests/model_test.py"", line 40, in train_step
    self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 472, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)
  File ""/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 1219, in _filter_grads
    ([v.name for _, v in grads_and_vars],))
ValueError: No gradients provided for any variable: ['sequential/dense/kernel:0', 'sequential/dense/bias:0', 'sequential/batch_normalization/gamma:0', 'sequential/batch_normalization/beta:0', 'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0', 'sequential/batch_normalization_1/gamma:0', 'sequential/batch_normalization_1/beta:0', 'sequential/dense_2/kernel:0', 'sequential/dense_2/bias:0'].
```

If you remove the minus sign before final loss in gen_tape.gradient, the code works fine.

```python
import tensorflow as tf
import numpy as np

layers = tf.keras.layers


def _create_mlp(
        hidden_size: int = 1024, output_size: int = 256
):
    return tf.keras.Sequential(
        [
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation(""relu""),
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation(""relu""),
            layers.Dense(output_size, activation=None, use_bias=True)
        ]
    )



class ModelTest(tf.keras.Model):
    def __init__(self):
        super(ModelTest, self).__init__()

        self.generator = _create_mlp(output_size=128)

    def train_step(self, inputs):

        with tf.GradientTape() as gen_tape:

            z = self.generator(inputs)

            final_loss = tf.reduce_mean(z)

        gen_gradients = gen_tape.gradient(final_loss, self.generator.trainable_variables)

        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))

        return {}


input_data = np.random.uniform(size=(200, 32))

dataset = tf.data.Dataset.from_tensor_slices(input_data)

dataset = dataset.batch(20)

model = ModelTest()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, run_eagerly=True)

model.fit(dataset, epochs=20)
```

The output is:

```

10/10 [==============================] - 0s 11ms/step
Epoch 2/20
10/10 [==============================] - 0s 11ms/step
Epoch 3/20
10/10 [==============================] - 0s 11ms/step
Epoch 4/20
10/10 [==============================] - 0s 11ms/step
Epoch 5/20
10/10 [==============================] - 0s 11ms/step
Epoch 6/20
10/10 [==============================] - 0s 11ms/step
Epoch 7/20
10/10 [==============================] - 0s 11ms/step
Epoch 8/20
10/10 [==============================] - 0s 11ms/step
Epoch 9/20
10/10 [==============================] - 0s 11ms/step
Epoch 10/20
10/10 [==============================] - 0s 11ms/step
Epoch 11/20
10/10 [==============================] - 0s 11ms/step
Epoch 12/20
10/10 [==============================] - 0s 11ms/step
Epoch 13/20
10/10 [==============================] - 0s 11ms/step
Epoch 14/20
10/10 [==============================] - 0s 11ms/step
Epoch 15/20
10/10 [==============================] - 0s 11ms/step
Epoch 16/20
10/10 [==============================] - 0s 11ms/step
Epoch 17/20
10/10 [==============================] - 0s 11ms/step
Epoch 18/20
10/10 [==============================] - 0s 11ms/step
Epoch 19/20
10/10 [==============================] - 0s 11ms/step
Epoch 20/20
10/10 [==============================] - 0s 11ms/step

```"
45603,BoostedTreesClassifier segmentation fault when evaluate or predict is called,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Os X 10.15.7
- TensorFlow installed from (source or binary): binary (pip installed)
- TensorFlow version (use command below): tried with both 2.3.1 and 2.3.0
- Python version: 3.8.5
- CUDA/cuDNN version: running on Mac, no GPU support
- GPU model and memory: as above

using this [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
```
== check python ===================================================
python version: 3.8.5
python branch:
python build version: ('default', 'Sep 30 2020 08:41:45')
python compiler version: Clang 11.0.0 (clang-1100.0.33.17)
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
Apple clang version 11.0.0 (clang-1100.0.33.17)
Target: x86_64-apple-darwin19.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                    1.18.5
protobuf                 3.14.0
tensorflow               2.3.0
tensorflow-estimator     2.3.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.3.0
tf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
run.sh:145: command not found: nvidia-smi

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.3.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/cesco/Desktop/venv/lib/python3.8/site-packages
Required-by:

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 5, 'final', 0)

== bazel version  ===============================================
```


**Describe the current behavior**
I took [this notebook](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding) and tried to run it on my local machine but it repeatedly crashed the jupyter kernel when trying to evaluate the model (the same happens with the .predict() method).
I then converted the notebook into a simple python script (I thought that perhaps is an issue with jupyter) using the command `ipython nbconvert --to python` and tried to run it.
This is the full output I got:
```
Feature value: ""Third""
2020-12-11 14:07:55.153062: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-11 14:07:55.167575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd8db882480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-11 14:07:55.167595: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
One-hot encoded:  [[0. 0. 1.]]
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/4w/m2lqr9ts0q1dtnzgsx9gr3_40000gn/T/tmpy159zb62
WARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

WARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1471: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:111: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

accuracy                  0.765152
accuracy_baseline         0.625000
auc                       0.832844
auc_precision_recall      0.789631
average_loss              0.478908
label/mean                0.375000
loss                      0.478908
precision                 0.703297
prediction/mean           0.350790
recall                    0.646465
global_step             100.000000
dtype: float64
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/4w/m2lqr9ts0q1dtnzgsx9gr3_40000gn/T/tmpwwx0o5_p
WARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:398: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
Instructions for updating:
The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
WARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/head.py:637: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.
Instructions for updating:
The value of AUC returned by this may race with the update so this is deprecated. Please use tf.keras.metrics.AUC instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
[1]    76205 segmentation fault  python boosted_trees.py
```

**Describe the expected behavior**
I would expect the jupyter notebook kernel not to crash, and the script not to have a segmentation fault.

**Standalone code to reproduce the issue**
The part that cause the crash is when the BoostedTreesClassifier is used for predict or estimate.
The shortest script I could come up to reproduce is:
```python
import pandas as pd

# Load dataset.
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
y_train = dftrain.pop('survived')
y_eval = dfeval.pop('survived')


import tensorflow as tf
tf.random.set_seed(123)


CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

def one_hot_cat_column(feature_name, vocab):
  return tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,
                                                 vocab))
feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  # Need to one-hot encode categorical features.
  vocabulary = dftrain[feature_name].unique()
  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name,
                                           dtype=tf.float32))


example = dict(dftrain.head(1))
class_fc = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('class', ('First', 'Second', 'Third')))


# Use entire batch since this is such a small dataset.
NUM_EXAMPLES = len(y_train)

def make_input_fn(X, y, n_epochs=None, shuffle=True):
  def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
    if shuffle:
      dataset = dataset.shuffle(NUM_EXAMPLES)
    # For training, cycle thru dataset as many times as need (n_epochs=None).
    dataset = dataset.repeat(n_epochs)
    # In memory training doesn't use batching.
    dataset = dataset.batch(NUM_EXAMPLES)
    return dataset
  return input_fn

# Training and evaluation input functions.
train_input_fn = make_input_fn(dftrain, y_train)
eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)


n_batches = 1
est = tf.estimator.BoostedTreesClassifier(feature_columns,
                                          n_batches_per_layer=n_batches)

# The model will stop training once the specified number of trees is built, not
# based on the number of steps.
est.train(train_input_fn, max_steps=100)

# Eval.
est.evaluate(eval_input_fn)
```

**Other info / logs** Include any logs or source code that would be helpful to
From pip freeze:
```
absl-py==0.10.0
appnope==0.1.2
argon2-cffi==20.1.0
astunparse==1.6.3
async-generator==1.10
attrs==20.3.0
backcall==0.2.0
bleach==3.2.1
cachetools==4.2.0
certifi==2020.12.5
cffi==1.14.4
chardet==3.0.4
cycler==0.10.0
decorator==4.4.2
defusedxml==0.6.0
dill==0.3.3
entrypoints==0.3
future==0.18.2
gast==0.3.3
google-auth==1.23.0
google-auth-oauthlib==0.4.2
google-pasta==0.2.0
googleapis-common-protos==1.52.0
grpcio==1.34.0
h5py==2.10.0
idna==2.10
importlib-resources==3.3.0
ipykernel==5.4.0
ipython==7.19.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
jedi==0.17.2
Jinja2==2.11.2
joblib==0.17.0
jsonschema==3.2.0
jupyter==1.0.0
jupyter-client==6.1.7
jupyter-console==6.2.0
jupyter-core==4.7.0
jupyterlab-pygments==0.1.2
Keras-Preprocessing==1.1.2
kiwisolver==1.3.1
Markdown==3.3.3
MarkupSafe==1.1.1
matplotlib==3.3.3
mistune==0.8.4
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.0.8
nest-asyncio==1.4.3
notebook==6.1.5
numpy==1.18.5
oauthlib==3.1.0
opt-einsum==3.3.0
packaging==20.7
pandas==1.1.5
pandocfilters==1.4.3
parso==0.7.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==8.0.1
prometheus-client==0.9.0
promise==2.3
prompt-toolkit==3.0.8
protobuf==3.14.0
ptyprocess==0.6.0
pyarrow==2.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
Pygments==2.7.3
pyparsing==2.4.7
pyrsistent==0.17.3
python-dateutil==2.8.1
pytz==2020.4
pyzmq==20.0.0
qtconsole==5.0.1
QtPy==1.9.0
requests==2.25.0
requests-oauthlib==1.3.0
rsa==4.6
scikit-learn==0.23.2
scipy==1.4.1
Send2Trash==1.5.0
six==1.15.0
sklearn==0.0
tensorboard==2.4.0
tensorboard-plugin-wit==1.7.0
tensorflow==2.3.0
tensorflow-estimator==2.3.0
termcolor==1.1.0
terminado==0.9.1
testpath==0.4.4
threadpoolctl==2.1.0
tornado==6.1
tqdm==4.54.1
traitlets==5.0.5
urllib3==1.26.2
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wrapt==1.12.1
```
"
45602,keras load_model Could not find matching function when compute_mask is used,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
https://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
colab

- TensorFlow installed from (source or binary):
tf-nightly

- TensorFlow version (use command below):
- Python version:



**Describe the current behavior**
ValueError exception is raised when attempting to load a saved model that uses a ```compute_mask``` method.

In this example https://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing
a custom layer copied from the documentation is added to a Sequential model. The model is compiled and saved. The function ```keras.models.load_model``` raises and exception when attempting to reconstruct the model.

It seems probable that the model is not taking computed masks into account when attempting to reconstruct the graph. The error message leads me to believe that it is unaware that a mask producer can pass a mask to a mask consumer.

**Describe the expected behavior**
Canonical examples described in the documentation should be able to be saved and loaded.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * Tensor(""inputs:0"", shape=(None, 16, 8), dtype=float32)
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 1 option(s):

Option 1:
  Positional arguments (2 total):
    * TensorSpec(shape=(None, 16, 8), dtype=tf.float32, name='inputs')
    * TensorSpec(shape=(None, 16), dtype=tf.bool, name='mask')
  Keyword arguments: {}
```

**Workaround:**
If the model is saved with the ```save_traces``` flag set to False  it can be successfully saved and loaded.
Example:
https://colab.research.google.com/drive/1WRPqSzu41waymZRr_9wqpojhVEi7EsCV?usp=sharing"
45601,Cannot convert numpy array to tensor in Image Data Generator,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: Installed using Lambda Stack
-   **TensorFlow version (use command below)**: 2.3.1
-   **Python version**: 3.8.5
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11
-   **GPU model and memory**: RTX 3080 10GB
-   **Exact command to reproduce**:

### Describe the problem
I am trying to use keras image data generator to feed data into a pretrained mobilenet v2 model followed by global average pooling and one dense layer. Inputs are 224 * 224 * 3 images and outputs are the coordinates of a rectangular enclosure in the image. The coordinates are in a numpy array. But when the data generator tries to load data during model.fit, the following error is shown.


### Source code / logs
```
datagen = ImageDataGenerator(brightness_range=[0.2,1.2])
train_generator = datagen.flow_from_dataframe(dataframe=df, 
                                              directory='./',
                                              x_col=""filename"", 
                                              y_col=""diagonals"", 
                                              class_mode=""raw"", 
                                              target_size=(224, 224), 
                                              batch_size=16,
                                              rescale=1.0/255.0)

```
```

model.compile(optimizer=Adam(lr=0.001),loss='mse')
model.fit(train_generator,
                    epochs = 200,
                    batch_size = 16,
                    callbacks=[es, checkpoint]
)
```

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-31-4293be728923> in <module>
----> 1 history = model.fit(train_generator,
      2                     epochs = 200,
      3                     batch_size = 16,
      4                     callbacks=[es, checkpoint]
      5 )

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1047          training_utils.RespectCompiledTrainableState(self):
   1048       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1049       data_handler = data_adapter.DataHandler(
   1050           x=x,
   1051           y=y,

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)
   1103 
   1104     adapter_cls = select_data_adapter(x, y)
-> 1105     self._adapter = adapter_cls(
   1106         x,
   1107         y,

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    907     self._keras_sequence = x
    908     self._enqueuer = None
--> 909     super(KerasSequenceAdapter, self).__init__(
    910         x,
    911         shuffle=False,  # Shuffle is handed in the _make_callable override.

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    786     peek, x = self._peek_and_restore(x)
    787     peek = self._standardize_batch(peek)
--> 788     peek = _process_tensorlike(peek)
    789 
    790     # Need to build the Model on concrete input shapes.

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs)
   1019     return x
   1020 
-> 1021   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)
   1022   return nest.list_to_tuple(inputs)
   1023 

/usr/lib/python3/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/lib/python3/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x)
   1014       if issubclass(x.dtype.type, np.floating):
   1015         dtype = backend.floatx()
-> 1016       return ops.convert_to_tensor(x, dtype=dtype)
   1017     elif scipy_sparse and scipy_sparse.issparse(x):
   1018       return _scipy_sparse_to_sparse_tensor(x)

/usr/lib/python3/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1497 
   1498     if ret is None:
-> 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1500 
   1501     if ret is NotImplemented:

/usr/lib/python3/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)
     50 def _default_conversion_function(value, dtype, name, as_ref):
     51   del as_ref  # Unused.
---> 52   return constant_op.constant(value, dtype, name=name)
     53 
     54 

/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    261     ValueError: if called on a symbolic tensor.
    262   """"""
--> 263   return _constant_impl(value, dtype, shape, name, verify_shape=False,
    264                         allow_broadcast=True)
    265 

/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    273       with trace.Trace(""tf.constant""):
    274         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
--> 275     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    276 
    277   g = ops.get_default_graph()

/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):
    299   """"""Implementation of eager constant.""""""
--> 300   t = convert_to_eager_tensor(value, ctx, dtype)
    301   if shape is None:
    302     return t

/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 
    100 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).

```"
45598,Please let tf.keras.metrics.MeanIoU support y_pred from logits,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
 tf.keras.metrics.MeanIoU doesn't have an option that allow y_pred passed as logits, we want to pass y_pred as a logits tensor, you can simply add an optional step into the ```update_state``` function of this class: 
```
    y_pred = math_ops.argmax(y_pred, axis=-1)
```
**Will this change the current api? How?**
Yes, add one parameter from_logits, you can keep default as False though:
```tf.keras.metrics.MeanIoU(num_classes, from_logits=False, name=None, dtype=None)```

**Who will benefit with this feature?**
Computer vision object detection/segmentation community

**Any Other info.**
"
45596,Tensorflow float32 conversion bug from python to tensorflow constants,"Tensorflow changes values well within tf.float32 range to nearby values.

Is this expected behaviour? Numpy doesn't have this issue and given the values are within float32 range I'm not sure why this is happening.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10, same issue on linux though**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **No**
- TensorFlow version (use command below): **version 2.1.0**
- Python version: 3.6.8
- Bazel version (if compiling from source): **Not build from source**
- GCC/Compiler version (if compiling from source): **Not build from source**
- CUDA/cuDNN version:  **running on cpu**
- GPU model and memory:  ****

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**
For example
`tf.constant(20200131, dtype=tf.float32)`
Becomes <tf.Tensor: shape=(), dtype=float32, numpy=20200132.0>

Why the extra 1?**

Expected behaviour:
**
`tf.constant(20200131, dtype=tf.float32)`
Becomes <tf.Tensor: shape=(), dtype=float32, numpy=20200131.0>
**

**Standalone code to reproduce the issue**
import tensorflow as tf
tf.constant(20200131, dtype=tf.float32)
"
45594,CancelledError: [_Derived_]RecvAsync is cancelled. ,"## Note

I am opening this issue because the error I am describing seem to affect quite some people. See **Related Issues**, but those have been closed ""due to inactivity"".

----

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16, 18
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce 1080 TI, 11 GB

**Describe the current behavior**

The training fails seemingly randomly with `CancelledError:  [_Derived_]RecvAsync is cancelled`. All cases seem to have recurrent layer in common (see **Related Issues**).

After starting, the training will run (in my case) for some time and then just crash with the above error. 

**Describe the expected behavior**

Don't crash.

**Standalone code to reproduce the issue**

There are some in https://github.com/tensorflow/tensorflow/issues/33721.

**Other info / logs**

```
2020-12-05 18:06:59.383572: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN
_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'
2020-12-05 18:06:59.383906: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'
2020-12-05 18:06:59.384114: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1484): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'
Traceback (most recent call last):
  File ""asr/bin/train_keras.py"", line 300, in <module>
    app.run(main)
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""asr/bin/train_keras.py"", line 236, in main
    model.fit(
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 807, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 545, in call
    outputs = execute.execute(
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.
         [[{{node div_no_nan/ReadVariableOp_5/_1620}}]]
         [[GroupCrossDeviceControlEdges_2/Identity_6/_1699]] [Op:__inference_train_function_159151]

Function call stack:
train_function
```

**Related Issues**

- https://github.com/tensorflow/tensorflow/issues/33721
- https://github.com/tensorflow/tensorflow/issues/35523
"
45592,Keras doesn't release the memory after fit (single thread and multiprocessing),"**System information**
- OS Platform: Windows, Linux
- TensorFlow version ('v2.3.0-54-gfcc4b966f1', '2.3.1')
- Python version: 3.8.6

**Current behavior**
This problem is unfortunately blocking my production code.
The example below is oversimplified but it's perfectly able to reproduce the issue.
I run many different models using different combinations of:
- hyperparameters 
- features
- samples

Finally I save all the models for further analysis.
In the example below, for the sake of simplicity, I fit and save the same model many times, but as I said, in the production code it would be a different model for each run. 
I provide both single thread and parallel implementation .

**1. Import** 
```
# built in
import psutil

# third party
import numpy as np
import tensorflow as tf

from dask import compute, delayed
from dask.distributed import Client
from scikeras._utils import make_model_picklable 
from tensorflow.keras import Model
from tensorflow.keras import regularizers
from tensorflow.keras.backend import clear_session
from tensorflow.keras.layers import (
    Dense,
    Input,
    GRU,
    BatchNormalization,
    Dropout,
    Conv1D,
    Flatten
)
```

**2. Keras model build function**
```
def build_keras_model(
        X,
        nodes=15,
        activation=""selu"",
        kernel_initializer='random_uniform',
        regularizer_l1=0.05,
        regularizer_l2=0.0,
        recurrent_dropout=0.0,
        dropout=0.0,
        dense_units=tuple(),
        batchnorm=True,
        batchnorm_trainable=True,
        batchnorm_training=False,
        use_bias=False,
        loss='mse',
        optimizer='adam'
):
    shape = X.shape[1:]
    inputs = Input(shape=shape, name='inputs')

    x = Dropout(dropout)(inputs) if dropout else inputs

    x = GRU(
        nodes,
        activation=activation,
        recurrent_dropout=recurrent_dropout,
        return_sequences=False,
        kernel_initializer=kernel_initializer,
        kernel_regularizer=regularizers.l1_l2(regularizer_l1, regularizer_l2),
        activity_regularizer=regularizers.l1(0.0),
        use_bias=use_bias
    )(x)

    dense_units = dense_units or ()
    for n in dense_units:
        x = Dense(
            n,
            activation=activation,
            name=f'extra_dense{n}',
            kernel_regularizer=regularizers.l1_l2(0.01, 0.01)
        )(x)

    if batchnorm:
        x = BatchNormalization(trainable=batchnorm_trainable)(x, training=batchnorm_training)

    x = Dense(
        1,
        activation='linear',
        use_bias=use_bias,
        kernel_initializer='random_uniform',
        name='prediction'
    )(x)

    model = Model(inputs=inputs, outputs=x)
    model.compile(
        optimizer=optimizer,
        loss=loss
    )
    make_model_picklable(model)
    return model
```

**3. Functions**
```
def get_memory_usage():
    m_perc = psutil.virtual_memory().percent
    m_used = round(psutil.virtual_memory().used / 10e8, 4)
    return m_used, m_perc


def print_memory_usage(msg=0):
    m_used, m_perc = get_memory_usage()
    print(f'{msg}: memory used is {m_used}, {m_perc}% of total memory')


def fit_keras(model, X, y, **fit_params):
    model.fit(X, y, batch_size=fit_params.pop('batch_size', len(X)), **fit_params)
    return model


def run_keras_single(X, y, n_runs):
    models = []
    for i in range(n_runs):
        print_memory_usage(i)
        model = build_keras_model(X)
        model.fit(X, y, batch_size=len(X), verbose=0)
        models.append(model)
    return models


def run_keras_multi(X, y, n_runs):
    print_memory_usage('before')
    keras_model = build_keras_model(X)
    models = compute(*[delayed(fit_keras)(keras_model, X, y, epochs=20, verbose=0) for i in range(n_runs)])
    print_memory_usage('after')
    return models
```

 **4. Data**
```
rng = np.random.default_rng(7)
n_samples = 500
n_timesteps = 10
n_features = 50
loc = 0
scale = 0.01

X = rng.normal(loc=loc, scale=scale, size=(n_samples, n_timesteps, n_features))
y = rng.normal(loc=loc, scale=scale, size=(n_samples,))
```

**5. Run**
```
n_runs = 1000

models_single = run_keras_single(X, y, n_runs)
models_multi = run_keras_multi(X, y, n_runs)
```

**Conclusion**
After running 1000 models and saving the results to the disk the saved models file size is about 10Mb. However the whole process both with and without multiprocessing requires almost 30GB of RAM which is not released after fitting.

Any suggestion on how to solve or mitigate the issue would be much appreciated.
Many thanks 
Gio"
45591,tfjs-models/face-landmarks-detection/demo/ TypeError: Fail to fetch,"Hi ,  I run this demo in my macbook, I don't have VPN,  How to synchronize the model and store it myself. thank you!
"
45590,Value 'sm_86' is not defined for option 'gpu-name',"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):docker
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):tf-nightly-gpu         2.5.0.dev20201210
- Python version:3.6.9
- CUDA/cuDNN version:11.0/8.0.4
- GPU model and memory:3090/24GB

when i inference my tf model,it appears hundreds of warnings like the following image 
about 1 minute later ,the normal inference begin
i watch the issus and try to used tf2.4re02 but it was useless
![image](https://user-images.githubusercontent.com/33931180/101855756-ef43c680-3b9e-11eb-807e-7756853c75ab.png)

"
45586,Object Detection doesn't work,
45585,Inconsistent results when using tf_upgrade_v2 script,"**System information**

- Run in Google Collab Notebook with macOS Big Sur
- There are two files run: 
    - a version running code with Tensorflow 2
    - a version running code with Tensorflow 1.x
- Python 3.6.9

**Current behavior**

We wanted to see if the result of the migration script from TF 1.x to TF 2 gives the same output. To evaluate this, we took code from Tensorflow 1.x taken from this tutorial (https://medium.com/@udolf15/mnist-digits-classification-with-tensorflow-7f7dcda0fc1e) and ran it with the migration script and got a compat version of the code. I set the graph-level and operation-level seeds to be able to have reproducible outputs on both the compat(TF2 using migration script) and original version (TF 1.x). 

The results from running the Original TF 1.x version are: 
```
(train_loss, train_accuracy)
0.8028196 0.91954
0.23598278 0.9466
0.1635617 0.96346
0.12261735 0.97066
0.09810413 0.97426
0.07586458 0.97328
0.057922967 0.98538
0.0451939 0.98892
0.036826767 0.99028
0.032206446 0.98698
0.027886271 0.99306
0.023077298 0.99478
0.016734073 0.99372
0.016260928 0.98992
0.015216767 0.99534
0.012952065 0.9955
0.008880938 0.99868
0.006781239 0.99742
0.006611649 0.99772
0.007158364 0.99824
```

The results from running the Migration Script TF 2 version are: 
```
(train_loss, train_accuracy)
0.86170954 0.91768
0.23816809 0.9488
0.16750608 0.96032
0.12643586 0.96756
0.0971571 0.97194
0.07221356 0.9815
0.05393147 0.9875
0.04196974 0.98938
0.03490297 0.98796
0.029905643 0.98764
0.027600855 0.98814
0.026849583 0.9914
0.021233767 0.99068
0.015700594 0.99518
0.0131430775 0.99696
0.010740909 0.99608
0.0077435044 0.99426
0.0060562133 0.99568
0.006112121 0.99866
0.0034680425 0.99874
```

As we can see, they are significantly different. We ran this multiple times, the original version gave us consistent results whereas the compat version gave us varying results.

From the migration script, we get a warning:
```
31:21: INFO: Changing labels arg of tf.nn.softmax_cross_entropy_with_logits to tf.stop_gradient(labels). Please check this transformation.
```

**Describe the expected behavior**

The expected behavior was that the results between the Migration Script TF 2 version would yield similiar results to the original TF 1.x results.

**Standalone code to reproduce the issue**

You can reproduce this results running them on the following Colab Notebooks: 

TF 1.x Original Version: https://colab.research.google.com/drive/1OL_qPqdF9Hfhksqnri2BpJmbRc5LT84d?usp=sharing

TF 2 Migration Script version (with tf.stop_gradient): https://colab.research.google.com/drive/12sao4pWQ_x5tLLazux0LkfqeoqhnaCuL?usp=sharing"
45584,Autograph warning seems to lead to reduced performance,"

**System information**
- I am using a toolbox called EEGNet and using my own script
- OS Platform and distribution: macOS catalina version 10.15.7 (19H2)
- TensorFlow installed from (source or binary) - i don't know how to check this
- Tensorflow version 2.3.1
- Python version: 3.7.4

- GPU model and memory: Radeon Pro 560X 4GB Intel UHD graphics 630 1536MB, and memory = 16GB 2400MHz DDR4


**Describe the current behavior**
I am getting the warning:
`WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd0e8162b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 
WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd0e8162b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause:`

I get this warning somewhat sporadically. When I reopen Spyder the issue is gone until the second iteration in which I need to call it. I am also noticing that the performance as seen through validation accuracy is very much lower when this warning appears.

I checked other posts and it looked like changing gast==0.2.2 fixed it. But not for me.

**Describe the expected behavior**
Ideally the warning shouldn't appear, as it does the very first time around when I reopen Spyder. And the validation accuracy should be higher

**Standalone code to reproduce the issue**
N/A
I am using a module/toolbox called EEGNet.  https://github.com/vlawhern/arl-eegmodels
but this seems to be the line that causes the warning:
`fittedModel = model.fit(X_train, Y_train, batch_size = 64, epochs = 300, 
                                    verbose = 2, validation_data=(X_validate, Y_validate),
                                    callbacks=[checkpointer], class_weight = class_weights)`

**Other info / logs**

This is when I set autograph to verbose, 3
`INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>
    args: (<tf.Tensor 'args_0:0' shape=(150,) dtype=int64>,)
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>
    args: (<tf.Tensor 'args_0:0' shape=(150,) dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>
    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(150, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(150, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(150,) dtype=int64>))
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>
    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(150, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(150, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(150,) dtype=int64>))
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>
    args: (<tf.Tensor 'args_0:0' shape=(75,) dtype=int64>,)
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>
    args: (<tf.Tensor 'args_0:0' shape=(75,) dtype=int64>,)
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>
    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(75, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(75, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(75,) dtype=int64>))
    kwargs: {}

Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>
    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(75, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(75, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(75,) dtype=int64>))
    kwargs: {}

INFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>: DoNotConvert rule for tensorflow
Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>: DoNotConvert rule for tensorflow
Train on 150 samples, validate on 75 samples
Epoch 1/300
INFO:tensorflow:Converted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>
    args: ()
    kwargs: {}

Converted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>
    args: ()
    kwargs: {}

INFO:tensorflow:Cache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> key <code object initialize_variables at 0x7fd17c1c40c0, file ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fd077f74d90>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmpaqdi9bka)
Cache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> key <code object initialize_variables at 0x7fd17c1c40c0, file ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fd077f74d90>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmpaqdi9bka)
INFO:tensorflow:Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 506, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 324, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 266, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 92, in get_factory
    assert self.module_name in sys.modules
AssertionError
Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 
WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: 
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 506, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 324, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 266, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 92, in get_factory
    assert self.module_name in sys.modules
AssertionError`

Can you help me troubleshoot this?"
45582,Layers with non-zero gradients are not updated with optimizer.apply_gradients(),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):
Tested with both 2.2.0 and a recent 2.5.0 nightly build
v1.12.1-47149-g0939f0b7a8a 2.5.0-dev20201208
v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.1.243, CUDNN 7.6.5
- GPU model and memory: GTX 1070Ti


**Describe the current behavior**

In the following example, loading a second Keras model causes a situation where layers of the map_model are not updated with optimizer.apply_gradients() even though the gradients are non-zero. This is a silent failure, so I have added a callback called WeightChecker to demonstrate the issue. Otherwise, this code is meant to closely follow this (working) example: https://keras.io/examples/generative/dcgan_overriding_train_step/

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras


class WeightChecker:
    """"""Automated health checks for training Keras models.""""""
    def __init__(self, model):
        self.initial_model = model
        self.var_names = [var.name for var in model.trainable_variables]
        self.prev_weights = model.get_weights()

    def check_epoch(self, model):
        """"""Checks to run at the end of an epoch""""""
        self.check_untrained_params(model)

    def check_untrained_params(self, model):
        """"""Compare self.model.trainable_variables to self.prev_weights""""""
        passed = True
        curr_weights = model.get_weights()
        for curr_var, prev_var, var_name in zip(curr_weights, self.prev_weights, self.var_names):
            eq = np.equal(curr_var, prev_var).all()
            if eq:
                passed = False
                print(f""\nWarning: Variable {var_name} was not updated with training. ""
                      f""Confirm that this layer is correctly ""
                      f""connected to the computation graph."")
        self.prev_weights = [w.copy() for w in curr_weights]
        return passed


class WeightCheckerCallback(keras.callbacks.Callback):
    """"""Check model initialization and run training checks.
    """"""
    def __init__(self):
        super().__init__()
        self.weight_check = None

    def setup_weight_checker(
            self,
            model: keras.Model = None):
        """"""Initialize the callback with an input_batch and targets.""""""
        self.weight_check = WeightChecker(model)

    def on_train_begin(self, logs=None):
        if self.weight_check is None:
            raise ValueError(""setup_weight_checker() must be called to use WeightCheckerCallback."")

    def on_epoch_end(self, epoch, logs=None):
        self.weight_check.check_epoch(self.model)


def data_gen():
    """"""Generate random data for training.""""""
    data = (np.random.random((audio_len, 1)).astype(np.float32),
            np.random.random((audio_len, 1)).astype(np.float32))
    while True:
        yield data


batch_size = 64
audio_len = 16000
steps_per_epoch = 10
dataset = tf.data.Dataset.from_generator(data_gen,
                                         (tf.float32, tf.float32),
                                         (tf.TensorShape((audio_len, 1)), tf.TensorShape((audio_len, 1))))
dataset = dataset.batch(batch_size)

map_model = tf.keras.Sequential([tf.keras.layers.Conv1D(
        64, 3, padding='same'
    ),
    tf.keras.layers.Conv1D(
        1, 3, padding='same'
    )])
map_model(np.random.random((batch_size, audio_len, 1)))

aux_model = tf.keras.Sequential([tf.keras.layers.Dense(1)])
aux_model.trainable = False
aux_model(np.random.random((batch_size, 1)))


class MainModel(tf.keras.Model):
    """"""Main Model.""""""
    def __init__(self, map_model, aux_model):
        super().__init__()
        self.feature_dim = 128
        self.aux_model = aux_model
        self.map_model = map_model
        self.global_step = 0

    def call(self, inputs, training=True):
        output = self.map_model(inputs)
        return output

    def train_step(self, data):
        mixed_audio = data[0]
        clean_audio = data[1]

        with tf.GradientTape() as tape:
            decoded_audio = self.map_model(mixed_audio)
            total_loss = tf.reduce_mean(tf.abs(clean_audio - decoded_audio))

        grads = tape.gradient(total_loss, self.trainable_variables)
        [tf.print(f'Gradient std for {tv.name}: '
                  f'{np.std(g.numpy())}')
         for (g, tv) in zip(grads, self.trainable_variables)
         if 'conv1d/' in tv.name]
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        losses = {
            'loss': total_loss,
        }
        # tf.summary.scalar('loss', total_loss, step=self.global_step)
        self.global_step += 1
        return losses


epochs = 5

model = MainModel(map_model, aux_model)
model.compile(
    loss='mae',
    optimizer='adam',
    run_eagerly=True
)

weight_checker = WeightCheckerCallback()
weight_checker.setup_weight_checker(model)

model.fit(
    dataset,
    epochs=epochs,
    callbacks=[weight_checker],
    steps_per_epoch=steps_per_epoch
)
```

Running this code demonstrates that the gradients for conv1d are non-zero,
```
Gradient std for sequential/conv1d/kernel:0: 0.003920636139810085
Gradient std for sequential/conv1d/bias:0: 0.00040249430458061397
```
but the layer weights of conv1d are not updated
```
Warning: Variable sequential/conv1d/kernel:0 was not updated with training. Confirm that this layer is correctly connected to the computation graph.
Warning: Variable sequential/conv1d/bias:0 was not updated with training. Confirm that this layer is correctly connected to the computation graph.
```
In this case, the aux_model is not doing anything, it is only defined. However, the problem remains even if it is used in the computation.

**Describe the expected behavior**
The expected behaviour can be seen by setting `aux_model = None` in the above code. When this is done, the conv1d layer updates as expected.


**Standalone code to reproduce the issue**
See problem description

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
45580,Cannot load saved model when using multiple tf.keras.layers.experimental.preprocessing.StringLookup layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 and Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.8.5 (Ubuntu) and 3.6.9 (Colab)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

`v2.3.0-54-gfcc4b966f1 2.3.1`

**Describe the current behavior**

I am unable to load a saved model that has multiple `tf.keras.layers.experimental.preprocessing.StringLookup` layers.

When I load the model, I get the following error:
```
ValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table
```

**Describe the expected behavior**

I expect to be able to load the model.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1BrlOjxYVyVM3lEJRmeK9lNvev8Z4MVwT?usp=sharing

The notebook does the following:
1. Create a model with two `tf.keras.layers.experimental.preprocessing.StringLookup` layers.
2. Save the model with `tf.saved_model.save` in the same Keras model
3. Reload the model with `tf.saved_model.load`
4. `ValueError` is thrown

**Other info / logs** Include any logs or source code that would be helpful to

- It works when using a single `StringLookup` layer.
- I have observed the same problem with `tf.keras.layers.experimental.preprocessing.TextVectorization`.

Full error stacktrace (from Colab): [error.txt](https://github.com/tensorflow/tensorflow/files/5674123/error.txt)"
45579,"""SAME"" padding for avg_pool2d yields different results than explicitly 0-padded input with ""VALID""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.8.5

**Describe the current behavior**

When using padding ""SAME"" with average pooling, the results on a small input look wrong. If I use average pooling on a completely empty matrix with only 1 value of the matrix at  1, using the ""SAME"" padding algorithm I don't get a correct average value. If I explicitly 0-pad the input and use ""VALID"", I get the correct value. 

**Describe the expected behavior**

The behavior should be the same using :
* an explicitly 0-padded matrix with `tf.pad` and the ""VALID"" algorithm
* The ""SAME"" padding algorithm

**Standalone code to reproduce the issue**

**""SAME""**
```python
import tensorflow as tf
import numpy as np
generated = np.zeros((4,4))
generated[2,1] = 1
gen_nhwc = tf.constant(generated[np.newaxis,:,:,np.newaxis])
pool = 3
res = tf.nn.avg_pool2d(gen_nhwc, (pool,pool), (1,1),""SAME"")
result = np.squeeze(res.numpy())
print(""Input:\n"",generated)
print(""Output\n"",result)
```
```
Input:
 [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 0. 0.]]
Output
 [[0.         0.         0.         0.        ]
 [0.16666667 0.11111111 0.11111111 0.        ]
 [0.16666667 0.11111111 0.11111111 0.        ]
 [0.25       0.16666667 0.16666667 0.        ]]
```

**`tf.pad` and ""VALID"" results:** 
```python
import tensorflow as tf
import numpy as np
generated = np.zeros((4,4))
generated[2,1] = 1
gen_nhwc = tf.constant(generated[np.newaxis,:,:,np.newaxis])
pool = 3
paddings = [[0,0],[pool//2,pool//2],[pool//2,pool//2],[0,0]]
gen_pad = tf.pad(gen_nhwc, paddings, ""CONSTANT"")
res = tf.nn.avg_pool2d(gen_pad, (pool,pool), (1,1),""VALID"")
result = np.squeeze(res.numpy())
print(generated)
print(result)
```

```
Input:
 [[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 0. 0.]]
Output
 [[0.         0.         0.         0.        ]
 [0.11111111 0.11111111 0.11111111 0.        ]
 [0.11111111 0.11111111 0.11111111 0.        ]
 [0.11111111 0.11111111 0.11111111 0.        ]]
```
"
45578,"""""TypeError: 'NoneType' object is not iterable"""" when instantiating keras.models.Model() or keras.models.Sequential()","Whenever i try to instantiate a keras model either Model() or Sequential() i get **TypeError: 'NoneType' object is not iterable**. 
As an example I've taken screenshots of a Sequential model i wanted to create on the Mnist data using PyCharm as follows.

![capture-20201210-164722](https://user-images.githubusercontent.com/72258001/101787332-a4c33b00-3b07-11eb-8c4c-23ad30821723.png)

After running i got the following error message, #Note the first 3 lines are not important as they're just saying that there is no GPU found on my laptop;;; 

![capture-20201210-164800](https://user-images.githubusercontent.com/72258001/101788484-d38de100-3b08-11eb-9708-fd66b50d495d.png)
![capture-20201210-164815](https://user-images.githubusercontent.com/72258001/101788736-1ea7f400-3b09-11eb-8c8e-69d3b68ad83c.png)

I think my error is emanating from line 9 `network = models.Sequential()`.
I also think my imports are fine.  I have also tried running the same code in my jupyter notebook with the same apparent error. I also tried instantiating `model = models.Model()` from my notebook but with the same apparent error. 

The following packages are the ones mainly involved in deep learning and their respective versions on my laptop;
- python v3.7.7
- tensorflow 2.3.1
- keras 2.4.3

Please comment with anymore information you may need to assist me.
Thanks in advance.

"
45577,Tutorials still mention boosted_trees_classifier_train_in_memory,"## URL(s) with the issue:

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimator/boosted_trees_model_understanding.ipynb

## Description of issue (what needs changing):

The tutorials says 

> For performance reasons, when your data fits in memory, we recommend use the boosted_trees_classifier_train_in_memory function. 

However, function `boosted_trees_classifier_train_in_memory` doesn't exist. It's actually removed in ffc25308ce2be84240ec90502b38800bc5a4dd60.

### Submit a pull request?

I'm not sure whether it's fine to simply remove the sentense. You guys may want to add something else."
45575,Error when building tensorflow from source,"
**System information**
- OS Platform and Distribution: ubuntu 20.4
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0 & 2.3.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: from source, via bazel 
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: no GPU


Hi! I'm trying to install tf in ubuntu 20.4. Since for some reason AVX is not supported on ubuntu (I don't know why), the pip installation results in a core dump in python. So I tried installing from source via bazel 3.1.0. The bazel compilation is successful. However, when I build tensorflow via the following command:  bazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package, I get the following error:

ERROR: An error occurred during the fetch of repository 'eigen_archive':
java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz, https://gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz] to /home/rey/.cache/bazel/_bazel_rey/7a2ccf9885a6b6731b7d3780dd19d183/external/eigen_archive/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz: GET returned 406 Not Acceptable

So it's as if it's not finding the tar file on that address (https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz). I actually went to that address, and it works correctly. So I'm not sure why it cannot get it. Do you think I could just get them manually and put them in /.cache/bazel/... ? Any help would be appreciated. 

Thanks,
Rey

"
45574,TF 2.4rc3 fails with CUDNN_STATUS_INTERNAL_ERROR on GeForce RTX 3070,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image tensorflow/tensorflow:2.4.0rc3-gpu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Docker binary
- TensorFlow version (use command below): v2.4.0-rc2-20-g68f236364c 2.4.0-rc3
- Python version: Python 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA Version: 11.1
- GPU model and memory: GeForce RTX 3070 computeCapability: 8.6 coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s

**Describe the current behavior**

Tensorflow fails in initialization with CUDNN_STATUS_INTERNAL_ERROR.

**Describe the expected behavior**

Tensorflow should start up and train convolutional network.

**Standalone code to reproduce the issue**

May be able to provide if useful. But it's a bit of work so please let me know if there is some trivial reason for this first.

**Other info / logs**

Log output:
```
2020-12-10 11:32:15.021193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-10 11:32:16.006649: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-10 11:32:16.007443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-12-10 11:32:16.065141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.065643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-10 11:32:16.065669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-10 11:32:16.067864: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-12-10 11:32:16.067966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-12-10 11:32:16.068967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-10 11:32:16.069211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-10 11:32:16.073175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-12-10 11:32:16.074124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-12-10 11:32:16.074283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-12-10 11:32:16.074455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.074971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.075363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-10 11:32:16.076134: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-10 11:32:16.076244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.076736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2020-12-10 11:32:16.076808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-10 11:32:16.076855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-12-10 11:32:16.076878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-12-10 11:32:16.076897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-12-10 11:32:16.076917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-12-10 11:32:16.076938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-12-10 11:32:16.076959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-12-10 11:32:16.076975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-12-10 11:32:16.077076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.077578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.078120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-12-10 11:32:16.078276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-12-10 11:32:16.934447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-10 11:32:16.934503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-12-10 11:32:16.934513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-12-10 11:32:16.934781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.935233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.935641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-12-10 11:32:16.936014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7148 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6)
2020-12-10 11:32:17.278390: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-12-10 11:32:17.298521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593615000 Hz
2020-12-10 11:32:23.537921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-12-10 11:32:23.970919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-12-10 11:32:23.973136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-12-10 11:32:24.772020: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-12-10 11:32:24.786874: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""train.py"", line 68, in <module>
    train_step(batch)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 2943, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 560, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node encoder/conv2d/Conv2D (defined at /code/model.py:101) ]] [Op:__inference_train_step_9520]

Errors may have originated from an input operation.
Input Source operations connected to node encoder/conv2d/Conv2D:
 truediv (defined at /code/model.py:8)

Function call stack:
train_step
```

Full environment:
```
== check python ===================================================
python version: 3.6.9
python branch: 
python build version: ('default', 'Oct  8 2020 12:12:24')
python compiler version: GCC 8.4.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020
os release version: 5.4.0-56-generic
os platform: Linux-5.4.0-56-generic-x86_64-with-Ubuntu-18.04-bionic
linux distribution: ('Ubuntu', '18.04', 'bionic')
linux os distribution: ('Ubuntu', '18.04', 'bionic')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='e377002ed67b', release='5.4.0-56-generic', version='#62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020', machine='x86_64', processor='x86_64')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                  1.19.4
protobuf               3.13.0
tensorflow-estimator   2.4.0rc0
tensorflow-gpu         2.4.0rc3

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.4.0-rc3
tf.version.GIT_VERSION = v2.4.0-rc2-20-g68f236364c
tf.version.COMPILER_VERSION = 7.3.1 20180303
        65:	find library=libc.so.6 [0]; searching
        65:	 search path=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls:/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/tls/x86_64/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:/usr/local/nvidia/lib/tls/x86_64/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/tls/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/tls/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/cuda/lib64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/tls/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/tls/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/tls/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/x86_64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
        65:	  trying file=/usr/local/nvidia/lib64/libc.so.6
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
        65:	
        65:	find library=libpthread.so.0 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libpthread.so.0
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libpthread.so.0
        65:	
        65:	find library=libdl.so.2 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libdl.so.2
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libdl.so.2
        65:	
        65:	find library=libutil.so.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libutil.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libutil.so.1
        65:	
        65:	find library=libexpat.so.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libexpat.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libexpat.so.1
        65:	
        65:	find library=libz.so.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libz.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libz.so.1
        65:	
        65:	find library=libm.so.6 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libm.so.6
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libm.so.6
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libpthread.so.0
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libc.so.6
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libm.so.6
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libz.so.1
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libexpat.so.1
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libutil.so.1
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libdl.so.2
        65:	
        65:	
        65:	initialize program: /usr/local/bin/python
        65:	
        65:	
        65:	transferring control: /usr/local/bin/python
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libffi.so.6 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libffi.so.6
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/usr/lib/x86_64-linux-gnu/libffi.so.6
        65:	
        65:	
        65:	calling init: /usr/lib/x86_64-linux-gnu/libffi.so.6
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libtensorflow_framework.so.2 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..		(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
        65:	
        65:	find library=librt.so.1 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..		(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/librt.so.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../librt.so.1
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/librt.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/librt.so.1
        65:	
        65:	find library=libstdc++.so.6 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..		(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libstdc++.so.6
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libstdc++.so.6
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libstdc++.so.6
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
        65:	
        65:	find library=libgcc_s.so.1 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..		(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libgcc_s.so.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libgcc_s.so.1
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libgcc_s.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1
        65:	
        65:	
        65:	calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/librt.so.1
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
        65:	
        65:	find library=libcudart.so.11.0 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..		(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libcudart.so.11.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libcudart.so.11.0
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libcudart.so.11.0
        65:	
        65:	
        65:	calling init: /usr/local/cuda/lib64/libcudart.so.11.0
        65:	
2020-12-10 12:08:45.967347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
        65:	find library=libcrypto.so.1.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libcrypto.so.1.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1
        65:	
        65:	
        65:	calling init: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libopenblasp-r0-ae94cfde.3.9.dev.so [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	
        65:	find library=libgfortran-2e0d59d6.so.5.0.0 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0
        65:	
        65:	find library=libquadmath-2d0c479f.so.0.0.0 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0
        65:	
        65:	find library=libz-eb09ad1d.so.1.2.3 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libbz2.so.1.0 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libbz2.so.1.0
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=liblzma.so.5 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/liblzma.so.5
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/liblzma.so.5
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/liblzma.so.5
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libmpdec.so.2 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libmpdec.so.2
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2
        65:	
        65:	
        65:	calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/fft/_pocketfft_internal.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/bit_generator.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_common.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_bounded_integers.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_mt19937.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_philox.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_pcg64.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_sfc64.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_generator.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	
        65:	
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_session.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_tf_stack.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_utils.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_exception_registry.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_bfloat16.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_dtypes.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_op_def_registry.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_events_writer.so
        65:	
        65:	find library=libuuid.so.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libuuid.so.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/lib/x86_64-linux-gnu/libuuid.so.1
        65:	
        65:	
        65:	calling init: /lib/x86_64-linux-gnu/libuuid.so.1
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_file_io.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_record_io.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_checkpoint_reader.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_parallel_device.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensor_float_32_execution.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_func.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/service/_pywrap_server_lib.so
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_device_lib.so
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_multiprocessing.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libssl.so.1.1 [0]; searching
        65:	 search path=/usr/local/cuda/lib64		(LD_LIBRARY_PATH)
        65:	  trying file=/usr/local/cuda/lib64/libssl.so.1.1
        65:	 search cache=/etc/ld.so.cache
        65:	  trying file=/usr/lib/x86_64-linux-gnu/libssl.so.1.1
        65:	
        65:	
        65:	calling init: /usr/lib/x86_64-linux-gnu/libssl.so.1.1
        65:	
        65:	
        65:	calling init: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_uarray/_uarray.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/fft/_pocketfft/pypocketfft.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_flow.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_matching.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libhdf5-beac1db3.so.103.0.0 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/libhdf5-beac1db3.so.103.0.0
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0
        65:	
        65:	find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
        65:	
        65:	find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/.		(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
        65:	
        65:	find library=libaec-2147abcd.so.0.0.4 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/.		(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
        65:	
        65:	find library=libz-a147dcb0.so.1.2.3 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/.		(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so
        65:	
        82:	find library=libc.so.6 [0]; searching
        82:	 search path=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls:/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/tls/x86_64/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:/usr/local/nvidia/lib/tls/x86_64/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64		(LD_LIBRARY_PATH)
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/extras/CUPTI/lib64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/tls/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/tls/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/cuda/lib64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/tls/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/tls/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/tls/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/x86_64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6
        82:	  trying file=/usr/local/nvidia/lib64/libc.so.6
        82:	 search cache=/etc/ld.so.cache
        82:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
        82:	
        82:	
        82:	calling init: /lib/x86_64-linux-gnu/libc.so.6
        82:	
        82:	
        82:	initialize program: /bin/sh
        82:	
        82:	
        82:	transferring control: /bin/sh
        82:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5pl.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_optimizer.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_cluster.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfprof.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_quantize_training.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_stacktrace_handler.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_util_port.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_debug_events_writer.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_mlir.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_python_op_gen.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_toco_api.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	find library=libopenblasp-r0-085ca80a.3.9.so [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/libopenblasp-r0-085ca80a.3.9.so
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so
        65:	
        65:	find library=libgfortran-ed201abd.so.3.0.0 [0]; searching
        65:	 search path=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs		(RPATH from file /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)
        65:	  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs_cxx.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/specfun.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_comb.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so
        65:	
        65:	
        65:	calling init: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so
        65:	
        65:	
        65:	calling fini: /usr/local/bin/python [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/x86_64-linux-gnu/libffi.so.6 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/cuda/lib64/libcudart.so.11.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/fft/_pocketfft_internal.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/bit_generator.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_common.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_bounded_integers.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_mt19937.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_philox.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_pcg64.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_sfc64.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_generator.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	
        65:	
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_session.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_tf_stack.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_utils.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_exception_registry.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_bfloat16.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_dtypes.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_op_def_registry.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_events_writer.so [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_file_io.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_record_io.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_checkpoint_reader.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_parallel_device.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensor_float_32_execution.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_func.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/service/_pywrap_server_lib.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_device_lib.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_multiprocessing.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/x86_64-linux-gnu/libssl.so.1.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_uarray/_uarray.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/fft/_pocketfft/pypocketfft.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_flow.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_matching.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5pl.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_optimizer.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_cluster.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfprof.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_quantize_training.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_stacktrace_handler.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_util_port.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_debug_events_writer.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_mlir.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_python_op_gen.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_toco_api.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs_cxx.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/specfun.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_comb.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0 [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so [0]
        65:	
        65:	
        65:	calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2 [0]
        65:	
        65:	
        65:	calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
        65:	
        65:	
        65:	calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
        65:	

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Dec 10 12:08:47 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3070    Off  | 00000000:08:00.0 Off |                  N/A |
|  0%   41C    P8    19W / 270W |     68MiB /  7979MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart_static.a

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 9, 'final', 0)

== bazel version  ===============================================

```"
45573,save_model/load_model pair does not preserve model's metric,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian testing
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.8.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
After saving and then loading a model it produces different metric results.

**Describe the expected behavior**
Everything should be the same 

**Standalone code to reproduce the issue**
Please see a colab notebook [here](https://colab.research.google.com/drive/1-vUEU1gFf7GRrhy95E-h6xw_UB80yDSB?usp=sharing)

Note that the metric of the loaded model is `accuracy: 1.0`, whereas original model produced `accuracy: 0.125'
"
45572,A quantized tf code that contain tf.where op cannot be converted to tflite (operator of type Where is not yet implemented.),"**System information**
- OS Platform and Distribution: ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 1.15.3


**Text output from tflite_convert**

```
2020-12-10 11:34:41.253749: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 83 operators, 206 arrays (1 quantized)
2020-12-10 11:34:41.277151: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Where for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted
```

**Any other info / logs**

When converting a model to tflite, got the error above.

- Added tf.where() op to the model code (as a custom output layer).
- Model: DeepLabV3 (using MobileNetV2 backbone) - [official itensorflow inplementation](https://github.com/tensorflow/models/tree/master/research/deeplab).
- Finetuned using Quantization-aware training.
- Convertion to tflite was done using the [provided script](https://github.com/tensorflow/models/blob/master/research/deeplab/convert_to_tflite.py).


Would appreciate any help."
45571,NCCL version is always 2.7.3,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / cudnn7.6
- GPU model and memory: Tesla V100-SXM3-32GB

**Describe the current behavior**
When I run [image classification model](https://github.com/tensorflow/models/tree/master/official/vision/image_classification) from models repository with the following command:

```
NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 mnist_main.py   --model_dir=modeldir   --data_dir=datadir   --train_epochs=10   --distribution_strategy=mirrored
```
I always get NCCL version as ""2.7.3+cudaCUDA_MAJOR.CUDA_MINOR"". The problem is that I don't have NCCL 2.7.3 but it still seems to be working because I can get ncclAllReduce calls with NCCL_DEBUG_SUBSYS=COLL. NCCL version of the machine that I am using is 2.7.8.   

I also cannot intercept NCCL collectives such as ncclAllReduce with LD_PRELOAD trick. Does Tensorflow use a static library or something else because in normal case I should be able to intercept any nccl call with LD_PRELOAD. It works with PyTorch, C++ NCCL apps etc. 

**Describe the expected behavior**
I expect to get NCCL version as 2.7.8
I expect to be able to intercept NCCL functions with a preloaded library (LD_PRELOAD trick)

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 mnist_main.py   --model_dir=modeldir   --data_dir=datadir   --train_epochs=10   --distribution_strategy=mirrored
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. 

```
2020-12-10 09:16:28.794595: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-12-10 09:16:31.039310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-12-10 09:16:31.360781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:34:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:31.363748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:be:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:31.366700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: 
pciBusID: 0000:e0:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:31.369582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties: 
pciBusID: 0000:e2:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:31.369617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-12-10 09:16:31.372516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-12-10 09:16:31.374946: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-12-10 09:16:31.375800: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-12-10 09:16:31.378188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-12-10 09:16:31.379703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-12-10 09:16:31.384111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-12-10 09:16:31.406596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3
2020-12-10 09:16:31.408615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-10 09:16:31.437675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz
2020-12-10 09:16:31.437913: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555595fabf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-10 09:16:31.437935: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-12-10 09:16:32.895096: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558f709a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-12-10 09:16:32.895130: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM3-32GB, Compute Capability 7.0
2020-12-10 09:16:32.895136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM3-32GB, Compute Capability 7.0
2020-12-10 09:16:32.895140: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM3-32GB, Compute Capability 7.0
2020-12-10 09:16:32.895145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM3-32GB, Compute Capability 7.0
2020-12-10 09:16:32.898035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:34:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:32.900943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:be:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:32.903831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: 
pciBusID: 0000:e0:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:32.906747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties: 
pciBusID: 0000:e2:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s
2020-12-10 09:16:32.906777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-12-10 09:16:32.906803: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-12-10 09:16:32.906813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-12-10 09:16:32.906823: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-12-10 09:16:32.906832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-12-10 09:16:32.906853: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-12-10 09:16:32.906863: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-12-10 09:16:32.928663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3
2020-12-10 09:16:32.928717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-12-10 09:16:35.890042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-10 09:16:35.890086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 2 3 
2020-12-10 09:16:35.890095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y Y Y 
2020-12-10 09:16:35.890101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N Y Y 
2020-12-10 09:16:35.890106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 2:   Y Y N Y 
2020-12-10 09:16:35.890112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 3:   Y Y Y N 
2020-12-10 09:16:35.904988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3070 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM3-32GB, pci bus id: 0000:34:00.0, compute capability: 7.0)
2020-12-10 09:16:35.909256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30099 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM3-32GB, pci bus id: 0000:be:00.0, compute capability: 7.0)
2020-12-10 09:16:35.913101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30099 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM3-32GB, pci bus id: 0000:e0:00.0, compute capability: 7.0)
2020-12-10 09:16:35.916761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 30099 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM3-32GB, pci bus id: 0000:e2:00.0, compute capability: 7.0)
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
I1210 09:16:35.924412 23456247883584 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
I1210 09:16:35.927785 23456247883584 dataset_info.py:362] Load dataset info from datadir/mnist/3.0.1
I1210 09:16:35.930115 23456247883584 dataset_info.py:411] Field info.citation from disk and from code do not match. Keeping the one from code.
I1210 09:16:35.930334 23456247883584 dataset_builder.py:528] Constructing tf.data.Dataset for split ['train', 'test'], from datadir/mnist/3.0.1
2020-12-10 09:16:36.191140: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-12-10 09:16:36.191203: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 4 GPUs
2020-12-10 09:16:36.192815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.1
2020-12-10 09:16:37.043438: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed
Epoch 1/10
WARNING:tensorflow:From /home/pakthar/.conda/envs/tfmodels/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
W1210 09:16:37.291702 23456247883584 deprecation.py:317] From /home/pakthar/.conda/envs/tfmodels/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
I1210 09:16:37.575582 23456247883584 cross_device_ops.py:699] batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:37.790755 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:37.793543 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:37.797662 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:37.800141 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
I1210 09:16:39.279846 23456247883584 cross_device_ops.py:699] batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:39.472889 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:39.475625 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:39.479524 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I1210 09:16:39.482020 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2020-12-10 09:16:40.931919: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-12-10 09:16:45.904130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
NCCL version 2.7.3+cudaCUDA_MAJOR.CUDA_MINOR
g001:35039:35203 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x1553c2000000 recvbuff 0x1553c2000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f80001340 [nranks=4] stream 0x15506756bc20
g001:35039:35205 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x1554192c6e00 recvbuff 0x1554192c6e00 count 1643370 datatype 7 op 0 root 0 comm 0x154f74001310 [nranks=4] stream 0x1550675b3630
g001:35039:35204 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x155108000000 recvbuff 0x155108000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f78001310 [nranks=4] stream 0x1550672ff1a0
g001:35039:35206 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x155058000000 recvbuff 0x155058000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f6c001310 [nranks=4] stream 0x15506756bca0
```
"
45570,failed to compile tensorflow (V2.4_RC4) in debug mode,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.117.bsk.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4_rc4
- Python version:Python 3.7.3
- Installed using virtualenv? pip? conda?: build from source 
- Bazel version (if compiling from source):bazel 3.1.0
- GCC/Compiler version (if compiling from source):gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version: Cuda compilation tools, release 11.1, V11.1.105
- GPU model and memory: V100(32G), host memory>120GB



**Describe the problem**
I want to compile Tensorflow from source code, using the debug model, when I pass args to the bazel, it ends up with failure as below:
ERROR: /home/chengyang/src/tf-src/tensorflow_2.4_rc4/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/chengyang/.cache/bazel/_bazel_zhoudongyan.daniel/b440c4e0619e08e4030082228cc9dfe0/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8 \
    LD_LIBRARY_PATH=/opt/XXX/jdk/jre/lib/amd64/server:/opt/XXX/lib/native:/opt/XXX/lib/native/ufs:/opt/XXX/lib/native:/opt/XXX/lib/native:/opt/XXX/lib:/usr/local/cuda/compat:/usr/local/cuda/lib64: \
    PATH=/home/chengyang/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/home/chengyang/software/bazelisk:/usr/local/cuda/bin:/home/chengyang/system_op/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tige
r/arnold/arnold_entrypoint/tools:/opt/XXX/bin:/opt/XXX/bin:/home/chengyang/.local/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \
    TF_NEED_CUDA=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_framework.so.2.4.0-2.params)
Execution platform: @local_execution_config_platform//:platform
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcublas_plugin.pic.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can n
ot be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: nonrepresentable section on output
collect2: error: ld returned 1 exit status
Target //tensorflow/compiler/xla/tools:run_hlo_module failed to build
INFO: Elapsed time: 111.012s, Critical Path: 93.19s
INFO: 2174 processes: 2174 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 ```bazel build //tensorflow/compiler/xla/tools:run_hlo_module  --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=cuda --compilation_mode=dbg --strip=never --copt=""-g"" --cxxopt=""-g"" --copt=""-O0"" --cxxopt=""-O0""  --verbose_failures```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I have successfully compiled the source code with ```--compilation_mode=opt --strip=never --copt=""-g"" --cxxopt=""-g""  --verbose_failures```,  however, it turns out to be that ""-O2"" is enabled, and I hope to pass ""-O0"" to track the execution with GDB. "
45569,Add RaggedTensor support to tf.stop_gradient and tf.reduce_logsumexp,"
**System information**
- TensorFlow version (you are using): 2.4.0rc3
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.** Tensorflow prohibits use of tf.stop_gradient or tf.reduce_logsumexp with RaggedTensor. Other tf.math.reduce_XXX methods support RaggedTensor though.

**Will this change the current api? How?** Should not.

**Who will benefit with this feature?** In my case I was implementing gumbel-softmax with straight-through estimator for seq2seq with sampling from multiple positions in the same sequence. Straight-through estimator requires tf.stop_gradient call. Doing this with RaggedTensor would've been much more efficient.

**Any Other info.**
"
45568,"Using mixed precision causes incorrect loss, metric values","Tensorflow version : 2.4.0-rc3, compiled from source
GPU : RTX 3080 10GB
CUDA / CUDNN : 11.1 / 8
bazel version : 3.1.0
Windows 10

I decided to mixed precision to speed up the training, but some issues were.
I used this code:
```
policy = keras.mixed_precision.Policy('mixed_float16')
keras.mixed_precision.set_global_policy(policy)

...

model.compile(optimizer = keras.mixed_precision.LossScaleOptimizer(optimizers.Adam(learning_rate=1e-2))
                        loss = dice_loss
                        metric = [dice_coefficient]
                                                )
```
**Training didn't show any nan loss and metric. But when evaluating the model or each epoch ended, val loss and metric were incorrect(they were fixed over epochs).** The colab notebook is **[here](https://colab.research.google.com/drive/1kLpn88HSn7WOnH5vN5l-GlLRTPVArsDw?usp=sharing)**

etc:
- The values of `model.predict(x)` and `model(x)` are different. When the former was nan, the latter seemed normal."
45566,"raise ValueError(""Shapes %s and %s are incompatible"" % (self, other)) ValueError: Shapes (2, 56, 56, 1024) and () are incompatible","Traceback (most recent call last):
  File ""main.py"", line 191, in <module>
    tf.app.run()
  File ""C:\Users\User\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 187, in main
    run_training()
  File ""main.py"", line 164, in run_training
    loss_align_reg, vs_train_op, vs_eval_op, offset_pred, loss_reg = model.construct_model()
  File ""C:\Users\User\Desktop\video search\TALL-master\ctrl_model.py"", line 167, in construct_model
    sim_reg_mat, sim_reg_mat_test = self.visual_semantic_infer(self.visual_featmap_ph_train, self.sentence_ph_train, self.visual_featmap_ph_test, self.sentence_ph_test)
  File ""C:\Users\User\Desktop\video search\TALL-master\ctrl_model.py"", line 78, in visual_semantic_infer
    cross_modal_vec_train = self.cross_modal_comb(transformed_clip_train_norm, transformed_sentence_train_norm, self.batch_size)
  File ""C:\Users\User\Desktop\video search\TALL-master\ctrl_model.py"", line 59, in cross_modal_comb
    concat_feature = tf.reshape(tf.concat(2,[vv_feature, ss_feature]),[batch_size, batch_size, self.semantic_size+self.semantic_size])
  File ""C:\Users\User\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\User\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\array_ops.py"", line 1297, in concat
    tensor_shape.scalar())
  File ""C:\Users\User\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\framework\tensor_shape.py"", line 1103, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (2, 56, 56, 1024) and () are incompatible

Can anyone fix this issue?"
45565,tf.SparseTensorSpec in input_signature of tf.function does not take effect,"**Describe the current behavior**
When we define a `tf.SparseTensorSpec` with some specific shape in the `input_signature` of `tf.function`, we still can pass a `tf.SparseTensor` with any arbitrary shape into the decorated function.

Related to #29198. For now, the shape of input sparse tensor is always `None` inside tf.function, which I think contradicts with the purpose of `input_signature`.

**Describe the expected behavior**
Like `tf.TensorSpec`, it should raise an error when the input `tf.SparseTensor` is not compatible with the `tf.SparseTensorSpec`.

**Standalone code to reproduce the issue**
See [this notebook](https://colab.research.google.com/drive/1ArjLQqS6VufIla_IN0JykHSPpYUEG7gW?usp=sharing).
"
45558,Keras optimizer uses non-trainable variables instead of constants for hyperparameters,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Keras OptimizerV2 converts hyperparameters which are python data types to non-trainable TF Variables. Is there a reason they are not converted to tf.constant instead? Here's the relevant piece of code from `optimizer_v2.py`:

```python
def _create_hypers(self):
    if self._hypers_created:
      return
    # Iterate hyper values deterministically.
    for name, value in sorted(self._hyper.items()):
      if isinstance(
          value, (ops.Tensor, tf_variables.Variable)) or callable(value):
        continue
      else:
        self._hyper[name] = self.add_weight(
            name,
            shape=[],
            trainable=False,
            initializer=value,
            aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)
    self._hypers_created = True
```

**Describe the expected behavior**

It seems unnecessary to create variables for hyperparameters that are known to be constant.

**Standalone code to reproduce the issue**

This code creates a variable for the learning rate in the graph.

```python
tf.keras.optimizers.SGD(learning_rate=0.01)
```"
45557,"Cannot load SavedModel when model trained with tfa.optimizers.NovoGrad: ValueError: Shapes (3, 8) and () are incompatible","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.6.10
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I'm training a model with the `tfa.optimizer.NovoGrad` (https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/NovoGrad). The model appears to train fine, but when I try to load the model using `tf.keras.models.load_model(""my_model"")`, I get a `ValueError` regarding the shapes. A minimal example to replicate this behavior is provided below:

```python
# Train a small model and save it as a SavedModel
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(2))
model.compile(optimizer=tfa.optimizers.NovoGrad(), loss=tf.keras.losses.sparse_categorical_crossentropy)
callbacks = [tf.keras.callbacks.ModelCheckpoint(""scratch"")]

x = np.random.random((2, 3))
y = np.random.randint(0, 2, (2,))
model.fit(x, y, batch_size=1, epochs=1, callbacks=callbacks)
```
Now close the python session, open a new one, and try to load the model:
```python
import tensorflow as tf
model = tf.keras.models.load_model(""scratch"")

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 121, in load
    path, options=options, loader_cls=KerasObjectLoader)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 633, in load_internal
    ckpt_options)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 131, in __init__
    self._restore_checkpoint()
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py"", line 328, in _restore_checkpoint
    self._checkpoint_options).expect_partial()
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 1320, in restore
    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 209, in restore
    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 908, in _restore_from_checkpoint_position
    visit_queue=visit_queue))
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 943, in _single_restoration_from_checkpoint_position
    if child_position.bind_object(trackable=local_object):
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 258, in bind_object 
    slot_name=slot_restoration.slot_name)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 1236, in _create_or_restore_slot_variable
    slot_variable_position.restore(slot_variable)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 209, in restore
    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 914, in _restore_from_checkpoint_position
    tensor_saveables, python_saveables))
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 297, in restore_saveables
    validated_saveables).restore(self.save_path_tensor, self.options)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py"", line 340, in restore
    restore_ops = restore_fn()
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py"", line 316, in restore_fn
    restore_ops.update(saver.restore(file_prefix, options))
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/functional_saver.py"", line 111, in restore
    restored_tensors, restored_shapes=None)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 127, in restore
    self.handle_op, self._var_shape, restored_tensor)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 311, in shape_safe_assign_variable_handle
    shape.assert_is_compatible_with(value_tensor.shape)
  File ""/home/tblstri/anaconda3/envs/TENSORFLOW_CPU/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 1134, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (3, 8) and () are incompatible
```

**Describe the expected behavior**
The saved model should be loaded without error. 

**Standalone code to reproduce the issue**
Standalone code provided above.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
