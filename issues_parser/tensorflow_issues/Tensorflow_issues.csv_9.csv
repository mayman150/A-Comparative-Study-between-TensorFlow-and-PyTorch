Issue Number,Issue Title,Issue Body
53556,tf.data.experimental.save not saving my created tf.data.Dataset ,"Tensorflow version: 2.7.0

**Description of the issue:**

i made use of the tf.data.experimental.CsvDataset class to create tf.data.Dataset from my csv data which i want to save with tf.data.experimental.save. upon attempting to save the dataset, the tf.data.experimental.save keeps running endlessly without saving the data. 

`csv data file size: 67108864 bytes`

**Description of the expected behavior:**
tf.data.experimental.save should save the tf.data.Dataset created to a file

**Code to create the tf.data.Dataset and save same to file:**
```
Dataset = tf.data.experimental.CsvDataset(
    my_data.csv', 
    record_defaults=column_types, 
    header=True) # this runs successfully
```

`tf.data.experimental.save(Dataset, ""./Dataset"", compression='GZIP')` # this runs endlessly without throwing an error nor saving the dataset  to file






"
53555,quantization model failed,"### 1. System information

- Windows10
- TensorFlow 2.7
- 

### 2. Code

YOLOV5 provides  tensorflow int8 optimization failure


```
# keras_model I wrote is no problem 
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
converter.target_spec.supported_types = [tf.float16]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
if int8:
      dataset = LoadImages(check_dataset(data)['train'], img_size=imgsz, auto=False)  # representative data
      converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib)
      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
      converter.target_spec.supported_types = []
      converter.inference_input_type = tf.uint8  # or tf.int8
      converter.inference_output_type = tf.uint8  # or tf.int8
      converter.experimental_new_quantizer = False
      f = str(file).replace('.pt', '-int8.tflite')

tflite_model = converter.convert()
open(f, ""wb"").write(tflite_model)
LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
```


TensorFlow Lite: export failure: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (16 != 1)Node number 3 (CONV_2D) failed to prepare.

"
53554,Typing tensorflow.data.Dataset,"**System information**
- TensorFlow version (you are using): 2.5
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

I have some functions that accept instances of tensorflow.data.Dataset. I would like to type those functions so that the caller knows what is expected. However, doing `data: tensorflow.data.Dataset`, is not good enough, because people wouldn't know what kind of structure is produced by the object (shape of tensors, whether is tuple or dictionary, etc).

There is another object, `tensorflow.data.DatasetSpec`, which can be used to document the objects that will be produces by the tensorflow.data.Dataset. However, I am not sure how to put those things together for a typing annotation, and I am left to describe the structure in docstring, which I do not think is ideal.

I would love to see something like a generic, where you pass the type it accepts, like `Dataset[Spec]`(like in lists we do `List[int]`), but I am happy to learn some other ways to achieve more descriptive typing than just `data: tensorflow.data.Dataset`.

**Will this change the current api? How?**
Only for typing purposes

**Who will benefit with this feature?**
A lot of people from the Machine Learning community using Tensorflow to build production systems. 

**Any Other info.**
I could help if provided with guidance. "
53552,DSP Overflow - high pixel values are being clamped when running on DSP,"Hi,
I trained a keras model to extract gray-level segmentation maps. 
I converted the model to TFLite and quantized the model.
The quantized model produces similar results on CPU and DSP HWs, if the values of the pixels are not very high (<<1).
If the maps produce pixel predictions with high values that are closer to 1, it seems that the values are being clamped when running on DSP, unlike in CPU which produces reasonable maps.

I repeated the tests on versions 2.2, 2.4, and 2.7 and all demonstrated the same results.

What can I do to change this?
Should I change the dynamic range of the maps?
"
53550,I failed  to install   tflite-model-maker,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Linux Ubuntu 18):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I want to install   tflite-model-maker  ,   i use ""pip3 install tflite-model-maker  "" or  ""pip install tflite-model-maker  "" 
print:
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
spyder 3.3.6 requires pyqt5<5.13; python_version >= ""3"", which is not installed.
spyder 3.3.6 requires pyqtwebengine<5.13; python_version >= ""3"", which is not installed.
conda 4.10.1 requires ruamel_yaml_conda>=0.11.14, which is not installed.
astroid 2.3.1 requires typed-ast<1.5,>=1.4.0; implementation_name == ""cpython"" and python_version < ""3.8"", which is not installed.
spleeter 1.5.0 requires librosa==0.7.2, but you have librosa 0.8.1 which is incompatible.
spleeter 1.5.0 requires pandas==0.25.1, but you have pandas 1.3.5 which is incompatible.
spleeter 1.5.0 requires tensorflow==1.15.0, but you have tensorflow 2.7.0 which is incompatible.
astroid 2.3.1 requires six==1.12, but you have six 1.16.0 which is incompatible.
astroid 2.3.1 requires wrapt==1.11.*, but you have wrapt 1.13.3 which is incompatible.

"
53549,This is spam,<spam removed>
53548,Failed to load the native TensorFlow runtime.,"Hi,
I am having an issue when importing tensorflow into jupyter notebook. I installed tensorflow using pip install tensorflow using a command prompt opened via anaconda. I checked which versions of python and tensorflow are installed, the command line says I have python 3.8.5 and tensorflow 2.7.0. I believe they are compatible with eachother.
The error I get when importing it is attached below. 
![Issue](https://user-images.githubusercontent.com/96695861/147415587-9a175ade-f747-449b-b04e-01666ba43b08.PNG)

How can this be solved?
Many thanks"
53546,Error when installing TF2.2 based on ROCm 3.3.0,"``**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.2.0
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 8.4.0
- ROCm version: 3.3.0
- GPU model and memory:  AMD gfx906



**Describe the problem**
Hello, everyone.
I got the following error when compiling the C++interface of TF2.2 based on ROCm 3.3.0.
Install command: bazel build --config opt --config=rocm --verbose_failures //tensorflow:libtensorflow_cc.so --jobs 30
Error massage:
`ERROR: /public/share/ac8zby7vk0/wankw/software/test1/tensorflow/tensorflow/BUILD:710:1: Linking of rule '//tensorflow:libtensorflow_cc.so.2.2.0' failed (Exit 1)
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<Eigen::half, float>::Run(Eigen::GpuDevice const&, Eigen::half const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<float, float>::Run(Eigen::GpuDevice const&, float const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<double, float>::Run(Eigen::GpuDevice const&, double const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.pic.o:debug_ops_gpu.cu.cc:function tensorflow::CurtHealthLaunch<short, float>::Run(Eigen::GpuDevice const&, short const*, int, float*): error: undefined reference to 'tensorflow::TfCheckOpHelperOutOfLine(tensorflow::Status const&, char const*)'
bazel-out/k8-opt/bin/external/com_github_grpc_grpc/_objs/gpr_base/string.pic.o:string.cc:function gpr_leftpad(char const*, char, unsigned long): warning: memset used with constant zero length parameter; this could be due to transposed parameters
bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/support/SmallPtrSet.pic.o:SmallPtrSet.cpp:function llvm::SmallPtrSetImplBase::Grow(unsigned int): warning: memset used with constant zero length parameter; this could be due to transposed parameters
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o:spacetobatch_functor_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o:spacetobatch_functor_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/depthtospace_op_gpu.cu.pic.o:depthtospace_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/spacetodepth_op_gpu.cu.pic.o:spacetodepth_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/diag_op_gpu/diag_op_gpu.cu.pic.o:diag_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/diag_op_gpu/diag_op_gpu.cu.pic.o:diag_op_gpu.cu.cc:function std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*): error: undefined reference to 'tensorflow::internal::CheckOpMessageBuilder::NewString()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_1.cu.pic.o:where_op_gpu_impl_1.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_1.cu.pic.o:where_op_gpu_impl_1.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_2.cu.pic.o:where_op_gpu_impl_2.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_3.cu.pic.o:where_op_gpu_impl_3.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/where_op_gpu/where_op_gpu_impl_4.cu.pic.o:where_op_gpu_impl_4.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, char const*, std::string, char const*, std::string>(char const*, char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:function tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to 'tensorflow::TensorShapeRep::DebugString() const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<Eigen::half>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<float>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<double>: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.pic.o:dynamic_partition_op_gpu.cu.cc:vtable for tensorflow::DynamicPartitionOpGPU<std::complex<float> >: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, Eigen::half>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeOp<Eigen::GpuDevice, double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResize<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, float, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op/crop_and_resize_op.pic.o:crop_and_resize_op.cc:function std::_Function_handler<void (), tensorflow::CropAndResizeGradImageOp<Eigen::GpuDevice, double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&): error: undefined reference to 'tensorflow::functor::CropAndResizeBackpropImage<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext const*, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long>, 16, Eigen::MakePointer>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.pic.o:bincount_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/bucketize_op_gpu/bucketize_op_gpu.cu.pic.o:bucketize_op_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/histogram_op_gpu/histogram_op_gpu.cu.pic.o:histogram_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPU<double, 3, 3, 1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPU<double, 3, 3, -1>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<double, (tensorflow::DepthwiseConv2dDirection)0, 3, 3, 2, false>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::LaunchDepthwiseConv2dGPUSmall<double, (tensorflow::DepthwiseConv2dDirection)0, 3, 3, 2, true>(tensorflow::OpKernelContext*, tensorflow::DepthwiseArgs const&, double const*, double const*, double*, tensorflow::TensorFormat): error: undefined reference to 'tensorflow::ToString(tensorflow::TensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_double.cu.pic.o:depthwise_conv_op_gpu_double.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*>(char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/depthwise_conv_op_gpu_float.cu.pic.o:depthwise_conv_op_gpu_float.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*>(char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/broadcast_to_op_gpu/broadcast_to_op_gpu.cu.pic.o:broadcast_to_op_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Unimplemented<char const*, std::string, char const*, std::string, char const*>(char const*, std::string, char const*, std::string, char const*): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::EventMgr::FreeMemory(absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> > const&): error: undefined reference to 'tensorflow::LogMemory::RecordRawDeallocation(std::string const&, long long, void*, tensorflow::Allocator*, bool)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::SummarizeValue(long long, bool) const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::SummarizeValue(long long, bool) const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::DataTypeString(tensorflow::DataType)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::CSRSparseMatrix::ValidateTypesAndShapes(tensorflow::DataType, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor const&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*, long long>(char const*, std::string, char const*, long long): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.pic.o:kernels_gpu.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, std::string, char const*, std::string>(char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::StrCat(tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::variant_op_registry_fn_registration::UnaryVariantUnaryOpRegistration<tensorflow::data::OptionalVariant>::UnaryVariantUnaryOpRegistration(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::variant_op_registry_fn_registration::UnaryVariantBinaryOpRegistration<tensorflow::data::OptionalVariant>::UnaryVariantBinaryOpRegistration(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::UnaryOpVariant<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantUnaryOp, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::UnaryOpVariant<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantUnaryOp, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::errors::Internal<char const*, tensorflow::VariantUnaryOp, char const*, std::string, char const*, std::string>(char const*, tensorflow::VariantUnaryOp, char const*, std::string, char const*, std::string): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function std::string absl::strings_internal::JoinAlgorithm<__gnu_cxx::__normal_iterator<tensorflow::Tensor const*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > >, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&>(__gnu_cxx::__normal_iterator<tensorflow::Tensor const*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > >, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&, absl::string_view, tensorflow::data::OptionalVariant::DebugString() const::{lambda(std::string*, tensorflow::Tensor const&)#1}&): error: undefined reference to 'tensorflow::Tensor::DebugString(int) const'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function void tensorflow::EncodeVariant<tensorflow::data::OptionalVariant>(tensorflow::data::OptionalVariant const&, std::string*): error: undefined reference to 'tensorflow::VariantTensorData::SerializeToString(std::string*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function bool tensorflow::DecodeVariant<tensorflow::data::OptionalVariant>(std::string*, tensorflow::data::OptionalVariant*): error: undefined reference to 'tensorflow::VariantTensorData::ParseFromString(std::string)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::errors::InvalidArgument<char const*, unsigned long, char const*, unsigned long, char const*>(char const*, unsigned long, char const*, unsigned long, char const*): error: undefined reference to 'tensorflow::strings::internal::CatPieces(std::initializer_list<absl::string_view>)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::BinaryOpVariants<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantBinaryOp, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::Status tensorflow::BinaryOpVariants<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::VariantBinaryOp, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*): error: undefined reference to 'tensorflow::DeviceName<Eigen::GpuDevice>::value'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/_objs/optional_ops_gpu/optional_ops.cu.pic.o:optional_ops.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::port::MaybeAbiDemangle(char const*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::TransformFilter<Eigen::GpuDevice, double, int, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, int>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::ReverseTransformFilter<Eigen::GpuDevice, double, 4>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::TransformFilter<Eigen::GpuDevice, double, int, 5>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 5, 1, int>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/conv_2d_gpu/conv_2d_gpu_double.cu.pic.o:conv_2d_gpu_double.cu.cc:function tensorflow::functor::ReverseTransformFilter<Eigen::GpuDevice, double, 5>::operator()(Eigen::GpuDevice const&, tensorflow::FilterTensorFormat, Eigen::TensorMap<Eigen::Tensor<double const, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<double, 5, 1, long>, 16, Eigen::MakePointer>): error: undefined reference to 'tensorflow::ToString(tensorflow::FilterTensorFormat)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function void tensorflow::EncodeVariant<tensorflow::TensorList>(tensorflow::TensorList const&, std::string*): error: undefined reference to 'tensorflow::VariantTensorData::SerializeToString(std::string*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function bool tensorflow::DecodeVariant<tensorflow::TensorList>(std::string*, tensorflow::TensorList*): error: undefined reference to 'tensorflow::VariantTensorData::ParseFromString(std::string)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterBinaryOpFn(tensorflow::VariantBinaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'
bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/list_kernels_gpu/list_kernels.cu.pic.o:list_kernels.cu.cc:function tensorflow::UnaryVariantOpRegistry::RegisterUnaryOpFn(tensorflow::VariantUnaryOp, std::string const&, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::OpKernelContext*, tensorflow::Variant const&, tensorflow::Variant*)> const&): error: undefined reference to 'tensorflow::UnaryVariantOpRegistry::PersistentStringStorage()'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 1571.556s, Critical Path: 229.26s
INFO: 4832 processes: 4832 local.
FAILED: Build did NOT complete successfully
`
How do I solve it? Thanks.
[nohup.zip](https://github.com/tensorflow/tensorflow/files/7776532/nohup.zip)

"
53545,Cross compiling TFLite with XNNPACK=ON for ARM with CMake,"**System information**
- Linux Ubuntu 20.04
- TensorFlow r2.8 (and master at the time of writing, being Dec 26th 2021)
- CMake 3.16.3
- gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2) toolchain recommendation)

**Describe the problem**
I want to cross compile tensorflow lite (with xnnpack=on) for arm using cmake. 

The build fails unless I force xnnpack to off.  I'm using the steps stated in the following articles:

- https://www.tensorflow.org/lite/guide/build_cmake
- https://www.tensorflow.org/lite/guide/build_arm
- https://www.tensorflow.org/lite/guide/build_cmake_arm

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Install clean Ubuntu 20.04 and latest updates

- Install Prerequisites 
```
sudo apt-get install curl git cmake
```

- Install Toolchain (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#download_toolchain_2))
```
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz -C ${HOME}/toolchains
```

- Clone Tensorflow (as per [this](https://www.tensorflow.org/lite/guide/build_cmake#step_2_clone_tensorflow_repository))
```
git clone https://github.com/tensorflow/tensorflow.git ${HOME}/tensorflow_src
mkdir ${HOME}/tflite_build && cd ${HOME}/tflite_build
```

- Run CMake (as per [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#run_cmake_2)) noting that this defaults to using -DTFLITE_ENABLE_XNNPACK=ON
```
ARMCC_FLAGS=""-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations""
ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
 -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
 -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
 -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
 -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
 -DCMAKE_SYSTEM_NAME=Linux \
 -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow_src/tensorflow/lite/
```

- Build (as per [this](https://www.tensorflow.org/lite/guide/build_cmake#step_5_build_tensorflow_lite))
```
cmake --build . -j
```

**Any other info / logs**
This build crashes around halfway through as per the output below. I can make it work by adding -DTFLITE_ENABLE_XNNPACK=OFF, however that will disable XNNPACK and considerably slow down inference time.  My ARM hardware has the NEON capabilitiy, so really need this XNNPACK compile to work. Thanks in  advance!

**Build Output**
```
[ 57%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/f32-ibilinear/gen/neonfma-c8.c.o
cd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-gemm/gen/1x8s4-minmax-neonfma.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-gemm/gen/1x8s4-minmax-neonfma.c
cd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-gemm/gen/6x8s4-minmax-neonfma.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-gemm/gen/6x8s4-minmax-neonfma.c
cd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-ibilinear-chw/gen/neonfma-p8.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-ibilinear-chw/gen/neonfma-p8.c
/home/tim/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c: In function ‘xnn_f16_f32_vcvt_ukernel__neonfp16_x16’:
cd /home/tim/tflite_build/_deps/xnnpack-build && /home/tim/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/tim/tflite_build/xnnpack/include -I/home/tim/tflite_build/xnnpack/src -I/home/tim/tflite_build/clog/deps/clog/include -I/home/tim/tflite_build/cpuinfo/include -I/home/tim/tflite_build/pthreadpool-source/include -I/home/tim/tflite_build/FXdiv-source/include -I/home/tim/tflite_build/FP16-source/include  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -O3 -DNDEBUG -fPIC   -Wno-psabi -pthread -std=gnu99  -marm  -march=armv7-a -mfpu=neon-vfpv4  -O2  -o CMakeFiles/XNNPACK.dir/src/f32-ibilinear/gen/neonfma-c8.c.o   -c /home/tim/tflite_build/xnnpack/src/f32-ibilinear/gen/neonfma-c8.c
/home/tim/tflite_build/xnnpack/src/f16-f32-vcvt/gen/vcvt-neonfp16-x16.c:31:11: error: unknown type name ‘float16x8_t’
     const float16x8_t vh0 = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;

```
"
53544,How can I prevent CPU overflow while optimizing tensorflow with particle swarm optimization?,"I am trying to optimize the following hyperparameters with pso for segmentation in modified Unet model.

Hyperparameters: Dropout, learning_rate, factor, min_lr

tensorflow==2.0.0

keras==2.3.1

PSO iterations number = 10

PSO Particle number = 10

10 models are running in each iteration (PSO iteration). This means a total of 100 models will run (model_1, model_2,...,model_100). But, when I go to the 2nd iteration, I get a memory overflow error on the 19th model. For the solution of this, I looked and applied similar problems on this platform, but the error did not go away. How can I avoid this memory overflow? I also tried the following code to solve this problem (for tensorflow 1.* version compatibility). First Soution Try:

```
import tensorflow as tf
from tensorflow.python.keras import backend as K
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=36,inter_op_parallelism_threads=2, allow_soft_placement=True, device_count = {'CPU': 36})
sess = tf.compat.v1.Session(config=config)
K.set_session(tf.compat.v1.Session(config=config))
```


THE CODE (Current State): The code block below is run on the calculation set with 36 Cores requested in the slurm script file.


```
def objective_function(x):
    _ = tf.Variable([1])
    context._context = None
    context._create_context()
    tf.config.threading.set_intra_op_parallelism_threads(36)
    tf.config.threading.set_inter_op_parallelism_threads(2)
    Dropoutt = x[0]
    Llearning_rate = x[1]
    Factorr = x[2]
    Mmin_lr = x[3]
    with tf.device('/CPU:0'):
        model = get_unet_mod(input_img, n_filters=16, dropout=Dropoutt, batchnorm=True)
        model_isim = model.name
        model.compile(optimizer=Adam(amsgrad=True, learning_rate=Llearning_rate), loss=jaccard_distance_loss, metrics=[""accuracy"", dice_coef, f1])
        Callbackss = [ReduceLROnPlateau(monitor=""loss"",mode=""min"",factor=Factorr, patience=10, min_lr=Mmin_lr, verbose=0)]
        results = model.fit(X_train, y_train, batch_size=8, verbose=0, epochs=30, callbacks=Callbackss, validation_data=(X_test, y_test))

PSO(objective_function, bounds, particle_size, iterations)
```
Error:

slurmstepd: error: step 8207716.4294967294 hit memory+swap limit at least once during execution. this may or may not result in some failure.
"
53543,Improving error message on CUDA version search and specification during build `./configure`,"This is from TF git version: 5e8cd1162cc8eeb9cf493d071293a621dcbb51f0 (Dec 22).

When building from sources and when the `./configure` step fails to find a CUDA version with a message below, could it actually report what it found? The message below leaves things ambiguous:

```
Could not find any cuda.h matching version 'CUDA 11' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
        'local/cuda/extras/CUPTI/include'
of:
        '/lib'
        '/lib/i386-linux-gnu'
        '/lib/x86_64-linux-gnu'
        '/lib32'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda/targets/x86_64-linux/lib'
        '/usr/local/lib'

Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:
```

When it says ""Could not find any cuda.h .... matching version .."", could it report what exactly was found and whether it found anything?  From this prompt itself, it isn't immediately clear whether one should enter ""11"" or ""11.2"" (with minor version), or ""CUDA 11"". I do have a `/usr/local/cuda/include/cuda.h` and 

```console
$ cat /usr/local/cuda/include/cuda.h  | grep CUDA_VERSION
#define CUDA_VERSION 11020
 * \defgroup CUDA_VERSION Version Management
/** @} */ /* END CUDA_VERSION */
```

Update: Finally, specifying ""11.2"" worked.



"
53542,Tensorboard load saved_model.pb failed,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
Windows

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
tag v2.6.0

**Describe the current behavior**
![image](https://user-images.githubusercontent.com/39380007/147376311-8278d9d7-1577-4d03-803b-e2e4931dbb49.png)


**Describe the expected behavior**
Show saved_model.pb on tensorboard

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53541,Feature Request: Providing Gradients for `layer.set_weights()`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): No, I am not experienced enough.



**Describe the feature and the current behavior/state.**

The feature is to make `set_weights()` differentiable. The current behavior is that the `model.set_weights()` function breaks the gradient. Given a model named `learner` and a batch of data `batch`

```
l = tf.constant(0.1)
with tf.GradientTape() as gt:
  gt.watch(l)
  new_weights = [w * l for w in learner.weights]

  learner.set_weights(new_weights)

  y_pred = learner(batch[""train""][""X""])
  loss = tf.keras.losses.categorical_crossentropy(batch[""train""][""y""], y_pred)
grads = gt.gradient(loss, l)
grads # returns None
```

This example returns None for the gradients. In fact, the gradient tape can't calculate the gradient of any variable related to calculating the `new_weights`.

So I was wondering if it's possible to provide gradients for `set_weights()` or if there are any work-arounds to this problem.

**Will this change the current api? How?**

I don't see how this feature would change the current api. It is more a modification on an existing function.

**Who will benefit with this feature?**

I believe adding this feature would make projects related to meta-learning easier to realize.

In my specific case, I was trying to implement [this paper](https://openreview.net/pdf?id=rJY0-Kcll) and translate the [pytorch implementation](https://github.com/markdtw/meta-learning-lstm-pytorch) to tensorflow, but I think this feature would generally benefit many projects that are related to the growing field of meta-learning.

**Any Other info.**
Thanks and Merry Christmas!!!
"
53540,Tensorflow,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
53538,ModuleNotFoundError: No module named 'tensorflow.python.keras.applications',"<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

I am running a script that is trying to import this:

``import tensorflow.python.keras.applications``

but it gives the bellow error:

```ModuleNotFoundError: No module named 'tensorflow.python.keras.applications'```

My TensorFlow version is 2.8.0 and Keras version is 2.8.0.
I'm trying to train an object detection model but I keep getting that error.

Here is my pip list: https://pastebin.com/xjfTW28W"
53534,Error in Hessian calculation using forward over backward propagation ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- TensorFlow installed from (source or binary):  both colab CPU and GPU mode
- TensorFlow version (use command below):2.7.0
- Python version: Python 3.7.12
- GPU model and memory: Tesla P100 


**Describe the current behavior**
I am trying to compute second derivative using the method provide by the [tutorial](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) using forward over backward propagation to calculate the hessian-vector-product. For memory reasons, we chose not to use the double backward method. In the code, we have nested loop and `tf.dynamic_partition`. Both the gradient and hessian works in the eager mode but when I try to decorate the function by `@tf.function` the error appears, and I found it's because the combination of using `tf.dynamic_partition` and `for loop` gives the error. 

Additionally, a different error is given when I try to decorate the `hvp` function. Without decorator, there's a `TypeError `, with a decorator, it gives a `SystemError `

- Briefly describe your candidate solution(if contributing): I have tried to find a workaround to use `tf.gather` or use a loop to replace the `tf.dynamic_partition`, different problems also prompt. 

**Standalone code to reproduce the issue**
I have made a reduced dummy code here and also in [colab](https://colab.research.google.com/drive/1k2-zp6DIzt4C4f9lgrqIasXE0SjdUJ3P?usp=sharing)

```python
import tensorflow as tf
import numpy as np
@tf.function # without the decorator, the function works fine in eager mode
def foo(mu):

  partitions = tf.constant([1, 0, 0])
  points = tf.dynamic_partition(mu, partitions, 2)[0]
  block = points
  # a dummy example of a loop
  for j in tf.range(1): # without this loop, the function works fine
    block =  points
  
  return block
# dummy input data
mu = tf.constant([[3.,2.,1.],[3.,2.,1.],[3.,2.,1.]])
foo(mu)

# gradient calculation 
@tf.function
def grad(mu):

    with tf.GradientTape(watch_accessed_variables=False) as t:
        t.watch(mu)
        property = foo(mu)
    loss = t.gradient(property,mu)

    return(loss)
gradient = grad(mu)

# hessian vector product
@tf.function # with/without the decorator, different error prompt
def hvp(mu,tangents):
    with tf.autodiff.ForwardAccumulator(mu, tangents) as acc:
        with tf.GradientTape(watch_accessed_variables=False) as t:
            t.watch(mu)
            property = foo(mu)
            print('tracing')
            tf.print('executing')
        loss = t.gradient(property,mu)
    hess = acc.jvp(loss)
    return(hess)
  
tangents = np.zeros(mu.shape)
tangents[0]=1
tangents = tf.convert_to_tensor(tangents,dtype=tf.float32)
hess = hvp(mu,tangents)

```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
StagingError                              Traceback (most recent call last)
<ipython-input-6-f99c3f212345> in <module>()
     14 tangents[0]=1
     15 tangents = tf.convert_to_tensor(tangents,dtype=tf.float32)
---> 16 hess = hvp(mu,tangents)
     17 
     18 

1 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

StagingError: in user code:

    File ""<ipython-input-4-f99c3f212345>"", line 9, in hvp  *
        loss = t.gradient(property,mu)

    SystemError: PyEval_EvalFrameEx returned a result with an error set
```
"
53533,Tensorflow conv1d computed incorrectly on GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.7
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda-11.2
- GPU model and memory: Tesla K80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Computing a single filter 1d convolution on GPU with kernel_size=15, padding='valid' and strides=1 on a (1, 64, 32) input tensor (1 sample X 64 coordinates X 32 input channels) results in data from the end of the input tensor effecting the beginning of the output vector. 

This behavior only happens when performing the calculation on GPU (specifically nvidia Telsa K80). 
On CPU results are fine.

**Describe the expected behavior**
Receptive field should be limited to the kernel size. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Issue reproduces on colab with GPU:
https://colab.research.google.com/drive/1pK1tNxWVtC9qilcX0u5sbpGmQEZiCDEX?usp=sharing


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[Tensorflow-bug.ipynb - Colaboratory.pdf](https://github.com/tensorflow/tensorflow/files/7769397/Tensorflow-bug.ipynb.-.Colaboratory.pdf)

"
53531,[PluggableDevice] TF_RESOURCE is not deserializable since 2.7,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.8.0
- Python version: 3.9

**Describe the current behavior**
When calling `TF_TensorData` in `TF >= 2.7.0` on a tensor containing a `TF_RESOURCE`, the returned data is the resource handle itself instead of being a string serialization of it. This seems to have been caused by [this commit](https://github.com/tensorflow/tensorflow/commit/6f8c6dbdf7bf9155a931aaefb77fdbcff3b06e16) which removed the `TF_RESOURCE` serialization logic from TF 2.

**Describe the expected behavior**
Calling `TF_TensorData` on a tensor containing a `TF_RESOURCE` should return some kind of cross-abi serialization of the resource handle or the resource proto. For example, reverting [this commit](https://github.com/tensorflow/tensorflow/commit/6f8c6dbdf7bf9155a931aaefb77fdbcff3b06e16) produces the expected behavior from a plugin standpoint.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): I can if needed
- Briefly describe your candidate solution(if contributing): 

**Standalone code to reproduce the issue**

```cpp
void* tensor_data = TF_TensorData(tensor);
size_t tensor_size = TF_TensorByteSize(tensor)
std::string serialized_data(reinterpret_cast<const char*>(tensor_data), tensor_size);
auto resource_handle = std::make_shared<tensorflow::ResourceHandleProto>();

if (!resource_handle->ParseFromString(serialized_data)) {
  std::abort();
}
```
"
53530,First tensorflow operation on numpy or list data takes long time after tf.GradientTape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.2 | 8.2.4.15-1+cuda11.4
- GPU model and memory: RTX 2080 8GB

**Describe the current behavior**

First tensorflow operation on numpy or list data after `with tf.GradientTape() as ...` takes much longer than subsequent operations, and is dependant on model size

**Describe the expected behavior**

Operation takes the same amount of time as all others. Note that in 2.6.0 both the first operation before, and the first operation after, would take a long time, but in 2.7.0 it is only the first operation after.

**Standalone code to reproduce the issue**

```
import time
import tensorflow as tf
from keras import Input, Model
from keras.layers import Conv2D
import numpy as np


def Backbone(filters):
    x = Input((None, None, 3))
    inp = x
    x = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu')(x)
    x = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu')(x)
    x = Conv2D(filters=filters*2, kernel_size=(3, 3), activation='relu')(x)
    x = Conv2D(filters=filters*2, kernel_size=(3, 3), activation='relu')(x)
    x = Conv2D(filters=filters*4, kernel_size=(3, 3), activation='relu')(x)
    x = Conv2D(filters=filters*4, kernel_size=(3, 3), activation='relu')(x)
    return Model(inp, x)


class TrainModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super(TrainModel, self).__init__(*args, **kwargs)

    def train_step(self, data):
        ts1 = time.time()
        test = tf.reduce_mean([1.0])
        te1 = time.time()

        ts2 = time.time()
        test = tf.reduce_mean([1.0])
        te2 = time.time()

        with tf.GradientTape() as tape:
            res = self(data)
            loss = tf.reduce_mean(res)

        ts3 = time.time()
        test = tf.reduce_mean([1.0])
        # test = tf.reduce_mean(np.random.randint(0, 100, (100, 100)))
        te3 = time.time()

        ts4 = time.time()
        test = tf.reduce_mean([1.0])
        te4 = time.time()

        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        print(f""before: {te1 - ts1:.05f}s {te2 - ts2:.05f}s, after: {te3 - ts3:.05f}s {te4 - ts4:.05f}s"")

        return {m.name: m.result() for m in self.metrics}


print(""filters = 8"")
model = Backbone(filters=8)
model = TrainModel(model.inputs, model.outputs)
model.compile(optimizer='adam', run_eagerly=True)
model.fit(x=tf.random.normal([32,224,224,3]),
          batch_size=4,
          epochs=1,
          verbose=0)


print(""filters = 32"")
model = Backbone(filters=32)
model = TrainModel(model.inputs, model.outputs)
model.compile(optimizer='adam', run_eagerly=True)
model.fit(x=tf.random.normal([32,224,224,3]),
          batch_size=4,
          epochs=1,
          verbose=0)
```

**Other info / logs**

Output from above:

```
filters = 8
before: 0.00079s 0.00030s, after: 0.00082s 0.00050s
before: 0.00042s 0.00034s, after: 0.00125s 0.00040s
before: 0.00051s 0.00038s, after: 0.00115s 0.00029s
before: 0.00039s 0.00035s, after: 0.00121s 0.00029s
before: 0.00039s 0.00035s, after: 0.00124s 0.00035s
before: 0.00039s 0.00040s, after: 0.00127s 0.00041s
before: 0.00048s 0.00041s, after: 0.00125s 0.00030s
before: 0.00048s 0.00047s, after: 0.00123s 0.00040s
filters = 32
before: 0.00075s 0.00049s, after: 0.00600s 0.00043s
before: 0.00049s 0.00042s, after: 0.01974s 0.00063s
before: 0.00057s 0.00043s, after: 0.01396s 0.00036s
before: 0.00043s 0.00036s, after: 0.01337s 0.00040s
before: 0.00052s 0.00040s, after: 0.01359s 0.00040s
before: 0.00062s 0.00039s, after: 0.01354s 0.00041s
before: 0.00054s 0.00042s, after: 0.01387s 0.00043s
before: 0.00060s 0.00062s, after: 0.01415s 0.00073s
```
"
53529,TRT Converter not working in 2.7.0 version of official image,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): docker image `tensorflow/tensorflow:2.7.0-gpu`
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: Python 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Nvidia GeForce 1050

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I tried to optimize a Tensorflow SavedModel using trt_converter. It works in official `tensorflow/tensorflow:2.3.0-gpu`, `tensorflow/tensorflow:2.4.0-gpu` yet fails in `tensorflow/tensorflow:2.7.0-gpu`
**Describe the expected behavior**
`trt_convert.TrtGraphConverterV2` should work out of box as in previous versions of official builds.
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
from tensorflow.python.compiler.tensorrt import trt_convert as trt

input_saved_model_dir = ""directory/of/tensorflow/saved_model""
output_saved_model_dir = ""tensorrt-test""
converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)
trt_graph = converter.convert()
converter.save(output_saved_model_dir);print(""====saved===="")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
2021-12-23 09:10:04.726658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such fil
e or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-23 09:10:04.726892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object fil
e: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-23 09:10:04.726909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:35] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the 
missing libraries mentioned above are installed properly.
ERROR:tensorflow:Tensorflow needs to be built with TensorRT support enabled to allow TF-TRT to operate.
Traceback (most recent call last):
  File ""trt.py"", line 10, in <module>
    converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1009, in __init__
    _check_trt_version_compatibility()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 221, in _check_trt_version_compatibility
    raise RuntimeError(""Tensorflow has not been built with TensorRT support."")
RuntimeError: Tensorflow has not been built with TensorRT support.
```"
53525,Type 'INT8' is not supported by tile.Node number 92 (TILE) failed to invoke,"### 1. System information

- OS Platform and Distribution: MacOS 10.15.7
- TensorFlow installation (pip package or built from source): pip3
- TensorFlow library (version): 2.7.0

### 2. Code

Code to run the inference on the int8 model (change `np.int8` to `np.float32` in you want to try float16 model):

```
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
generator_interpreter = tf.lite.Interpreter(model_path=""kp_detector.tflite"")
generator_interpreter.allocate_tensors()

# Get input and output tensors.
input_details = generator_interpreter.get_input_details()
output_details = generator_interpreter.get_output_details()

for i in range(0, len(input_details)):
    # Test the model on random input data.
    input_shape = input_details[i]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.int8)
    generator_interpreter.set_tensor(input_details[i]['index'], input_data)

generator_interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = generator_interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

#### 3. Failure after conversion
I have converted a `.pb` model to `.tflite` with float16 and int8 conversions. The inference is correct on float16, but not the int8. Let me know if you want me to provide the models.  
"
53522,line 123 of build_pip_package_with_cmake.sh to be adapted,"**System information**
- Linux Ubuntu 20.04 on WSL2
- Cmake 3.16
- Host system with i5-10210u processor, 4 cores

Line 123 of the script as shown on
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh#L123
causes the script to crash as result of not limiting number of processors. 
Error message: ERRO[0169] error waiting for container: invalid character ‘u’ looking for beginning of value

In a similar way, running the cmake build command directly on the Linux host (i.e. without using a docker container) causes compiler errors if the number of processors (4 in this case) is not specified correctly. (need to use ""cmake --build . -j 4"" as correct command)

If ${BUILD_NUM_JOBS} is replaced by the actual number of processors/cores available (in my case 4) then both the script to create the pip wheel as well as the basic cmake command perform as desired and complete without errors.

See [this thread](https://discuss.tensorflow.org/t/instructions-for-cmake-on-raspberry-pi-zero-are-inaccurate/6610/12) for further details.



"
53520,[TF:TRT]How to write my input_fn when I conver my tf model to trt? ,"
use nvcr.io/nvidia/tensorflow:19.12-tf2-py3 in docker
my model is

max_batch_size: 512
input [{
	name: ""dense_input""
	data_type: TYPE_FP32
	format: FORMAT_NONE
	dims: [-1]
	is_shape_tensor: false
	allow_ragged_batch: false
}, {
	name: ""sparse_ids_input""
	data_type: TYPE_INT32
	format: FORMAT_NONE
	dims: [-1]
	is_shape_tensor: false
	allow_ragged_batch: false
}, {
	name: ""seq_input""
	data_type: TYPE_INT32
	format: FORMAT_NONE
	dims: [-1, -1]
	is_shape_tensor: false
	allow_ragged_batch: false
}, {
	name: ""sparse_wgt_input""
	data_type: TYPE_FP32
	format: FORMAT_NONE
	dims: [-1]
	is_shape_tensor: false
	allow_ragged_batch: false
}]
output: [{
	name: ""tf_op_layer_Sigmoid""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_pctr""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_dapan_action""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_pcvr_ctr""
	data_type: TYPE_FP32
	dims: [2]
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_pctcvr_1""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_delay_time""
	data_type: TYPE_FP32
	dims: [2]
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_pcvr""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_pctcvr""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}, {
	name: ""tf_op_layer_Sigmoid_1""
	data_type: TYPE_FP32
	dims: [1]
	reshape: {
		shape: []
	}
	label_filename: """"
	is_shape_tensor: false
}]
I want to know how to write my input_fn when I conver it to trt, my code is blew but not ok :

# -*- coding: utf-8 -*-
import os

from tensorflow import make_tensor_proto
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import tensorflow as tf
import numpy as np

os.environ['TF_CPP_MIN_LOG_LEVEL'] = ""3""

if __name__ == ""__main__"":
    input_saved_model_dir = ""./TF-recommend/1634309909""
    output_saved_model_dir = ""./TF-recommend-trt/""

    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS
    conversion_params = conversion_params._replace(
        max_workspace_size_bytes=(1 << 32))
    conversion_params = conversion_params._replace(precision_mode=""FP16"")
    conversion_params = conversion_params._replace(
        maximum_cached_engines=100)

    converter = trt.TrtGraphConverterV2(
        input_saved_model_dir=input_saved_model_dir,
        conversion_params=conversion_params)

    converter.convert()


    def my_input_fn():
        dense_input = np.ones([438]).astype('float32')
        sparse_ids_input = np.ones([79]).astype('int32')
        sparse_wgt_input = np.ones([79]).astype('float32')

        req = {
            ""dense_input"": make_tensor_proto(dense_input, shape=(dense_input.shape)),
            ""sparse_ids_input"": make_tensor_proto(sparse_ids_input, shape=(sparse_ids_input.shape)),
            ""sparse_wgt_input"": make_tensor_proto(sparse_wgt_input, shape=(sparse_wgt_input.shape)),
        }
        yield req


    converter.build(input_fn=my_input_fn)
    converter.save(output_saved_model_dir)"
53519,Tensorflow ist not using full gpu memory,"Systeminformation:

ubuntu-server 20.04
gpu: rtx3060ti
tensor-flow: 2.7.0
driver version: 495.44
cuda: 11.2
cudnn: 8.1.0

I expect that tensorflow would use nearly the full gpu memory not only 6435MiB from 8GB.

test code (jupyter notebook):

```
import tensorflow as tf
import time
import sys

print('TensorFlow:', tf.__version__)
print('Python:', sys.version)

print('\n***CUDA***\n')
!nvcc --version

print('\n***CUDNN***\n')
!ls /usr/local/cuda/lib64/libcudnn.so* -l

!nvidia-smi
time.sleep(2)

model = tf.keras.models.Sequential()
time.sleep(2)

!nvidia-smi

```

Output:

```
TensorFlow: 2.7.0
Python: 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0]

***CUDA***

nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0

***CUDNN***

lrwxrwxrwx 1 root root     13 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.8
lrwxrwxrwx 1 root root     17 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8 -> libcudnn.so.8.1.0
-rwxr-xr-x 1 root root 158264 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8.1.0
Wed Dec 22 11:36:00 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| 30%   39C    P0    N/A / 200W |      0MiB /  7972MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

2021-12-22 11:36:03.614764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:03.632056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:03.632548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:03.634278: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-12-22 11:36:03.635303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:03.635783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:03.636194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:04.027651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:04.027831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:04.027964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-22 11:36:04.028084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6

Wed Dec 22 11:36:06 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   39C    P2    41W / 200W |   6437MiB /  7972MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2917      C   ...ensorflow/venv/bin/python     6435MiB |
+-----------------------------------------------------------------------------+

```"
53517,Can All_gather of collective_ops be used in deep learning model building and reverse optimization of the model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux CentOS 7.6.1810
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.15.4
- Python version:3.6.8
- Bazel version (if compiling from source):None
- GCC/Compiler version (if compiling from source):None
- CUDA/cuDNN version:None
- GPU model and memory:None

**Describe the current behavior**
I want to implement a data exchange function between workers,So I use Send/Recv of collective_ops while building a model.But errors occured when computing gradients.
> No gradient defined for operation 'CollectiveGather_1' (op type: CollectiveGather)

**Describe the expected behavior**
Normal training.

**Code to reproduce the issue**
``` python
 import tensorflow as tf
 from tensorflow.python.ops import collective_ops
 FLAGS = tf.app.flags.FLAGS
 worker_replicas=2
 def shuffle(tensor):
    batch_size = tf.shape(tensor)[0]
    rank = FLAGS.task_index
    with tf.device('/cpu:0'):
        all_idx = tf.range(worker_replicas * batch_size)
        shuffle_idx = tf.random.shuffle(all_idx)
        if FLAGS.task_index == 0:
            index_broadcast = collective_ops.broadcast_send(shuffle_idx, shape=shuffle_idx.shape,
                                                            dtype=shuffle_idx.dtype,
                                                            group_size=worker_replicas, group_key=3, instance_key=100)
        else:
            index_broadcast = collective_ops.broadcast_recv(shape=shuffle_idx.shape, dtype=shuffle_idx.dtype,
                                                            group_size=worker_replicas, group_key=3, instance_key=100)
        my_idxs = tf.slice(index_broadcast, [rank * batch_size], [batch_size])
        all_tensor = collective_ops.all_gather(
            tensor, worker_replicas, worker_replicas, worker_replicas)

    return tf.gather(all_tensor, my_idxs), shuffle_idx


```

**Other info / logs**
I insert the code snippet while model building,it shuffles tensors between workers,but the log as follows.
```
File ""/home/shangyw/git_rep/test_model/deploy/model_deploy.py"", line 435, in _optimize_clone
    clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)
  File ""/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/training/optimizer.py"", line 512, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py"", line 637, in _GradientsHelper
    (op.name, op.type))
2021-12-22 11:04:07,317 - DEBUG - No gradient defined for operation 'CollectiveGather_1' (op type: CollectiveGather)
```



"
53512,TF2.7 failed build for rocm 4.5,"**System information**
- OS Platform and Distribution : Gentoo OS
- TensorFlow installed from: source
- TensorFlow version: 2.7.0
- Python version: 3.9.9
- Installed using : portage
- Bazel version: 4.2.2
- GCC/Compiler version: 11.2.0
- GPU model and memory: AMD Vega Frontier

**Describe the problem**
```
mlir-tblgen failed: error executing command
CommandLine Error: Option 'd' registered more than once!
LLVM ERROR: inconsistency in registered CommandLine options
FAILED: Build did NOT complete successfully

```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
cd /var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/work/tensorflow-2.7.0-bazel-base/execroot/org_tensorflow && \
  exec env - \
    HOME=/var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/homedir \
    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/temp/.keras \
    PATH=/usr/lib/portage/python3.9/ebuild-helpers/xattr:/usr/lib/portage/python3.9/ebuild-helpers:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/usr/lib/llvm/12/bin:/usr/lib64/subversion/bin:/opt/cuda/bin:/opt/rocm-4.5.2/hip/bin:/opt/rocm-4.5.2/llvm/bin \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/opt/rocm-4.5.2/lib/library \
    ROCM_PATH=/opt/rocm-4.5.2 \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,curl,cython,dill_archive,double_conversion,enum34_archive,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,libjpeg_turbo,lmdb,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \
  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-op-decls -dialect tfg tensorflow/core/ir/ops.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I ./tensorflow/core/ir/include -I bazel-out/k8-opt/bin/./tensorflow/core/ir/include -I ./tensorflow/core/ir/types/include -I bazel-out/k8-opt/bin/./tensorflow/core/ir/types/include -I ./ -I bazel-out/k8-opt/bin/./ -I tensorflow/core/ir -I bazel-out/k8-opt/bin/tensorflow/core/ir -o bazel-out/k8-opt/bin/tensorflow/core/ir/ops.h.inc
```
**Any other info / logs**
[strace](https://gist.githubusercontent.com/raw/1b3a9b4f520aa3450f5cf1c5bad47f76)
[bazelrc](https://gist.github.com/raw/346a6a4e32d0da1514d7fe2f0fc0ae4c)
[environment](https://gist.github.com/raw/b53debf6a768aa99032b455d487b0ad8)
[build.log](https://gist.github.com/raw/e4009002bbdd26493b31a63ce6fff26f)
"
53510, AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call',"Hello.
I am importing the following libraries while running the example that comes as the [basic text classification ](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es-419) in the official Tensorflow documentation. But I get an error message. My version of tensorflow is 2.7.0

Running code for which the error occurred:
```
epochs = 10
history = model.fit( train_ds, validation_data=val_ds, epochs=epochs)
```

Libraries loads:
```
import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
```
Error message:
```
AttributeError: in user code:

    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\engine\training.py"", line 1525, in test_function  *
        return step_function(self, iterator)
    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\engine\training.py"", line 1514, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\engine\training.py"", line 1507, in run_step  **
        outputs = model.test_step(data)
    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\engine\training.py"", line 1474, in test_step
        return self.compute_metrics(x, y, y_pred, sample_weight)
    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\engine\training.py"", line 961, in compute_metrics
        result = metric.result()
    File ""C:\Users\Octavio\anaconda3\lib\site-packages\keras\utils\metrics_utils.py"", line 124, in decorated
        tf.__internal__.distribute.strategy_supports_no_merge_call()):
```
Thank you
"
53506,Specify what is ag__ in the output of tf.autograph.to_code,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/autograph/to_code

## Description of issue (what needs changing):
I tried to look everywhere but I could not find what is `ag__`, how could I use the output of `tf.autograph.to_code` with the python interpreter.
I thought the name `ag__` was `tf.autograph` but this module does not have `FunctionScope` for example.
tf.compat.v1.autograph also did not work.
I did not seem to be able to find online any explanation of this shorthand `ag__`

### Parameters defined
Could this variable be defined to know how can we interpret the outputted source code?"
53505,tf.random is broken on macOS Monterey 12.1,"On a Mac M1 with Tensorflow-metal 0.3.0 and Tensorflow-2.7.0

this code used to generate different sequences but now generate the same sequence : 
```
import tensorflow as tf

x = tf.random.uniform((10,))
y = tf.random.uniform((10,))

tf.print(x)
tf.print(y)
[0.178906798 0.8810848 0.384304762 ... 0.162458301 0.64780426 0.0123682022]
[0.178906798 0.8810848 0.384304762 ... 0.162458301 0.64780426 0.0123682022]
```

It works on CPU, it works on collab, it works on cuda, it used to works on MacOS 12.0.

I tried many workaround like using a generator, but generator can't be used on M1 GPU, eg  : 
```
randomgen = tf.random.Generator.from_non_deterministic_state()
#%%
for _ in range(10):
        g2 = tf.random.get_global_generator()
        x = g2.uniform((10,),(1,2))
        y = g2.uniform((10,),(3,4))
        tf.print(x)
        tf.print(y)
```

```
NotFoundError: No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}
	.  Registered:  device='CPU'
 [Op:RngReadAndSkip]
```

other people reported other problems with various code that used to works in 12.0 but the problems are likely to be related to randomness.
a broken random is obviously bad in machine learning ^^

I opened an apple feedback as well as a discussion : https://developer.apple.com/forums/thread/697057?answerId=699100022#699100022
"
53503,Build broken by recent commit updating llvm-raw,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build errors out with

INFO: Repository 'llvm-raw' used the following cache hits instead of downloading the corresponding file.
 * Hash 'd68b5fa9b851cc0b8a5f84f7659404380f477efc859f21951bc3554476047245' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/ec0e4545caa18e0b55e4f25db913a10a4782fdc3.tar.gz
If the definition of 'llvm-raw' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'llvm-raw':
   Traceback (most recent call last):
	File ""/home/andrew/src/tensorflow/third_party/repo.bzl"", line 75, column 30, in _tf_http_archive_impl
		ctx.patch(patch_file, strip = 1)
Error in patch: Error applying patch /home/andrew/src/tensorflow/third_party/llvm/Bazel-update-build-files-for.patch: Incorrect Chunk: the chunk content doesn't match the target
**Original Position**: 840

**Original Content**:
    copts = llvm_copts,
    deps = [
        "":Core"",
        "":Support"",
        "":config"",
    ],

**Revised Content**:
    copts = llvm_copts,
    deps = [
        "":Core"",
        "":DebugInfoDWARF"",
        "":Support"",
        "":config"",
    ],
ERROR: no such package '@llvm-raw//utils/bazel': Error applying patch /home/andrew/src/tensorflow/third_party/llvm/Bazel-update-build-files-for.patch: Incorrect Chunk: the chunk content doesn't match the target
**Original Position**: 840

**Original Content**:
    copts = llvm_copts,
    deps = [
        "":Core"",
        "":Support"",
        "":config"",
    ],

**Revised Content**:
    copts = llvm_copts,
    deps = [
        "":Core"",
        "":DebugInfoDWARF"",
        "":Support"",
        "":config"",
    ],
INFO: Elapsed time: 64.208s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)


**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --verbose_failures -- //tensorflow/core/ir/...

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

This commit seems to be implicated
https://github.com/tensorflow/tensorflow/commit/403b4b19f9d143b4868d8a822c325acb6fdd8704
"
53501,Batches handled differently in the int reference fully connected kernel,"**System information**
-- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): SHA1: 403b4b19f9d143b4868d8a822c325acb6fdd8704

**Describe the current behavior**
Almost all fully connected kernels handle batches differently than the int8 reference fully connected kernel. Is this really intentional or a bug?

We have seen models (with keep_num_dims=True for fully_connected) where batches can be different in fully connected when a model has been converted from float to int8.

find . -name ""fully_connected.h"" |xargs grep ""const int batches""|grep -v ""//""
./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
./lite/kernels/internal/reference/integer_ops/fully_connected.h:  const int batches = output_shape.Dims(0);
./lite/kernels/internal/reference/integer_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);
./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches = thread_end - thread_start;
./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches =
./lite/kernels/internal/optimized/integer_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);

The typical case is const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1)
In the int8 ref case: const int batches = output_shape.Dims(0);

**Describe the expected behavior**
Batches are handled the same way for int and float fc kernels.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
Change const int batches = output_shape.Dims(0);
To
const int output_dims_count = output_shape.DimensionsCount();
const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);

"
53500,The TensorFlow contrib module will not be included in TensorFlow 2.0.,"WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1221 15:55:51.620390 140392233633600 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

 and i checked that version of tensorflow in my system is 1.15

"
53499,Doc Improvement,"https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/eager/backprop.py#L812

dz_dy = tape.gradient(z, w)
The Variable `dz_dy` could be more intuitive by renaming to `dz_dw`."
53498,SparseTensor generator for tensorflow keras model training,"We had large data to fit for the model training. And in the model construction, we used tf.keras.experimental.SequenceFeatures, which required the input should be SpareTensor. I tired using generaotr, but did not work for SparsTensor. 
Here is the sample code:
```
from models.model_attention import AttentionModel
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dropout, Dense


inputs = {'f1': tf.keras.layers.Input(name='f1', sparse=True, shape=(40, 1), dtype='float32'),
          'f2': tf.keras.layers.Input(name='f2', sparse=True, shape=(40, 1), dtype='float32')}

features = [tf.feature_column.sequence_numeric_column('f1', dtype=tf.float32),
            tf.feature_column.sequence_numeric_column('f2', dtype=tf.float32)]

input_layer, _ = tf.keras.experimental.SequenceFeatures(features)(inputs)
lstm_out = LSTM(128, return_sequences=False)(input_layer)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = Dense(1, activation='tanh')(lstm_out)
model = tf.keras.models.Model(inputs, lstm_out)
model.compile(loss='mse', metrics='mae', optimizer='Adam')


def gen():
    batch = 4
    while True:
        x1 = tf.sparse.from_dense(np.random.random((batch, 40, 1)))
        x2 = tf.sparse.from_dense(np.random.random((batch, 40, 1)))
        x = {'f1': x1, 'f2': x2}
        y = np.random.random((batch, 1))
        yield x, y


x, y = gen().__next__()
# x, y yielded from generator works
model.fit(x, y, epochs=2, verbose=2)
g = gen()
# TypeError: Input must be a SparseTensor.
model.fit(g, steps_per_epoch=2, epochs=2, verbose=2, validation_data=g, validation_steps=2)
```

gen() function used to work for normal numpy array generator, but did not work for SparseTensor input. Any advices to the issue?"
53497,Range of parameters while converting to TFLite models,"Hi,

We are trying to apply an INT8 TFLite model to Arm NPU; however, there are some requirements in the operations. For example, convolution must satisfy the (requirements below)[https://github.com/ARM-software/ethos-n-driver-stack/blob/master/SUPPORTED.md#convolution-2d]:
```
I*W/O must be between 2.33e-10 and 1, where:
I is the input quantization scale.
W is the weight quantization scale.
O is the output quantization scale. 
```

As a result, is it possible to set the values of input, weight, and output quantization scales in convolution and depthwise convolution layers while converting the pb files to INT8 TFLite models?"
53494,Tensorflow `tf.function` fails if function is called with two identical arguments,"In my TF model, my call functions calls an external energy function which is dependent on a function where single parameter is passed twice (see simplified version below):
```
import tensorflow as tf

@tf.function
def calc_sw3(gamma,gamma2, cutoff_jk):
    E3 = 2.0
    return E3

@tf.function
def calc_sw3_noerr( gamma0, cutoff_jk):
    E3 = 2.0
    return E3

@tf.function # without tf.function this works fine
def energy(coords, gamma):
    xyz_i = coords[0, 0 : 3]
    xyz_j = coords[0, 3 : 6]
    rij = xyz_j - xyz_i
    norm_rij = (rij[0]**2 + rij[1]**2 + rij[2]**2)**0.5
    E3 = calc_sw3( gamma,gamma,norm_rij)    # repeating gamma gives error
    # E3 = calc_sw3_noerr( gamma, norm_rij) # this gives no error
    return E3



class SWLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.gamma = tf.Variable(2.51412, dtype=tf.float32)

    def call(self, coords_all):
        total_conf_energy = energy( coords_all, self.gamma)
        return total_conf_energy
# =============================================================================


SWL = SWLayer()
coords2 = tf.constant([[
                        1.9434,  1.0817,  1.0803,  
                        2.6852,  2.7203,  1.0802,  
                        1.3807,  1.3573,  1.3307]])

with tf.GradientTape() as tape:
    tape.watch(coords2)
    E = SWL( coords2)
```

Here if gamma is passed only once, or if I do not use `tf.function` decorator. But with `tf.function` and passing same variable twice, I get the following error:
```
Traceback (most recent call last):
  File ""temp_tf.py"", line 47, in <module>
    E = SWL( coords2)
  File ""...venv/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""temp_tf.py"", line 34, in call
    total_conf_energy = energy( coords_all, self.gamma)
tensorflow.python.autograph.impl.api.StagingError: Exception encountered when calling layer ""sw_layer"" (type SWLayer).

in user code:

    File ""temp_tf.py"", line 22, in energy  *
        E3 = calc_sw3( gamma,gamma,norm_rij)    # repeating gamma gives error

    IndexError: list index out of range


Call arguments received:
  • coords_all=tf.Tensor(shape=(1, 9), dtype=float32)
```

Is this expected behaviour?

**System information**
- MacOS Mojave
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.7"
53491,Error occur: undefined reference when trying to ndk-build to run tflite example,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399 embedded board
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.7
- Python version: 3.7
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): NDK r21

---

**Describe the problem**

I'm trying to NDK build simple tflite example code.

When I build, this error is occured.

```error: undefined reference to 'tflite::InterpreterBuilder::operator()(std::__1::unique_ptr<tflite::Interpreter, std::__1::default_delete<tflite::Interpreter> >*)'```

---

**Provide the exact sequence of commands / steps that you executed before running into the problem**

First, I build tflite. I type command ```bazel build --config=elinux_aarch64 -c opt //tensorflow/lite:libtensorflowlite.so``` and libtensorflow.so file is created.

And Secondely, I prepare tflite code.
```C
  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_file.c_str());

  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  std::unique_ptr<tflite::Interpreter> interpreter;
  tflite::InterpreterBuilder(*model, resolver)(&interpreter);    <<<<<<----- This is where the error occurs. 

  interpreter->SetNumThreads(num_thread);
  interpreter->AllocateTensors();

  // Set input
  float* input = interpreter->typed_input_tensor<float>(0);   
  for(int y=0; y<height; y++)
  {
    for(int x=0; x<width; x++)
    {
      *(input + y*width + x) = ((float)img.data[3*y*width + 3*x]);
      *(input + y*width + x + 1 * height*width) = ((float)img.data[3*y*width + 3*x + 1]);
      *(input + y*width + x + 2 * height*width) = ((float)img.data[3*y*width + 3*x + 2]);
    }
  }

  TfLiteTensor* output_tensor = nullptr;
  interpreter->Invoke();
  output_tensor = interpreter->tensor(interpreter->outputs()[0]);
  printf(""Elapsed: %f\n\n"",sec.count()*1000);
```

and then, I link the libtensorflow.so file.

I add two line in my Android.mk

```
TFLITE_INSTALL_PATH := packages/apps/3rdparty/tflite
LOCAL_LDLIBS:= $(TFLITE_INSTALL_PATH)/lib_android/libtensorflowlite.so
```

---

What I tried to solve the error is followed

1) change ndk version
Now I'm using ndk ver 21, and I try another version(13, 15, 18) too.

2) re-build tensorflow lite
I add option such as --config=monolithic, --config=dynamic_kernels and --cxxopt=--std=c++14 And I also change option --config=elinux_aarch64 to --config=android_arm64

In this situation, Is there anything to check? I don't know what to do anymore. Is this problem of Android.mk setting? or build bazel setting?

---

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53489,Off by one discrepancy for tf.nn.conv1d between cpu and gpu,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information (CPU Tensorflow code)**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.6 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version: **Python 3.9.0**
-  Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**


**System information (GPU Tensorflow code)**
- Running Docker image **tensorflow/tensorflow:latest-gpu-jupyter**, imageID: **8da916739a38**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.3 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **unsure**
- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version: **3.8.10**
- Bazel version (if compiling from source): **unsure**
- GCC/Compiler version (if compiling from source): **unsure**
- CUDA/cuDNN version: 
  - Cuda: **11.2.152** build **cuda_11.2.r11.2/compiler.29618528_0**
  - CUDNN: **8100**
- GPU model and memory: **GeForce GTX 1060** w/ **6078MiB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**:
Code attached takes as input a binary sequence and convolves a filter over it to compute the decimal representation of each length 5 binary substring. For example, the binary sequence` [0, 1, 1, 0, 1, 1]` gets converted to `[13, 27]`.

When the length of the string is 33 or greater, the CPU and GPU outputs differ. CPU is correct and GPU is off-by-one in some cases. There is no noticeable pattern for when the GPU is off-by-one:

**CPU:**
Input binary sequence: `[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]`
Converted decimal sequence `[1, 3, 7, 14, 28, 24, 16, 0, 0, 1, 3, 7, 15, 31, 31, 31, 31, 31, 31, 31, 30, 29, 27, 22, 12, 25, 19, 6, 13, 27, 23, 14, 28]`

**GPU:**
Input binary sequence: `[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]`
Converted decimal sequence: `[1, 3, 7, 13, 28, 23, 16, 0, 0, 1, 3, 6, 14, 30, 30, 30, 30, 31, 30, 30, 30, 29, 26, 22, 12, 25, 19, 6, 12, 26, 22, 13, 27]`


**Describe the expected behavior**

Both converted decimal sequences should agree. The CPU sequence is the correct one.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **no**
- Briefly describe your candidate solution(if contributing): **N/A**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import numpy as np


def base_2_accumulator(length: int):
    powers_of_2 = tf.bitwise.left_shift(1, tf.range(length))
    return powers_of_2[::-1]

rng = np.random.default_rng(0)

dump_file = './cpu_outputs.txt'
# dump_file = './gpu_outputs.txt'

f = open(dump_file, 'w')

batch_size = 1000
block_len=33
msg = tf.constant(rng.integers(0, 2, size=(batch_size, block_len, 1)), dtype=tf.int32)
print('msg', file=f)
print(msg, file=f)

window = 5
base_2 = base_2_accumulator(window)
print('base_2', file=f)
print(base_2, file=f)
print(base_2)

base_2_filter = tf.cast(base_2[:, None, None], dtype=tf.float32)
print('base_2_filter', file=f)
print(base_2_filter, file=f)

msg_prepended = tf.pad(msg[:, :, 0], paddings=tf.constant([[0, 0], [window-1, 0]]))
print('msg_prepended', file=f)
print(msg_prepended, file=f)

conv_msg_input = tf.cast(msg_prepended[:,:,None], dtype=tf.float32)
print('conv_msg_input', file=f)
print(conv_msg_input, file=f)

state_sequence = tf.cast(tf.nn.conv1d(conv_msg_input, base_2_filter, stride=1, padding='VALID'), dtype=tf.int32)[:,:,0]
print('state_sequence', file=f)
print(state_sequence, file=f)
print('first binary sequence', file=f)
print(msg_prepended[0].numpy().tolist(), file=f)
print('first state sequence', file=f)
print(state_sequence[0].numpy().tolist(), file=f)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53488,Tensorflow.fit method is using 3 times more memory than it should,"Hi! 

I am running two near-identical scripts. The code is provided in the following links:
script1.py: https://pastebin.com/66i8kEnK
script2.py: https://pastebin.com/AWfQUguq

The code for the helper-function which is in some_file_1.py:
helper.py: https://pastebin.com/fXgWcuz5

script1.py is using a 3 times more memory than script2.py. The only main difference between the two is that, in script1, I am converting my data and params to tf.tensors as I pass into model.fit method and in script2, I convert before passing into the .fit method.

My python version is 3.9.7 and tensorflow version is 2.4.1. 

The OS is: Red Hat Enterprise Linux
The Kernel is: Linux 3.10.0-1160.2.1.el7.x86_64
The architecture is: x86-64

The GPU is Nvidia either P100 or Nvidia V100. 

"
53487,F2 Score and mAP metric for object detection model.,"I didn't find a metric regarding the object detection task in keras. 

How to implement the above two metrics in keras and use them? In this competition, https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview/evaluation uses the F2 score for its object detection task but I don't know how to evaluate my model locally with this score? 

> The metric sweeps over IoU thresholds in the range of 0.3 to 0.8 with a step size of 0.05, calculating an F2 score at each threshold. For example, at a threshold of 0.5, a predicted object is considered a ""hit"" if its IoU with a ground truth object is at least 0.5.
"
53486,No way to unstack ragged tensors,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: NO
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: COLAB
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: N/A
-   **TensorFlow version (use command below)**: latest COLAB
-   **Python version**: latest COLAB
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: COLAB
-   **CUDA/cuDNN version**: COLAB
-   **GPU model and memory**: COLAB
-   **Exact command to reproduce**:




### Describe the problem
I want to be able to unstack a ragged tensor so I can pass the unstacked tensors to a function with multiple inputs.

Here is some example code of what I want to do.

```
import tensorflow as tf
X = tf.ragged.constant([[0, 1, 2], [0, 1]])

def sum_func(x1,x2):
  return tf.reduce_sum(x1) + tf.reduce_sum(x2)

sum_func(tf.unstack(X))
```

> ValueError: TypeError: object of type 'RaggedTensor' has no len()

Please note I know there are other ways to achieve the above with this function but this is just a minimal example"
53485,Saved model evaluate and validation always giving 0.50 accuracy despite reaching 96% on exact same data during training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 21h2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below):  v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.9.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3/8.2.1
- GPU model and memory: RTX 3080Ti 12GB

**Describe the current behavior**
When training a normal CNN model using the horse VS human dataset (binary image classification), the training accuracy reached 0.96+, but validation is forever stuck at 0.5. What's more interesting is after I load the saved model and called `model.evaluate` on the training set, it also gives 0.50, despite the training set being exactly same and was yielding 0.96+ during the actual training.

**Describe the expected behavior**
model to perform normally in both training and testing

model weight: https://drive.google.com/file/d/11oqaGvy4vV0v77WmWzQxhtQSgEH4PUhA/view?usp=sharing

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf
import urllib
import zipfile
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import *


def download_data():
    _TRAIN_URL = ""https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip""
    _TEST_URL = ""https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip""
    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')
    local_zip = 'horse-or-human.zip'
    zip_ref = zipfile.ZipFile(local_zip, 'r')
    zip_ref.extractall('tmp/horse-or-human/')
    zip_ref.close()
    urllib.request.urlretrieve(_TEST_URL, 'testdata.zip')
    local_zip = 'testdata.zip'
    zip_ref = zipfile.ZipFile(local_zip, 'r')
    zip_ref.extractall('tmp/testdata/')
    zip_ref.close()


def solution_model():
    train_datagen = ImageDataGenerator(rescale=1. / 255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')
    test_datagen = ImageDataGenerator(rescale=1. / 255)

    train_generator = train_datagen.flow_from_directory(
        'tmp/horse-or-human',  # This is the source directory for training images
        target_size=(300, 300),
        batch_size=16,
        class_mode='binary')

    print('Validation data')
    validation_generator = test_datagen.flow_from_directory(
        'tmp/testdata',
        target_size=(300, 300),
        batch_size=32)

    opt = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-6)
    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
    epoch = 20
    batch_size = 16
    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1,
                                         mode='auto', baseline=None, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.05, patience=5, verbose=1)
    ]

    xInput = Input([300, 300, 3])
    x = tf.cast(xInput, tf.float32)
    resnet50 = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet')
    x = resnet50(x)
    x = Flatten()(x)
    x = Dropout(0.2)(x)
    x = Dense(256)(x)
    x = BatchNormalization(epsilon=1.001e-5)(x)
    x = Activation('relu')(x)
    xOutput = Dense(1)(x)  # from logits so no need activation
    model = tf.keras.models.Model(xInput, xOutput)

    model.compile(optimizer=opt, loss=loss, metrics='accuracy')
    model.summary()
    model.fit(train_generator, validation_data=validation_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=epoch, callbacks=callbacks, verbose=1)
    return model



if __name__ == '__main__':
    # download_data()  # only need to run this once
    model = solution_model()
    model.save(""mymodel.h5"")
```

Evaluation code:

```
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import load_model

test_datagen = ImageDataGenerator(rescale=1. / 255)

print('Validation data')
validation_generator = test_datagen.flow_from_directory(
    'tmp/horse-or-human',
    target_size=(300, 300),
    batch_size=32,
    class_mode='categorical')

model = load_model('mymodel.h5')
model.summary()
model.evaluate(validation_generator)  # should be >0.96
```

**Other info / logs** Include any logs or source code that would be helpful to
`33/33 [==============================] - 13s 125ms/step - loss: 0.7544 - accuracy: 0.5000`

As you can see, loss value is actually normal (it started training with 2+ loss, now only 0.75)
"
53482,ImportError: DLL load failed while importing trace: The specified module could not be found.,"OS Name	Microsoft Windows 10 Pro
Version	10.0.19044 Build 19044
Other OS Description 	Not Available
OS Manufacturer	Microsoft Corporation
System Name	DESKTOP-045H3MF
System Manufacturer	LENOVO
System Model	80XL
System Type	x64-based PC
System SKU	LENOVO_MT_80XL_BU_idea_FM_ideapad 320-15IKB
Processor	Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz, 2712 Mhz, 2 Core(s), 4 Logical Processor(s)
BIOS Version/Date	LENOVO 4WCN47WW, 6/30/2020
SMBIOS Version	3.0
Embedded Controller Version	1.47
BIOS Mode	UEFI
BaseBoard Manufacturer	LENOVO
BaseBoard Product	LNVNB161216
BaseBoard Version	NO DPK
Platform Role	Mobile
Secure Boot State	Off
PCR7 Configuration	Elevation Required to View
Windows Directory	C:\Windows
System Directory	C:\Windows\system32
Boot Device	\Device\HarddiskVolume1
Locale	United States
Hardware Abstraction Layer	Version = ""10.0.19041.1151""
User Name	DESKTOP-045H3MF\LENOVO
Time Zone	Bangladesh Standard Time
Installed Physical Memory (RAM)	8.00 GB
Total Physical Memory	7.88 GB
Available Physical Memory	1.69 GB
Total Virtual Memory	14.9 GB
Available Virtual Memory	5.98 GB
Page File Space	7.00 GB
Page File	C:\pagefile.sys
Kernel DMA Protection	Off
Virtualization-based security	Not enabled
Device Encryption Support	Elevation Required to View
Hyper-V - VM Monitor Mode Extensions	Yes
Hyper-V - Second Level Address Translation Extensions	Yes
Hyper-V - Virtualization Enabled in Firmware	Yes
Hyper-V - Data Execution Protection	Yes
###import requests
import cartopy.crs as ccrs 
import matplotlib.pyplot as plt
import os
from PIL import Image
import random
import pyttsx3
engine = pyttsx3.init('sapi5')
voices = engine.getProperty('voices')
engine.setProperty('voices',voices[0].id)

def Speak(audio):
    print("" "")
    print(f"": {audio}"")
    engine.say(audio)
    engine.runAndWait()
    print("" "")

Api_Key = ""FxwAGG1PWubblMdnFrHZAgHy0KdT86QPDRwLz2Is""

def NasaNews(Date):

    Speak(""Extracting Data From Nasa . "")

    Url = ""https://api.nasa.gov/planetary/apod?api_key="" + str(Api_Key)

    Params = {'date':str(Date)}
    
    r = requests.get(Url,params = Params)

    Data = r.json()

    Info = Data['explanation']

    Title = Data['title']

    Image_Url = Data['url']

    Image_r = requests.get(Image_Url)

    FileName = str(Date) + '.jpg'

    with open(FileName,'wb') as f:

        f.write(Image_r.content)

    Path_1 = ""C:\\AI\\"" + str(FileName)

    Path_2 = ""C:\\AI\\DataBase\\NasaDataBase\\"" + str(FileName)

    os.rename(Path_1, Path_2)

    img = Image.open(Path_2)

    img.show()

    Speak(f""Title : {Title}"")
    Speak(f""According To Nasa : {Info}"")

def Summary(Boby):

    list__ = ('2','3','4','5')

    value = random.choice(list__)

    path = ""C:\\AI\\DataBase\\NasaDataBase\\ImageUsed\\"" + str(value) + "".jpg""

    os.startfile(path)

    name = str(Boby)

    url = ""https://hubblesite.org/api/v3/glossary/"" + str(name)

    r = requests.get(url)

    Data = r.json()

    if len(Data) != 0:
        retur =  Data['definition']

        Speak(f""According To The Nasa : {retur}"")

    else:

        Speak(""No Data Available , Try Again Later!"")
def MarsImage():

    name = 'curiosity' 

    date = '2020-12-3'

    Api_ = str(Api_Key)

    url = f""https://api.nasa.gov/mars-photos/api/v1/rovers/{name}/photos?earth_date={date}&api_key={Api_}""
    r = requests.get(url)

    Data = r.json()

    Photos = Data['photos'][:20]

    try:

        for index , photo in enumerate(Photos):

            camera = photo['camera']

            rover = photo['rover']

            rover_name = rover['name']

            camera_name = camera['name']

            full_camera_name = camera['full_name']

            date_of_photo = photo['earth_date']

            img_url = photo['img_src']

            p = requests.get(img_url)

            img = f'{index}.jpg'

            with open(img,'wb') as file:
                file.write(p.content)

            Path_1 = ""C:\\AI\\"" + str(img)

            Path_2 = ""C:\\AI\\DataBase\\NasaDataBase\\MarsImage\\"" + str(img)

            os.rename(Path_1,Path_2)

            os.startfile(Path_2)

            Speak(f""This Image Was Captured With : {full_camera_name}"")

            Speak(f""This Image Was Captured On : {date_of_photo}"")

    except:
        Speak(""There IS An Error!"")
def IssTracker():

    url = ""http://api.open-notify.org/iss-now.json""

    r = requests.get(url)

    Data = r.json()

    dt = Data['timestamp']

    lat = Data['iss_position']['latitude']

    lon = Data['iss_position']['longitude']

    print(f""Time And Date : {dt}"")
    print(f""Latitude : {lat}"")
    print(f""Longitude : {lon}"")

    plt.figure(figsize=(10,8))

    ax = plt.axes(projection = ccrs.PlateCarree())

    ax.stock_img()

    plt.scatter(float(lon),float(lat),color = 'blue' , marker= 'o')

    plt.show()
IssTracker() 
   ###

PS C:\AI> pip install numpy
Collecting numpy
  Using cached numpy-1.21.4-cp310-cp310-win_amd64.whl (14.0 MB)
Installing collected packages: numpy
Successfully installed numpy-1.21.4
PS C:\AI> & C:/Users/LENOVO/AppData/Local/Programs/Python/Python310/python.exe c:/AI/Nasa.py
Traceback (most recent call last):
  File ""c:\AI\Nasa.py"", line 2, in <module>
    import cartopy.crs as ccrs
  File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python310\lib\site-packages\cartopy\__init__.py"", line 109, in <module>
    import cartopy.crs
  File ""C:\Users\LENOVO\AppData\Local\Programs\Python\Python310\lib\site-packages\cartopy\crs.py"", line 26, in <module>
    import cartopy.trace
ImportError: DLL load failed while importing trace: The specified module could not be found.
PS C:\AI> 
"
53478,how to change model from default efficientnet_lit0 in tflite model maker image_classifier.create(),"Hi,

I wanted to know in tflite model maker, if i want to use a custom model and not the default model to train and quantize with custom data, how do i do that? image.classifier.create() seems to be using efficientnet_lite0 as default and i cannot seem to change it to any other model, say mobilenet_v2.

thanks"
53476,Performance regression for FP16 Relu kernel from TF2.6 to TF2.7,"**Current behavior**
Migrating from TF2.6 to TF2.7 FP16 Relu kernel has changed from EigenMetaKernel to MLIR. 
Tested on Volta and Ampere, the memory throughput gets halved and compute throughput is also reduced. Collected by nsight compute. 
| Throughput %                    | compute | memory |
|---------------------------------|---------|--------|
| EigenMetaKernel                 | 35.79   | 84.20  | 
| Relu_GPU_DT_HALF_DT_HALF_kernel | 28.17   | 45.02  |

A roughly 2x slowdown of the forward pass is observed. The better bandwidth utilization of EigenMetaKernel is contributed from wider vectors for GPU-half implementation for libeigen. 

"
53475,Remove references to HCC,"https://github.com/tensorflow/tensorflow/blob/b56e6db5e7cccfffa824bbc1b5e018c6cc413c21/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc#L739

HCC was removed from ROCm in Summer 2020; references to it are likely cruft / removable.

Mark"
53472,error in running application and this error is showing,"Traceback (most recent call last):
  File ""E:\New folder\face_mask\facemask1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\New folder\face_mask\app\app.py"", line 7, in <module>
    from deeplearning import face_mask_prediction
  File ""E:\New folder\face_mask\app\deeplearning.py"", line 3, in <module>
    import tensorflow as tf
  File ""E:\New folder\face_mask\facemask1\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""E:\New folder\face_mask\facemask1\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""E:\New folder\face_mask\facemask1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""E:\New folder\face_mask\facemask1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
53471,Can't install packages within docker image 2.7.0-gpu on Mac OS,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 12.0.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): docker (Docker version 20.10.10)
- TensorFlow version: 2.7.0
- Python version: from docker
- Installed using virtualenv? pip? conda?: docker
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory:

**Describe the problem**

Hello, I have issues when using the docker image `tensorflow/tensorflow:2.7.0-gpu` on Mac OS. I'm using this image even if I don't have a GPU on my local machine because I use the same Dockerfile on the Linux machines with GPUs I use remotely.  

My issue is that when using this docker image on my Mac, I have an error (see below) when trying to install a package with `apt`. This issue *doesn't happen on Mac* when using the image `tensorflow/tensorflow:2.6.0-gpu` or `tensorflow/tensorflow:2.7.0`, and it *doesn't happen on Debian GNU/Linux 10* when using the image `tensorflow/tensorflow:2.7.0-gpu`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

You can reproduce this issue on Mac with the following Dockerfile:

```Dockerfile
FROM tensorflow/tensorflow:2.7.0-gpu

RUN apt-get update --fix-missing && apt-get install -y ssh
```

and doing `docker build .`

**Any other info / logs**

```bash
$ docker build .
[+] Building 10.8s (6/10)
 => [internal] load build definition from Dockerfile                                                                                                                                        0.1s
 => => transferring dockerfile: 839B                                                                                                                                                                   0.1s
 => [internal] load .dockerignore                                                                                                                                                                      0.0s
 => => transferring context: 35B                                                                                                                                                                       0.0s
 => [internal] load metadata for docker.io/tensorflow/tensorflow:2.7.0-gpu                                                                                                                             0.7s
 => [internal] load build context                                                                                                                                                                      0.0s
 => => transferring context: 234B                                                                                                                                                                      0.0s
 => CACHED [1/6] FROM docker.io/tensorflow/tensorflow:2.7.0-gpu@sha256:fc5eb0604722c7bef7b499bb007b3050c4beec5859c2e0d4409d2cca5c14d442                                                                0.0s
 => ERROR [2/6] RUN apt-get update --fix-missing && apt-get install -y ssh                                                                                                                             9.9s
------
 > [2/6] RUN apt-get update --fix-missing && apt-get install -y ssh:
#5 0.603 Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease
#5 0.624 Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
#5 0.668 Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
#5 0.681 Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease
#5 0.775 Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
#5 0.981 Get:6 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [797 kB]
#5 1.293 Get:7 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.6 kB]
#5 1.305 Get:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1108 kB]
#5 1.371 Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease
#5 1.466 Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release [696 B]
#5 1.510 Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [828 kB]
#5 1.559 Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release
#5 1.602 Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Release.gpg [836 B]
#5 1.738 Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1758 kB]
#5 2.086 Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [517 kB]
#5 2.394 Get:17 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [733 kB]
#5 2.893 Get:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [21.7 kB]
#5 2.908 Get:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [50.8 kB]
#5 2.912 Get:20 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1335 kB]
#5 3.371 Get:21 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.1 kB]
#5 3.471 Fetched 7550 kB in 3s (2502 kB/s)
#5 3.471 Reading package lists...
#5 4.458 Reading package lists...
#5 5.473 Building dependency tree...
#5 5.678 Reading state information...
#5 5.923 The following additional packages will be installed:
#5 5.924   libbsd0 libcbor0.6 libedit2 libfido2-1 libwrap0 libx11-6 libx11-data libxau6
#5 5.925   libxcb1 libxdmcp6 libxext6 libxmuu1 ncurses-term openssh-client
#5 5.926   openssh-server openssh-sftp-server python3-distro ssh-import-id ucf wget
#5 5.926   xauth
#5 5.928 Suggested packages:
#5 5.928   keychain libpam-ssh monkeysphere ssh-askpass molly-guard ufw
#5 5.982 The following NEW packages will be installed:
#5 5.983   libbsd0 libcbor0.6 libedit2 libfido2-1 libwrap0 libx11-6 libx11-data libxau6
#5 5.984   libxcb1 libxdmcp6 libxext6 libxmuu1 ncurses-term openssh-client
#5 5.985   openssh-server openssh-sftp-server python3-distro ssh ssh-import-id ucf wget
#5 5.985   xauth
#5 6.086 0 upgraded, 22 newly installed, 0 to remove and 27 not upgraded.
#5 6.086 Need to get 2841 kB of archives.
#5 6.086 After this operation, 15.9 MB of additional disk space will be used.
#5 6.086 Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libbsd0 amd64 0.10.0-1 [45.4 kB]
#5 6.201 Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libedit2 amd64 3.1-20191231-1 [87.0 kB]
#5 6.250 Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libcbor0.6 amd64 0.6.0-0ubuntu1 [21.1 kB]
#5 6.258 Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libfido2-1 amd64 1.3.1-1ubuntu2 [47.9 kB]
#5 6.276 Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-client amd64 1:8.2p1-4ubuntu0.3 [671 kB]
#5 6.493 Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.3 [51.5 kB]
#5 6.518 Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 ucf all 3.0038+nmu1 [51.6 kB]
#5 6.529 Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libwrap0 amd64 7.6.q-30 [46.3 kB]
#5 6.554 Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.3 [377 kB]
#5 6.630 Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 ssh all 1:8.2p1-4ubuntu0.3 [5080 B]
#5 6.633 Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libxau6 amd64 1:1.0.9-0ubuntu1 [7488 B]
#5 6.635 Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libxdmcp6 amd64 1:1.1.3-0ubuntu1 [10.6 kB]
#5 6.638 Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb1 amd64 1.14-2 [44.7 kB]
#5 6.652 Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libx11-data all 2:1.6.9-2ubuntu1.2 [113 kB]
#5 6.689 Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libx11-6 amd64 2:1.6.9-2ubuntu1.2 [575 kB]
#5 6.876 Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 libxext6 amd64 2:1.3.4-0ubuntu1 [29.1 kB]
#5 6.886 Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 libxmuu1 amd64 2:1.1.3-0ubuntu1 [9728 B]
#5 6.889 Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 wget amd64 1.20.3-1ubuntu2 [348 kB]
#5 7.005 Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 xauth amd64 1:1.1-0ubuntu1 [25.0 kB]
#5 7.018 Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 ncurses-term all 6.2-0ubuntu2 [249 kB]
#5 7.093 Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 python3-distro all 1.4.0-1 [14.6 kB]
#5 7.097 Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]
#5 7.214 debconf: Perl may be unconfigured (strict.pm did not return a true value at (eval 1) line 2.
#5 7.214 BEGIN failed--compilation aborted at (eval 1) line 2.
#5 7.214 ) -- aborting
#5 7.279 Fetched 2841 kB in 1s (2498 kB/s)
#5 7.315 Selecting previously unselected package libbsd0:amd64.
(Reading database ... 5660 files and directories currently installed.)
#5 7.345 Preparing to unpack .../00-libbsd0_0.10.0-1_amd64.deb ...
#5 7.355 Unpacking libbsd0:amd64 (0.10.0-1) ...
#5 7.410 Selecting previously unselected package libedit2:amd64.
#5 7.416 Preparing to unpack .../01-libedit2_3.1-20191231-1_amd64.deb ...
#5 7.423 Unpacking libedit2:amd64 (3.1-20191231-1) ...
#5 7.480 Selecting previously unselected package libcbor0.6:amd64.
#5 7.483 Preparing to unpack .../02-libcbor0.6_0.6.0-0ubuntu1_amd64.deb ...
#5 7.489 Unpacking libcbor0.6:amd64 (0.6.0-0ubuntu1) ...
#5 7.534 Selecting previously unselected package libfido2-1:amd64.
#5 7.539 Preparing to unpack .../03-libfido2-1_1.3.1-1ubuntu2_amd64.deb ...
#5 7.547 Unpacking libfido2-1:amd64 (1.3.1-1ubuntu2) ...
#5 7.599 Selecting previously unselected package openssh-client.
#5 7.603 Preparing to unpack .../04-openssh-client_1%3a8.2p1-4ubuntu0.3_amd64.deb ...
#5 7.617 Unpacking openssh-client (1:8.2p1-4ubuntu0.3) ...
#5 7.742 Selecting previously unselected package openssh-sftp-server.
#5 7.745 Preparing to unpack .../05-openssh-sftp-server_1%3a8.2p1-4ubuntu0.3_amd64.deb ...
#5 7.752 Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.3) ...
#5 7.835 Selecting previously unselected package ucf.
#5 7.838 Preparing to unpack .../06-ucf_3.0038+nmu1_all.deb ...
#5 7.847 Moving old data out of the way
#5 7.849 Unpacking ucf (3.0038+nmu1) ...
#5 7.906 Selecting previously unselected package libwrap0:amd64.
#5 7.909 Preparing to unpack .../07-libwrap0_7.6.q-30_amd64.deb ...
#5 7.917 Unpacking libwrap0:amd64 (7.6.q-30) ...
#5 7.967 Selecting previously unselected package openssh-server.
#5 7.969 Preparing to unpack .../08-openssh-server_1%3a8.2p1-4ubuntu0.3_amd64.deb ...
#5 7.996 Unpacking openssh-server (1:8.2p1-4ubuntu0.3) ...
#5 8.083 Selecting previously unselected package ssh.
#5 8.086 Preparing to unpack .../09-ssh_1%3a8.2p1-4ubuntu0.3_all.deb ...
#5 8.097 Unpacking ssh (1:8.2p1-4ubuntu0.3) ...
#5 8.136 Selecting previously unselected package libxau6:amd64.
#5 8.139 Preparing to unpack .../10-libxau6_1%3a1.0.9-0ubuntu1_amd64.deb ...
#5 8.147 Unpacking libxau6:amd64 (1:1.0.9-0ubuntu1) ...
#5 8.189 Selecting previously unselected package libxdmcp6:amd64.
#5 8.194 Preparing to unpack .../11-libxdmcp6_1%3a1.1.3-0ubuntu1_amd64.deb ...
#5 8.200 Unpacking libxdmcp6:amd64 (1:1.1.3-0ubuntu1) ...
#5 8.249 Selecting previously unselected package libxcb1:amd64.
#5 8.252 Preparing to unpack .../12-libxcb1_1.14-2_amd64.deb ...
#5 8.259 Unpacking libxcb1:amd64 (1.14-2) ...
#5 8.295 Selecting previously unselected package libx11-data.
#5 8.299 Preparing to unpack .../13-libx11-data_2%3a1.6.9-2ubuntu1.2_all.deb ...
#5 8.305 Unpacking libx11-data (2:1.6.9-2ubuntu1.2) ...
#5 8.398 Selecting previously unselected package libx11-6:amd64.
#5 8.401 Preparing to unpack .../14-libx11-6_2%3a1.6.9-2ubuntu1.2_amd64.deb ...
#5 8.405 Unpacking libx11-6:amd64 (2:1.6.9-2ubuntu1.2) ...
#5 8.506 Selecting previously unselected package libxext6:amd64.
#5 8.510 Preparing to unpack .../15-libxext6_2%3a1.3.4-0ubuntu1_amd64.deb ...
#5 8.515 Unpacking libxext6:amd64 (2:1.3.4-0ubuntu1) ...
#5 8.563 Selecting previously unselected package libxmuu1:amd64.
#5 8.566 Preparing to unpack .../16-libxmuu1_2%3a1.1.3-0ubuntu1_amd64.deb ...
#5 8.570 Unpacking libxmuu1:amd64 (2:1.1.3-0ubuntu1) ...
#5 8.606 Selecting previously unselected package wget.
#5 8.609 Preparing to unpack .../17-wget_1.20.3-1ubuntu2_amd64.deb ...
#5 8.614 Unpacking wget (1.20.3-1ubuntu2) ...
#5 8.675 Selecting previously unselected package xauth.
#5 8.682 Preparing to unpack .../18-xauth_1%3a1.1-0ubuntu1_amd64.deb ...
#5 8.686 Unpacking xauth (1:1.1-0ubuntu1) ...
#5 8.729 Selecting previously unselected package ncurses-term.
#5 8.734 Preparing to unpack .../19-ncurses-term_6.2-0ubuntu2_all.deb ...
#5 8.739 Unpacking ncurses-term (6.2-0ubuntu2) ...
#5 9.139 Selecting previously unselected package python3-distro.
#5 9.144 Preparing to unpack .../20-python3-distro_1.4.0-1_all.deb ...
#5 9.150 Unpacking python3-distro (1.4.0-1) ...
#5 9.199 Selecting previously unselected package ssh-import-id.
#5 9.203 Preparing to unpack .../21-ssh-import-id_5.10-0ubuntu1_all.deb ...
#5 9.209 Unpacking ssh-import-id (5.10-0ubuntu1) ...
#5 9.265 Setting up libxau6:amd64 (1:1.0.9-0ubuntu1) ...
#5 9.281 Setting up python3-distro (1.4.0-1) ...
#5 9.339 Traceback (most recent call last):
#5 9.340   File ""/usr/bin/py3compile"", line 34, in <module>
#5 9.340     from debpython.version import SUPPORTED, debsorted, vrepr, \
#5 9.341 ImportError: cannot import name 'SUPPORTED' from 'debpython.version' (/usr/share/python3/debpython/version.py)
#5 9.347 dpkg: error processing package python3-distro (--configure):
#5 9.347  installed python3-distro package post-installation script subprocess returned error exit status 1
#5 9.347 Setting up wget (1.20.3-1ubuntu2) ...
#5 9.366 dpkg: dependency problems prevent configuration of ssh-import-id:
#5 9.366  ssh-import-id depends on python3-distro; however:
#5 9.366   Package python3-distro is not configured yet.
#5 9.366
#5 9.366 dpkg: error processing package ssh-import-id (--configure):
#5 9.366  dependency problems - leaving unconfigured
#5 9.366 Setting up libcbor0.6:amd64 (0.6.0-0ubuntu1) ...
#5 9.374 Setting up libwrap0:amd64 (7.6.q-30) ...
#5 9.386 Setting up libx11-data (2:1.6.9-2ubuntu1.2) ...
#5 9.395 Setting up ucf (3.0038+nmu1) ...
#5 9.411 strict.pm did not return a true value at /usr/share/debconf/frontend line 5.
#5 9.411 BEGIN failed--compilation aborted at /usr/share/debconf/frontend line 5.
#5 9.413 dpkg: error processing package ucf (--configure):
#5 9.413  installed ucf package post-installation script subprocess returned error exit status 255
#5 9.413 Setting up libfido2-1:amd64 (1.3.1-1ubuntu2) ...
#5 9.425 Setting up libbsd0:amd64 (0.10.0-1) ...
#5 9.442 Setting up ncurses-term (6.2-0ubuntu2) ...
#5 9.461 Setting up libxdmcp6:amd64 (1:1.1.3-0ubuntu1) ...
#5 9.475 Setting up libxcb1:amd64 (1.14-2) ...
#5 9.489 dpkg: dependency problems prevent configuration of openssh-server:
#5 9.493  openssh-server depends on ucf (>= 0.28); however:
#5 9.493   Package ucf is not configured yet.
#5 9.493
#5 9.493 dpkg: error processing package openssh-server (--configure):
#5 9.495  dependency problems - leaving unconfigured
#5 9.496 Setting up libedit2:amd64 (3.1-20191231-1) ...
#5 9.511 Setting up libx11-6:amd64 (2:1.6.9-2ubuntu1.2) ...
#5 9.526 Setting up libxmuu1:amd64 (2:1.1.3-0ubuntu1) ...
#5 9.543 dpkg: dependency problems prevent configuration of ssh:
#5 9.544  ssh depends on openssh-server (>= 1:8.2p1-4ubuntu0.3); however:
#5 9.544   Package openssh-server is not configured yet.
#5 9.546
#5 9.546 dpkg: error processing package ssh (--configure):
#5 9.547  dependency problems - leaving unconfigured
#5 9.549 Setting up openssh-client (1:8.2p1-4ubuntu0.3) ...
#5 9.588 warnings.pm did not return a true value at /usr/sbin/addgroup line 32.
#5 9.588 BEGIN failed--compilation aborted at /usr/sbin/addgroup line 32.
#5 9.590 dpkg: error processing package openssh-client (--configure):
#5 9.590  installed openssh-client package post-installation script subprocess returned error exit status 255
#5 9.591 Setting up libxext6:amd64 (2:1.3.4-0ubuntu1) ...
#5 9.607 Setting up xauth (1:1.1-0ubuntu1) ...
#5 9.620 dpkg: dependency problems prevent configuration of openssh-sftp-server:
#5 9.620  openssh-sftp-server depends on openssh-client (= 1:8.2p1-4ubuntu0.3); however:
#5 9.621   Package openssh-client is not configured yet.
#5 9.622
#5 9.622 dpkg: error processing package openssh-sftp-server (--configure):
#5 9.623  dependency problems - leaving unconfigured
#5 9.626 Processing triggers for libc-bin (2.31-0ubuntu9.2) ...
#5 9.634 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc-builtins.so.11.2.152 is not an ELF file - it has the wrong magic bytes at the start.
#5 9.635
#5 9.638 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc.so.11.2.152 is not an ELF file - it has the wrong magic bytes at the start.
#5 9.640
#5 9.643 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc-builtins.so.11.2 is not an ELF file - it has the wrong magic bytes at the start.
#5 9.643
#5 9.645 /sbin/ldconfig.real: /usr/local/cuda/targets/x86_64-linux/lib/libnvrtc.so.11.2 is not an ELF file - it has the wrong magic bytes at the start.
#5 9.647
#5 9.663 /sbin/ldconfig.real: File /usr/local/cuda/lib64/stubs/libcuda.so is empty, not checked.
#5 9.664 /sbin/ldconfig.real: File /usr/local/cuda/lib64/stubs/libcuda.so.1 is empty, not checked.
#5 9.685 Errors were encountered while processing:
#5 9.685  python3-distro
#5 9.685  ssh-import-id
#5 9.685  ucf
#5 9.685  openssh-server
#5 9.685  ssh
#5 9.685  openssh-client
#5 9.685  openssh-sftp-server
#5 9.705 E: Sub-process /usr/bin/dpkg returned an error code (1)
------
executor failed running [/bin/bash -c apt-get update --fix-missing && apt-get install -y ssh]: exit code: 100
```
"
53470,EinsumDense layer ignores `global_dtype_policy` when imported in a certain way,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.5.1
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): `2.6.2`
- Python version: `3.8`

**Describe the current behavior**

I tried using the `EinsumDense` layer in a `mixed_float16` training and it seems like that the way you import this layer is very crucial, as it tends to ignore the `dtype_policy` when I import it with `from tensorflow.python.keras.layers.einsum_dense import EinsumDense`.

**Describe the expected behavior**
All layers should use the `global_dtype_policy`, regardless of the way they are imported.


**Standalone code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras import mixed_precision
from tensorflow.keras.layers.experimental import EinsumDense as EinsumDense1
from tensorflow.python.keras.layers.einsum_dense import EinsumDense as EinsumDense2
from keras.layers.einsum_dense import EinsumDense as EinsumDense3

policy = mixed_precision.Policy(""mixed_float16"")
mixed_precision.set_global_policy(policy)

layer1 = EinsumDense1(""abc,cd->abd"",
                      output_shape=(None, 64),
                      bias_axes=""d"")

layer2 = EinsumDense2(""abc,cd->abd"",
                      output_shape=(None, 64),
                      bias_axes=""d"")

layer3 = EinsumDense3(""abc,cd->abd"",
                      output_shape=(None, 64),
                      bias_axes=""d"")

layer4 = tf.keras.layers.experimental.EinsumDense(""abc,cd->abd"",
                      output_shape=(None, 64),
                      bias_axes=""d"")

input_tensor = tf.keras.Input(shape=[32, 128])

print('from tensorflow.keras.layers.experimental import EinsumDense')
print(layer1(input_tensor))
print(layer1.dtype_policy) # float16
print('------------------------------------------------------------')

print('from tensorflow.python.keras.layers.einsum_dense import EinsumDense')
print(layer2(input_tensor))
print(layer2.dtype_policy) # float32

print('------------------------------------------------------------')

print('from keras.layers.einsum_dense import EinsumDense')
print(layer3(input_tensor))
print(layer3.dtype_policy) # float16
print('------------------------------------------------------------')

print('from keras.layers.einsum_dense import EinsumDense')
print(layer4(input_tensor))
print(layer4.dtype_policy) # float16
```
"
53467,MacOS: Problem getting numpy include path.,"<em>Please make sure that this is a bug. As per our
**Describe the current behavior**
I am able to build on my development Mac. However, running on the CI Mac node, we get an error that looks like this:

```
2021-12-17T01:27:56.6979400Z Analyzing: target //tensorflow/lite/ios:TensorFlowLiteC_static_framework (1 packages loaded, 0 targets configured)
2021-12-17T01:27:58.6232140Z Analyzing: target //tensorflow/lite/ios:TensorFlowLiteC_static_framework (21 packages loaded, 10 targets configured)
2021-12-17T01:27:58.8729360Z INFO: Repository local_execution_config_python instantiated at:
2021-12-17T01:27:58.8731780Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/WORKSPACE:15:14: in <toplevel>
2021-12-17T01:27:58.8734480Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace
2021-12-17T01:27:58.8737750Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:84:27: in _tf_toolchains
2021-12-17T01:27:58.8739200Z   /private/var/tmp/_bazel_runner2/14b6f7e45d646cdf7b1c6e3a405fb4ce/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs
2021-12-17T01:27:58.8740200Z   /private/var/tmp/_bazel_runner2/14b6f7e45d646cdf7b1c6e3a405fb4ce/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config
2021-12-17T01:27:58.8740920Z Repository rule local_python_configure defined at:
2021-12-17T01:27:58.8742180Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>
2021-12-17T01:27:58.8812530Z INFO: Repository local_config_python instantiated at:
2021-12-17T01:27:58.8814680Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/WORKSPACE:15:14: in <toplevel>
2021-12-17T01:27:58.8816660Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:1108:19: in workspace
2021-12-17T01:27:58.8818170Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/tensorflow/workspace2.bzl:94:21: in _tf_toolchains
2021-12-17T01:27:58.8818980Z Repository rule python_configure defined at:
2021-12-17T01:27:58.8820110Z   /Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>
2021-12-17T01:27:58.8864240Z ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
2021-12-17T01:27:58.8865240Z    Traceback (most recent call last):
2021-12-17T01:27:58.8867660Z 	File ""/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl"", line 213, column 39, in _create_local_python_repository
2021-12-17T01:27:58.8869530Z 		numpy_include = _get_numpy_include(repository_ctx, python_bin) + ""/numpy""
2021-12-17T01:27:58.8870840Z 	File ""/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/py/python_configure.bzl"", line 187, column 19, in _get_numpy_include
2021-12-17T01:27:58.8871650Z 		return execute(
2021-12-17T01:27:58.8872840Z 	File ""/Users/runner2/.conan/data/tensorflow-lite/2.5.0.18+5089e313/ci/branch_feat_2.7.0/build/3b95326f653baf403ae4d70dae74814d9c4fef8a/tensorflow/third_party/remote_config/common.bzl"", line 219, column 13, in execute
2021-12-17T01:27:58.8873970Z 		fail(
2021-12-17T01:27:58.8874300Z Error in fail: Problem getting numpy include path.
2021-12-17T01:27:58.8874970Z Traceback (most recent call last):
2021-12-17T01:27:58.8875640Z   File ""<string>"", line 1, in <module>
2021-12-17T01:27:58.8876820Z ModuleNotFoundError: No module named 'numpy'
2021-12-17T01:27:58.8877460Z Is numpy installed?
```

The only difference is, CI node uses Homebrew python 3.8, while the dev system is using system python. 
I've tried using venv on both machines, which doesn't change the outcome. 
I've also attempted setting `PYTHON_BIN_PATH` and `PYTHON_LIB_PATH` (for both venv and direct Homebrew python scenario) with no luck.

**Describe the expected behavior**
I'd expect it to build no matter which python is being used.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53466,tf.debugging.assert_type throws error when checking type of RaggedTensor,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.6 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version: **3.9.0**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When calling `tf.debugging.assert_type` on a `RaggedTensor`, a `ValueError: TypeError: object of type 'RaggedTensor' has no len()` is thrown.

**Describe the expected behavior**
The assertion check should pass if the ragged tensor is the correct type or fail otherwise.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **no**
- Briefly describe your candidate solution(if contributing): **N/A**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

a = tf.ragged.constant([[1, 2], [1]])
tf.debugging.assert_type(a, tf_type=tf.int32)  # Should pass
>>> ValueError: TypeError: object of type 'RaggedTensor' has no len()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Workaround**
```
import tensorflow as tf

a = tf.ragged.constant([[1, 2], [1]])
assert a.dtype == tf.int32
```"
53464,TensorFlow Lite Model Maker has version conflict during install on RPi02W,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/lite/guide/model_maker

Please provide a link to the documentation entry, for example:
[Model Maker Object Detection](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/)

## Description of issue (what needs changing):

Cannot install **tflite_model_maker** and **tflite_support** on Raspberry Pi Zero 2 W

` uname -a
Linux raspbari17 5.10.63-v7+ #1488 SMP Thu Nov 18 16:14:44 GMT 2021 armv7l GNU/Linux
`
### Clear description

To fix this you could try to:
1. _loosen_ the range of package versions you've specified
2. _remove_ package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies


### Correct links

Is the link to the source code correct? n/a

### Parameters defined

Are all parameters defined and formatted correctly? Yes

### Returns defined

Are return values defined? n/a

### Raises listed and defined

ERROR: Cannot install tflite-model-maker==0.1.0, tflite-model-maker==0.1.1, tflite-model-maker==0.1.2, tflite-model-maker==0.2.0, tflite-model-maker==0.2.1, tflite-model-maker==0.2.2, tflite-model-maker==0.2.3, tflite-model-maker==0.2.4, tflite-model-maker==0.2.5, tflite-model-maker==0.3.0, tflite-model-maker==0.3.1, tflite-model-maker==0.3.2, tflite-model-maker==0.3.3 and tflite-model-maker==0.3.4 because these package versions have conflicting dependencies.

The conflict is caused by:
    tflite-model-maker 0.3.4 depends on tensorflow-addons>=0.11.2
    tflite-model-maker 0.3.3 depends on tensorflow-addons>=0.11.2
    tflite-model-maker 0.3.2 depends on tensorflow-addons>=0.11.2
    tflite-model-maker 0.3.1 depends on tensorflow-addons>=0.11.2
    tflite-model-maker 0.3.0 depends on tensorflow-addons>=0.11.2
    tflite-model-maker 0.2.5 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.4 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.3 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.2 depends on tflite-support==0.1.0rc3.dev2
    tflite-model-maker 0.2.1 depends on tf-nightly==2.4.0.dev20200811
    tflite-model-maker 0.2.0 depends on tf-nightly==2.4.0.dev20200810
    tflite-model-maker 0.1.2 depends on tf-nightly
    tflite-model-maker 0.1.1 depends on tf-nightly
    tflite-model-maker 0.1.0 depends on tf-nightly


### Usage example

Is there a usage example?

`sudo python3 -m pip install tflite-model-maker`

### Request visuals, if applicable

Are there currently visuals? No but terminal output entered above.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? No, beyond my job grade. Sorry.
"
53463,Cannot create hexagon delegate on my tablet,"Hello I'm trying to create a dsp/hexagon delegate on my android tablet, but for some reason ioctrl fails as reported on device log
Any help!
Regards,
Walter

Relevant C++ code:

//TfLiteHexagonInit(); // also desnt works
TfLiteHexagonInitWithPath(""dsp;/data/dsp;/system/vendor/lib/rfsa/adsp"");
TfLiteHexagonDelegateOptions hexagon_opts = {0};
TfLiteDelegate* hexagonDelegate = TfLiteHexagonDelegateCreate(&hexagon_opts)  // fails here

Log:

I/tflite: Initialized TensorFlow Lite runtime.
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:688::error: -1: 0 == ioctl(dev, FASTRPC_IOCTL_INIT, (unsigned long)&init)
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:764::error: -1: !apps_dev_init(domain)
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:431::error: -1: -1 != open_dev(domain)
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:688::error: -1: 0 == ioctl(dev, FASTRPC_IOCTL_INIT, (unsigned long)&init)
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:764::error: -1: !apps_dev_init(domain)
E/adsprpc: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:329::error: -1: -1 != (dev = open_dev(domain))
W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
I/tflite: Hexagon Delegate is not supported.

cat /proc/cpuinfo | grep Hardware
Hardware        : Qualcomm Technologies, Inc APQ8096

adb shell getprop ro.product.device
msm8996

adb shell getprop ro.board.platform
msm8996

Using Hexagon libraries v1.20.0.1 downladed from here https://www.tensorflow.org/lite/performance/hexagon_delegate

Benchmark app throws this log:

STARTING!
Log parameter values verbosely: [0]
Graph: [mobilenet_v1_0.25_192_quant.tflite]
Use Hexagon: [1]
Loaded model mobilenet_v1_0.25_192_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
loaded libadsprpc.so
Func remote_handle_control not available on this device (NULL).
WARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
INFO: Hexagon Delegate is not supported.

Could not create Hexagon delegate: platform may not support delegate or required libraries are missing
The input model file size (MB): 0.497264
Initialized session in 285.927ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=68 first=17405 curr=6455 min=6361 max=17405 avg=7354.15 std=2021

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=143 first=6432 curr=6750 min=6371 max=12065 avg=6967.24 std=676

Inference timings in us: Init: 285927, First inference: 17405, Warmup (avg): 7354.15, Inference (avg): 6967.24
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=0.972656 overall=2.53125"
53462,"tensorflow-cpu install from conda-forge fails with 'requires grpc-cpp >=1.39.1,<1.40.0a0, but none of the providers can be installed'","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.3 LTS
- TensorFlow installed from (source or binary): binary, `tensorflow-cpu` latest as of 12 Dec 2021
- TensorFlow version: latest (should install 2.6.2, installs 2.6.0 due to existing packages)
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: `mamba` `/ conda`
- GPU model and memory: none, CPU only

**Describe the problem**

Run following Docker container:

```
docker container run -it --rm --entrypoint=bash --user=0 \
  jupyter/all-spark-notebook
```

Run these commands:

```
cat << EOF > req.txt
jupyterhub
nbgitpuller
pyspark
boto3
elasticsearch
altair
beautifulsoup4
bokeh
bottleneck
cloudpickle
cython
dask
h5py
ipympl
ipywidgets
matplotlib-base
nltk
gensim
numba
numexpr
pandas
patsy
protobuf
pytables
scikit-learn
scikit-image
scipy
seaborn
statsmodels
xlrd
EOF

mamba install -q --file req.txt -y
mamba install -v -y -c conda-forge tensorflow-cpu
```

Will result in:

```
info     Problem count: 1
Encountered problems while solving:
  - package tensorflow-base-2.6.0-cpu_py39h7e79a0b_2 requires grpc-cpp >=1.39.1,<1.40.0a0, but none of the providers can be installed
```

Notice that:

- 2.6.2 is latest version of tensorflow, but this appears to install 2.6.0
- grpc-cpp 1.42.0 is installed already prior to tensorflow-cpu attempted install

So let's try:

```
mamba install -v -y -c conda-forge tensorflow-cpu==2.6.2
```

Result:

```
info     Problem count: 1
Encountered problems while solving:
  - package grpc-cpp-1.42.0-ha1441d3_1 requires libprotobuf >=3.19.1,<3.20.0a0, but none of the providers can be installed
```

Funny thing is, `libprotobuf` 3.19.1 _is_ installed:

```
mamba list | grep proto
# libprotobuf               3.19.1               h780b84a_0    conda-forge
```"
53461,[TFLite] iOS app crashes with EXC_RESOURCE when building with XCode 13,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: iOS 15
- Mobile device if the issue happens on mobile device: iPhone
- TensorFlow installed from: source
- TensorFlow version: 2.6.0
- Python version: Python 3.9.7
- Bazel version: 4.2.1-homebrew
- GCC/Compiler version: Apple clang version 13.0.0 (clang-1300.0.29.3)
- CUDA/cuDNN version: n.a.
- GPU model and memory: n.a.

**Describe the current behavior**
The app crashes with `EXC_RESOURCE` when running TFLite model on iOS device.
* This problem occurs only when the app is built with XCode 13.0 / 13.1 / 13.2 and using Core ML backend.
* The problem does not occur when the app is built with XCode 12.

**Describe the expected behavior**
App run without crash when built with XCode 13

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):

@freedomtan has identified the issue:
> FYR. I upgraded my macOS, iOS, and Xcode to 12.1, 15.2, and Xcode 13.2 yesterday. Then, I spent some time profiling and found that the culprit is [CoreML delegate's invoke function].(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate_kernel.mm#L207-L230). Adding `@autoreleasepool {}` to it resolved the EXC_RESOURCE issue. Tested on iPhone 11 Pro and iPhone 13 running iOS 15.2. Supposedly, it worked for earlier Xcode 13.x and iOS.

**Standalone code to reproduce the issue**
Not possible since the app need to be built and run.

**Other info / logs**

The app used is located here: https://github.com/mlcommons/mobile_app_open

Crash log:
```
flutter: Running Benchmark:IS_float32 in performance mode...

...

li:cpp/flutter/main.cc:252@run_backend4
2021-10-02 14:21:30.769776: I cpp/backends/external.cc:135] Using default allocator
Enabling CoreML delegate 0x2836b5a00
CoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.
INFO: CoreML delegate: 76 nodes delegated out of 77 nodes, with 1 partitions.

li:cpp/flutter/main.cc:257@run_backend4
2021-10-02 14:21:31.456619: E cpp/datasets/ade20k.cc:76] Failed to list all the ground truth files in provided path. Only measuring performance.
li:cpp/flutter/main.cc:285@run_backend4
li:cpp/flutter/main.cc:289@run_backend4
* thread #26, name = 'DartWorker', queue = 'com.apple.CoreMLBatchProcessingQueue', stop reason = EXC_RESOURCE RESOURCE_TYPE_MEMORY (limit=2098 MB, unused=0x0)
    frame #0: 0x00000001f37d864c libsystem_platform.dylib`_platform_memmove + 76
libsystem_platform.dylib`_platform_memmove:
->  0x1f37d864c <+76>: stnp   q0, q1, [x3]
    0x1f37d8650 <+80>: add    x3, x3, #0x20             ; =0x20 
    0x1f37d8654 <+84>: ldnp   q0, q1, [x1]
    0x1f37d8658 <+88>: add    x1, x1, #0x20             ; =0x20 
Target 0: (Runner) stopped.
```
![135967965-1e284466-a072-4b15-8895-9bbd6001a70a](https://user-images.githubusercontent.com/85728587/146400916-bc8d5e6d-5053-41fb-b4ba-2df5bca58fc1.png)

"
53459,"UnimplementedError:  Fusion is not implemented: [BiasAdd,Tanh] (only when tanh activation functions are used)","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Linux Ubuntu 18.04.6 LTS)
- TensorFlow version (use command below): 2.4.1
- Python version: Python 3.8.5


I am trying to build a keras model to solve a problem in which I define a custom loss function, the model has tanh activation functions and in the custom loss function I generate random input values and then I do some manipulations to prepare the input of the model that takes 3 inputs and outputs just one output. The loss function is then defined as a function of the output and some of its derivatives with respect to the inputs which are computed using tf.gradients. 

The code is shown below

```python
import sys
import os.path
import tensorflow as tf
import math as m
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time
from itertools import product, combinations
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Layer, Activation, Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam


inputs = tf.keras.Input(shape=(3,),dtype='float32')
layer1 = Dense(units=200, activation='tanh',dtype='float32')(inputs)
layer2 = Dense(units=200, activation='tanh',dtype='float32')(layer1)
layer3 = Dense(units=200, activation='tanh',dtype='float32')(layer2)
layer4 = Dense(units=200, activation='tanh',dtype='float32')(layer3)
layer5 = Dense(units=200, activation='tanh',dtype='float32')(layer4)
layer6 = Dense(units=200, activation='tanh',dtype='float32')(layer5)
layer7 = Dense(units=200, activation='tanh',dtype='float32')(layer6)
layer8 = Dense(units=200, activation='tanh',dtype='float32')(layer7)
layer9 = Dense(units=200, activation='tanh',dtype='float32')(layer8)
predictions = Dense(units=1, activation='linear',dtype='float32')(layer9)
model = tf.keras.Model(inputs=inputs, outputs=predictions)

# Define custom loss
def custom_loss():

    # Create a loss function
    def loss(y_true,y_pred):
        
        # Write here the loss function

        x0 = tf.random.uniform(shape=[2,100],maxval = 1)
        y0 = tf.random.uniform(shape=[2,100],maxval = 1)
        t0 = tf.random.uniform(shape=[100,2],maxval = 1)
        pY0 = tf.transpose(tf.concat([tf.ones([1,y0.shape[1]]) * y0[0,:], tf.zeros([1,y0.shape[1]], dtype='float32'), tf.ones([1,y0.shape[1]]) * y0[1,:]], 0))
        pY1 = tf.transpose(tf.concat([tf.ones([1,y0.shape[1]]) * y0[0,:], tf.ones([1,y0.shape[1]], dtype='float32'), tf.ones([1,y0.shape[1]]) * y0[1,:]], 0))
        pX0 = tf.transpose(tf.concat([tf.zeros([1,x0.shape[1]], dtype='float32'), tf.ones([1,x0.shape[1]]) * x0[0,:], tf.ones([1,x0.shape[1]]) * x0[1,:]], 0))
        pX1 = tf.transpose(tf.concat([tf.ones([1,x0.shape[1]], dtype='float32'), tf.ones([1,x0.shape[1]]) * x0[0,:], tf.ones([1,x0.shape[1]]) * x0[1,:]], 0))
        pIC = tf.concat([t0, tf.zeros([t0.shape[0],1], dtype='float32')], 1)
        pF = tf.random.uniform(shape=[1000,3],maxval = 1)
        uX0 = model(pX0, training=True)
        uX1 = model(pX1, training=True)
        uY0 = model(pY0, training=True)
        uY1 = model(pY1, training=True)
        uIC = model(pIC, training=True)
        u = model(pF, training=True)

        # compute the derivatives of u
        u_grad = tf.gradients(u, pF)[0]
        u_Dot = tf.gradients(u, pF)[0]
        u_t = u_grad[:,2]
        u_x = u_grad[:,0]
        u_y = u_grad[:,1]
        u_xx = (tf.gradients(u_x, pF))[0][:,0]
        u_yy = (tf.gradients(u_y, pF))[0][:,1]

        return tf.reduce_mean(tf.square(uX0)) + tf.reduce_mean(tf.square(uX1)) + \
        tf.reduce_mean(tf.square(uY0)) + tf.reduce_mean(tf.square(uY1)) + \
        tf.reduce_mean(tf.square(uIC - tf.transpose(tf.math.sin(pi*pIC[:,0])*tf.math.sin(pi*pIC[:,1]) * tf.ones([1,1], dtype='float32')) )) + \
        tf.reduce_mean(tf.square(u_t-u_xx-u_yy))

    # Return a function
    return loss

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4) ,
              loss=custom_loss(),
              # Call the loss function with the selected layer
              metrics=['accuracy'])

input_T = tf.linspace([0.0, 0.0, 0.0], [1.0, 1.0, 1.0], 1000) # We define a dummy input in order to use the fit method
output_T = tf.linspace([0.0], [1.0], 1000) # We define a dummy output
model.fit(input_T, output_T,epochs=10, batch_size = input_T.shape[0])

````


I tried to run the code on my PC using the CPU and I got the following error 

```
`File ""NewTest_HeatEq.py"", line 169, in <module>
    model.fit(input_T, output_T,epochs=10, batch_size = input_T.shape[0])
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2942, in __call__
    return graph_function._call_flat(
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 555, in call
    outputs = execute.execute(
  File ""/home/saddam/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError:  Fusion is not implemented: [BiasAdd,Tanh]
	 [[node loss/model/dense/Tanh_3 (defined at NewTest_HeatEq.py:100) ]] [Op:__inference_train_function_4519]

Function call stack:
train_function
```

However, the code worked when I ran it on a GPU machine, and also the code works when I run it on my PC changing only the activation functions to sigmoid ones. I find the behavior not normal, do you have any idea on what has caused that and how to fix the issue. 


"
53458,Deprecate non-tuple nd-indices,"This is deprecated in NumPy (numpy/numpy#9686) and in Jax already removed (google/jax#4867) but TensorFlow still uses this feature as can be seen by using NumPy arrays and Jax arrays:
```py
>>> from tensorflow.keras import Sequential
>>> from jax.numpy import array
>>> Sequential().predict(array([0]))
[...]
TypeError: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[array(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.
[...]
>>> from numpy import array
>>> Sequential().predict(array([0]))
array([0])
```
For now, I opened an issue on Jax (google/jax#8980) and asked to revert this change, but anyway I think TensorFlow really should accept the deprecation."
53456,Writing Tensor objects to Numpy arrays in Tensorflow 1.12,"I am building a model which is supposed to be a composition of different models. I need the output of one model to be fed as input to another. To do so, I followed the standard approach of extending tf.keras.Model and modifying the call() function suitably. However, that didn’t work out since Model2 expects an input of the type np.ndarray whereas Model1 returns an output of the type tf.tensor hence their composition isn’t possible.

I have tried multiple ways of doing the above including converting the tensor to a tf.Variable and using the Variable.assign() method, writing out values to a list and then using np.stack() to get a np.tensor as output. Nothing has worked so far.

I request the community to kindly help me out in solving the above issue."
53455,"When I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0.How can I do?","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports) 11.6 (20G165)
- TensorFlow installed from (source or binary):TensorFlowLiteSwift','2.6.0'
- TensorFlow version (or github SHA if from source):TensorFlowLiteSwift','2.6.0'

question:
When I use ObjectDetection Demo,it tips Invalid tensor index 1, max index is 0,How can I do?  
I do not know where is Error ,I just use my tflitemodel to replace Demo tflitemodel ,it show last tips:

Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.
Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.
Failed to invoke the interpreter with error: Invalid tensor index 1, max index is 0.
..."
53454,ERROR: gen_sparse_csr_matrix_ops not exists in tensorflow.python.ops.linalg.sparse,"
https://github.com/tensorflow/tensorflow/blob/1e3cc64881fc65226f98b7fc1c8ac6f5debac7df/tensorflow/python/ops/linalg/sparse/sparse_csr_matrix_ops.py#L32

from tensorflow.python.ops.linalg.sparse import gen_sparse_csr_matrix_ops always returns error"
53453,Description error of 'non_max_suppression_with_scores',"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py

## Description of issue (what needs changing):

### Clear description
1. In line 3755，I think it we should modify `tf.image.non_max_suppression_padded` to be `tf.image.non_max_suppression_with_scores`;
2. In line 3760，I think it we should modify `tf.image.non_max_suppression_padded` to be `tf.image.non_max_suppression_with_scores`;
"
53451,Tensor shape mismatches the associate numpy array shape,"The following code demonstrates a simple TF graph with transpose, pad and another transpose that gives the output data with shape that mismatches tensor shape.

```
import numpy as np
import tensorflow.compat.v1 as tf

if __name__ == '__main__':
    tf.disable_eager_execution()
    tf.reset_default_graph()

    input_ = tf.placeholder(shape=(5, 3, 2, 4), dtype=tf.float32)

    permutation1 = tf.constant(value=(0, 2, 3, 1))
    transpose1 = tf.transpose(a=input_, perm=permutation1)

    pad_shape = tf.constant(value=((0, 0), (0, 0), (0, 0), (0, 0)))
    # pad_shape = tf.constant(value=((0, 0), (10, 0), (0, 0), (0, 0)))  # THIS VERSION WORKS!!!

    pad = tf.pad(tensor=transpose1, paddings=pad_shape)

    identity1 = tf.identity(input=pad)

    permutation2 = tf.constant(value=(0, 3, 1, 2))
    out_tensor = tf.transpose(a=identity1, perm=permutation2)

    tf.identity(out_tensor)

    input_data = np.random.randint(low=-127, high=127, size=np.prod(input_.shape)).reshape(input_.shape).astype(
        np.float32)

    with tf.Session() as session:
         tf_out_arrays = session.run(fetches=[out_tensor], feed_dict={input_: input_data})

    output_data = tf_out_arrays[0]

    assert input_.shape == input_data.shape, 'Input data shape'
    assert output_data.shape == out_tensor.shape, f'Output data shape mismatch {output_data.shape} != {out_tensor.shape}'
```

Gives the assertion error:
```
AssertionError: Output data shape mismatch (5, 4, 3, 2) != (5, 3, 2, 4)
```

The transposition was not applied by the TF and tensor shape now mismatches the data shape.

Interestingly if the padding values are not zeros, that is if the line
```
  pad_shape = tf.constant(value=((0, 0), (0, 0), (0, 0), (0, 0)))
```
is changed to 
```
pad_shape = tf.constant(value=((0, 0), (10, 0), (0, 0), (0, 0)))
```
the code works.

The TF version is:
```
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
2021-12-16 11:45:20.516535: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-12-16 11:45:20.516563: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
v2.4.0-49-g85c8b2a817f 2.4.1
```
but I have also tested it against 2.4.4, 2.5.2, 2.6.2  and 2.7.0 - the mismatch reproduces. 

The code was tested in Ubuntu 20.04 and ArchLinux.

No GPU was used.
"
53447,[TFLite] Support building external delegate with CMake,"TensorFlow Lite supports external delegate but not supported in CMake build, do we have plan to support it?"
53446,[PluggableDevice] Cross-device copies support,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-69032-gfb972503145 2.8.0-dev20211220
- Python version: 3.9
- Bazel version (if compiling from source): 3.7.2

**Describe the current behavior**

When using a PluggableDevice plugin with 2 physical graphics cards, the `memcpy_dtod` function is invoked. This function, however, only has a single `SP_Device` as a parameter, even when the `src` and `dst` memory regions are from different devices.

**Describe the expected behavior**

The `memcpy_dtod` function should have 2 `SP_Device` parameters in case the copy is done cross-adapter to give the plugin a chance to handle cross-device copies.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): Adding a `memcpy_dtod_cross_adapter` function for cross-device memory copies, or adding a second `SP_Device` parameter to `memcpy_dtod` (although the latter wouldn't be backward compatible).

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

a = tf.compat.v1.placeholder(dtype=tf.float32, shape=[3,1])
a_values = [[1],[2],[3]]

b = tf.compat.v1.placeholder(dtype=tf.float32, shape=[3,1])
b_values = [[4],[6],[11]]

c = tf.compat.v1.placeholder(dtype=tf.float32, shape=[1,3])
c_values = [[2,4,5]]

with tf.device(""/device:CUSTOM:0""):
    y1 = tf.raw_ops.AddN(inputs=[a,b], name=""MyAdd"")
with tf.device(""/device:CUSTOM:1""):
    y2 = tf.raw_ops.MatMul(a=y1, b=c, name=""MyMultiply"")

with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as s:
    print(s.run(y2, feed_dict={a:a_values, b:b_values, c:c_values}))
```"
53445,Can not run the first test,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


def execute2_lds(plan, start, capacity):
# letter count
value = 0
points = []
points.append(start)
# count the letter
for i in range(len(plan)):
exvalue = 0
if plan[i][0] > capacity:
exxvalue = abs(plan[i][2][0] - plan[i][1][0])
exyvalue = abs(plan[i][2][1] - plan[i][1][1])
# determine capacity
exunits = (exxvalue +exyvalue) * (plan[i][0] % capacity + 1) * 2
exvalue += exunits
value += exvalue
nplan = plan[i][1:]
for j in nplan:
# put all coodinates into a list
points.append(j)
for z in range(len(points) - 1):
xunits = abs(points[z + 1][0] - points[z][0])
yunits = abs(points[z + 1][1] - points[z][1])
# add together
value += (xunits + yunits)
return value

1 + 4 + 2 + 3
... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 9))
10

1 + 4 + 2 + 3 + 3 + 3 + 3 + 3
... print(execute2_lds([(3, (1, 0), (3, 2)), (7, (2, 1), (0, 0))], (0, 0), 3))
22

5 + 5 + 5 + 4 + 1 + 1
... print(execute2_lds([(8, (0, 0), (1, 4)), (2, (1, 4), (1, 0)), (4, (0, 0), (1, 0))], (0, 0), 4))
21"
53444,for i,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53443,Support partial parameter warm-start from pretrained checkpoints in tensorflow v2 (non-estimator mode),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tensorflow 2.x
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Warm-starting from an existing checkpoint is an important feature for all kinds of model training that's well supported under tf.estimator framework. The latter unfortunately seems relegated to second class status in tensorflow 2.x, since it's not eager (at least not natively?). 

The closest way to achieve warm-start in tf2.x seems to be via tf.train.Checkpoint, a more directly approach than tf.estimator.warm_start_utils, that is, if implemented well. So far however I see that it supports loading all the parameters in a checkpoint altogether, but not loading only some of the parameters. 

**Will this change the current api? How?**
Possibly. Maybe provide a kwarg in Checkpoint initializer called parameter_list.

**Who will benefit with this feature?**
Anyone who wants to reuse a pretrained model but not all of its parameters.

**Any Other info.**
There is a lot of emphasis on eagerness, efficiency, in tensorflow v2, but in my opinion not sufficient focus on flexibility and directness of usage so far. Warm-starting is one example.

Update: my colleague found an unofficial implementation of flexible warm-start from a checkpoint in the TF2 version of BERT [here](https://github.com/kpe/bert-for-tf2/blob/master/bert/loader.py#L236). I would recommend adding some official examples along similar lines."
53441,Tensorflow-metal error when running .fit(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.9.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Apple Metal 32gb (unified)

The error occurs when I attempt to train the model using the GPU. The error does not happen when I specify `with tf.device('/cpu:0'):`, so CPU training appears to be fine, and returns similar results when cross-checked with the results obtained on Google Colab. This appears to be an issue specifically due to tensorflow-metal. My tensorflow-metal version is `0.3.0`. 

**Standalone code to reproduce the issue**
The example on [3D image classification from CT scans](https://keras.io/examples/vision/3D_image_classification/)
More specifically, the section that starts model training:

<pre><code># Compile model.
initial_learning_rate = 0.0001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)
model.compile(
    loss=""binary_crossentropy"",
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    metrics=[""acc""],
)

# Define callbacks.
checkpoint_cb = keras.callbacks.ModelCheckpoint(
    ""3d_image_classification.h5"", save_best_only=True
)
early_stopping_cb = keras.callbacks.EarlyStopping(monitor=""val_acc"", patience=15)

# Train the model, doing validation at the end of each epoch
epochs = 100


model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=epochs,
    shuffle=True,
    verbose=2,
    callbacks=[checkpoint_cb, early_stopping_cb],
)</code></pre>

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[error.txt](https://github.com/tensorflow/tensorflow/files/7723232/error.txt)

"
53438,RNN predict() raised error if shape is not the same as training set for the first time,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): 2.7.0 source
- TensorFlow version (use command below): 2.7.0
- Python version: 3.9.7
- Bazel version (if compiling from source): 4.2.2
- GCC/Compiler version (if compiling from source): msvc v1916
- CUDA/cuDNN version: 11.5/ 8.2
- GPU model and memory: GTX1060 6GB
- 
**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1Baaf74EzBjmuCUyY8MwFOGHAJwK2BUSP?usp=sharing

**Describe the current behavior**
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
sample, sample_label = x_train[0], y_train[0]

model = Sequential()

# Layers
model.add(LSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

# Optimizer
opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)

#Compile
model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

#Fit
model.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test))
```

and the run

```python
model.predict(x_test[0:1, :10, :])
```

raised error

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_5600/1128729238.py in <module>
      1 ans = []
      2 for i in range(1, 28):
----> 3     ans.append(model.predict(x_test[0:1, :i, :])[0])

~\miniconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\miniconda3\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1621, in predict_function  *
        return step_function(self, iterator)
    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1604, in run_step  **
        outputs = model.predict_step(data)
    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\training.py"", line 1572, in predict_step
        return self(x, training=False)
    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\Henry\miniconda3\lib\site-packages\keras\engine\input_spec.py"", line 263, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer ""{layer_name}"" is '

    ValueError: Input 0 of layer ""sequential_1"" is incompatible with the layer: expected shape=(None, 28, 28), found shape=(None, 1, 28)
```

But then if you run this first

```python
model.predict(x_test[0:1])
```

and then run the same code gives expected result

```python
model.predict(x_test[0:1, :10, :])
```

**Describe the expected behavior**

```python
model.predict(x_test[0:1, :10, :])
```
should run fine the first time without running `model.predict(x_test[0:1])` first.

**Briefly describe your candidate solution(if contributing):**

Something (a shape checking) is probably wrong when building the predict function for the model? thats why after the first predict() ran without error, the code will work fine."
53435,Memory leak in saving and loading a keras model containing CategoricalEncoding and Lookup layers,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.9.2009 (Core)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: not applicable
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.6.0 (v2.6.0-rc2-32-g919f693420e), 2.7.0 (v2.7.0-rc1-69-gc256c071bb2)
-   **Python version**: 3.7.11, 3.8.12, 3.9.6
-   **Bazel version (if compiling from source)**: not applicable
-   **GCC/Compiler version (if compiling from source)**: not applicable
-   **CUDA/cuDNN version**: no GPU available
-   **GPU model and memory**: no GPU available
-   **Exact command to reproduce**: See code below

### Describe the problem

To solve a binary classification problem, I have a keras model that processes categorical input (as as well as numeric input).  
I need to save (`model.save`) and load (`tf.keras.models.load_model`) the model multiple times (performig training of the model inbetween).  

I expect that the model consumes constant disk space and constant RAM everytime I load the model since the architecture does not change (only the parameter values change).
This does not happen when the model contains an `IntegerLookup` layer followed by a `CategoryEncoding` layer. 

The issue can be reproduced without training the model at all.  
Here is a minimal code example that creates a model and saves it to disk:

    import tensorflow as tf
    import numpy as np

    input_layer = tf.keras.Input(shape=(1,), dtype=""int32"") 
    index = tf.keras.layers.IntegerLookup(max_values=2)
    index.adapt(np.array(range(2)))
    encoder = tf.keras.layers.CategoryEncoding(max_tokens=index.vocab_size())
    encoded_layer = encoder(index(input_layer))
    output_layer = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)(encoded_layer)
    model = tf.keras.Model(input_layer, output_layer)
    model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"")
    model.save(""model"")
    
Every time I execute

    model = tf.keras.models.load_model(""model"")
    model.save(""model"")

the space that the model consumes on disk increases by approx. 8 kB.
The even worse: When I load the model, the RAM useage increases by approx. 9 MB in each iteration.
So after 100 iterations, the model needs approx. 1 MB on disk and 950 MB RAM (which *is* problematic).

This also happens if I start a new python process in each iteration.

In my application, the memory consumption grows even faster because the model has several input layers and also several inner layers.
This makes the model unusable after some iterations because I cannot load it anymore.

Additionaly, of course, the load and save cycles are getting slower with each repetition.

So far, I could reproduce this issue on tensorflow versions 2.6 and 2.7 running on python 3.7, 3.8 or 3.9. The behavior is identical."
53434,Documentation for unit testing is not clear,"## URL(s) with the issue:

Official TF tests documentation page: https://www.tensorflow.org/community/contribute/tests

## Description of issue (what needs changing):

It is not at all clear how one should run unit tests. In fact, some unit tests are _very_ hard to execute singularly.

For example, I have been working on a PR that requires some testing on `//tensorflow/core/lib/io` libraries, and I have spent half an hour understanding how to run the unit test [`zlib_buffers_test.cc`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/core/lib/io/BUILD#L382).

This is how the exploration went:

`zlib_buffers_test.cc` is included in the `filegroup` with name `legacy_lib_io_all_tests`. This in turn is included in [`//tensorflow/core:low_level_library_tests`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/core/BUILD#L1856).

This set of tests however is defined using `tf_cc_tests` (mind the final ""s"" here), which behavior is declared in [`//tensorflow/tensorflow.bzl`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/tensorflow.bzl#L1520), which basically allows me to run the tests under `legacy_lib_io_all_tests` like this:

```
bazel test //tensorflow/core:__tensorflow_core_lib_io_legacy_lib_io_all_tests
```

which is, at least for me, not really intuitive. And I still haven't understood how to effectively just test the `zlib_buffers_test.cc` unit test, if not by adding the flag `--test_filter=<filter>`.

I couldn't find any resource online, nor on any issue here, that mentions the behavior of unit tests or a guideline on how they should be structured and tested for using `bazel`.

I found, however, [a guy with my same issue](https://www.cxybb.com/article/weixin_44351244/116571161), so at least I know I am not alone 😅

### Clear description

> For example, why should someone use this method? How is it useful?

Anytime someone wants to make improvements on a unit test, it should be relatively easy to execute it, and relevant documentation about how that process work can really speed up development and contributions to existing parts of the project.

### Submit a pull request?

I am very happy to help! If there is some existing documentation about the unit testing process, I would happily refer to that one. If it doesn't exist, but the testing process is ""known"" somehow, I would gladly add a piece of documentation to the existing page mentioning how to generally run unit tests."
53432,Tf Keras Dense layer output nan with kernel_initializer='random_normal' before fitting,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2
- GPU model and memory:

**Describe the current behavior**
Before fitting, I used model.predict with my train set, It was ensure that train set was free of nan values. Next, I printed output of each layer.
 The output is:
```
0 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 ...
 [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00
   0.000000e+00  1.000000e+00]]
1 [[ 1.000000e-03 -1.310000e-01 -9.504200e-02 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 0.000000e+00 -1.000000e-02  1.556190e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 1.000000e+00  5.763000e+00  3.027034e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 ...
 [ 8.930000e-01 -5.760000e-01  4.266911e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 1.000000e+00  5.978000e+00  4.650479e+00 ...  0.000000e+00
   0.000000e+00  1.000000e+00]
 [ 8.500000e-01  8.910000e-01  7.704330e-01 ...  0.000000e+00
   0.000000e+00  1.000000e+00]]
2 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 ...
 [nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]]
```

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
inputs = Input(shape=data.shape)
x = Flatten()(inputs)
x = Dense(data.classnum, kernel_initializer='random_normal')(x)
x = tf.keras.layers.Softmax()(x)
model = tf.keras.Model(inputs=inputs, outputs=x)

for idx in range(len(model.layers)):
        # print(model.layers[idx].name,model.layers[idx].get_weights())
        intermediate_layer_model = keras.Model(inputs=model.input,
                                    outputs=model.layers[idx].output)
        intermediate_output = intermediate_layer_model.predict(data.train_dataset)
        print(idx,intermediate_output)
```

"
53431,Could not load library cudnn_cnn_infer64_8.dll. Error code 126,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  binary
- TensorFlow version: 2.7.0
- Python version: 3.9.8
- Installed using virtualenv? pip? conda?:  pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):  vs2022
- CUDA/cuDNN version:  cuda11.5/cudnn8.3
- GPU model and memory:  RTX3060TI 8G



**Describe the problem**
I try to run the model but get the such error:
![image](https://user-images.githubusercontent.com/35481439/146149453-0cb4ba2a-bc6c-42d7-8003-684f3fb9d98c.png)

but i am sure i have installed the cuda and cudnn successfully. Here is the result of testing cuda.
**
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.5\extras\demo_suite>deviceQuery.exe
deviceQuery.exe Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""NVIDIA GeForce RTX 3060 Ti""
  CUDA Driver Version / Runtime Version          11.5 / 11.5
  CUDA Capability Major/Minor version number:    8.6
  Total amount of global memory:                 8192 MBytes (8589410304 bytes)
  (38) Multiprocessors, (128) CUDA Cores/MP:     4864 CUDA Cores
  GPU Max Clock rate:                            1665 MHz (1.66 GHz)
  Memory Clock rate:                             7001 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 3145728 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               zu bytes
  Total amount of shared memory per block:       zu bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1536
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          zu bytes
  Texture alignment:                             zu bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.5, CUDA Runtime Version = 11.5, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3060 Ti
Result = PASS
**

And when i search for the version of tensorflow with cuda and cudnn, i found there is not any information for tensorflow-gpu-2.7.0.
![image](https://user-images.githubusercontent.com/35481439/146150973-c7644279-a912-4884-9b68-2e42c93b7524.png)

Any suggestions? thanks!


**Provide the exact sequence of commands / steps that you executed before running into the problem**



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
53430,"Segmentation fault and no log information available, add some tips about segmentation fault","**System information**
- TensorFlow version (you are using): tensorflow 1.12.0, python 3.6.13
- Are you willing to contribute it (Yes/No): Yes. 



**Describe the feature and the current behavior/state.**
I have trouble retraining my colleague's code from a year ago. I notice that when I try to load my model in the former code base on *libtensorflow-cpu-linux-x86_64-1.12.0* it reports a `Segmentation fault`, without any further information. Upon inspecting my code, I notice that it stops when I load the model, function `TF_SessionRun`. I'm not sure what to do to fix it. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. We fixed it after changing the placeholder name.

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
Those who are not familiar with tensorflow v1's place holder.
**Any Other info.**
No."
53427,Is it possible to report warning when plugin device cannot load instead of error?,"Hi,
I found TF would crash when load plugin device fail, Is it possible to change it with warning instead of crash and do not enable plugin device? Just like the GPU device behavior.  
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py#L150"
53426,M1 Pro After installing TF based on Official apple guide gives ERROR for even importing TF,"SYSTEM:
- MacBook Pro 14 (M1 Apple Silicon)
- MacOS 12.0.1

DONE:
- https://developer.apple.com/metal/tensorflow-plugin/
- I have followed this for ARM M1 apple silicon for my 14""
- (I had anaconda installed before, that may causing the error but I DO NOT want to delete my anaconda all together)

CODE
```
import tensorflow as tf
```

ERROR
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~/miniforge3/lib/python3.9/site-packages/numpy/core/__init__.py in <module>
     21 try:
---> 22     from . import multiarray
     23 except ImportError as exc:

~/miniforge3/lib/python3.9/site-packages/numpy/core/multiarray.py in <module>
     11 
---> 12 from . import overrides
     13 from . import _multiarray_umath

~/miniforge3/lib/python3.9/site-packages/numpy/core/overrides.py in <module>
      6 
----> 7 from numpy.core._multiarray_umath import (
      8     add_docstring, implement_array_function, _get_implementing_args)

ImportError: dlopen(/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib
  Referenced from: /Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so
  Reason: tried: '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
/var/folders/yp/mq9ddgh54gjg2rp7mw_t015c0000gn/T/ipykernel_95218/3793406994.py in <module>
----> 1 import tensorflow as tf

~/miniforge3/lib/python3.9/site-packages/tensorflow/__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py in <module>
     39 
     40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 41 from tensorflow.python.eager import context
     42 
     43 # pylint: enable=wildcard-import

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/context.py in <module>
     28 
     29 from absl import logging
---> 30 import numpy as np
     31 import six
     32 

~/miniforge3/lib/python3.9/site-packages/numpy/__init__.py in <module>
    138     from . import _distributor_init
    139 
--> 140     from . import core
    141     from .core import *
    142     from . import compat

~/miniforge3/lib/python3.9/site-packages/numpy/core/__init__.py in <module>
     46 """""" % (sys.version_info[0], sys.version_info[1], sys.executable,
     47         __version__, exc)
---> 48     raise ImportError(msg)
     49 finally:
     50     for envkey in env_added:

ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.9 from ""/Users/ps/miniforge3/bin/python""
  * The NumPy version is: ""1.19.5""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: dlopen(/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libcblas.3.dylib
  Referenced from: /Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so
  Reason: tried: '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/python3.9/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/lib/libcblas.3.dylib' (no such file), '/Users/ps/miniforge3/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)
```"
53424,Enabling tf.debugging.set_log_device_placement(True) does not print output,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): `pip install tensorflow` / wheel
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.2
- GPU model and memory: NVIDIA TU102 [GeForce RTX 2080 Ti Rev. A]

**Describe the current behavior**

I expect to see physical device placement output when running the examples in the [GPU documentation](https://tensorflow.google.cn/guide/gpu?hl=en).

**Describe the expected behavior**

I do not see any additional output when enabling the logging for device placement.

- Do you want to contribute a PR? (yes/no): unsure
- Briefly describe your candidate solution(if contributing): n/a

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
tf.debugging.set_log_device_placement(True)
print(tf.__version__)
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

# Place tensors on the CPU
with tf.device('/CPU:0'):
  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
  c = tf.matmul(a, b)
print(c)

# Place tensors on the GPU
with tf.device('/GPU:0'):
  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
  c = tf.matmul(a, b)
print(c)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Running the above gives me the following output

```
2.7.0
Num GPUs Available:  1
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
```

cudnn is 8.2.

```
$ cat /usr/include/cudnn_version.h  | grep -e '\(PATCHLEVEL\|MAJOR\|MINOR\)'
#define CUDNN_MAJOR 8
#define CUDNN_MINOR 2
#define CUDNN_PATCHLEVEL 0
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
```

and `nvcc`

```
$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
```"
53423,Hexagon delegate fails on a known good model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.7
- Python version: 3.7
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When running inference with TFlite and Hexagon delegate,
initialization fails with the following message:

09-19 08:24:18.566  4958  4958 I tflite  : Created TensorFlow Lite XNNPACK delegate for CPU.
09-19 08:24:18.568  4958  4958 E tflite  : Calling prepare multiple times
09-19 08:24:18.568  4958  4958 E tflite  : Node number 90 (TfLiteHexagonDelegate) failed to prepare.
09-19 08:24:18.775  4958  4958 E tflite  : Restored original execution plan after delegate application failure.
09-19 08:24:18.782  4958  4958 E tflite  : Error in applying the default TensorFlow Lite delegate indexed at 0, and all previously applied delegates are reverted.

**Describe the expected behavior**

When using the same model with the same code compiled against TensorFlow 2.3, it works fine.
See attached the model in question.
[model.zip](https://github.com/tensorflow/tensorflow/files/7713641/model.zip)"
53417,TFLite GPU failed to delegate with ssd_mobilenet_v2_fpnlite_320 model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  2.6.0
- Python version: 3.9.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: GeForce GTX 1050Ti 4G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I downloaded SSD MobileNet V2 FPNLite 320x320 pretrained model from [TF Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) , then used export_tflite_graph_tf2.py script to convert checkpoints to saved model.

```
python export_tflite_graph_tf2.py \
    --pipeline_config_path ssd_mobilenet_v2_fpnlite_320/pipeline.config \
    --trained_checkpoint_dir ssd_mobilenet_v2_fpnlite_320/checkpoint \
    --output_directory ssd_mobilenet_v2_fpnlite_320 \
    --config_override "" \
            model{ \
            ssd{ \
              post_processing { \
                batch_non_max_suppression { \
                        score_threshold: 0.0 \
                        iou_threshold: 0.5 \
                } \
             } \
          } \
       } \
       ""
```

Then convert saved model to tflite
```
converter = tf.lite.TFLiteConverter.from_saved_model(ssd_mobilenet_v2_fpnlite_320/saved_model)
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.target_spec.supported_types = [tf.float16]
converter.experimental_new_converter = True
tflite_model = converter.convert()
open(output_name, ""wb"").write(tflite_model)
```

However, when use NNAPI to delegate this model, it's failed with error :
```
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
DEQUANTIZE: 
327 operations will run on the GPU, and the remaining 2 operations will run on the CPU.
ERROR: TfLiteGpuDelegate Init: PACK: Tensor ""ssd_mobile_net_v2fpn_keras_feature_extractor/FeatureMaps/top_down/nearest_neighbor_upsampling/nearest_neighbor_upsampling/w_stack"" has bad input dims size: 5.
INFO: Created 0 GPU delegate kernels.
ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized
ERROR: Node number 329 (TfLiteGpuDelegateV2) failed to prepare.
```

**Describe the expected behavior**
TFLite NNAPI could delegate ssd_mobilenet_v2_fpnlite_320 model.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53413,TF_SessionRun() from C API crashes when not enough RAM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Microsoft Windows 10 Enterprise, version: 10.0.18363 N/A Build 18363
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.
- TensorFlow installed from (source or binary): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.4.0.zip
- TensorFlow version (use command below): bug is tested and reproducible with tensorflow C API of versions: 2.4.0, 2.7.0
- Python version: 3.8
- CUDA/cuDNN version: used CPU
- GPU model and memory: used CPU

**Describe the current behavior**
I'm using tensorflow C API in my C++ code. I'm using microsoft visual studio 2019 (compiler version - msvc 14.26).
When there's not enough memory, TF_SessionRun() crashes. Visual Studio debugger shows that TF_SessionRun() throws std::bad_alloc.  If I debug code and place break immediately before executing TF_SessionRun() and immediately after it, after executing TF_SessionRun()  I get the following exception message in Debugger
""Exception thrown at 0x00007FFB9F7EA859 in MyProject.exe: Microsoft C++ exception: std::bad_alloc at memory location 0x000000F789CFD580""
And here's the callstack from where the exception is thrown.
  	[External Code]	
>	vcruntime140.dll!00007ffb85ba6480()	Unknown
 	tensorflow-2.4.0.dll!00007ffb108e335f()	Unknown
 	tensorflow-2.4.0.dll!00007ffb10863eaf()	Unknown
 	tensorflow-2.4.0.dll!00007ffb108641b3()	Unknown
 	tensorflow-2.4.0.dll!00007ffb1086c119()	Unknown
 	tensorflow-2.4.0.dll!00007ffb10866624()	Unknown
 	tensorflow-2.4.0.dll!00007ffb1432f562()	Unknown
 	tensorflow-2.4.0.dll!00007ffb1433d9fa()	Unknown
 	tensorflow-2.4.0.dll!00007ffb1433c248()	Unknown
 	tensorflow-2.4.0.dll!00007ffb14343589()	Unknown
 	tensorflow-2.4.0.dll!00007ffb16b2860b()	Unknown
 	tensorflow-2.4.0.dll!00007ffb16b26531()	Unknown
 	tensorflow-2.4.0.dll!00007ffb17117799()	Unknown
 	tensorflow-2.4.0.dll!00007ffb17117c31()	Unknown
 	tensorflow-2.4.0.dll!00007ffb17111128()	Unknown
 	ucrtbase.dll!thread_start<unsigned int (__cdecl*)(void *),1>()	Unknown
 	kernel32.dll!BaseThreadInitThunk()	Unknown
 	ntdll.dll!RtlUserThreadStart()	Unknown

And the worst thing is that I cannot wrap this call to TF_SessionRun() in try-catch, because the compiler optimizes it away, since we are calling C code, and C code isn't allowed to throw exceptions. But even if could be caught, it is technically undefined behavior, and I cannot rely on catching exceptions from tensorflow.dll, because 1) it was built with a different compiler 2) it is C API, it should not throw anything.

I attached files.zip, which contains main.cpp file that reproduces the problem. It reads neural network protobuf (.pb) file and tries to run it. Script, that generated neural network, and the neural network itself are also inside files.zip

On line 11 of main.cpp there is variable SIZE (const size_t SIZE = 500;) On my machine TF_SessionRun() crashes when SIZE is around 500. It can vary from machine to machine, try different values to reproduce the problem on your machine.

So, the problem is that if SIZE is sufficiently big, a call to TF_SessionRun() on line 63 crashes, and we do not reach the code below TF_SessionRun() that would print either ""FINISHED SUCCESSFULLY"" or ""FINISHED WITH ERROR"".
(But if SIZE is sufficiently small, it successfully finishes and prints ""FINISHED SUCCESSFULLY"")

**Describe the expected behavior**
If there's not enough memory, TF_SessionRun() should return error code via TF_Status*, and it should not crash. 
If we reached call to TF_SessionRun() in main.cpp, then after the call it should print either ""FINISHED SUCCESSFULLY"" or ""FINISHED WITH ERROR""


**Standalone code to reproduce the issue**
I attached an archive - files.zip. It contains 3 files:
main.cpp - C++ code that reproduces the problem
frozen_graph.pb - neural network protobuf file, that is used inside main.cpp
generate_graph.py - script, that generates frozen_graph.pb

I think you can reproduce this bug (where TF_SessionRun() crashes because there isn't enough RAM) with any sufficiently complex neural network, that consumes lots of RAM. I actually encountered this bug with a different neural network, I cannot share it with you, because it is not my Intellectual Property.

[files.zip](https://github.com/tensorflow/tensorflow/files/7707904/files.zip)

"
53411,[r2.6][PR] Cannot find real source of dependency `No such file or directory`  error when building from scratch,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.6
- Python version: 3.9.5
- Installed using virtualenv? pip? conda?: `miniconda`
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0

**Describe the problem**

I am currently working on a relatively large PR to [introduce ZSTD support in TF](https://github.com/tensorflow/tensorflow/pull/53385) for `TFRecordWriter`, which currently supports `ZLIB` and `GZIP`.

To introduce my changes, I am of course writing some tests, and as I am working my way up to the dependencies chain, I have stumbled upon this error:

```
(tensorflow) ubuntu@tensorflow-compression-build-1:~/Workspace/tensorflow$ bazel test //tensorflow/core/lib/io/zstd:zstd_test --test_filter=* --verbose_failures
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=143
INFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/miniconda3/envs/tensorflow/bin/python --action_env PYTHON_LIB_PATH=/home/ubuntu/miniconda3/envs/tensorflow/lib/python3.9/site-packages --python_path=/home/ubuntu/miniconda3/envs/tensorflow/bin/python
INFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:linux in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ef409778dc6f6079679ade72bc957ee0/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Analyzed target //tensorflow/core/lib/io/zstd:zstd_test (1 packages loaded, 70 targets configured).
INFO: Found 1 test target...
ERROR: /home/ubuntu/Workspace/tensorflow/tensorflow/core/BUILD:1610:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 1): gcc failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ef409778dc6f6079679ade72bc957ee0/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/ubuntu/.vscode-server/bin/7db1a2b88f7557e0a43fec75b6ba7e50b3e9f77e/bin:/home/ubuntu/miniconda3/envs/tensorflow/bin:/home/ubuntu/miniconda3/condabin:/home/ubuntu/.vscode-server/bin/7db1a2b88f7557e0a43fec75b6ba7e50b3e9f77e/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/ubuntu/miniconda3/envs/tensorflow/bin/python \
    PYTHON_LIB_PATH=/home/ubuntu/miniconda3/envs/tensorflow/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.o' -fPIC -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/snappy -iquotebazel-out/k8-opt/bin/external/snappy -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-DTENSORFLOW_USE_XLA=1' '-DINTEL_MKL=1' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/util/events_writer.cc -o bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from ./tensorflow/core/lib/io/record_writer.h:29:0,
                 from ./tensorflow/core/util/events_writer.h:23,
                 from tensorflow/core/util/events_writer.cc:16:
./tensorflow/core/lib/io/zstd/zstd_outputbuffer.h:19:10: fatal error: zstd.h: No such file or directory
 #include <zstd.h>
          ^~~~~~~~
compilation terminated.
Target //tensorflow/core/lib/io/zstd:zstd_test failed to build
INFO: Elapsed time: 0.926s, Critical Path: 0.28s
INFO: 5 processes: 5 internal.
FAILED: Build did NOT complete successfully
//tensorflow/core/lib/io/zstd:zstd_test                         FAILED TO BUILD

FAILED: Build did NOT complete successfully
```

And it did happen before, so I just went into the `BUILD` file that the error references, and look if there is somewhere I might have missed a dependency somehow. I have yet to find the real source of the dependency, and I have gone through every possible place where I should have put my dependencies.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Clone my fork and go to my branch, with commit sha `9d7b6a5e223c59d8d9687ce128dd1ebc5a6ab908` or `build-issue-adrian-compression-r2.6`:

```
git clone https://github.com/IAL32/tensorflow
git checkout 9d7b6a5e223c59d8d9687ce128dd1ebc5a6ab908
```

Build from source however you like, and execute the test:

```
bazel test //tensorflow/core/lib/io/zstd:zstd_test
```

You should now see the error above.

Finally, if you revert the last commit, which changes `tensorflow/core/lib/io/record_writer.cc` and `tensorflow/core/lib/io/record_writer.h` to accommodate my `ZSTD` class, launching the test again works fine.

An overview of the stuff I have changed can be found here: https://github.com/tensorflow/tensorflow/compare/r2.6...IAL32:build-issue-adrian-compression-r2.6?expand=1

I believe this can be solved by including some header as a dependency, but I have been struggling to understand **where** exactly.

Thanks for any help!"
53410,tf2 stops working under gpu mode with 3rd order derivative included in loss function,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 20H2, OS build 19042.1348
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): anaconda pip install
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: Python 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuda10.0/cudnn11.5
- GPU model and memory: 2080TI & 1080TI

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
In the case included here, the manually defined loss function includes 3rd order derivative. Attached please find the zip pack that includes the python scripts: NS_tf2.py. 
In NS_tf2.py this term is in the form of 
<img src=""https://latex.codecogs.com/svg.latex?\frac{\partial^{3}&space;(pred)}{\partial&space;(var1)\partial&space;(var2)\partial&space;(var2)}"" title=""\frac{\partial^{3} (pred)}{\partial (var1)\partial (var2)\partial (var2)}"" />
where pred represents the output of sequential network and var1 and var2 are tensors. 

At the beginning of the python script 
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
is used to choose between cpu and gpus. 
The above code work well under cpu mode:
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
but get stalled when setting hardware to either 1080ti or 2080ti.

**Describe the expected behavior**
Code works under gpu mode

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Please find the code in attached pack. Please first cd to scripts' directory to run the scripts.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Please find the code in attached pack. Please first cd to scripts' directory to run the scripts.

[tf_bug.zip](https://github.com/tensorflow/tensorflow/files/7707751/tf_bug.zip)

"
53408,Tuner tutorial rebuilds model rather than saving it,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/keras/keras_tuner

## Description of issue (what needs changing):
The bottom of this tutorial says `Re-instantiate the hypermodel and train it with the optimal number of epochs from above.` But there's no reason to train the model twice when you could use the callback to save the model at the best epoch. I and others have speculated [here](https://twitter.com/HamelHusain/status/1470492047404580865) that the call to re-train the model is supposed to be on the full data (with no validation split). If that's the intention, the resulting change would be to remove `validation_split=0.2` from the final `fit` call.

Otherwise, it seems you should add a `ModelCheckpoint` callback to the save the best model when you first fit the model... and then not refit the model."
53407,Bazel Tensorflow installation with TensorRT from source throws error although there is output file,"I want to build Tensorflow with TensorRT support with Bazel from source.

System specifications:

Tensorflow 2.7
Python 3.8.0
CUDA 11.5
Ubuntu 18.04
Geforce RTX 2060
I tried the following bazel versions: 3.7.2, 4.2.1, 4.2.2 and all threw the same error.

It starts the building process but then throws the following 2 errors regarding the BUILD file:

```
ERROR: /home/ros/.cache/bazel/_bazel_ros/f58d1d645696850c769c1c78103710da/external/local_config_python/BUILD:78:8: declared output 'external/local_config_python/*.*' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /home/ros/.cache/bazel/_bazel_ros/f58d1d645696850c769c1c78103710da/external/local_config_python/BUILD:78:8: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
Here is the BUILD file from which the error occurs

```
licenses([""restricted""])

package(default_visibility = [""//visibility:public""])

load(""@bazel_tools//tools/python:toolchain.bzl"", ""py_runtime_pair"")

py_runtime(
    name = ""py2_runtime"",
    interpreter_path = ""/home/ros/tfenv/bin/python3"",
    python_version = ""PY2"",
)

py_runtime(
    name = ""py3_runtime"",
    interpreter_path = ""/home/ros/tfenv/bin/python3"",
    python_version = ""PY3"",
)

py_runtime_pair(
    name = ""py_runtime_pair"",
    py2_runtime = "":py2_runtime"",
    py3_runtime = "":py3_runtime"",
)

toolchain(
    name = ""py_toolchain"",
    toolchain = "":py_runtime_pair"",
    toolchain_type = ""@bazel_tools//tools/python:toolchain_type"",
    target_compatible_with = [],
    exec_compatible_with = [],
)

cc_import(
    name = ""python_lib"",
    interface_library = select({
        "":amd64"": "":python_import_lib"",
        # A placeholder for Unix platforms which makes --no_build happy.
        ""//conditions:default"": ""not-existing.lib"",
    }),
    system_provided = 1,
)

cc_library(
    name = ""python_headers"",
    hdrs = ["":python_include""],
    deps = select({
        "":amd64"": ["":python_lib""],
        ""//conditions:default"": [],
    }),
    includes = [""python_include""],
)


alias(
    name = ""headers"",
    actual = "":python_headers"",
)

cc_library(
    name = ""numpy_headers"",
    hdrs = ["":numpy_include""],
    includes = [""numpy_include""],
)

config_setting(
    name = ""amd64"",
    values = {""cpu"": ""amd64""},
    visibility = [""//visibility:public""],
)

genrule(
    name = ""python_include"",
    outs = [
        ""*.*""
    ],
    cmd = """"""

   """""",
)

genrule(
    name = ""numpy_include"",
    outs = [
        ""numpy_include/numpy/__multiarray_api.h"",
        ""numpy_include/numpy/__ufunc_api.h"",
        ""numpy_include/numpy/_neighborhood_iterator_imp.h"",
        ""numpy_include/numpy/_numpyconfig.h"",
        ""numpy_include/numpy/arrayobject.h"",
        ""numpy_include/numpy/arrayscalars.h"",
        ""numpy_include/numpy/halffloat.h"",
        ""numpy_include/numpy/libdivide/LICENSE.txt"",
        ""numpy_include/numpy/libdivide/libdivide.h"",
        ""numpy_include/numpy/multiarray_api.txt"",
        ""numpy_include/numpy/ndarrayobject.h"",
        ""numpy_include/numpy/ndarraytypes.h"",
        ""numpy_include/numpy/noprefix.h"",
        ""numpy_include/numpy/npy_1_7_deprecated_api.h"",
        ""numpy_include/numpy/npy_3kcompat.h"",
        ""numpy_include/numpy/npy_common.h"",
        ""numpy_include/numpy/npy_cpu.h"",
        ""numpy_include/numpy/npy_endian.h"",
        ""numpy_include/numpy/npy_interrupt.h"",
        ""numpy_include/numpy/npy_math.h"",
        ""numpy_include/numpy/npy_no_deprecated_api.h"",
        ""numpy_include/numpy/npy_os.h"",
        ""numpy_include/numpy/numpyconfig.h"",
        ""numpy_include/numpy/old_defines.h"",
        ""numpy_include/numpy/oldnumeric.h"",
        ""numpy_include/numpy/random/bitgen.h"",
        ""numpy_include/numpy/random/distributions.h"",
        ""numpy_include/numpy/ufunc_api.txt"",
        ""numpy_include/numpy/ufuncobject.h"",
        ""numpy_include/numpy/utils.h"",
    ],
    cmd = """"""
cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/__multiarray_api.h"" ""$(@D)/numpy_include/numpy/__multiarray_api.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/__ufunc_api.h"" ""$(@D)/numpy_include/numpy/__ufunc_api.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h"" ""$(@D)/numpy_include/numpy/_neighborhood_iterator_imp.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/_numpyconfig.h"" ""$(@D)/numpy_include/numpy/_numpyconfig.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h"" ""$(@D)/numpy_include/numpy/arrayobject.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h"" ""$(@D)/numpy_include/numpy/arrayscalars.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/halffloat.h"" ""$(@D)/numpy_include/numpy/halffloat.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt"" ""$(@D)/numpy_include/numpy/libdivide/LICENSE.txt"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/libdivide/libdivide.h"" ""$(@D)/numpy_include/numpy/libdivide/libdivide.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/multiarray_api.txt"" ""$(@D)/numpy_include/numpy/multiarray_api.txt"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h"" ""$(@D)/numpy_include/numpy/ndarrayobject.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h"" ""$(@D)/numpy_include/numpy/ndarraytypes.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/noprefix.h"" ""$(@D)/numpy_include/numpy/noprefix.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h"" ""$(@D)/numpy_include/numpy/npy_1_7_deprecated_api.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_3kcompat.h"" ""$(@D)/numpy_include/numpy/npy_3kcompat.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_common.h"" ""$(@D)/numpy_include/numpy/npy_common.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_cpu.h"" ""$(@D)/numpy_include/numpy/npy_cpu.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_endian.h"" ""$(@D)/numpy_include/numpy/npy_endian.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_interrupt.h"" ""$(@D)/numpy_include/numpy/npy_interrupt.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_math.h"" ""$(@D)/numpy_include/numpy/npy_math.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h"" ""$(@D)/numpy_include/numpy/npy_no_deprecated_api.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_os.h"" ""$(@D)/numpy_include/numpy/npy_os.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/numpyconfig.h"" ""$(@D)/numpy_include/numpy/numpyconfig.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/old_defines.h"" ""$(@D)/numpy_include/numpy/old_defines.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/oldnumeric.h"" ""$(@D)/numpy_include/numpy/oldnumeric.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/random/bitgen.h"" ""$(@D)/numpy_include/numpy/random/bitgen.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/random/distributions.h"" ""$(@D)/numpy_include/numpy/random/distributions.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ufunc_api.txt"" ""$(@D)/numpy_include/numpy/ufunc_api.txt"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"" ""$(@D)/numpy_include/numpy/ufuncobject.h"" && cp -f ""/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/utils.h"" ""$(@D)/numpy_include/numpy/utils.h""
   """""",
)
```"
53406,"throw exception :TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'. when tf.keras.backend.set_floatx(""float64"")","**my code**
``` python
import tensorflow as tf
tf.keras.backend.set_floatx(""float64"")
.....
model.fit(.....)
```
the tf version is `v2.6.1-9-gc2363d6d025 2.6.2`
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `model.fit`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04`
- TensorFlow installed from (source or binary): `docker`
- TensorFlow version (use command below): `2.6.2`
- Python version: 3.6.9

**throw exception:**
```
/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/parameter_server_strategy_v2.py:863 _call_for_each_replica
        args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica
        return _call_for_each_replica(strategy, fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:246 _call_for_each_replica
        coord.join(threads)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /usr/local/lib/python3.6/dist-packages/six.py:703 reraise
        raise value
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:346 run
        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:792 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.6/dist-packages/keras/engine/compile_utils.py:457 update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:73 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/keras/metrics.py:177 update_state_fn
        return ag_update_state(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/keras/metrics.py:2292 update_state  **
        label_weights=label_weights)
    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:636 update_confusion_matrix_variables
        thresholds_with_epsilon=thresholds_with_epsilon)
    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:391 _update_confusion_matrix_variables_optimized
        true_labels = tf.multiply(y_true, weights)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:530 multiply
        return gen_math_ops.mul(x, y, name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6246 mul
        ""Mul"", x=x, y=y, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper
        inferred_from[input_arg.type_attr]))

    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.
```
**Describe the expected behavior**

- Do you want to contribute a PR? (yes/no): `yes`
- Briefly describe your candidate solution(if contributing):

https://github.com/tensorflow/tensorflow/blob/c2363d6d025981c661f8cbecf4c73ca7fbf38caf/tensorflow/python/keras/utils/metrics_utils.py#L379-L413

cast the `weights` to y_true.dtype。
``` python
weights = tf.cast(weights, dtype=y_true.dtype)
true_labels = math_ops.multiply(y_true, weights)
false_labels = math_ops.multiply((1.0 - y_true), weights)
```"
53405,throw exception :TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53404,ERROR: Node number 27 (AVERAGE_POOL_2D) failed to invoke,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Android10
- TensorFlow installed from (source or binary):source 
- TensorFlow version (or github SHA if from source):github tensorflow-r.2.3.1

All the way,
the sample label_image and the sample model  test on Ubuntu18.04 is OK ,and On Android use NNAPI is OK .
but **use tflite to  test on Android10 is wrong**  .

**Provide the text output from tflite_convert**
**when Label_image run mobilenet_v1_1.0_224_quant.tflite on Android ,AVERAGE_POOL_2D failed to invoke**
**label_image**: tensorflow\lite\examples\label_image\label_image.cc
**mobilenet_v1_1.0_224_quant**.tflite:http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz
```
# Copy and paste here
ud710_2h10:/data/local/tmp/android-test # **./label_image_1210   -m model/tf_mobilenet_v1_1.0_224_quant.tflite    -i data/grace_hopper.bmp -l label/labels.txt -a 0 -d 0 -f 0 -g 0  -b 0 -s 1  -p 1  -v 1**
Loaded model model/tf_mobilenet_v1_1.0_224_quant.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
load tflite mode
average time: 1.559 ms
tensors size: 89
nodes size: 31
inputs: 1
input(0) name: input
0: MobilenetV1/Logits/AvgPool_1a/AvgPool, 1024, 3, 0.0235285, 0
1: MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd, 1001, 3, 0.166099, 66
2: MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias, 4004, 2, 0.000117327, 0
3: MobilenetV1/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars, 1025024, 3, 0.0049866, 74
4: MobilenetV1/Logits/SpatialSqueeze, 1001, 3, 0.166099, 66
5: MobilenetV1/Logits/SpatialSqueeze_shape, 8, 2, 0, 0
6: MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias, 128, 2, 0.000170521, 0
7: MobilenetV1/MobilenetV1/Conv2d_0/Relu6, 401408, 3, 0.0235285, 0
8: MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0218267, 151
9: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6, 100352, 3, 0.0235285, 0
10: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_Fold_bias, 2048, 2, 0.000572435, 0
11: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0243294, 134
12: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000227253, 0
13: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6, 100352, 3, 0.0235285, 0
14: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00965865, 99
15: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6, 100352, 3, 0.0235285, 0
16: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_Fold_bias, 2048, 2, 0.000455672, 0
17: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0193668, 106
18: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000128159, 0
19: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6, 100352, 3, 0.0235285, 0
20: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00544699, 153
21: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6, 25088, 3, 0.0235285, 0
22: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_Fold_bias, 2048, 2, 0.00018436, 0
23: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.00783559, 126
24: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_Fold_bias, 4096, 2, 0.000192445, 0
25: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6, 50176, 3, 0.0235285, 0
26: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/FakeQuantWithMinMaxVars, 524288, 3, 0.00817923, 130
27: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6, 50176, 3, 0.0235285, 0
28: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_Fold_bias, 4096, 2, 0.00296857, 0
29: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/FakeQuantWithMinMaxVars, 9216, 3, 0.126169, 211
30: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold_bias, 4096, 2, 0.000424646, 0
31: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6, 50176, 3, 0.0235285, 0
32: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/FakeQuantWithMinMaxVars, 1048576, 3, 0.0180482, 95
33: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, 401408, 3, 0.0235285, 0
34: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_Fold_bias, 128, 2, 0.006875, 0
35: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/FakeQuantWithMinMaxVars, 288, 3, 0.292199, 110
36: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_Fold_bias, 256, 2, 0.000715759, 0
37: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6, 802816, 3, 0.0235285, 0
38: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/FakeQuantWithMinMaxVars, 2048, 3, 0.0304209, 121
39: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6, 200704, 3, 0.0235285, 0
40: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_Fold_bias, 256, 2, 0.00947663, 0
41: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/FakeQuantWithMinMaxVars, 576, 3, 0.402773, 130
42: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_Fold_bias, 512, 2, 0.000356414, 0
43: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6, 401408, 3, 0.0235285, 0
44: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/FakeQuantWithMinMaxVars, 8192, 3, 0.0151482, 104
45: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6, 401408, 3, 0.0235285, 0
46: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_Fold_bias, 512, 2, 0.00142435, 0
47: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/FakeQuantWithMinMaxVars, 1152, 3, 0.0605373, 160
48: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_Fold_bias, 512, 2, 0.000323645, 0
49: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6, 401408, 3, 0.0235285, 0
50: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/FakeQuantWithMinMaxVars, 16384, 3, 0.0137555, 94
51: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6, 100352, 3, 0.0235285, 0
52: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_Fold_bias, 512, 2, 0.000394292, 0
53: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/FakeQuantWithMinMaxVars, 1152, 3, 0.0167581, 123
54: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_Fold_bias, 1024, 2, 0.00017886, 0
55: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6, 200704, 3, 0.0235285, 0
56: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/FakeQuantWithMinMaxVars, 32768, 3, 0.00760185, 151
57: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6, 200704, 3, 0.0235285, 0
58: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_Fold_bias, 1024, 2, 0.000965968, 0
59: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0410553, 129
60: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_Fold_bias, 1024, 2, 0.000151326, 0
61: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6, 200704, 3, 0.0235285, 0
62: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/FakeQuantWithMinMaxVars, 65536, 3, 0.00643161, 122
63: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6, 50176, 3, 0.0235285, 0
64: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_Fold_bias, 1024, 2, 0.000316712, 0
65: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0134608, 122
66: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000215785, 0
67: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6, 100352, 3, 0.0235285, 0
68: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/FakeQuantWithMinMaxVars, 131072, 3, 0.00917122, 109
69: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6, 100352, 3, 0.0235285, 0
70: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_Fold_bias, 2048, 2, 0.000869019, 0
71: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0369348, 132
72: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000124702, 0
73: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6, 100352, 3, 0.0235285, 0
74: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00530005, 140
75: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6, 100352, 3, 0.0235285, 0
76: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_Fold_bias, 2048, 2, 0.00100255, 0
77: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0426099, 94
78: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000116779, 0
79: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6, 100352, 3, 0.0235285, 0
80: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00496329, 127
81: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6, 100352, 3, 0.0235285, 0
82: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_Fold_bias, 2048, 2, 0.000667241, 0
83: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0283589, 127
84: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000182837, 0
85: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6, 100352, 3, 0.0235285, 0
86: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.0077709, 89
87: MobilenetV1/Predictions/Reshape_1, 1001, 3, 0.00390625, 0
88: input, 150528, 3, 0.0078125, 128
len: 940650
width, height, channels: 517, 606, 3
input: 88
number of inputs: 1
number of outputs: 1
Interpreter has 90 tensors and 31 nodes
Inputs: 88
Outputs: 87

Tensor   0 MobilenetV1/Logits/AvgPool_1a/AvgPool kTfLiteUInt8  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 1 1 1024
Tensor   1 MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1 1 1001
Tensor   2 MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias kTfLiteInt32   kTfLiteMmapRo       4004 bytes ( 0.0 MB)  1001
Tensor   3 MobilenetV1/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo    1025024 bytes ( 1.0 MB)  1001 1 1 1024
Tensor   4 MobilenetV1/Logits/SpatialSqueeze kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1001
Tensor   5 MobilenetV1/Logits/SpatialSqueeze_shape kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2
Tensor   6 MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        128 bytes ( 0.0 MB)  32
Tensor   7 MobilenetV1/MobilenetV1/Conv2d_0/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 112 112 32
Tensor   8 MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        864 bytes ( 0.0 MB)  32 3 3 3
Tensor   9 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  10 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  11 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  12 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  13 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  14 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512
Tensor  15 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  16 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  17 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  18 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  19 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  20 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512
Tensor  21 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      25088 bytes ( 0.0 MB)  1 7 7 512
Tensor  22 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  23 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  24 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024
Tensor  25 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024
Tensor  26 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 1 1 512
Tensor  27 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024
Tensor  28 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024
Tensor  29 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       9216 bytes ( 0.0 MB)  1 3 3 1024
Tensor  30 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024
Tensor  31 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024
Tensor  32 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo    1048576 bytes ( 1.0 MB)  1024 1 1 1024
Tensor  33 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 112 112 32
Tensor  34 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        128 bytes ( 0.0 MB)  32
Tensor  35 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        288 bytes ( 0.0 MB)  1 3 3 32
Tensor  36 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64
Tensor  37 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     802816 bytes ( 0.8 MB)  1 112 112 64
Tensor  38 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  64 1 1 32
Tensor  39 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 56 56 64
Tensor  40 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64
Tensor  41 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        576 bytes ( 0.0 MB)  1 3 3 64
Tensor  42 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128
Tensor  43 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128
Tensor  44 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       8192 bytes ( 0.0 MB)  128 1 1 64
Tensor  45 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128
Tensor  46 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128
Tensor  47 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       1152 bytes ( 0.0 MB)  1 3 3 128
Tensor  48 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128
Tensor  49 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128
Tensor  50 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  128 1 1 128
Tensor  51 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 28 28 128
Tensor  52 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128
Tensor  53 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       1152 bytes ( 0.0 MB)  1 3 3 128
Tensor  54 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  55 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256
Tensor  56 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      32768 bytes ( 0.0 MB)  256 1 1 128
Tensor  57 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256
Tensor  58 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  59 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2304 bytes ( 0.0 MB)  1 3 3 256
Tensor  60 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  61 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256
Tensor  62 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      65536 bytes ( 0.1 MB)  256 1 1 256
Tensor  63 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 14 14 256
Tensor  64 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  65 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2304 bytes ( 0.0 MB)  1 3 3 256
Tensor  66 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  67 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  68 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  512 1 1 256
Tensor  69 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  70 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  71 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  72 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  73 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  74 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512
Tensor  75 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  76 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  77 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  78 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  79 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  80 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512
Tensor  81 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  82 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  83 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512
Tensor  84 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  85 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512
Tensor  86 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512
Tensor  87 MobilenetV1/Predictions/Reshape_1 kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1001
Tensor  88 input                kTfLiteUInt8  kTfLiteArenaRw     150528 bytes ( 0.1 MB)  1 224 224 3
Tensor  89 (null)               kTfLiteUInt8  kTfLiteArenaRw     338688 bytes ( 0.3 MB)  1 112 112 27

Node   0 Operator Builtin Code   3 CONV_2D
  Inputs: 88 8 6
  Outputs: 7
  Temporaries: 89
Node   1 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 7 35 34
  Outputs: 33
Node   2 Operator Builtin Code   3 CONV_2D
  Inputs: 33 38 36
  Outputs: 37
Node   3 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 37 41 40
  Outputs: 39
Node   4 Operator Builtin Code   3 CONV_2D
  Inputs: 39 44 42
  Outputs: 43
Node   5 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 43 47 46
  Outputs: 45
Node   6 Operator Builtin Code   3 CONV_2D
  Inputs: 45 50 48
  Outputs: 49
Node   7 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 49 53 52
  Outputs: 51
Node   8 Operator Builtin Code   3 CONV_2D
  Inputs: 51 56 54
  Outputs: 55
Node   9 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 55 59 58
  Outputs: 57
Node  10 Operator Builtin Code   3 CONV_2D
  Inputs: 57 62 60
  Outputs: 61
Node  11 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 61 65 64
  Outputs: 63
Node  12 Operator Builtin Code   3 CONV_2D
  Inputs: 63 68 66
  Outputs: 67
Node  13 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 67 71 70
  Outputs: 69
Node  14 Operator Builtin Code   3 CONV_2D
  Inputs: 69 74 72
  Outputs: 73
Node  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 73 77 76
  Outputs: 75
Node  16 Operator Builtin Code   3 CONV_2D
  Inputs: 75 80 78
  Outputs: 79
Node  17 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 79 83 82
  Outputs: 81
Node  18 Operator Builtin Code   3 CONV_2D
  Inputs: 81 86 84
  Outputs: 85
Node  19 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 85 11 10
  Outputs: 9
Node  20 Operator Builtin Code   3 CONV_2D
  Inputs: 9 14 12
  Outputs: 13
Node  21 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 13 17 16
  Outputs: 15
Node  22 Operator Builtin Code   3 CONV_2D
  Inputs: 15 20 18
  Outputs: 19
Node  23 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 19 23 22
  Outputs: 21
Node  24 Operator Builtin Code   3 CONV_2D
  Inputs: 21 26 24
  Outputs: 25
Node  25 Operator Builtin Code   4 DEPTHWISE_CONV_2D
  Inputs: 25 29 28
  Outputs: 27
Node  26 Operator Builtin Code   3 CONV_2D
  Inputs: 27 32 30
  Outputs: 31
Node  27 Operator Builtin Code   1 AVERAGE_POOL_2D
  Inputs: 31
  Outputs: 0
Node  28 Operator Builtin Code   3 CONV_2D
  Inputs: 0 3 2
  Outputs: 1
Node  29 Operator Builtin Code  22 RESHAPE
  Inputs: 1 5
  Outputs: 4
Node  30 Operator Builtin Code  25 SOFTMAX
  Inputs: 4
  Outputs: 87
**ERROR: tensorflow/lite/kernels/pooling.cc:173 optimized_ops::AveragePool(op_params, GetTensorShape(input), GetTensorData<uint8_t>(input), GetTensorShape(output), GetTensorData<uint8_t>(output)) was not true.
ERROR: Node number 27 (AVERAGE_POOL_2D) failed to invoke.**

Failed to invoke tflite!
invoked
average time: 36.978 ms
1: 941 941:spaghetti squash
1: 924 924:plate
1: 886 886:velvet
1: 833 833:stupa, tope
1: 649 649:medicine chest, medicine cabinet
```


**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

"
53403,New dtype: bcomplex32 ,"


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): Probably yes



**Describe the feature and the current behavior/state.**
Add bcomplex32 dtype.
Currently only a real (like in Math, real field) version is supported (bfloat16)

**Will this change the current api? How?**
Each function that supports the dtype paramter, should also support the new bcomplex32
**Who will benefit with this feature?**
Anyone who uses the complex plane and wants a speedup for further calculations on supported hardware such as GPUs with compatibility level of 7 and above, for example while using FFT.
**Any Other info.**
"
53402,When I use   tensorflow_lite(1.13.1) it has report error  it looks ...,"error tip : Model provided has model identifier 'AFRG', should be 'TFL3'

I have model.tflite when I creat     
model = tflite::FlatBufferModel::BuildFromBuffer(newmodel_data, modellength);

How Can I do?


"
53401,AFRG,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
53400,00M report after I upgrade tensorflow for tf2.5 to tf2.7,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): tf2.5 and tf 2.7
- Python version: 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  CUDA11.2.2/cudnn8.1.0.77 for both tf2.5 and tf2.7 
- GPU model and memory: NVIDIA RTX A6000, 43660 MB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I upgrade tensorflow from version 2.5 to 2.7. The code that can run in tf2.5 enviroment cannot run in tf2.7 enviroment since when run in tf2.7, the GPU memory exhaust.
**Describe the expected behavior**
The code can also run in tf2.7 without reporting OOM happens.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
The link of code script is here   [https://colab.research.google.com/drive/1zb_SAqKZPGinzdWWl5CUySwzX5T_CatU?usp=sharing](url). It cannot reporduce the isuue in colab since the code needs about 50G RAM memory.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
2021-12-13 13:04:10.364463: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *************************************************************************************************___
2021-12-13 13:04:10.364557: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at gather_nd_op.cc:47 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[772,753,768,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 220, in <module>
    train(epoch, batch, iternum, restore=0,
  File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 210, in train
    hist = Model.fit(x, y, batch_size=batch, epochs=epoch, validation_split=0.1,
  File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[772,753,768,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node GatherNd_35
 (defined at /media/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/fan_radon_iradon_line.py:8)
]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_train_function_12544]

Errors may have originated from an input operation.
Input Source operations connected to node GatherNd_35:
In[0] mul_129:
In[1] GatherNd_5/indices:

Operation defined at: (most recent call last)
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 220, in <module>
>>>     train(epoch, batch, iternum, restore=0,
>>> 
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 210, in train
>>>     hist = Model.fit(x, y, batch_size=batch, epochs=epoch, validation_split=0.1,
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py"", line 1216, in fit
>>>     tmp_logs = self.train_function(iterator)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py"", line 878, in train_function
>>>     return step_function(self, iterator)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py"", line 867, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py"", line 860, in run_step
>>>     outputs = model.train_step(data)
>>> 
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 109, in train_step
>>>     predictions = self(x, training=1)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File ""/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 98, in call
>>>     for i in range(1, len(self.oneiterstack) - 1):
>>> 
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 102, in call
>>>     Au = self.radon(u)
>>> 
>>>   File ""/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py"", line 82, in radon
>>>     self.w11,self.w12,self.w21,self.w22,u)
>>> 
>>>   File ""/media/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/fan_radon_iradon_line.py"", line 8, in radon_fan_line
>>>     pf = tf.gather_nd(f * 1.0, ind11) * w11 + \
>>> 

Function call stack:
train_function -> call
"
53399,tf.reduce_logsumexp throws exception when reducing over RaggedTensor,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 18.04.6 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**
- Python version: **Python 3.9.0**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` throws a `ValueError: TypeError: object of type 'RaggedTensor' has no len()`.

**Describe the expected behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` should successfully reduce and return a non-ragged tensor. This matches the behavior of other reduce operations like `tf.reduce_sum` or `tf.reduce_max`/

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): **no**
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**:
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

a = tf.ragged.constant([[0, 0, 0], [0, 0]], dtype=tf.float32)
a.shape  # TensorShape([2, None])
tf.reduce_sum(a, axis=1)
# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>
tf.reduce_logsumexp(a, axis=1)  
# >>> ValueError: TypeError: object of type 'RaggedTensor' has no len()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This same error shows up when you try to transpose a ragged tensor.

A simple workaround:
```python
a_max = tf.reduce_max(a, axis=1, keepdims=True)
tf.math.log(tf.math.reduce_sum(tf.math.exp(a - a_max), axis=1)) + a_max
# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.0986123, 0.6931472], dtype=float32)>
```
```"
53398,To support casting int32 in HEX format ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.x
- Are you willing to contribute it (Yes/No): Maybe



**Describe the feature and the current behavior/state.**
I want something like hex_tensor = tf.cast(int32_tensor, dtype=tf.hex) which allows me to get the hex value representation and can be used for further computation. This becomes relevant for several data where label information is in hex colours while the label images are in PNG (RGB) format. It is one of the basic op in python [Python hex](https://docs.python.org/3/library/functions.html#hex).


**Will this change the current api? How?**
No, extends the functionality.


**Who will benefit with this feature?**
Any tensor op that needs to work with hex values, o

**Any Other info.**
Same as #30493 seems it is closed before finishing."
53397,[BUG] Retrained model on new session cause garbage for saving ,"I still get this issues at Tensorflow 2.5+, Tensorflow-GPU 2.5+, Keras-2.4.3. h5py=3.1.0
Even reinstalling Tensorflow, keras, and h5py does not resolve the problem.

The model I made is just a stack of Dense layer without anything special. 
Similar to all people, I can evaluate and predict good in the same training kernel after training. But saving a model by `model.save()` after training, and then reload the trained model for re-training cause weight reinitialization. Despite the arg `compile=True` or `compile=False`, or even `model.reset_states()` or not, The error still occurred

Note 1: This error appeared too long, from 2016 till now but it is not resolved
Note 2: Test have been made on TensorFlow 2.5, 2.6, ... but errors still being found

Similar Issue: # 4875 in Keras (keras-team/keras#4875)

The pipeline is similar. (This model is pure of stack of Dense layers)
1) Create the model
2) Compile
2) Train model
3) Save the model. 

-> The model still fine for prediction

---------------------------------------------------------------------------------

NEW KERNEL:
4) Re-load the model (3)
5) Save the model (4)
-> Still ok

BUT
---------------------------------------------------------------------------------

NEW KERNEL:
4) Re-load the model (3)
5) Retrain the model by `model.fit()` (4) -> Cause new additional weight initializer: 
I believe this is made by weight += new_initialized_weights
6) Save the model --> Model used the weight from beginning at the first step of (5) after retraining

Although I have digged into the source code but no sign of this behaviour `weight += new_initialized_weights` was found. 
But the Keras loss at the first batch during re-training phase is large enough so that I could notify this error. I believe this may come from the global variables state maintained in the source, 

The callbacks contained the same configuration ModelCheckpoint at two phases

"
53396,module 'tensorflow.python.compiler.tensorrt.utils' has no attribute 'versionTupleToString',"# Tensorrt conversion fails on Ubuntu 

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 2.7 binary (via pip)
- TensorFlow version (use command below): 2.7
- Python version: v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.5
- GPU model and memory:  Geforce RTX 2060 Super

**Describe the current behavior**
The tensorrt conversion fails when the installed version 7.2.3 is higher than the linked version 7.2.2, because the python code in the logger statements is wrong.

**Describe the expected behavior**
Tensorrt conversion is expected to succeed.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Not needed here, just look at `trt_convert.py:229ff` - the function `trt_utils.versionTupleToString` is called multiple times but is not defined, there is only `trt_utils._version_tuple_to_string`. The logger only gets called when there is a version mismatch.  A simple 

```python
versionTupleToString = _version_tuple_to_string
```
in `utils.py` will solve it.

File `trt_convert.py:229ff`:
```python
  def raise_trt_version_deprecated(version_type, trt_version):
    assert version_type in [
        ""linked"", ""loaded""
    ], (""Incorrect value received for version_type: %s. Accepted: ['linked', ""
        ""'loaded']"") % version_type

    logging.error(
        ""The {version_type} version of TensorRT: `{trt_version}` has now ""
        ""been removed. Please upgrade to TensorRT 7 or more recent."".format(
            version_type=version_type,
            trt_version=trt_utils.versionTupleToString(trt_version)))

    raise RuntimeError(""Incompatible %s TensorRT versions"" % version_type)

  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated(""linked"", linked_version)

  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated(""loaded"", loaded_version)

  if (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):
    logging.error(
        ""Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few ""
        ""requirements must be met:\n""
        ""\t-It is required to use the same major version of TensorRT during ""
        ""compilation and runtime.\n""
        ""\t-TensorRT does not support forward compatibility. The loaded ""
        ""version has to be equal or more recent than the linked version."",
        trt_utils.versionTupleToString(loaded_version),
        trt_utils.versionTupleToString(linked_version))
    raise RuntimeError(""Incompatible TensorRT major version"")

  elif loaded_version != linked_version:
    logging.info(
        ""Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is ""
        ""supported because TensorRT minor/patch upgrades are backward ""
        ""compatible."", trt_utils.versionTupleToString(loaded_version),
        trt_utils.versionTupleToString(linked_version))

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53395,tf.sparse.sparse_dense_matmul vs tf.matmul has numerical error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: MacOs Cataline 10.15.7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tested with 2.4.1, 2.6.0 2.7.0
- Python version: 3.7.12
- CUDA/cuDNN version: Used cpu
- GPU model and memory:

**Bug**

Numerical errors appear when using `tf.sparse.sparse_dense_matmul` compared to `tf.matmul`. This happens when the number of sparse elements becomes large ~100. Code to reproduce:

```
d = 10
matrix = tf.reshape(tf.linspace(0.,1.,d*d), [d,d])
sparse_matrix = tf.sparse.from_dense(matrix)

x = tf.reshape(tf.linspace(0.,1.,d), [d,1])

y = tf.matmul(matrix,x)
sy = tf.sparse.sparse_dense_matmul(sparse_matrix,x)

error = tf.reduce_mean(tf.abs(y-sy))
=> error = 1e-8
```

Couldn't seem to find this issue anywhere apart from [here](https://www.gitclear.com/open_repos/tensorflow/tensorflow/release/v2.5.0-rc2), where they mention that there is truly random noise when using tf.sparse.sparse_dense_matmul using GPU. But it seems like here it also appears with CPU


"
53394,Not able to restore the model from config with tf.einsum operation,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.4.3 (also reproduced with 2.5.0 and 2.7.0)
- Python version: 3.8.10
- CUDA/cuDNN version: 11.0

**Describe the current behavior**
When restoring the model from config getting
`ValueError: Got 0 inputs for equation ""bmhwf,bmoh->bmowf"", expecting 2`
Although if the `tf.einsum` op is wrapped as a Keras Lambda layer, it works (able to dump to config and restore).
**Describe the expected behavior**
Should be able to restore the model from config.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): Yes
- Briefly describe your candidate solution(if contributing): Not sure of how the solution might look like.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/10X2dDb_EGLL64w-MyMU4g9dSrHLw9PvI?usp=sharing
```
import tensorflow as tf
from tensorflow import keras


x1 = keras.Input(shape=(2, 4, 4, 1))
x2 = keras.Input(shape=(2, 2, 4))
x = tf.einsum('bmhwf,bmoh->bmowf', x1, x2)
model = keras.Model(inputs=[x1, x2], outputs=x)
model = tf.keras.Model.from_config(model.get_config())
```

**Other info / logs**
Log from colab
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-5a94aa47793c> in <module>()
      7 x = tf.einsum('bmhwf,bmoh->bmowf', x1, x2)
      8 model = keras.Model(inputs=[x1, x2], outputs=x)
----> 9 model = tf.keras.Model.from_config(model.get_config())

4 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)
   2446     with generic_utils.SharedObjectLoadingScope():
   2447       input_tensors, output_tensors, created_layers = (
-> 2448           functional.reconstruct_from_config(config, custom_objects))
   2449       # Initialize a model belonging to `cls`, which can be user-defined or
   2450       # `Functional`.

/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)
   1336         while layer_nodes:
   1337           node_data = layer_nodes[0]
-> 1338           if process_node(layer, node_data):
   1339             layer_nodes.pop(0)
   1340           else:

/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py in process_node(layer, node_data)
   1280         input_tensors = (
   1281             base_layer_utils.unnest_if_single_tensor(input_tensors))
-> 1282       output_tensors = layer(input_tensors, **kwargs)
   1283 
   1284       # Update node index map.

/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/special_math_ops.py in _einsum_v2_parse_and_resolve_equation(equation, input_shapes)
   1279   if len(input_shapes) != len(input_labels):
   1280     raise ValueError('Got {} inputs for equation ""{}"", expecting {}'.format(
-> 1281         len(input_shapes), equation, len(input_labels)))
   1282 
   1283   # Special case: if there are no '->', then we create output subscripts from

ValueError: Exception encountered when calling layer ""tf.einsum"" (type TFOpLambda).

Got 0 inputs for equation ""bmhwf,bmoh->bmowf"", expecting 2

Call arguments received:
  • equation='bmhwf,bmoh->bmowf'
  • inputs=<class 'inspect._empty'>
  • kwargs=<class 'inspect._empty'>
```
"
53393,Can't run inference with YAMNET based model that's been converted to TFLite,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Not a Mobile Device
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.8.5
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: using CPU
- GPU model and memory: using CPU

**Describe the current behavior**
I used [this tutorial](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio) to train a model.
Then I combined the classification model and YAMNet model using the instructions in the tutorial.
Next, I converted the model to TFLite using int8 quantization.
The first problem I faced was that the input shape of YAMNet is variable and the TFLite model's shape is now 1.
![image](https://user-images.githubusercontent.com/1261332/145704398-248d5793-8441-4a4c-a4c5-08c1286474d8.png)

So I had to resize the input to accept 16k audio samples.

```
model.resize_tensor_input(
    input_details[0]['index'], (16000,))
```

But then I encountered another error I couldn't fix:

**Pad value has to be greater than equal to 0.Node number 17 (PAD) failed to invoke.**

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

from yamnet_commons import filenames, model_path, load_wav_for_map, AUTOTUNE, commands

files_ds = tf.data.Dataset.from_tensor_slices(filenames)
files_ds = files_ds.map(load_wav_for_map, num_parallel_calls=AUTOTUNE)
files_ds = files_ds.cache()

model = tf.lite.Interpreter(str(model_path.joinpath('./model.tflite')))
input_details = model.get_input_details()
output_details = model.get_output_details()

model.resize_tensor_input(
    input_details[0]['index'], (16000,))

model.allocate_tensors()

results = {}

for frame_data, lbl_idx in files_ds:
    lbl_idx = lbl_idx.numpy()
    model.set_tensor(input_details[0]['index'],
                     tf.cast(frame_data * 255 - 128, tf.int8))
    model.invoke()
    prediction = model.get_tensor(output_details[0]['index'])
    print(prediction)
    sm = tf.nn.softmax(prediction.astype(np.float) / 128)
    idx = np.argmax(sm)

    if lbl_idx not in results:
        results[lbl_idx] = {
            'n_total': 0,
            'n_ok': 0
        }

    results[lbl_idx]['n_total'] += 1
    if idx == lbl_idx:
        results[lbl_idx]['n_ok'] += 1


print(results)

for lbl, v in results.items():
    print(""{0} accuracy: {1}"".format(commands[lbl], v['n_ok']/v['n_total']))
```
"
53392,exit code 409 when trying to run through tensorflow example in pycharm,"I am trying to go through the DCGAN example on the tensorflow website https://www.tensorflow.org/tutorials/generative/dcgan. it seems to run fine up until the step where it uses the generator generated_image = generator(noise, training=False). At that point it exits with error code Process finished with exit code -1073740791 (0xC0000409).

I am running on Windows 10 using pycharm. I have tried messing with the batch size in case this is a memory issue, but even setting it to 1 gives the same results. I have also tried running pycharm as administrator."
53391,AttributeError: 'Sequential' object has no attribute 'predict_classes',"```
seed_text = ""I've got a bad feeling about this""
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = """"
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += "" "" + output_word
print(seed_text)
```

This codeblock is giving me the error shown below:


```
AttributeError                            Traceback (most recent call last)
<ipython-input-52-70399a5f93d3> in <module>()
      5         token_list = tokenizer.texts_to_sequences([seed_text])[0]
      6         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
----> 7         predicted = model.predict_classes(token_list, verbose=0)
      8         output_word = """"
      9         for word, index in tokenizer.word_index.items():

AttributeError: 'Sequential' object has no attribute 'predict_classes'
```

How can I solve this error? Please help.

Here is the link of the full code:
https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbDVSTGdJczNoa2ZBeXpObWxobGJldGd5Q2pLd3xBQ3Jtc0trSVRobkVBM2EtV3lTajlQNzhXbEhGTjN3WXhFTGo3d1U4b3h1Rjl4Rk9ubUVVWG95cUNfaWFoTHpmRlpEM1VmM2NrOXFqc21WYkRKZ0c3WWgtcE5VRV9kZF9pZDZMUDVmWnRtNUtMS3NzUnZodFB3RQ&q=https%3A%2F%2Fgoo.gle%2F3aSTLGx"
53390,Cuda 10.2 compatible with which tensorlow-gpu version?,"I am getting error while importing tensorflow where it is looking cuda 11.0 version, I have cuda with 10.2, unable to find compatible TF version with 10.2.

Can anyone confirm the TF version ?

"
53388,cl.exe failed on fast_module_type.so exit 2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 19044.1387
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Python version: 3.9.2
- Installed using virtualenv? pip? conda?: bazel
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): VS2019 (14.29.30133)
- CUDA/cuDNN version: 11.2 / 8.2
- CPU model and memory: R7 2600X 16GB DDR4
- GPU model and memory: GTX 2060 6GB GDDR4



**Describe the problem**
I'm trying to compile tensorflow from source to deploy a model in C++ trained on Google Collab. I tried to compile different versions of tensorflow from v2.4.0 to master and r2.4 to master but all of them give me error, now I'm trying to compile r2.6.
I just need TF on CPU so when I run configure.py select No on cuda an cuDNN support. I already use python 3.8, 3.9, 3.9.2, VS2019, VS2017, between each test use `bazel clean` and then `bazel clean --expunge` and also tried with a clean install of Windows 10 for compiling but all of them give the exact same error
Also followed issue #50950 and #51573

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Clonning repository
```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.6
```

Running ` python .\configure.py`
```
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is C:\Users\sixto\AppData\Local\Programs\Python\Python39\python.exe]:


Found possible Python library paths:
  C:\Users\sixto\AppData\Local\Programs\Python\Python39\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\sixto\AppData\Local\Programs\Python\Python39\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

Installing requirements which I copy from `tensorflow\tools\pip_package\setup.py`
`pip3 install -r requirements.txt`

And compile just for CPU
`bazel --output_user_root=D:\tmp_tensorflow build --config=opt //tensorflow/tools/pip_package:build_pip_package`

At first run it give me this error
```
ERROR: D:/tensorflow/tensorflow/compiler/xla/service/BUILD:340:11: C++ compilation of rule '//tensorflow/compiler/xla/service:hlo_evaluator' failed (Exit 2): cl.exe failed: error executing command
  cd D:/tmp_tensorflow/26orbg4z/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/python.exe
    SET PYTHON_LIB_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\sixto\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\sixto\AppData\Local\Temp
```

Then re-run the same compiling command and give me this error
```
ERROR: D:/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command
  cd D:/tmp_tensorflow/26orbg4z/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/python.exe
    SET PYTHON_LIB_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\sixto\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\sixto\AppData\Local\Temp
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[second_run.log](https://github.com/tensorflow/tensorflow/files/7696321/second_run.log)
[first_run.log](https://github.com/tensorflow/tensorflow/files/7696322/first_run.log)

"
53387,🐛🪲,"> <em>Please make sure that this is a bug. As per our

[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53384,Perspective transformation data augmentation for object detection,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version :2.7.0
- Are you willing to contribute it 🆗 



**Perspective transformation data augmentation for object detection.**

**Will this change the current api? How? No, the change will be done only in the preprocessing layer.

**Who will benefit with this feature? As it has been described in [this article](https://ieeexplore.ieee.org/abstract/document/8943416), this feature could be helpful to make the models more general in the object detection projects.

"
53383,I think nvidia-driver is not working.,"I think nvidia-driver is not working.

You have to check command `nvidia-smi` 

If these command is not working, you have reboot your computer.

Maybe `nvidia-smi` works correctly.

_Originally posted by @SnowMasaya in https://github.com/tensorflow/tensorflow/issues/255#issuecomment-384846967_"
53382,FAILED: Build did NOT complete successfully on windows 10,"<em>I am facing error trying to build tensorflow from source in windows 10. My goal is to use tensorflow API in C++. I am following [this doc](https://www.tensorflow.org/install/source_windows) for that. But whenever I run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` I get `ERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command`. I have also posted an issue where I am getting error message while trying to build tensorflow in Ubuntu 18.04 [here](https://github.com/tensorflow/tensorflow/issues/53379).</em>

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7
- Python version: 3.8
- Bazel version: 4.2.2


**Describe the problem**
I have tried to build tensorflow in Ubuntu 18.04 created in Oracle VM VirtualBox. When I tried to run the `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` command to build the pip package tensorflow cpu-only version, The following error message persists:
```
ERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command
  cd C:/users/black ops/_bazel_black ops/r7janahi/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v8.1A\bin\NETFX 4.5.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe
    SET PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\BLACKO~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\BLACKO~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe @bazel-out/x64_windows-opt/bin/external/com_google_protobuf/_objs/protoc_lib/zip_writer.obj.params
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9002 : ignoring unknown option '/experimental:preprocessor'
fatal error C1007: unrecognized flag '-ReducedOptimizeHugeFunctions' in 'p2'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1.472s, Critical Path: 0.74s
INFO: 10 processes: 10 internal.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
According to [the doc](https://www.tensorflow.org/install/source_windows) I have created a conda environment of Python 3.8 and installed the TensorFlow package dependencies:
`pip3 install -U six numpy wheel`
`pip3 install -U keras_preprocessing --no-deps`
Then I have installed Bazel 4.2.2 from [bazel github](https://github.com/bazelbuild/bazel/releases?page=6) using _bazel-4.2.2-windows-x86_64.exe_.
Then I have installed _MSYS2_ and run the following command:
`pacman -S git patch unzip`
After that, I have cloned the [Tensorflow repo](https://github.com/tensorflow/tensorflow.git) and checked into _r2.7_ branch.
Then I have run `python ./configure.py` and the following is the output:
```
You have bazel 4.2.2 installed.
Please specify the location of python. [Default is I:\Anaconda\envs\tf_c++\python.exe]:


Found possible Python library paths:
  I:\Anaconda\envs\tf_c++\lib\site-packages
Please input the desired Python library path to use.  Default is [I:\Anaconda\envs\tf_c++\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.

```

After that, I have run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` command and the following is the output:
```
WARNING: Output user root ""C:/Users/BLACK OPS/_bazel_BLACK OPS"" contains a space. This will probably break the build. You should set a different --output_user_root.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=171
INFO: Reading rc options for 'build' from i:\tf_for_c_env\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=I:/Anaconda/envs/tf_c++/python.exe
INFO: Reading rc options for 'build' from i:\tf_for_c_env\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from i:\tf_for_c_env\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe --action_env PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages --python_path=I:/Anaconda/envs/tf_c++/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from i:\tf_for_c_env\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file i:\tf_for_c_env\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file i:\tf_for_c_env\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file i:\tf_for_c_env\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file i:\tf_for_c_env\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file i:\tf_for_c_env\tensorflow\.bazelrc: --define framework_shared_object=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  I:/tf_for_c_env/tensorflow/WORKSPACE:23:14: in <toplevel>
  I:/tf_for_c_env/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  C:/users/black ops/_bazel_black ops/r7janahi/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  C:/users/black ops/_bazel_black ops/r7janahi/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command
  cd C:/users/black ops/_bazel_black ops/r7janahi/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v8.1A\bin\NETFX 4.5.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe
    SET PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\BLACKO~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\BLACKO~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe @bazel-out/x64_windows-opt/bin/external/com_google_protobuf/_objs/protoc_lib/zip_writer.obj.params
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9002 : ignoring unknown option '/experimental:preprocessor'
fatal error C1007: unrecognized flag '-ReducedOptimizeHugeFunctions' in 'p2'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1.472s, Critical Path: 0.74s
INFO: 10 processes: 10 internal.
FAILED: Build did NOT complete successfully
```
**Any other info / logs**
I have Visual Studio 2019 installed from previous uses in my Windows environment, so I have skipped the [Install Visual C++ Build Tools 2019](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019) part from the doc."
53381, bazel-genfiles/ and *.pb.h  not been generated  with bazel-building tensorflow2.4.1 from source,"**System information**
- OS Platform and Distribution ( Linux Ubuntu 18.04):
- TensorFlow installed from (source)
- TensorFlow version:        Tags v2.4.1
- Python version:      Python 3.7.6
- Installed using virtualenv? pip? conda?:       conda
- Bazel version (if compiling from source):        have tried bazel 3.4.0 、 bazel 3.1.0
- GCC/Compiler version (if compiling from source):  gcc (GCC) 7.4.0
- CUDA/cuDNN version:        cuda_11.1 , cudnn 8.0.5
- GPU model and memory:   rtx3060
- nvidia-driver version:     460.91
- eigen  version :      eigen-3.3.90


I want to build  tensorflow c++ api with bazel, no errors were reported during  bazel-build process .
the command I used: `bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so`

however,while compiling a C++ program, I got the following error:
`fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory
  #include ""tensorflow/core/framework/device_attributes.pb.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
`  

After that I  checked the  tensorflow/core/framework  folder and **found no *.pb.h  files in it ,only   some .proto files,**(etc. device_attributes.proto ) .  **Not even the bazel-genfiles folder.** 
The *.pb.h files should be generated by protobuf ,  what's wrong with my protobuf ?
the protobuf version and url refer to [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/workspace.bzl)


I don't  know how to solve it.  Thanks for your help.



"
53380,UnsatisfiedLinkError when attempting to use HexagonDelegate,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using example from https://www.tensorflow.org/lite/performance/hexagon_delegate
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 7T Pro
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): `org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT` and `org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT` with hexagon libraries `v1.20.0.1`
- Python version: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a, but the 7T Pro has a Snapdragon 855+ with a Hexagon 690

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Attempting to create `HexagonDelegate` results in a `java.lang.UnsatisfiedLinkError`

**Describe the expected behavior**

HexagonDelegate should work on the device in question

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Create a new project, and add the Java API as described here: https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_java_api

You should end up with the three skel binaries in app/src/main/jniLibs, and the tensorflow-lite & tensorflow-lite-hexagon nightly AARs downloaded and built into the app, resulting in the following lib folder after installation:

```
OnePlus7TPro:/data/app/~~an-4OVJU49vmn0JQVRH4TQ==/<pname>-QzF_0vIwaxg6M2SwPMqdug==/lib/arm64 #
ls
libhexagon_interface.so  libhexagon_nn_skel_v65.so  libtensorflowlite_hexagon_jni.so
libhexagon_nn_skel.so    libhexagon_nn_skel_v66.so  libtensorflowlite_jni.so
```

Add code to create a HexagonDelegate, as follows:

```
try {
   HexagonDelegate(context)
}catch (e: UnsupportedOperationException){
   //Fall back to NNAPI delegate
}
```

Run the app, it will crash with an exception:

```
12-10 01:21:39.599 20658 20658 E AndroidRuntime: FATAL EXCEPTION: main
12-10 01:21:39.599 20658 20658 E AndroidRuntime: Process: <pname>, PID: 20658
12-10 01:21:39.599 20658 20658 E AndroidRuntime: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""dlopen"" referenced by ""/data/app/~~an-4OVJU49vmn0JQVRH4TQ==/<pname>-QzF_0vIwaxg6M2SwPMqdug==/lib/arm64/libtensorflowlite_hexagon_jni.so""...
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1087)
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1008)
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.System.loadLibrary(System.java:1664)
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.ensureNativeLibraryLoaded(HexagonDelegate.java:66)
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.<init>(HexagonDelegate.java:35)
12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at <line of HexagonDelegate creation>(....kt:89)
```

As shown with the `ls` above, all the required libraries are present in that directory.

This behaviour is identical to #51872, which was closed as stale without a solution. The linked issues from the comments on that issue do not solve this problem, as for issue 1 the crash is occurring before the actual DSP check, which would be caught and handled by my sample code. Issue 2 is for Ubuntu Mobile and Python, and thus totally irrelevant.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53379,Python Configuration Error: Problem getting numpy include path while building with Bazel,"<em>I am facing error trying to build tensorflow from source. My goal is to use tensorflow API in C++. I am following [this doc](https://www.tensorflow.org/install/source) for that. But whenever I run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` I get `Python Configuration Error: Problem getting numpy include path.`</em>

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 2.1
- Python version: 3.6.9
- Bazel version: 0.29.1



**Describe the problem**
I have tried to build tensorflow in Ubuntu 18.04 created in Oracle VM VirtualBox.  When I tried to run the `bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package` command to build _tensorflow cpu-only_ version, The following error message persists:
```
ERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD
INFO: Elapsed time: 0.268s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)
```
Then according to [this reply](https://github.com/tensorflow/tensorflow/issues/44876#issuecomment-727388512) in another issue, I changed the command into `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`. But then the following error persists:
```
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
INFO: Elapsed time: 0.966s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (2 packages loaded, 5 targets con\
figured)

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
According to [the doc](https://www.tensorflow.org/install/source) I have followed the following steps to install Python and the TensorFlow package dependencies:
`sudo apt install python3-dev python3-pip`
`pip install -U --user pip numpy wheel`
`pip install -U --user keras_preprocessing --no-deps`
Then I have installed Bazel 0.29.1 from [bazel github](https://github.com/bazelbuild/bazel/releases?page=6) using _bazel-0.29.1-linux-x86_64_.
After that, I have cloned the [Tensorflow repo](https://github.com/tensorflow/tensorflow.git) and checked into _r2.1_ branch.
Then I have run `./configure` and the following is the output:
```
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.29.1 installed.
Please specify the location of python. [Default is /usr/bin/python]: 


Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

```

After that, I have run `bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package` command and the following is the output:
```
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=79
INFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2
INFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc:
  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
ERROR: Skipping '[--config=option]': no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD
ERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/niaz/Desktop/TF_C++_API/tensorflow/BUILD
INFO: Elapsed time: 2.548s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)
    currently loading: tensorflow/tools/pip_package
```
Then according to [this reply](https://github.com/tensorflow/tensorflow/issues/44876#issuecomment-727388512) in another issue, I changed the command to `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`. But then the following is the output:
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=79
INFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2
INFO: Reading rc options for 'build' from /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc:
  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /home/niaz/Desktop/TF_C++_API/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /home/niaz/Desktop/TF_C++_API/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true
INFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl:347:20):
 - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:77:5
 - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 345
		_create_local_python_repository(repository_ctx)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 56, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
INFO: Call stack for the definition of repository 'gast_archive' which is a tf_http_archive (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/repo.bzl:121:19):
 - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:336:5
 - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1
INFO: Call stack for the definition of repository 'astor_archive' which is a tf_http_archive (rule definition at /home/niaz/Desktop/TF_C++_API/tensorflow/third_party/repo.bzl:121:19):
 - /home/niaz/Desktop/TF_C++_API/tensorflow/tensorflow/workspace.bzl:312:5
 - /home/niaz/Desktop/TF_C++_API/tensorflow/WORKSPACE:19:1
ERROR: While resolving toolchains for target //tensorflow:libtensorflow_framework.so.2.1.4: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 345
		_create_local_python_repository(repository_ctx)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 56, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 345
		_create_local_python_repository(repository_ctx)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 56, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/niaz/Desktop/TF_C++_API/tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
INFO: Elapsed time: 3.397s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (70 packages loaded, 127 targets \
configured)

```



**Any other info / logs**
I have tried this with Ubuntu 20.04 first and Ubuntu 18.04 after that as according to [this bazel build doc](https://docs.bazel.build/versions/main/install-ubuntu.html) Bazel is testedly supported for Ubuntu 18.04 and Ubuntu 16.04, but the problem still persists.
I have also checked with `pip3 freeze` and `pip3 list` that `numpy==1.19.5` version exists in my environment. I am not using any virtual environment currently. I have checked some related issues but none of them solved my problem.
"
53377,ValueError: Unsupported data type 14 in tensor,"_<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>_

**System information**
- Have I written custom code : `interpreter = tf.lite.Interpreter(model_path=""models/saved_model.tflite"")`
- OS Platform and Distribution : **`RaspberryPi 3B+`**
- TensorFlow Git version : `v2.4.0-0-g582c8d2` 
- TensorFlow version : `2.4.0`
- Installation Source : [`https://github.com/bitsy-ai/tensorflow-arm-bin/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl`](https://github.com/bitsy-ai/tensorflow-arm-bin/releases/download/v2.4.0/tensorflow-2.4.0-cp37-none-linux_armv7l.whl)
- Python version: `Python 3.7.3`
- More Info about the **tflite** Model : Custom trained **EfficientDet-B0** on `40` classes

#### ERROR : 
```
Traceback (most recent call last):
  File ""detect.py"", line 8, in <module>
    interpreter = tf.lite.Interpreter(model_path=""models/saved_model.tflite"")
  File ""/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py"", line 209, in __init__
    model_path, self._custom_op_registerers))
ValueError: Unsupported data type 14 in tensor
Unsupported data type 14 in tensor
Unsupported data type 14 in tensor
Unsupported data type 14 in tensor
Unsupported data type 14 in tensor
Unsupported data type 14 in tensor
```

> Note : If you are suggesting me to upgrade or downgrade the packages, please provide necessary code with it.

Thanks @tilakrayal !!



"
53373,tfjs-v3.12.0 and Wasm 3.12.0 for tfjs-models/pose-detection/  MoveNet  multipose wasm,"When I try to run the demo of the model : tfjs-models/pose-detection/  : https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet 

With tfjs-v3.12.0 and Wasm 3.12.0 and configuration model MoveNet type Multipose and backend Wasm  i got the following error :

Error: Kernel 'Reciprocal' not registered for backend 'wasm'

![image](https://user-images.githubusercontent.com/32233417/145433791-63df6bdb-87a1-4e08-a7cb-b72e41773c18.png)

The demo Woks well for the singlepose type (lightning).

Regards"
53372,How to train specific variables in SavedModel in Tensorflow format ?,"I have a pre-trained model in Tensorflow (not keras) format which I want to retrain .
[mobilenetv2.zip](https://github.com/tensorflow/tensorflow/files/7685965/mobilenetv2.zip)

However, in this model the list of variables is empty. The re-training needs to be done on a certain set of variables but those variables are not declared as trainable. How to access the variables from this model and make them trainable?


"
53370,Quantized Convolution Layers Operation in TF-lite,"Hello to everyone, for academic and research purposes I am trying to understand the operation behind a quantized convolution layer in Tensorflow Lite. For this purpose, I chose EffiecientNet-lite0 model. So I downloaded pretrained EfficientNet-lite0 float32 and int8 tflite files from the official [repository](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite) and run inference of these models using a sample .jpg image. Firstly, I checked model's architecture and some details using Netron tool and decided to pick first Conv2D layer as my case study.

![netron](https://user-images.githubusercontent.com/57605047/145392585-5a4393f8-5ab5-4e36-864a-9927c9782a63.png)

I started with fp32 model inference as I thought it will be simplier and used the code below for preprocess the image and for the inference of the model.
```
import tensorflow as tf
import numpy as np
import PIL.Image as Image


MEAN_RGB = 127.0
STDDEV_RGB = 128.0
CROP_PADDING = 32
IMAGE_SIZE = 224

def _decode_and_center_crop(image, image_size, resize_method=Image.BICUBIC):
    """"""Crops to center of image with padding then scales image_size.""""""
    image_width, image_height = image.size
    padded_center_crop_size = int((image_size / (image_size + CROP_PADDING)) * min(image_height, image_width))
    offset_height = ((image_height - padded_center_crop_size) + 1) // 2
    offset_width = ((image_width - padded_center_crop_size) + 1) // 2
    crop_window = [offset_width, offset_height, 
                   offset_width + padded_center_crop_size, 
                   offset_height + padded_center_crop_size]
    resized_image = image.crop(crop_window)
    resized_image = resized_image.resize((image_size, image_size), resize_method)
    return resized_image

with open('image_net_classes.txt') as f:
    lines = f.readlines()

image = Image.open('beagle.jpg')
resized_image = _decode_and_center_crop(image, IMAGE_SIZE)
resized_image = np.array(resized_image).astype(np.float32)
resized_image -= MEAN_RGB
resized_image /= STDDEV_RGB
resized_image = np.expand_dims(resized_image, axis=0)


# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""efficientnet-lite0-fp32.tflite"", experimental_preserve_all_tensors=True)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
interpreter.set_tensor(input_details[0]['index'], resized_image)

interpreter.invoke()

for t in interpreter.get_tensor_details():
  if t['index'] == 102:
    test = interpreter.get_tensor(t['index'])
print(test.shape)

output_data = interpreter.get_tensor(output_details[0]['index'])
string = str(np.argmax(output_data))

for line in lines:
    if string in line:
        print('The image is a', line)
        break
```
So after I downloaded Conv2D layer's parameters(kernel) I implented fused Relu6 Conv2D layer using simple Python and later came back to compare results and everything was working pretty good.

So the next step was to implement quantized Conv2D layer of EfficientNet-lite0-int8 and used the code below.

```
MEAN_RGB = 127.0
STDDEV_RGB = 128.0
IMAGE_SIZE = 224
scale = 0.012566016986966133
zero_point = 131

image = Image.open('beagle.jpg')
resized_image = _decode_and_center_crop(image, IMAGE_SIZE)
resized_image = np.array(resized_image).astype(np.float32)
resized_image -= MEAN_RGB
resized_image /= STDDEV_RGB
resized_image = np.expand_dims(resized_image, axis=0)
resized_image = resized_image / scale + zero_point
resized_image = np.array(resized_image).astype(np.uint8)

interpreter = tf.lite.Interpreter(model_path=""efficientnet-lite0-int8.tflite"", experimental_preserve_all_tensors=True)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
interpreter.set_tensor(input_details[0]['index'], resized_image)


interpreter.invoke()
for t in interpreter.get_tensor_details():
  if t['index'] == 102:
    test = interpreter.get_tensor(t['index'])
print(test.shape)

output_data = interpreter.get_tensor(output_details[0]['index'])


for line in lines:
    if string in line:
        print('The image is a', line)
        break
```
I also studied this [paper](https://arxiv.org/pdf/1712.05877.pdf) that provided this Conv layer's implemetation [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h#L248-L314). So in my understanding, the quantized Convolution Operation is the same as the full precision one but you have also to take into account offsets and scales. 

I was able to extract input,output,kernel and biases scales and offsets and managed to transform double multipliers to quantized multipliers and right shift using fuctions defined [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/quantization_util.cc) to be able to do the needed operation below.
```
acc = MultiplyByQuantizedMultiplierSmallerThanOne(
              acc, output_multiplier, output_shift);
```

That I assume is a per-axis operation. **So my question here, is that for this operation the multiplier we use is a quantized multiplier that equals Mo = (Sinput * Skernel) / Soutput or it's something else?**

So after I wrote a Python implementation of the above quantized Con2D layer and checked the results using
```
for t in interpreter.get_tensor_details():
  if t['index'] == 102:
    test = interpreter.get_tensor(t['index'])
```
there was a big deviation. Firstly, I thought that the above layer was a fused Relu6 Conv2D layer so I was expecting output tensor's values to be between 0 and 6 but that was not the case.

**Could you please provide me a more detailed description of a quantized fused Relu6 Conv2D layer's operation?**

### Parameters defined

The sample image I used for inference.
![beagle](https://user-images.githubusercontent.com/57605047/145397592-f2d5b484-d91e-4dbd-be53-93f1135a7844.jpg)

I am using Google Collaboratory to run the above code snippets.

**Tensorflow Version:** 2.7.0
**Python version:**  3.7.12
**Numpy version:** 1.19.5"
53366,tensorflow/c/c_api_experimental.cc:698:41: error: 'class tensorflow::EagerContext' has no member named 'StoreCollectiveOpsServer',"编译  tensorflow 1.14版本，提示
tensorflow/c/c_api_experimental.cc:698:41: error: 'class tensorflow::EagerContext' has no member named 'StoreCollectiveOpsServer'"
53365,Interpreter->invoke() calls Segmentation Fault,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 with USB Coral. 
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): v2.5.0, v2.6.0

**Description**
When i calls `initTfLiteInterpreter()`, tfLite work is correct, output has all information (boxes, labels and classes), time of invoke is good (15 ms). But when i calls `processingFrame(cv::Mat)` in other classes with equalent code (i get Seg fault on `Interpreter->invoke()`):
```
        // From other class 
for(int i = 0; i< 10; i++)
    {
        cv::Mat testImage = cv::imread(TestClass->EXAMPLE_FRAME);
        TestClass->processingFrame(testImage);
    }
```
I get this error with TF 2.5.0, 2.6.0. 

**Source of my programm**
**TestClass.h:**
```
/// Build EDGE Interpreter for Coral
    void BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,
                                edgetpu::EdgeTpuContext *edgetpu_context);

    // Load graph to coral
    void initTfLiteInterpreter();

    // Processing the received frame
    void processingFrame(cv::Mat& frame);

    int num_threads = 1;
    std::unique_ptr<tflite::Interpreter> interpreter;
    std::shared_ptr<edgetpu::EdgeTpuContext> tpu_context;

    TfLiteTensor* input_tensor;
    TfLiteTensor* output_locations;
    TfLiteTensor* output_classes;
    TfLiteTensor* output_scores;
    TfLiteTensor* num_detections_;

    int height;
    int width;
    int channels;
    int row_elems;
```
**TestClass.cxx:**
```
void TestClass::BuildEdgeTpuInterpreter(const tflite::FlatBufferModel &model,
                                                               edgetpu::EdgeTpuContext *edgetpu_context)
{
    tflite::ops::builtin::BuiltinOpResolver resolver;
    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());
    if (tflite::InterpreterBuilder(model, resolver)(&interpreter) != kTfLiteOk) {
        std::cerr << ""Failed to build interpreter."" << std::endl;
        return;
    }
    // Allocate tensor buffers.
    // Bind given context with interpreter.
    interpreter->SetExternalContext(kTfLiteEdgeTpuContext, edgetpu_context);
    interpreter->SetNumThreads(1);
    if (interpreter->AllocateTensors() != kTfLiteOk)
    {
      std::cerr << ""Failed to allocate tensors."" << std::endl;
    }
}

void TestClass::initTfLiteInterpreter(void)
{
    auto model = tflite::FlatBufferModel::BuildFromFile(GRAPH.c_str());

    tpu_context = edgetpu::EdgeTpuManager::GetSingleton()->OpenDevice();
    std::cout << ""Checking readiness of Coral device"" << std::endl;
    if(!tpu_context->IsReady())
    {
        std::cout << ""Coral device is not ready"" << std::endl;
        throw -1;
    }
    std::cout << ""EDGE TPU path: "" << tpu_context->GetDeviceEnumRecord().path << std::endl;
    BuildEdgeTpuInterpreter(*model, tpu_context.get());

    input_tensor = interpreter->tensor(interpreter->inputs()[0]);
    output_locations = interpreter->tensor(interpreter->outputs()[0]);
    output_classes = interpreter->tensor(interpreter->outputs()[1]);
    output_scores = interpreter->tensor(interpreter->outputs()[2]);
    num_detections_ = interpreter->tensor(interpreter->outputs()[3]);

    height = input_tensor->dims->data[1];
    width = input_tensor->dims->data[2];
    channels = input_tensor->dims->data[3];
    row_elems = width * channels;

    for(int i = 0; i< 10; i++)
    {
        cv::Mat testImage = cv::imread(EXAMPLE_FRAME);
        processingFrame(testImage);
    }
    
    Utils::dual_write(""CNN is ready, example frame was processed"");
    m_readyFlag.store(true);
}

void TestClass::processingFrame(cv::Mat& frame)
{
    Q_ASSERT(q_ptr);
    const clock_t begin_time = clock();
    QMutexLocker locker(&m_mutex);
    qDebug() << ""cv mat size: "" << width << height;
    cvtColor(frame, frame, cv::COLOR_BGR2RGB);
    // Resize for model input
    cv::resize(frame, frame, cv::Size(width, height));

    if (input_tensor->type != kTfLiteUInt8 ||           //
        input_tensor->dims->data[0] != 1 ||             //
        input_tensor->dims->data[1] != height ||  //
        input_tensor->dims->data[2] != width ||   //
        input_tensor->dims->data[3] != channels) {
    std::cerr << ""Input tensor shape does not match input image"" << std::endl;
    return;
    }

    uint8_t* dst = input_tensor->data.uint8;
    for (int row = 0; row < height; row++) {
        memcpy(dst, frame.ptr(row), row_elems);
        dst += row_elems;
    }

    if(interpreter->Invoke() != kTfLiteOk)
        qDebug() << ""Invoke is broken"";
    qDebug() << ""Invoke is done!"";
    const float* detection_locations = output_locations->data.f;
    const float* detection_classes = output_classes->data.f;
    const float* detection_scores = output_scores->data.f;
    const int num_detections = *(num_detections_->data.f);
    for (int i = 0; i < num_detections; i++) {
        const float score = detection_scores[i];
        const std::string label = std::to_string(uint8_t(detection_classes[i]));
        const float yMin = detection_locations[4 * i + 0];
        const float xMin = detection_locations[4 * i + 1];
        const float yMax = detection_locations[4 * i + 2];
        const float xMax = detection_locations[4 * i + 3];
        if (score > thresholdScore) {
            std::cout << label << "" score:"" << score << std::endl;
            emit q_ptr->returnBoundingBoxes(frame, yMin, xMin, yMax, xMax, score, label, true);
        }
    }
    std::cout << ""time: "" << float( clock () - begin_time ) /  CLOCKS_PER_SEC << std::endl;
    emit q_ptr->finishedCNNProcessing(frame);
}
```
**LOGS**
```
Checking readiness of Coral device
EDGE TPU path: /sys/bus/usb/devices/2-1
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.022215
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.012465
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.011841
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.011659
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.014413
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.011502
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.012496
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.012136
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.012898
cv mat size:  640 480
Invoke is done!
1 score:0.902344
time: 0.012129
Thu Dec  9 11:52:59 2021:  CNN is ready, example frame was processed
cv mat size:  640 480
Segmentation fault (core dumped)
```
**GDB out**
```
0x000000000067d47c in tflite::ops::custom::detection_postprocess::DecodeCenterSizeBoxes(TfLiteContext*, TfLiteNode*, tflite::ops::custom::detection_postprocess::OpData*)
```
"
53364,Selective build for iOS does not produce TensorFlowLiteC_framework.zip,"### System information
-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 11.4
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: building from source using latest TF source code
-   **Python version**: 3.9.4
-   **Bazel version (if compiling from source)**: 4.2.1
-   **GCC/Compiler version (if compiling from source)**: Apple clang 13.0.0
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**: 
```
bash tensorflow/lite/ios/build_frameworks.sh --input_models=model.tflite --target_archs=x86_64,armv7,arm64
```

### Describe the problem
We are performing a selective build for iOS to reduce TF binary size following the instructions here:
https://www.tensorflow.org/lite/guide/reduce_binary_size#selective_build_for_ios

Recently we have followed the instructions for Android on the same page and used the same machine to successfully compile the selective build AAR. For iOS, we followed a similar procedure of running `./configure` with iOS support, then running the selective build script to generate the framework using the command above. 

According to the documentation, the framework should be located at `bazel-bin/tensorflow/lite/ios/tmp/TensorFlowLiteC_framework.zip`, but the file does not exist after the build is complete. There are no errors in the output, and it states that the build is successful at the end.

The issue does not seem to be related to our specific model, since we also tried it with the MobileNet provided here but encountered the same issue:
https://www.tensorflow.org/lite/guide/reduce_binary_size#overview


### Source code / logs
The complete output is attached. Also attached the log when using the provided MobileNet on a fresh repo. That one had some warnings the first time it was run but still stated that the build was successful (no output still), not sure if it's significant.

[tf-lite-log.txt](https://github.com/tensorflow/tensorflow/files/7682282/tf-lite-log.txt)
[tf-lite-log-mobilenet.txt](https://github.com/tensorflow/tensorflow/files/7682384/tf-lite-log-mobilenet.txt)

"
53362,Could not load dynamic library 'libcudart.so.11.0' cannot open shared object file: No such file or directory,"- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: pip3
- CUDA/cuDNN version: 11.5
- GPU model and memory: Gtx 1060 
i have tensorflow and tensorflow-gpu installed

code source : 

```
import numpy as np
import keras
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from keras.applications.vgg16 import VGG16
import tensorflow as tf

# def predict(img_path):
def getPrediction(filename):
     model = tf.keras.models.load_model(""/classrepo/HomeWork_out/Project3_ManuelaClone/UCF-PROJECT-03/final_model_weights.hdf5"")
     img = load_img('/classrepo/HomeWork_out/Project3_ManuelaClone/UCF-PROJECT-03/static/'+filename, target_size=(180, 180))
     img = img_to_array(img)
     img = img / 255
     img = np.expand_dims(img,axis=0)
     category = model.predict_classes(img)
     answer = category[0]
     probability = model.predict(img)
     probability_results = 0

     if answer == 1:
          answer = ""Recycle""
          probability_results = probability[0][1]
     else:
          answer = ""Organic""
          probability_results = probability[0][0]

     answer = str(answer)
     probability_results=str(probability_results)

     values = [answer, probability_results, filename]
     return values[0], values[1], values[2]
```




```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   33C    P8     2W /  N/A |      6MiB /  6078MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       833      G   /usr/lib/Xorg                       4MiB |
+-----------------------------------------------------------------------------+
```

**i'm using manjaro and i get this error when i want to run my code by python3 main.py :**

```
2021-12-09 00:35:36.425837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-12-09 00:35:36.425863: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
```

any help please ??
"
53361,"Running quantized TF Lite model on Android receives error ""Failed to run on the given Interpreter""","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: tested on Samsung Galaxy device and Pixel 4 emulator
-   **TensorFlow installed from (source or binary)**: tried both (binary compiled on macOS 11.4)
-   **TensorFlow version (use command below)**: tried 2.2.0, 2.5,0, 2.7.0, and nightly
-   **Python version**: Python 3.9
-   **Bazel version (if compiling from source)**: 4.2.1
-   **GCC/Compiler version (if compiling from source)**: Apple clang 13.0.0
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
When running inference on a quantized LSTM TF Lite model in Android, we receive the following error:
```
java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:55 op_context->perm->dims->data[0] != dims (3 != 2)
    Node number 9 (TRANSPOSE) failed to prepare.
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:241)
        at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:135)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:80)
        at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:128)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:80)
```

The unquantized model works fine in Android. The quantized model works fine in iOS and Python.
We have tried using 2.2.0, 2.5.0, 2.7.0, and nightly to quantize the model in Python and include the respective versions in the Android build.gradle dependencies:
```
implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'
```
We have also tried using selective builds with the latest TF source code.

### Source code / logs
Code for quantizing the model in Python:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_quantizer = True
converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
```

We referenced the text classification example for Android in the repo for performing inference in Java:
```
ByteBuffer buffer = loadModelFile(assetManager, modelPath);
private Interpreter tflite = new Interpreter(buffer);
float[][] output = new float[1][labels.size()];
tflite.run(input, output);

private static MappedByteBuffer loadModelFile(AssetManager assetManager, String modelPath)
    throws IOException {
  try (AssetFileDescriptor fileDescriptor = assetManager.openFd(modelPath);
      FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor())) {
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
  }
}
```"
53360,Autoformat entire codebase?,"There are two popular ways of applying and enforcing automatic source code formatting a codebase like yours, these are:
- [clang-format](https://clang.llvm.org/docs/ClangFormat.html) (e.g., for your C++ and protobuf files)
- [black](https://github.com/psf/black) (for your Python and Jupyter Notebook files)

Which is as easy as:
```sh
$ python -m black .
$ fd -eh -ec -ehpp -ecpp -eproto -x clang-format -i --style=Google {} \;
```

Review changes:

  - https://github.com/offscale/tensorflow/tree/clang-format-apply from `clang-format` 13.0.0 at 9ebfeab
  - https://github.com/offscale/tensorflow/tree/black from `python -m black` 21.12b0 also at 9ebfeab

---

**Related**:

   - https://github.com/tensorflow/addons does a bunch of auto format and auto linting
   - My recently merged PR: #53329

**Disadvantages**:

  - Impact just about every file in the codebase
  - Require open PRs to be modified and forks to be updated

**Mitigation of disadvantages**:

  - To avoid stepping on toes, one can write a pull request scanner to avoid touching any file that has a current pull request lodged against it. I'm happy to write this script, but will also need someone to run this internally so it knows what files are being affected by non public PRs

**Advantages**:

  - Easier for automated tooling to make automated fixes (whitespace / prettyprinting issues don't get in the way)
  - More consistent codebase (which is somewhat of a quality metric)"
53358,Batched sparse2sparse (CSRSparseMatrix) multiplication error ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): preinstalled in Colab
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.12
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.1/7.6.5
- GPU model and memory: n/a

**Describe the current behavior**
When multiplying 2 sparse 3D tensors (batched matrices x single 2D matrix) of type tensorflow.python.ops.linalg.sparse.sparse_csr_matrix_ops.CSRSparseMatrix, with the function tensorflow.python.ops.linalg.sparse.sparse_csr_matrix_ops.matmul on a GPU, if batch dimension of the first tensor is bigger than 16, we get: 

```
InternalError: tensorflow/core/util/cuda_sparse.cc:866 (cusparseXcsrgemm2Nnz( *gpusparse_handle_, m, n, k, descrA, nnzA, csrSortedRowPtrA, csrSortedColIndA, descrB, nnzB, csrSortedRowPtrB, csrSortedColIndB, descrA, 0, null_ptr<int>(), null_ptr<int>(), descrC, csrSortedRowPtrC, nnzTotalDevHostPtr, info, workspace)): cuSparse call failed with status CUSPARSE_STATUS_EXECUTION_FAILED [Op:SparseMatrixSparseMatMul]
```

**Describe the expected behavior**
The tensorflow.python.ops.linalg.sparse.sparse_csr_matrix_ops.matmul function should support matrix multiplication of arbitrary batch dimension, as long there is enough GPU memory. If the problem is the GPU memory, the error message should be more descriptive.  

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): n/a

**Standalone code to reproduce the issue**
Here is a link to colab: 
https://colab.research.google.com/drive/1Abwf6esrNTWW_YmdZpWgnmEuCQ20umfA?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-8-80d3c194e0ae> in <module>()
      3 c = sparse_csr_matrix_ops.matmul(
      4     sparse_csr_matrix_ops.CSRSparseMatrix(a),
----> 5     sparse_csr_matrix_ops.CSRSparseMatrix(b))
      6 c = c.to_sparse_tensor()
      7 a.shape, b.shape, c.shape

2 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/sparse/sparse_csr_matrix_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, name)
    204           adjoint_a=adjoint_a,
    205           adjoint_b=adjoint_b,
--> 206           type=a.dtype)
    207 
    208       # In eager mode, shape inference functions are not called, and the output

/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py in sparse_matrix_sparse_mat_mul(a, b, type, transpose_a, transpose_b, adjoint_a, adjoint_b, name)
   1095       return _result
   1096     except _core._NotOkStatusException as e:
-> 1097       _ops.raise_from_not_ok_status(e, name)
   1098     except _core._FallbackException:
   1099       pass

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   7105 def raise_from_not_ok_status(e, name):
   7106   e.message += ("" name: "" + name if name is not None else """")
-> 7107   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   7108 
   7109 

InternalError: tensorflow/core/util/cuda_sparse.cc:866 (cusparseXcsrgemm2Nnz( *gpusparse_handle_, m, n, k, descrA, nnzA, csrSortedRowPtrA, csrSortedColIndA, descrB, nnzB, csrSortedRowPtrB, csrSortedColIndB, descrA, 0, null_ptr<int>(), null_ptr<int>(), descrC, csrSortedRowPtrC, nnzTotalDevHostPtr, info, workspace)): cuSparse call failed with status CUSPARSE_STATUS_EXECUTION_FAILED [Op:SparseMatrixSparseMatMul]
SEARCH STACK OVERFLOW
"
53357,Tutorial for autoencoder might be applying denoising to the wrong input data,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/generative/autoencoder#second_example_image_denoising

## Description of issue (what needs changing):

The model was trained to remove noise using the `x_train_noisy` as input and `x_train` as target output.
Later when demonstrating the denoising capabilities `x_test` is used as input, which is the original (**not noisy**) test data. In the plot it is also presented as `original + noise` (noisy images) vs the reconstruction [*from the noisy images*]. 

If I am not missing something, right now the autoencoder is used to denoise the already **not noisy** images instead of denoising the **noisy** images which are also shown as a comparison besides the reconstructed images.

### Clear description

Use the autoencoder to denoise the actually noisy images, in the same way as it is presented below and the same way as the model is trained and validated before.

Change this
```python
encoded_imgs = autoencoder.encoder(x_test).numpy()
decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()
```
to this
```python
encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()
decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()
```"
53356,Passing empty tensors to TFLite converted signatures fails with an exception,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.8.0-dev20211203 and 2.7 are both affected

### 2. Code

```python
import tensorflow as tf

class TestModel(tf.keras.models.Model):
  @tf.function
  def test(self, x):
    return x

test_model = TestModel()
signatures = [test_model.test.get_concrete_function(tf.TensorSpec([None], tf.float32))]

converter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# This raises ""ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first""
result = interpreter.get_signature_runner()(x=tf.zeros([0], tf.float32))
```

### 3. Failure after conversion
Conversion seems to work fine, but trying to run the signature fails with the following exception if an empty tensor is passed.

`ValueError: Cannot set tensor: Tensor is unallocated. Try calling allocate_tensors() first`

Despite the message, calling `allocate_tensors()` before has no effect. Passing a tensor that is not empty does not cause the failure.

While passing an empty tensor might seem absurd at first, I'm hitting this issue in much more subtle scenarios like passing shapes of tensors (it fails if the shape is for a scalar because it's empty), or when using ragged tensors where one of its internal row splits happens to be empty.

Note that returning empty tensors from within a TFLite converted function also fails. In that case you get this other exception:

`ValueError: Invalid tensor size.`

A workaround would be much appreciated if possible, though it seems unlikely without a proper fix."
53355,tf.random.uniform can't generate random tensor of type tf.int32,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.10
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.7.0-rc1-69-gc256c071bb2 2.7.0

**Describe the current behavior**

tf.random.uniform can generate random tensor of type tf.float32 with distinct minvals and maxvals, but can't do it for tf.int32.

**Describe the expected behavior**

tf.random.uniform should generate random tensor for both tf.float32 and tf.int32 with distinct minvals and maxvals.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3
import tensorflow as tf;
# generate random tensor of tf.float32 is OK
a = tf.random.uniform(minval = (1,2,3,4), maxval = (3,4,5,6), shape = (4,), dtype = tf.float32);
# generate random tensor of tf.int32 is not OK
a = tf.random.uniform(minval = (1,2,3,4), maxval = (3,4,5,6), shape = (4,), dtype = tf.int32);
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: minval must be 0-D, got shape [4] [Op:RandomUniformInt]
```"
53354,[Help] how to fix i tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] none of the mlir optimization passes are enabled (registered 2),"i used tf-cpu 2.6.0, python 3.9 run program in vscode env for conda find fault 
**i tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] none of the mlir optimization passes are enabled (registered 2)**

how this fix ??
chatbot with LSTM
thanks"
53353,How to build static library libtensorflowlite.a for arm?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
53352,Abnormally long loading/calling time for Tensorflow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version:CUDA: 11.1 / 8.1.4
- GPU model and memory: RTX 3080, 10GB x 2

**Describe the current behavior**
We've found that it takes an unexpectedly long loading time when calling Tensorflow or machine learning models. Once the library/model is loaded, it takes significantly less time afterwards. However, whenever we are restarting the kernel we run into the exact same problem. 

**Standalone code to reproduce the issue**
The reproducible code is provided below:
```
import tensorflow as tf
import time

print(f""TensorFlow version: {tf.__version__}"")

start = time.time()
print(tf.reduce_sum(tf.random.normal([1000, 1000])))
end = time.time()

print(f""it took = {end - start} seconds"")
```
Below is the result of the above code block:
```
TensorFlow version: 2.4.1
tf.Tensor(973.3261, shape=(), dtype=float32)
it took = 273.7537660598755 seconds
```
As you can see it takes way too much time for such a simple task. This issue applies the same when we are trying to call functions such as `model.fit` or `model.summary()`. The reproducible code below is another example of the problem. We used a simple CNN model with the cifar10 dataset. It took extensive time to just load the model for some reason. Additionally, when we are calling the `fit` function it takes forever time to actually run Epochs.
```
import random
import numpy as np

import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import time

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

X_train = X_train.reshape(50000, 32, 32, 3).astype(""float32"") / 255
X_test = X_test.reshape(10000, 32, 32, 3).astype(""float32"") / 255

def CNN():
    input_layer = keras.Input(shape=(32,32,3))
    conv = keras.layers.Conv2D(32, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=1))(input_layer)
    max_pool = keras.layers.MaxPooling2D((2, 2))(conv)
    flatten = keras.layers.Flatten()(max_pool)
    dense = keras.layers.Dense(128, activation='relu', kernel_initializer = keras.initializers.HeUniform(seed=1))(flatten)
    output_layer = keras.layers.Dense(10, activation='softmax')(dense)
    model = keras.Model(inputs=input_layer, outputs=output_layer, name = 'CNN')
    model.compile(loss='categorical_crossentropy', optimizer= keras.optimizers.Adam(learning_rate=0.001),  metrics=['acc', 'AUC'])
    return model 

start = time.time()
model = CNN()
end = time.time()
print(f""it took = {end - start} seconds"")
>>> it took = 109.86318612098694 seconds

start = time.time()
model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))
end = time.time()
print(f""it took = {end - start} seconds"")
>>> it took = 809.008472442627 seconds
```

We would love to figure out what is causing such a problem and a way to fix this. We greatly appreciate your help and support in advance. "
53351,sound_classification example can not work on iOS 15.0+,"Source:
https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios

I tested it on an iOS14 device and it worked fine.

I changed several iOS15.0+ devices, but they couldn't work. The phenomenon is that the progress bar jumps randomly and the data is messy.

I am sure that this is a bug that only appears on iOS 15.0+ and will recur 100%."
53345,CTC Loss throwing errors in half/mixed precision,"Hello there :wave: 

I have been running some experiments with automatic mixed-precision using TensorFlow recently. And I encountered an issue with the CTC loss, which throws an obscure dtype incompatibility issue. And I'm not sure how to solve this :thinking: 
Any ideas?

**System information**
- Have I written custom code: yes, the code snippet
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: binary, via pip
- TensorFlow version: 2.6.0
- Python version: 3.8
- CUDA/cuDNN version: CUDA 11.4 (cuDNN 8.2.0)
- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design

**Describe the current behavior**

As of now, running the snippet further down below throws the following error:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-2c1811c7c1ed> in <module>
     13 model_output = tf.random.uniform((batch_size, out_chans, vocab_size + 1), minval=0, maxval=1, dtype=tf.float16)
     14 
---> 15 tf.nn.ctc_loss(gt, model_output, seq_len, input_length, logits_time_major=False, blank_index=vocab_size)

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    205     try:
--> 206       return target(*args, **kwargs)
    207     except (TypeError, ValueError):
    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_v3(labels, logits, label_length, logit_length, logits_time_major, unique, blank_index, name)
    960     blank_index = 0
    961 
--> 962   return ctc_loss_dense(
    963       labels=labels,
    964       logits=logits,

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_dense(labels, logits, label_length, logit_length, logits_time_major, unique, blank_index, name)
   1094       return result[0], grad
   1095 
-> 1096     return compute_ctc_loss(*args)
   1097 
   1098 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in __call__(self, *a, **k)
    307 
    308   def __call__(self, *a, **k):
--> 309     return self._d(self._f, a, k)
    310 
    311 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in decorated(wrapped, args, kwargs)
    261 
    262     if context.executing_eagerly():
--> 263       return _eager_mode_decorator(wrapped, args, kwargs)
    264     else:
    265       return _graph_mode_decorator(wrapped, args, kwargs)

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py in _eager_mode_decorator(f, args, kwargs)
    489   """"""Implement custom gradient decorator for eager mode.""""""
    490   with tape_lib.VariableWatcher() as variable_watcher:
--> 491     result, grad_fn = f(*args, **kwargs)
    492   args = nest.flatten(args)
    493   all_inputs = list(args) + list(kwargs.values())

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in compute_ctc_loss(logits_t, labels_t, label_length_t, logit_length_t, *unique_t)
   1086       if unique_t:
   1087         kwargs[""unique""] = unique_t
-> 1088       result = ctc_loss_and_grad(**kwargs)
   1089       def grad(grad_loss):
   1090         grad = [array_ops.reshape(grad_loss, [1, -1, 1]) * result[1]]

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in ctc_loss_and_grad(logits, labels, label_length, logit_length, unique)
    689 
    690   ilabel_log_probs = nn_ops.log_softmax(logits)
--> 691   state_log_probs = _ilabel_to_state(labels, num_labels, ilabel_log_probs)
    692   state_trans_probs = _ctc_state_trans(labels)
    693   initial_state_log_probs, final_state_log_probs = ctc_state_log_probs(

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py in _ilabel_to_state(labels, num_labels, ilabel_log_probs)
    591   one_hot = array_ops.expand_dims(one_hot, axis=0)
    592   ilabel_log_probs = array_ops.expand_dims(ilabel_log_probs, axis=2)
--> 593   state_log_probs = math_ops.reduce_sum(ilabel_log_probs * one_hot, axis=3)
    594   state_log_probs = array_ops.concat([state_log_probs, blank], axis=2)
    595   return array_ops.pad(

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
   1365         #   r_binary_op_wrapper use different force_same_dtype values.
   1366         x, y = maybe_promote_tensors(x, y, force_same_dtype=False)
-> 1367         return func(x, y, name=name)
   1368       except (TypeError, ValueError) as e:
   1369         # Even if dispatching the op failed, the RHS may be a tensor aware

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)
   1708     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)
   1709   else:
-> 1710     return multiply(x, y, name=name)
   1711 
   1712 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    205     try:
--> 206       return target(*args, **kwargs)
    207     except (TypeError, ValueError):
    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)
    528   """"""
    529 
--> 530   return gen_math_ops.mul(x, y, name)
    531 
    532 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)
   6234       return _result
   6235     except _core._NotOkStatusException as e:
-> 6236       _ops.raise_from_not_ok_status(e, name)
   6237     except _core._FallbackException:
   6238       pass

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6939   message = e.message + ("" name: "" + name if name is not None else """")
   6940   # pylint: disable=protected-access
-> 6941   six.raise_from(core._status_to_exception(e.code, message), None)
   6942   # pylint: enable=protected-access
   6943 

~/miniconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Mul]
```

**Describe the expected behavior**

the snippet running smoothly

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.keras import mixed_precision

mixed_precision.set_global_policy('mixed_float16')

batch_size = 2
out_chans = 32
vocab_size = 10

gt = tf.zeros((batch_size, 5), dtype=tf.int32)
seq_len = [3, 4]
input_length = tf.fill((batch_size,), out_chans)
model_output = tf.random.uniform((batch_size, out_chans, vocab_size + 1), minval=0, maxval=1, dtype=tf.float16)

tf.nn.ctc_loss(gt, model_output, seq_len, input_length, logits_time_major=False, blank_index=vocab_size)

```"
53344,Debugger-V2 crashes with KeyError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Docker image based on Ubuntu 18.04 Bionic
- TensorFlow installed from (source or binary): built from patched source code
- TensorFlow version (use command below): `v2.1.3-0-g77f47d6ed6c 2.1.3`
- Python version: `3.6.9`
- Bazel version (if compiling from source): `4.2.1`
- GCC/Compiler version (if compiling from source): `g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0`
- CUDA/cuDNN version:  `libcudnn8/unstable 8.0.5.39-1+cuda11.1 amd64`
- GPU model and memory: Nvidia GeForce RTX 2080

**Describe the current behavior**

I am trying to set up Debugger-V2. My project currently uses Tensorflow 2.1.3 and, as far as I understand, the corresponding Tensorboard version does not support Debugger-V2. Therefore I have set up a separate Conda environment with Tensorboard 2.7.0, where I load logs written using Tensorflow 2.1.3. The ""system information"" block above provides details on the environment in which the rest of my project runs.

In this setup, the Debugger-V2 UI comes up empty (e.g. it says `Graph Executions (0)` in the top right corner), and there is the following traceback in stdout:

```
> tensorboard --logdir tensorboard_logs/train/ --port 6010
2021-12-07 18:07:08.627387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0

NOTE: Using experimental fast data loading logic. To disable, pass
    ""--load_fast=false"" and report issues on GitHub. More details:
    https://github.com/tensorflow/tensorboard/issues/4784

Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.7.0 at http://localhost:6010/ (Press CTRL+C to quit)
Exception in thread Thread-5:
Traceback (most recent call last):
  File ""<redacted>/.miniconda3/envs/dl-tensorflow/lib/python3.9/threading.py"", line 954, in _bootstrap_inner
    self.run()
  File ""<redacted>/.miniconda3/envs/dl-tensorflow/lib/python3.9/threading.py"", line 892, in run
    self._target(*self._args, **self._kwargs)
  File ""<redacted>/.miniconda3/envs/dl-tensorflow/lib/python3.9/site-packages/tensorboard/plugins/debugger_v2/debug_data_multiplexer.py"", line 62, in _run_repeatedly
    target()
  File ""<redacted>/.miniconda3/envs/dl-tensorflow/lib/python3.9/site-packages/tensorflow/python/debug/lib/debug_events_reader.py"", line 1116, in update
    self._load_graphs()
  File ""<redacted>/.miniconda3/envs/dl-tensorflow/lib/python3.9/site-packages/tensorflow/python/debug/lib/debug_events_reader.py"", line 1013, in _load_graphs
    debugged_graph = self._graph_by_id[op_creation_proto.graph_id]
KeyError: '0410a4d0-d88b-44f9-b7f5-a49008436b15'
```

The error comes from [this line](https://github.com/tensorflow/tensorflow/blob/d1c4acc743e6f5710e38595ea0c6c876205e8399/tensorflow/python/debug/lib/debug_events_reader.py#L1009).

The directory structure of `tensorboard_logs` is as follows:

```
tensorboard_logs/
└── train
    ├── events.out.tfevents.1638889539.<redacted>.118102.192768.v2
    ├── tfdbg_events.1638889418.<redacted>.execution
    ├── tfdbg_events.1638889418.<redacted>.graph_execution_traces
    ├── tfdbg_events.1638889418.<redacted>.graphs
    ├── tfdbg_events.1638889418.<redacted>.metadata
    ├── tfdbg_events.1638889418.<redacted>.source_files
    └── tfdbg_events.1638889418.<redacted>.stack_frames
```

and I set up debug logging with the following line:

```
        tf.debugging.experimental.enable_dump_debug_info(
            os.path.join(<tensorboard_logs>, 'train'),
            tensor_debug_mode='FULL_TENSOR',  # b/c TF 2.1 doesn't support FULL_HEALTH
            circular_buffer_size=10000)
```

I won't be able to share the log files that cause this crash, and I don't have a minimal reproducing example (yet?). Hopefully this is debuggable without the two.

Aside from the obvious problem with this crash, I am also not sure where I should put debug info (what the first parameter of `enable_dump_debug_info` should be). My logging is done via Keras, and the typical directory structure is as follows:

```
tensorboard_logs/
└── train
└── validation
```

I see 3 places where I could put debug info:

```
tensorboard_logs/  # <- here
└── train  # <- here
└── validation
└── debug_info  # <- here
```

and then I could point Tensorboard to any of those directories. Which is the preferred way?"
53343,Could not load dynamic library 'libcusparse.so.11,"I am facing issue while trying to use the GPU with tensorflow :-

flow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-12-06 18:16:03.990545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-12-06 18:16:03.990614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-12-06 18:16:04.041156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-12-06 18:16:04.048538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-12-06 18:16:04.127700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-12-06 18:16:04.127970: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib
2021-12-06 18:16:04.129752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-12-06 18:16:04.129778: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.

I installed CUDA from https://www.tensorflow.org/install/gpu for my tensorflow version (CUDA 11)

Tensorflow version : 2.4.1
OS : Ubuntu 18.04
CUDA : 11
cudnn : 8

Any tips or leads will be helpful.

Thanks in advance
"
53342,Program of odd and even in third place ,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
![IMG20211207164046](https://user-images.githubusercontent.com/85215308/145018910-6ae35a91-b3cf-4c02-baa6-d1d87a2c6a3b.jpg)




"
53341,failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6.13
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.0.130/7.4.2
- GPU model and memory: GTX 2080Ti  12GB
 Output of nvidia-smi：
`ubuntu@init:~$ nvidia-smi
Tue Dec  7 10:01:07 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |
| 30%   29C    P8    16W / 250W |     41MiB / 11016MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
`

**Describe the problem**

After I installed tensorflow-gpu using pip，I tested it in terminal，but it seems gpu is not available

`(tfgpu114) ubuntu@init:~$ python
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
[GCC 7.5.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>> tensorflow.test.is_gpu_available()
2021-12-07 09:01:22.849145: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-12-07 09:01:22.888771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2021-12-07 09:01:22.949108: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-12-07 09:01:22.949204: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: init
2021-12-07 09:01:22.949225: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: init
2021-12-07 09:01:22.949349: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.86.0
2021-12-07 09:01:22.949407: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.86.0
2021-12-07 09:01:22.949428: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.86.0
2021-12-07 09:01:22.960938: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500025000 Hz
2021-12-07 09:01:22.964071: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed09dbc440 executing computations on platform Host. Devices:
2021-12-07 09:01:22.964121: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
False
>>> 
`

Does anyone meet this condition，please tell me a solution，thank you！"
53339,TensorFlow TextVectorization producing Ragged Tensor with no padding after loading it from pickle.,"This is my TextVectorization layer:

```
strip_chars = string.punctuation + '¿'
strip_chars = strip_chars.replace('[', '')
strip_chars = strip_chars.replace(']', '')

vocab_size = 15000
sequence_length = 20
batch_size = 64

def custom_standardization(input_string):
  lowercase = tf.strings.lower(input_string)
  return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(strip_chars), '')

eng_vectorization = TextVectorization(max_tokens = vocab_size,
                                      output_mode = 'int',
                                      output_sequence_length = sequence_length)

spa_vectorization = TextVectorization(max_tokens = vocab_size,
                                      output_mode = 'int',
                                      output_sequence_length = sequence_length + 1,
                                      standardize = custom_standardization)

train_eng_texts = [pair[0] for pair in train_pairs]
train_spa_texts = [pair[1] for pair in train_pairs]

eng_vectorization.adapt(train_eng_texts)
spa_vectorization.adapt(train_spa_texts)
```

I have saved it using:

```
pickle.dump({'config': eng_vectorization.get_config(), 'weights': eng_vectorization.get_weights()},
             open(""english_vocab.pkl"", ""wb""))
```

But after loading it again:

```
from_disk = pickle.load(open(""english_vocab.pkl"", ""rb""))
new_eng = TextVectorization.from_config(from_disk['config'])
new_eng.adapt(tf.data.Dataset.from_tensor_slices([""xyz""]))
new_eng.set_weights(from_disk['weights'])
```

It is not behaving as the original one. It is outputting `RaggedTensor`, how to resolve this? 
Here is the link to my Google Colab - https://colab.research.google.com/drive/1rEkPEnG1odsEObzQrRFVVll7L1Kz33ud?usp=sharing"
53337,Issue created for Rollback of PR #52543: Add GPU implementation for Range op,"Merged PR #52543 is rolled back in f50654f7030c8952b9598a7f930daeb41a00bec5.
    Please follow up with the reviewer and close this issue once its resolved."
53336,Can I repeat a 1-D tensor in segment form?,"According to the doc, TF has: tf.segment_xxx and tf.repeat. But can I repeat a 1-D tensor in segment form?

#### Example:
segment = [0 0 0 1 1]
value = [0 1 2 3 4]

#### [ 1 ] If repeat_cnt = [2 2 2 1 1]
With tf.repeat, I can easily repeat value to: [0 0 1 1 2 2 3 4]

#### [ 2 ] But I want a segment repeat, which means: segment-0 repeat 2 times, segment-1 repeat 1 time.
That is to say, if repeat_cnt_new = [2 1], and I want to repeat value to: [0 1 2 0 1 2 3 4].

#### I mean, repeat blocks in a vector: [0 1 2] * 2 + [3 4] * 1. Are there any TF func or api can help?
Thanks!"
53335,Getting [java.lang.IllegalArgumentException: Internal error: Error applying delegate] error while applying NNAPI Delegate,"**Describe the current behavior**

I am using the tensorflow [model_personalization ](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization) example and I have modified the android application code ([LiteMultipleSignatureModel.java](https://github.com/tensorflow/examples/blob/master/lite/examples/model_personalization/android/transfer_api/src/main/java/org/tensorflow/lite/examples/transfer/api/LiteMultipleSignatureModel.java)) to run the TF Lite interpreter on NNAPI delegate in the following manner. 

```
Interpreter.Options options = (new Interpreter.Options());
NnApiDelegate nnApiDelegate = null;
if(Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {
  nnApiDelegate = new NnApiDelegate();
  options.addDelegate(nnApiDelegate);
}
this.interpreter = new Interpreter(tfLiteModel, options);

```

I am getting the following error on initializing the interpreter with the NNAPI delegate option : 

```
12-07 15:35:51.585 11444 11444 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Error applying delegate: 
12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:93)
12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)
12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:44)
12-07 15:35:51.585 11444 11444 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)

```

I suspect that this error is arising due to the ""train"" signature of the tfliteModel which uses the [optimizer.apply_gradients](https://github.com/tensorflow/examples/blob/1e624527d2fea0333894156ad7a59d8d455b2c73/lite/examples/model_personalization/transfer_learning/generate_training_model.py#L96) function to update the model weights, as on creating a different tfliteModel with  this particular line commented, the above error is not observed.  

**Describe the expected behavior**

Is there any way to add NNAPI delegate to the TF Lite interpreter for this trainable model?"
53334,"c++ tensorflow api inference .pb model is too slower, What can I do to speed it up ?","<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10  
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary):  tensorflow c++ api 
- TensorFlow version (use command below):  tensorflow 1.15 c++ api

**Describe the current behavior**

I compiled the api of tensorflow1.15_cpu version c++ and successfully called my pb model on the windows system. The problem now is that the speed of inference cannot meet my requirements.
I use deeplabv3+ (a model for semantic segmentation of graphics), which was trained on an Ubuntu system computer. GPU acceleration is used in the training process of this model.
The configuration of the Ubuntu system is as follows
-GPU: GeFore RTX 2080
-Memory: 7979MiB
The size of the pb model is 175MB. I call the pb model for inference on windows. I tested the time of seesion->run. It takes 3.5 seconds to infer a photo. This is too expensive. What can I do to shorten this time.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
`
void cvMat2tfTensor(cv::Mat input, tensorflow::Tensor& outputTensor)   
{
	auto outputTensorMapped = outputTensor.tensor<unsigned char, 4>();
	input.convertTo(input, CV_32FC3);
	cv::resize(input, input, cv::Size(1024, 1448));              

	int height = input.size().height;
	int width = input.size().width;
	int depth = input.channels();

	const float* data = (float*)input.data;
	for (int y = 0; y < height; ++y)
	{
		const float* dataRow = data + (y * width * depth);
		for (int x = 0; x < width; ++x)
		{
			const float* dataPixel = dataRow + (x * depth);
			for (int c = 0; c < depth; ++c)
			{
				const float* dataValue = dataPixel + c;
				outputTensorMapped(0, y, x, c) = *dataValue;
			}
		}
	}
}
`
`
int main()
{
	const std::string model_path = ""D:/sunquan/home/home/TensorflowTest/tensorflowTest/frozen_inference_graph_wuhan.pb"";	
	const std::string image_path = ""D:/sunquan/home/home/TensorflowTest/segmantation/*.jpg"";		// inference  image
	const std::string input_name = ""ImageTensor"";						  // input name
	const std::string output_name = ""SemanticPredictions"";				 // outpur name

	vector<String> image_files;   //inference image
	glob(image_path, image_files);  

	GraphDef graph_def;
	TF_CHECK_OK(ReadBinaryProto(Env::Default(), model_path, &graph_def));
	//	intial  tensorflow session
	std::unique_ptr<tensorflow::Session> session(tensorflow::NewSession(tensorflow::SessionOptions()));
	TF_CHECK_OK(session->Create(graph_def));

	for (int i = 0; i < image_files.size(); i++)
	{
		cv::Mat rgbImage = read_image(image_files[i]); 
		int channels = rgbImage.channels();
		tensorflow::Tensor input_tensor(DT_UINT8, TensorShape({ 1, 1448, 1024, channels })); 	
		cvMat2tfTensor(rgbImage, input_tensor); 
		std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = { { input_name, input_tensor } };
		std::vector<tensorflow::Tensor> outputs;
		clock_t start = clock();
		session->Run(inputs, { output_name }, {}, &outputs);
		clock_t end = clock();
		double endtime = (double)(end - start) / CLOCKS_PER_SEC;
		cout << "" The cost of time is : "" << endtime << "" S "" << endl;			
	}
	system("" pause "");
	return 0;
}
`
![image](https://user-images.githubusercontent.com/48703273/144982916-e6c16652-ba7a-49ef-9064-b5ebdf854bdd.png)

As you can see, except for the first photo that takes 8s to load the model, each of the remaining photos takes about 3.5s, which is too expensive
"
53333,"op requires element_shape to be static during TF Lite transformation pass <unknown>:0: note: loc(""StatefulPartitionedCall"")","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Macbook Pro 
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):  version 2.7.0

### 2. Code

`from tensorflow import lite
converter = lite.TFLiteConverter.from_keras_model(h_model)

tfmodel = converter.convert()
open('model.tflite', 'wb').write(tfmodel)`


### 3. Failure after conversion

ConverterError: /Users/shahadsulaiman/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1315:0: error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
/Users/shahadsulaiman/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1315:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n converter._experimental_lower_tensor_list_ops = False



### 4. (optional) RNN conversion support
`
inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)

inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)

decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

h_model = Model(inputs=[inputs1, inputs2], outputs=outputs)`"
53332,can't execute cc1plus,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:None
- TensorFlow installed from (source or binary):source
- TensorFlow version:master
- Python version:3.8.8
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):4.2.1
- GCC/Compiler version (if compiling from source):9.4.0
- CUDA/cuDNN version:None
- GPU model and memory:None

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://user-images.githubusercontent.com/31653717/145007716-676e1e62-9cce-4ff9-aeb6-2e28f400aaf3.png)
"
53326,Support multiple validation sets in Model.fit ,"Moving this feature from TF repository https://github.com/tensorflow/tensorflow/issues/38803

Are you willing to contribute it (Yes/No): No
Describe the feature and the current behavior/state.
Currently there's no way to use multiple validation sets with independent tracking of metrics.

Will this change the current api? How?

Simplest way I can think of is to accept a list of datasets in the validation_data parameter in Model.fit. Ideally there should also be a way to specify the name of each set so that the logs indicate what set each validation step corresponds to.

Who will benefit with this feature?

Anyone training with multiple validation sets.
"
53324,"Build from source failing for TF 2.7.0 (urgent, its been 18 days and I didn't get any response to my first post)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.9 (Nitrogen)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0 (also tried 2.6.0, 2.5.0, 2.3.0)
- Python version: 3.8
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.7.2 
- GCC/Compiler version (if compiling from source): 7.4.0 (also tried 7.3.0, 5.3.0, 6.3.0)
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080ti



**Describe the problem**
This is really urgent and I am reposting my previous issue that got no responses from anyone. If anyone could point me towards the right direction it would be very helpful!

I've been trying to install TensorFlow from source for about a month on a cluster in which the native GCC version is old (4.8.0). If I don't build my own GCC version, the installation fails every time because of this issue  #38718 . I've been using this [guide](https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445) to install the TensorFlow from source, but while I don't face the earlier error, the compilation **always** fails with some error related to Eigen, regardless of whichever GCC or TensorFlow version I use.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm BAZEL_CXXOPTS=-std=c++17 ../bazel-3.7.2/output/bazel build  //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures --config=nonccl --config=opt 

**Any other info / logs**

`ERROR: /global/home/groups/co_noneq/deepmd/tensorflow/tensorflow/core/kernels/BUILD:3535:18: C++ compilation of rule '//tensorflow/core/kernels:cwise_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /global/scratch/users/ansingh/_bazel_ansingh/ccb34797bcab4c95cac85d6d3e6f44f2/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/global/software/sl-7.x86_64/modules/langs/cuda/10.2 \
    GCC_HOST_COMPILER_PATH=/global/home/groups/co_noneq/deepmd/gcc/bin/gcc \
    LD_LIBRARY_PATH=/global/home/groups/co_noneq/deepmd/binutils/lib:/global/home/groups/co_noneq/deepmd/gcc/lib64:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/lib64/stubs:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/lib64:/global/home/users/ansingh/openmm/lib:/global/home/users/ansingh/openmm/lib \
    PATH=/global/home/groups/co_noneq/deepmd/binutils/bin:/global/home/groups/co_noneq/deepmd/gcc/bin:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/bin:/global/home/users/ansingh/miniconda3/bin:/global/home/users/ansingh/miniconda3/condabin:/global/software/sl-7.x86_64/modules/tools/sq/0.1.0/bin:/global/software/sl-7.x86_64/modules/tools/emacs/25.1/bin:/global/software/sl-7.x86_64/modules/tools/vim/7.4/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/global/home/groups/allhands/bin:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate/scripts:/global/home/users/ansingh/bin:/global/home/groups/allhands/bin:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate/scripts \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/global/home/users/ansingh/miniconda3/bin/python3 \
    PYTHON_LIB_PATH=/global/home/users/ansingh/miniconda3/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \
    TF_CUDA_PATHS=/global/software/sl-7.x86_64/modules/cuda/10.1/cudnn/7.6,/global/software/sl-7.x86_64/modules/langs/cuda/10.2 \
    TF_CUDA_VERSION=10 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.o' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' '-DHAVE_STRERROR_R=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_DEREGISTER_FRAME=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_POSIX_FALLOCATE=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -iquote external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/DiscRalPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/LmhloPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/MhloPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/transforms_pass_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/DLTIBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/legalize_to_standard_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lower_complex_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/disc_ral_ops_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmNeonIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/InstCombineTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/X86VectorIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAttrUtilsGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/EmitCAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/EmitCOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorAttrDefsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCodeGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCommonTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXInfo -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXUtilsAndDesc -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmNeonConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/X86VectorConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CodeGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CommonTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86Info -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86UtilsAndDesc -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86DisassemblerInternalHeaders -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem tensorflow/compiler/mlir/hlo/include -isystem bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/include -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/MathToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/MathToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/TosaToLinalg -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToLinalg -isystem external/llvm-project/mlir/lib/Conversion/TosaToSCF -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToSCF -isystem external/llvm-project/mlir/lib/Conversion/TosaToStandard -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToStandard -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/include -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/X86 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_35' '--cuda-gpu-arch=sm_35' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' -DMLIR_GENERATED_CPU_KERNELS_ENABLED -DMLIR_GENERATED_GPU_KERNELS_ENABLED -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/cwise_op_gpu_zeta.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h: In member function 'std::size_t std::hash<float>::operator()(float) const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:22: error: expected ')' before numeric constant
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
                      ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:15: error: invalid operands of types 'float' and 'double(const char*) throw ()' to binary 'operator!='
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
         ~~~~~~^~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:64: error: expected ')' before ';' token
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h: In member function 'std::size_t std::hash<double>::operator()(double) const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:22: error: expected ')' before numeric constant
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
                      ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:15: error: invalid operands of types 'double' and 'double(const char*) throw ()' to binary 'operator!='
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
         ~~~~~~^~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:63: error: expected ')' before ';' token
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: At global scope:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:35: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                   ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:32: error: could not convert 'nan' from 'double(const char*) throw ()' to 'float'
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                               ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:57: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                                         ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:54: error: could not convert 'nan' from 'double(const char*) throw ()' to 'float'
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                                     ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:36: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                    ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:33: error: could not convert 'nan' from 'double(const char*) throw ()' to 'double'
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                ~^~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:58: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                                          ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:55: error: could not convert 'nan' from 'double(const char*) throw ()' to 'double'
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                                      ~^~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:41: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
                                         ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:38: error: could not convert 'nan' from 'double(const char*) throw ()' to 'long double'
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
                                     ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1360:11: error: expected ')' before numeric constant
      long double __i = 0.0L)
           ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1360:8: error: could not convert 'nan' from 'double(const char*) throw ()' to 'long double'
      long double __i = 0.0L)
       ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<float> std::literals::complex_literals::operator""""if(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:28: error: no matching function for call to 'std::complex<float>::complex(<brace-enclosed initializer list>)'
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                            ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<long double>&)
   complex<float>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<double>&)
   complex<float>::complex(const complex<double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note: candidate: constexpr std::complex<float>::complex(float, float)
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note: candidate: constexpr std::complex<float>::complex(std::complex<float>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(const std::complex<float>&)
     struct complex<float>
                   ^~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(std::complex<float>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:26: error: expected primary-expression before '{' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                          ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:26: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:31: error: expected ')' before numeric constant
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                               ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:63: error: expected ';' before '}' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<float> std::literals::complex_literals::operator""""if(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:28: error: no matching function for call to 'std::complex<float>::complex(<brace-enclosed initializer list>)'
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                            ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<long double>&)
   complex<float>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<double>&)
   complex<float>::complex(const complex<double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note: candidate: constexpr std::complex<float>::complex(float, float)
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note: candidate: constexpr std::complex<float>::complex(std::complex<float>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(const std::complex<float>&)
     struct complex<float>
                   ^~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(std::complex<float>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:26: error: expected primary-expression before '{' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                          ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:26: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:31: error: expected ')' before numeric constant
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                               ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:63: error: expected ';' before '}' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<double> std::literals::complex_literals::operator""""i(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:29: error: no matching function for call to 'std::complex<double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                             ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<long double>&)
   complex<double>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note: candidate: constexpr std::complex<double>::complex(double, double)
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note: candidate: constexpr std::complex<double>::complex(std::complex<double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(const std::complex<double>&)
     struct complex<double>
                   ^~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(std::complex<double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:27: error: expected primary-expression before '{' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:27: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:32: error: expected ')' before numeric constant
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:64: error: expected ';' before '}' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<double> std::literals::complex_literals::operator""""i(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:29: error: no matching function for call to 'std::complex<double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                             ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<long double>&)
   complex<double>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note: candidate: constexpr std::complex<double>::complex(double, double)
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note: candidate: constexpr std::complex<double>::complex(std::complex<double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(const std::complex<double>&)
     struct complex<double>
                   ^~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(std::complex<double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:27: error: expected primary-expression before '{' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:27: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:32: error: expected ')' before numeric constant
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:64: error: expected ';' before '}' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<long double> std::literals::complex_literals::operator""""il(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:34: error: no matching function for call to 'std::complex<long double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<long double>{0.0L, __num}; }
                                  ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<double>&)
       _GLIBCXX_CONSTEXPR complex(const complex<double>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note: candidate: constexpr std::complex<long double>::complex(long double, long double)
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(const std::complex<long double>&)
     struct complex<long double>
                   ^~~~~~~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:32: error: expected primary-expression before '{' token
   { return std::complex<long double>{0.0L, __num}; }
                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:32: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:37: error: expected ')' before numeric constant
   { return std::complex<long double>{0.0L, __num}; }
                                     ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:48: error: expected ';' before '}' token
   { return std::complex<long double>{0.0L, __num}; }
                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<long double> std::literals::complex_literals::operator""""il(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:34: error: no matching function for call to 'std::complex<long double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                  ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<double>&)
       _GLIBCXX_CONSTEXPR complex(const complex<double>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note: candidate: constexpr std::complex<long double>::complex(long double, long double)
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(const std::complex<long double>&)
     struct complex<long double>
                   ^~~~~~~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:32: error: expected primary-expression before '{' token
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:32: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:37: error: expected ')' before numeric constant
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                     ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:75: error: expected ';' before '}' token
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits: In static member function 'static constexpr long double std::numeric_limits<long double>::denorm_min()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits:1799:65: error: expected ')' before numeric constant
       denorm_min() _GLIBCXX_USE_NOEXCEPT { return __LDBL_DENORM_MIN__; }
                                                                 ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits:1799:68: error: cannot convert 'double (*)(const char*) throw ()' to 'long double' in return
       denorm_min() _GLIBCXX_USE_NOEXCEPT { return __LDBL_DENORM_MIN__; }
                                                                    ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h: In function 'Packet Eigen::internal::psqrt_complex(const Packet&)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:80: error: expected ')' before numeric constant
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:63: error: expected primary-expression before '(' token
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:80: error: expected ')' before numeric constant
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                                   ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:94: error: expected ')' before numeric constant
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:58: error: expected primary-expression before '(' token
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                          ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:94: error: expected ')' before numeric constant
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:103: error: expected ')' before ';' token
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:103: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h: In static member function 'static Packet Eigen::internal::pchebevl<Packet, N>::run(Packet, const typename Eigen::internal::unpacket_traits<T>::type*)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:54: error: expected ')' before numeric constant
     Packet b1 = pset1<Packet>(static_cast<Scalar>(0.f));
                                                      ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:60: error: expected ')' before ';' token
     Packet b1 = pset1<Packet>(static_cast<Scalar>(0.f));
                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:60: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_i1e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:42: error: expected ')' before numeric constant
     return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);
                                          ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:64: error: expected ')' before ';' token
     return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);
                                                                ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_i1e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:395:42: error: expected ')' before numeric constant
     return pselect(pcmp_lt(x, pset1<T>(0.0)), pnegate(y), y);
                                          ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:395:63: error: expected ')' before ';' token
     return pselect(pcmp_lt(x, pset1<T>(0.0)), pnegate(y), y);
                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:501:42: error: expected ')' before numeric constant
     return pselect(
                                          ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:501:102: error: expected ')' before ';' token
     return pselect(
                                                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:576:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:576:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:746:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:746:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:904:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:904:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1068:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1068:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y0<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:74: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                                                                          ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y0<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1461:47: error: expected ')' before numeric constant
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1461:76: error: expected ')' before ';' token
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_j1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1574:46: error: expected ')' before numeric constant
     y_gt_two = pselect(
                                              ^  
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1574:82: error: expected ')' before ';' token
     y_gt_two = pselect(
                                                                                  ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_j1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1662:47: error: expected ')' before numeric constant
     y_gt_five = pselect(
                                               ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1662:84: error: expected ')' before ';' token
     y_gt_five = pselect(
                                                                                    ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);
                                              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:75: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);
                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1855:47: error: expected ')' before numeric constant
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1855:76: error: expected ')' before ';' token
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::digamma_impl_maybe_poly<float>::run(float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:153:21: error: expected ')' before numeric constant
     } else return 0.0f;
                     ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:153:24: error: cannot convert 'double (*)(const char*) throw ()' to 'float' in return
     } else return 0.0f;
                        ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::digamma_impl_maybe_poly<double>::run(double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:176:12: error: expected ')' before numeric constant
     else return 0.0;
            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:176:14: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in return
     else return 0.0;
              ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In function 'T Eigen::internal::flipsign(const T&, const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:46: error: expected ')' before numeric constant
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:50: error: wrong type argument to unary minus
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                                  ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:51: error: expected ')' before ';' token
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                                   ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::machep()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:687:60: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar machep() { assert(false && ""machep not supported for this type""); return 0.0; }
                                                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::big()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:689:57: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar big() { assert(false && ""big not supported for this type""); return 0.0; }
                                                         ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::biginv()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:691:60: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar biginv() { assert(false && ""biginv not supported for this type""); return 0.0; }
                                                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:26: error: expected ')' before numeric constant
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
                          ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:61: error: expected ')' before ';' token
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
                                                             ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1394:10: error: 'one' was not declared in this scope
         if( x == one )
          ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1394:10: note: suggested alternative: 'conj'
         if( x == one )
          ^~~
          conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1397:9: error: 'one' was not declared in this scope
         if( x < one )
         ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1397:9: note: suggested alternative: 'conj'
         if( x < one )
         ^~~
         conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1433:22: error: 'one' was not declared in this scope
         s += b*w/(x-one);
                      ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1433:22: note: suggested alternative: 'conj'
         s += b*w/(x-one);
                      ^~~
                      conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1434:12: error: expected primary-expression before '*' token
         s -= half * b;
            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:20: error: expected ')' before numeric constant
         Scalar zero = 0.0, one = 1.0;
                    ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:39: error: expected ')' before ';' token
         Scalar zero = 0.0, one = 1.0;
                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1484:20: error: 'one' was not declared in this scope
         Scalar nplus = n + one;
                    ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1484:20: note: suggested alternative: 'conj'
         Scalar nplus = n + one;
                    ^~~
                    conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::betainc_helper<float>::incbps(float, float, float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1782:9: error: expected ')' before numeric constant
     s = 0.0f;
         ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1782:12: error: cannot convert 'double(const char*) throw ()' to 'float' in assignment
     s = 0.0f;
            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1786:14: error: expected ')' before numeric constant
       if (b == 0.0f) {
              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1786:7: error: invalid operands of types 'float' and 'double(const char*) throw ()' to binary 'operator=='
       if (b == 0.0f) {
       ^~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1789:1: error: expected ')' before 'a'
       a += 1.0f;
 ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::betainc_impl<float>::run(float, float, float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1805:14: error: expected ')' before numeric constant
     if (a <= 0.0f) return nan;
              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1806:1: error: expected ')' before 'if'
     if (b <= 0.0f) return nan;
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1824:1: error: expected primary-expression before '}' token
   }
 ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::betainc_helper<double>::incbps(double, double, double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1841:9: error: expected ')' before numeric constant
     s = 0.0;
         ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1841:11: error: cannot convert 'double(const char*) throw ()' to 'double' in assignment
     s = 0.0;
           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::betainc_impl<double>::run(double, double, double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1878:16: error: expected ')' before numeric constant
     if (aa <= 0.0 || bb <= 0.0) {
                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1882:1: error: expected ')' before 'if'
     if ((xx <= 0.0) || (xx >= 1.0)) {
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1889:1: error: expected ')' before 'if'
     if ((bb * xx) <= 1.0 && xx <= 0.95) {
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1921:13: error: expected ')' before numeric constant
     if (y < 0.0) {
             ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1923:3: error: expected ')' before 'else'
     } else {
   ^ ~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h: In member function 'double std::random_device::entropy() const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h:1606:14: error: expected ')' before numeric constant
     { return 0.0; }
              ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h:1606:16: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in return
     { return 0.0; }
                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::poisson_distribution<_IntType>::result_type std::poisson_distribution<_IntType>::operator()(_UniformRandomNumberGenerator&, const std::poisson_distribution<_IntType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1319:18: error: expected ')' before numeric constant
   double __w = 0.0;
                  ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::binomial_distribution<_IntType>::result_type std::binomial_distribution<_IntType>::_M_waiting(_UniformRandomNumberGenerator&, _IntType, double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1503:20: error: expected ')' before numeric constant
  double __sum = 0.0;
                    ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::normal_distribution<_RealType>::result_type std::normal_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::normal_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::normal_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::normal_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::gamma_distribution<_RealType>::result_type std::gamma_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::gamma_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2354:19: error: expected ')' before numeric constant
      while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2354:23: error: expected ')' before ';' token
      while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2369:19: error: expected ')' before numeric constant
      while (__u == 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2369:23: error: expected ')' before ';' token
      while (__u == 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::gamma_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::gamma_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2403:19: error: expected ')' before numeric constant
     while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2403:23: error: expected ')' before ';' token
     while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2424:19: error: expected ')' before numeric constant
     while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2424:23: error: expected ')' before ';' token
     while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2435:19: error: expected ')' before numeric constant
        while (__u == 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2435:23: error: expected ')' before ';' token
        while (__u == 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::discrete_distribution<_IntType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2652:78: error: expected ')' before numeric constant
       const double __sum = std::accumulate(_M_prob.begin(),
                                                                              ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2652:82: error: expected ')' before ';' token
       const double __sum = std::accumulate(_M_prob.begin(),
                                                                                  ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_constant_distribution<_RealType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2805:76: error: expected ')' before numeric constant
       const double __sum = std::accumulate(_M_den.begin(),
                                                                            ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2805:80: error: expected ')' before ';' token
       const double __sum = std::accumulate(_M_den.begin(),
                                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::piecewise_constant_distribution<_RealType>::result_type std::piecewise_constant_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::piecewise_constant_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2902:71: error: expected ')' before numeric constant
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2902:75: error: expected ')' before ';' token
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_constant_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::piecewise_constant_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2935:71: error: expected ')' before numeric constant
      const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2935:75: error: expected ')' before ';' token
      const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_linear_distribution<_RealType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3030:20: error: expected ')' before numeric constant
       double __sum = 0.0;
                    ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::piecewise_linear_distribution<_RealType>::result_type std::piecewise_linear_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::piecewise_linear_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3125:71: error: expected ')' before numeric constant
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3125:75: error: expected ')' before ';' token
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h: In member function 'Eigen::TensorOpCost Eigen::TensorEvaluator<const Eigen::TensorPairReducerOp<ReduceOp, Dims, XprType>, Device>::costPerCoeff(bool) const':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:65: error: expected ')' before numeric constant
     const double compute_cost = 1.0 +
                                                                 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected ':' before ';' token
     const double compute_cost = 1.0 +
                                                                                                                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected primary-expression before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: At global scope:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:595:61: error: expected ')' before numeric constant
   const RealScalar m_sin_PI_div_n_LUT[32] = {
                                                             ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:595:1414: error: expected ')' before '}' token
   const RealScalar m_sin_PI_div_n_LUT[32] = {
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:631:69: error: expected ')' before numeric constant
   const RealScalar m_minus_sin_2_PI_div_n_LUT[32] = {
                                                                     ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:631:1404: error: expected ')' before '}' token
   const RealScalar m_minus_sin_2_PI_div_n_LUT[32] = {
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: In member function 'void Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::butterfly_4(Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::ComplexScalar*)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:26: error: expected primary-expression before '(' token
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                          ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:31: error: expected ')' before numeric constant
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:70: error: expected ')' before ';' token
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:70: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:26: error: expected primary-expression before '(' token
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                          ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:31: error: expected ')' before numeric constant
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:69: error: expected ')' before ';' token
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                                                                     ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:69: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: In member function 'void Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::butterfly_1D_merge(Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::ComplexScalar*, Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::Index, Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::Index)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:515:28: error: expected ')' before numeric constant
     ComplexScalar w(1.0, 0.0);
                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:515:32: error: expected ')' before ';' token
     ComplexScalar w(1.0, 0.0);
                                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h: In member function 'Eigen::TensorOpCost Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::costPerCoeff(bool) const':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:64: error: expected primary-expression before '(' token
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:82: error: expected ')' before numeric constant
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                                  ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:251: error: expected ')' before ';' token
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                                                                                                                                                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:251: error: expected ')' before ';' token
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xlogy_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:696:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:699:1: error: expected ')' before 'return'
     return x * numext::log(y);
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:699:26: error: expected ')' before ';' token
     return x * numext::log(y);
                          ^
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xlog1py_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:727:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:730:1: error: expected ')' before 'return'
     return x * numext::log1p(y);
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:730:28: error: expected ')' before ';' token
     return x * numext::log1p(y);
                            ^
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xdivy_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:762:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:765:1: error: expected ')' before 'return'
     return x / y;
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:765:13: error: expected ')' before ';' token
     return x / y;
             ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar) [with Scalar = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1985:114:   required from 'typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::zeta(const Scalar&, const Scalar&) [with Scalar = float; typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsBFloat16.h:22:86:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:14: error: cannot convert 'double (*)(const char*) throw ()' to 'const float' in initialization
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
              ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar) [with Scalar = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1991:119:   required from 'typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::polygamma(const Scalar&, const Scalar&) [with Scalar = float; typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsBFloat16.h:25:91:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:8: error: cannot convert 'double (*)(const char*) throw ()' to 'float' in initialization
         Scalar zero = 0.0, one = 1.0;
        ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar) [with Scalar = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1985:114:   required from 'typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::zeta(const Scalar&, const Scalar&) [with Scalar = double; typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = double]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/arch/GPU/SpecialFunctions.h:60:34:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:14: error: cannot convert 'double (*)(const char*) throw ()' to 'const double' in initialization
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
              ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar) [with Scalar = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1991:119:   required from 'typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::polygamma(const Scalar&, const Scalar&) [with Scalar = double; typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = double]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/arch/GPU/SpecialFunctions.h:74:39:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:8: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in initialization
         Scalar zero = 0.0, one = 1.0;
        ^~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 866.600s, Critical Path: 166.89s
INFO: 11958 processes: 2116 internal, 9842 local.
FAILED: Build did NOT complete successfully`
"
53322,Unit test //tensorflow/python/ops/ragged:ragged_dispatch_test fails on AARCH64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.8.10
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 11.2.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails with the following error
FAIL: testRaggedDispatch18 (op=<function unsorted_segment_sqrt_n at 0xffff18e41f70>, kwargs={'data': tf.RaggedTensorValue(values=array([1., 2., 3., 4., 6.]), row_splits=array([0, 2, 5])), 'segment_ids': tf.RaggedTensorValue(values=array([0, 1, 0, 0, 0]), row_splits=array([0, 2, 5])), 'num_segments': 2}, expected=[7.0, 2.0]) (_main_.RaggedDispatchTest)
RaggedDispatchTest.testRaggedDispatch18 (op=<function unsorted_segment_sqrt_n at 0xffff18e41f70>, kwargs={'data': tf.RaggedTensorValue(values=array([1., 2., 3., 4., 6.]), row_splits=array([0, 2, 5])), 'segment_ids': tf.RaggedTensorValue(values=array([0, 1, 0, 0, 0]), row_splits=array([0, 2, 5])), 'num_segments': 2}, expected=[7.0, 2.0])
testRaggedDispatch(op=<function unsorted_segment_sqrt_n at 0xffff18e41f70>, kwargs={'data': tf.RaggedTensorValue(values=array([1., 2., 3., 4., 6.]), row_splits=array([0, 2, 5])), 'segment_ids': tf.RaggedTensorValue(values=array([0, 1, 0, 0, 0]), row_splits=array([0, 2, 5])), 'num_segments': 2}, expected=[7.0, 2.0])
----------------------------------------------------------------------

Traceback (most recent call last):
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/ops/ragged/ragged_dispatch_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1407, in decorated
f(self, *args, **kwargs)
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/ops/ragged/ragged_dispatch_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
return test_method(self, **testcase_params)
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/ops/ragged/ragged_dispatch_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_dispatch_test.py"", line 892, in testRaggedDispatch
assert_fn(result, expected)
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/ops/ragged/ragged_dispatch_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1447, in decorated
return f(*args, **kwds)
File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/ops/ragged/ragged_dispatch_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3163, in assertAllEqual
np.testing.assert_array_equal(a, b, err_msg=""\n"".join(msgs))
File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 930, in assert_array_equal
assert_array_compare(operator._eq_, x, y, err_msg=err_msg,
File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
raise AssertionError(msg)
AssertionError:
Arrays are not equal

not equal where = (array([0, 1]),)
not equal lhs = array([7., 2.])
not equal rhs = array([7., 2.])
Mismatched elements: 2 / 2 (100%)
Max absolute difference: 8.8817842e-16
Max relative difference: 1.26882631e-16
x: array([7., 2.])
y: array([7., 2.])

**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): Add a tolerance value to the test to allow minor difference in value

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --verbose_failures -- //tensorflow/python/ops/ragged:ragged_dispatch_test

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53321,`tf.keras.layers.experimental.EinsumDense` gives different results depending on batch size,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.6.0 and 2.7.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

when using `tf.keras.layers.experimental.EinsumDense` layer produce different result depending on the batch size

**Describe the expected behavior**

the output of the layer should be independent of batch dimension

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
dense = tf.keras.layers.experimental.EinsumDense(
        ""bac,acd->bda"",
        output_shape=[64,4],
        bias_axes=""da""
)
x = tf.random.uniform((80,4,32))
print(dense(x)[0, :4].numpy())
print(dense(x[:1])[0, :4].numpy())
```
I've got different result in 7th-8th order:
```
[[ 0.12284548  0.2814498  -0.34291047  0.00810905]
 [ 0.23635934 -0.08506497  0.12073331  0.33535597]
 [-0.30136532  0.34854767  0.41540402  0.1328382 ]
 [-0.04340287 -0.07197566  0.17427945 -0.29642397]]

[[ 0.12284552  0.2814498  -0.34291047  0.00810904]
 [ 0.23635934 -0.08506497  0.12073331  0.33535597]
 [-0.3013654   0.3485477   0.415404    0.1328382 ]
 [-0.04340289 -0.07197568  0.17427945 -0.29642397]]
```
Having several of Einsum layers leads to accumulated error and wrong predictions

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53320,"I am having this error in my code, ""ValueError: No gradients provided for any variable:""","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are









Epoch 1/2
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Users\BLACKP~1\AppData\Local\Temp/ipykernel_15028/4197043661.py in <module>
----> 1 history = model.fit(X_tr, np.array(y_tr), batch_size=22, epochs=2, validation_split=0.1, verbose=1)

c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

ValueError: in user code:

    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\engine\training.py"", line 878, in train_function  *
        return step_function(self, iterator)
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\engine\training.py"", line 867, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\engine\training.py"", line 860, in run_step  **
        outputs = model.train_step(data)
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\engine\training.py"", line 816, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 633, in apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    File ""c:\users\blackpearl\appdata\local\programs\python\python38\lib\site-packages\keras\optimizer_v2\utils.py"", line 73, in filter_empty_gradients
        raise ValueError(f""No gradients provided for any variable: {variable}. ""

    ValueError: No gradients provided for any variable: (['embedding_2/embeddings:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'time_distributed/kernel:0', 'time_distributed/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'embedding_2/embeddings:0' shape=(12962, 20) dtype=float32>), (None, <tf.Variable 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0' shape=(20, 300) dtype=float32>), (None, <tf.Variable 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0' shape=(75, 300) dtype=float32>), (None, <tf.Variable 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0' shape=(300,) dtype=float32>), (None, <tf.Variable 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0' shape=(20, 300) dtype=float32>), (None, <tf.Variable 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0' shape=(75, 300) dtype=float32>), (None, <tf.Variable 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0' shape=(300,) dtype=float32>), (None, <tf.Variable 'time_distributed/kernel:0' shape=(150, 75) dtype=float32>), (None, <tf.Variable 'time_distributed/bias:0' shape=(75,) dtype=float32>)).
"
53319,Build Debug Version of TF 2.4,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow version: 2.4
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.2



**Command**
 bazel build --config=cuda --config=dbg //tensorflow/tools/pip_package:build_pip_package


**Error Message**
ERROR: /tensorflow/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.4' failed (Exit 1)
/usr/bin/ld: bazel-out/k8-dbg/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 42.168s, Critical Path: 26.85s
INFO: 1059 processes: 1059 local.
FAILED: Build did NOT complete successfully

**Question**
Is there a convenient/systematic way to build debug version of tf 2.4 with CUDA support?
"
53318,Is Tensorflow Library Performance NVidia Verified?,"Hi,
Has Tensorflow library been  tested by Nvidia team to check if Tensorflow libraries efficiently utilize GPU capabilities?
I use prebuilt tensorflow libraries in the link: https://www.tensorflow.org/install/lang_c
It would be great if I can access to any performance report containing GPU experiments and benchmarks prepared by Nvidia Team.
"
53317,"how to fix  undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>
Hi all:
   I build libtensorflow with source as follow:
  1: i build grpc first.
  2:  copy  pb, absl the include, and lib to /usr/lib64
  3: bazel build --action_env TF_SYSTEM_LIBS='com_google_protobuf,absl_py' //tensorflow:libtensorflow_cc.so
  4 build my owner example with the libtensorflow.so,but it's wrong.
   In function `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view)':
/opensource/tf/tensorflow/core/platform/status.h:49: undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_20211102::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'
  
pls, how to fix the bug.
ths.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53316,Wrong information,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/text/tutorials/text_classification_rnn

## Description of issue (what needs changing):
""Run a prediction on a new sentence:

If the prediction is >= 0.0, it is positive else it is negative.""
### Clear description

If the prediction is >= 0.5, it is positive else it is negative.
"
53315,[TFLite] Conversion support for 3D operators with channels first data format,"Hello,

TensorFlow Lite currently supports the conversion of 2D operators like `tf.keras.layers.Conv2D`, `tf.keras.layers.MaxPool2D`, ... with `data_format=""channels_first""` through the insertion of `TRANSPOSE` operators as the TFLite kernels only support the channels last format.

Would it be possible to add something similar for 3D operators like  `tf.keras.layers.Conv3D`, `tf.keras.layers.MaxPool3D`, ... that use `data_format=""channels_first""`?

Example script:
```python
import tensorflow as tf

input = tf.keras.Input(shape=[1, 3, 8, 8, 8])
output = tf.keras.layers.Conv3D(
    filters=12,
    kernel_size=(2, 2, 2),
    data_format=""channels_first"",
    use_bias=False,
)(input)
model = tf.keras.Model(input, output)


converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

Conversion error with TF 2.7.0 (note that setting `data_format=""channels_last""` works):
```
error: 'tf.Conv3D' op is neither a custom op nor a flex op
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Conv3D
Details:
        tf.Conv3D(tensor<?x3x8x8x8xf32>, tensor<2x2x2x3x12xf32>) -> (tensor<?x12x7x7x7xf32>) : {data_format = ""NCDHW"", device = """", dilations = [1, 1, 1, 1, 1], padding = ""VALID"", strides = [1, 1, 1, 1, 1]}
```

Thibaut"
53314,How to build tflite for arm,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are
not verified bugs in TensorFlow, please go to
[Discourse](https://discuss.tensorflow.org/).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
53313,"""Create an op"" documentation is outdated","
## URL(s) with the issue:

https://www.tensorflow.org/guide/create_op
https://github.com/tensorflow/custom-op

## Description of issue (what needs changing):

> `g++ -std=c++11 ...`

The option `std-c++11` is wrong, I think. It needs at least `std-c++14` for TensorFlow >=2.7.

This issue also exists in the [custom-op](https://github.com/tensorflow/custom-op) repo.

I think C++14 as default was introduced here: https://github.com/tensorflow/tensorflow/commit/f09d3e299752226f7a28c09a8e2018430b691a36

It can be seen here: https://github.com/tensorflow/tensorflow/blob/908e8010ff2c4249f0c81ed85f1013bbd4b7682e/.bazelrc#L320

> Note on gcc version >=5: gcc uses the new C++ ABI since version 5. The binary pip packages available on the TensorFlow website are built with gcc4 that uses the older ABI. If you compile your op library with gcc>=5, add -D_GLIBCXX_USE_CXX11_ABI=0 to the command line to make the library compatible with the older abi.

This is long outdated. The TF pip packages use a more recent GCC version since a long time (I don't exactly know since when).
"
53312,Unable to find version specific documentation for building TFLite,"

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/lite/guide/

## Description of issue (what needs changing):
Unable to find version-specific documentation to build TFLite building.

### Clear description
For version 2.4.1, There used to be documentation which is based on Makefile present in tensorflow/lite/tools/make
But this became redundant in the current release.
Bazel and cmake build process were introduced later.
Bazel build on host to generate TFLite shared library was initially kept and removed from the site.

For example, why should someone use this method? How is it useful?
Need version specific documentation to build TFLite library.
It will be helpful when users are using previous releases and help them in building TFLite library.

### Correct links
https://www.tensorflow.org/lite/guide/
https://github.com/tensorflow/tensorflow 


"
53309,Mac M1 tensorflow installation error,"**System information**
- OS Platform and Distribution: Mac OS Big Sur Version 11.6 Apple M1 chip Memory 8G

Here is how I install:
```
conda create -n tensorflow python=3.9.5
conda activate tensorflow
conda install -c apple tensorflow-deps
conda install -c apple tensorflow-deps==2.6.0
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
brew install libjpeg
conda install -y matplotlib jupyterlab
```
Then I run:
```
python
import tensorflow as tf
```
Then I got error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow/__init__.py"", line 449, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 155, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: _OBJC_CLASS_$_MPSGraphCompilationDescriptor
  Referenced from: /Users/zhangjing/miniforge3/envs/tf3.7/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib (which was built for Mac OS X 12.0)
  Expected in: /System/Library/Frameworks/MetalPerformanceShadersGraph.framework/Versions/A/MetalPerformanceShadersGraph
```


"
53307,How to pass a switch python argument into input_signature when using tf.function?,"Hi, I’m using Tensorflow 2.7.0 currently. There is no any other configuration things related to my issue.

I’m trying to set a switch for a tf.function, like set the input_signature for the call method of a Karas Model, so I can use the tf.saved_model.save API to save it. But it seems like I can not pass a python argument into the input_signature.

For example, I built a Transformer translation Model, I set an argument named “istraining” for the call method of this Model. When istraining = True, the Model run train mode and use the “teacher forcing” pattern to decode target language input. When istraining = False,  the model run translating mode and use the “auto regression” pattern to generate target language.

But I can not find a way to pass the python True/False argument into the input_signature of the tf.function decorator.

My currently solution is set a translating class function and a training class function separately and run get_concrete_function for these two functions separately when saving them. 

Is there a way to integrate these two functions into the model’s call method, and use a python argument True/False to control it after I save the model and reload it?

More generally, there is a reserved argument named “training” in Karas Layers and Karas Model, which is a python argument. if I wanna decorate the call method of a Karas Model, how should I set the input signature for this “training” argument?
"
53306,cannot get high precision arithmetic to work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.0.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.2
- Python version: 3.9.7
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
High precision when casting with numpy, but low precision when casting with tf:
```python
>>> import numpy as np; import tensorflow as tf
>>> tf.math.sin(np.array(np.pi, dtype=np.float64))
<tf.Tensor: shape=(), dtype=float64, numpy=1.2246467991473532e-16>

>>> tf.math.sin(tf.cast(np.pi, dtype=tf.float64))
<tf.Tensor: shape=(), dtype=float64, numpy=-8.742278000372475e-08>  # why low precision?

>>> tf.math.sin(tf.cast(3.141592653589793, dtype=tf.float64))
<tf.Tensor: shape=(), dtype=float64, numpy=-8.742278000372475e-08>  # how to get high precision??
```


**Describe the expected behavior**
I would expect `tf.cast(np.pi, dtype=tf.float64)` to behave like a float64 value when passed to functions.

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
see above
"
53305,Could not find the DLL(s) 'msvcp140_1.dll',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: (Windows 10)
- TensorFlow installed from: binary
- TensorFlow version: 2.6
- Python version: 3.8.0
- Installed using virtualenv? pip? conda?: pip (on a fresh anaconda environment)
- CUDA/cuDNN version: CUDA 11.2.2/ cuDNN 8.1.0.77
- GPU model and memory: Nvidia GeForce GTX 1650 Ti with 4096 MB GDDR6 



**Describe the problem**

`import tensorflow`


**Any other info / logs**

```
Traceback (most recent call last):

  File ""C:\Users\Muril\AppData\Local\Temp/ipykernel_6432/4294963926.py"", line 1, in <module>
    import tensorflow

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    self_check.preload_check()

  File ""C:\Users\Muril\anaconda3\envs\tf-keras-gpu\lib\site-packages\tensorflow\python\platform\self_check.py"", line 54, in preload_check
    raise ImportError(

ImportError: Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading ""Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019"" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads
```

I have installed the DLLs from that link, restarted my PC, but the problem remains.

The msvcp140_1.dll is in C:\Windows\System32, wich is already in my path environemnt variable.

![Capturar1](https://user-images.githubusercontent.com/40600889/144713654-f1f7e0f4-c657-4479-97c8-c12bc6dbb00d.JPG)


"
53304,Warnings with Arduino Nano 33 BLE,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): with pip
- TensorFlow version (or github SHA if from source): 2.7.0

**Standalone code to reproduce the issue** 
Run the DigitsExample.ino with eloquenttinyml library in arduino of version 0.0.10.

**Any other info / logs**

When I try compiling the code, I'm getting the following warnings. It's ultimately compiling the code but showing the following warnings:

In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/EloquentTinyML.h:13,
from C:\Users\saisa\UROP\EloquentTinyML-master\EloquentTinyML-master\examples\DigitsExample\DigitsExample.ino:1:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/EloquentTinyML.h:13,
from C:\Users\saisa\UROP\EloquentTinyML-master\EloquentTinyML-master\examples\DigitsExample\DigitsExample.ino:1:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\op_resolver.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\op_resolver.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\flatbuffer_conversions.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\flatbuffer_conversions.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\op_resolver.cpp: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&, tflite::ErrorReporter*, const TfLiteRegistration**)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\core\api\op_resolver.cpp:29:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]
builtin_code < BuiltinOperator_MIN) {
~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\debug_log_numbers.cpp: In function 'char* {anonymous}::FastFloatToBufferLeft(float, char*)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\debug_log_numbers.cpp:117:53: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
const uint32_t u = reinterpret_cast<uint32_t>(&f);
^
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_allocator.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_allocator.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/flatbuffer_conversions.h:24,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_allocator.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_allocator.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/memory_helpers.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\memory_helpers.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/memory_helpers.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\memory_helpers.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_interpreter.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp:15:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_interpreter.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp:15:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_interpreter.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_optional_debug_tools.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_optional_debug_tools.cpp:15:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_interpreter.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_optional_debug_tools.h:20,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_optional_debug_tools.cpp:15:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::input(size_t)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp:215:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]
if ((index < 0) || (index >= length)) {
~~~~~~^
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::output(size_t)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp:226:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]
if ((index < 0) || (index >= outputs->size())) {
~~~~~~^
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp: In member function 'TfLiteTensor* tflite::MicroInterpreter::tensor(size_t)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\micro_interpreter.cpp:236:14: warning: comparison of unsigned expression < 0 is always false [-Wtype-limits]
if ((index < 0) || (index >= tensors_size())) {
~~~~~~^
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/simple_memory_allocator.h:21:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\simple_memory_allocator.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/simple_memory_allocator.h:21:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\simple_memory_allocator.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/test_helpers.h:23:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\test_helpers.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/test_helpers.h:23:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\test_helpers.cpp:16:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\kernels\all_ops_resolver.cpp:13:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:407:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < TensorType_FLOAT32 || e > TensorType_INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:437:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/core/api/op_resolver.h:20:0,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/micro_mutable_op_resolver.h:19,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/all_ops_resolver.h:16,
from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\kernels\all_ops_resolver.cpp:13:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameDimensionType(tflite::DimensionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < DimensionType_DENSE || e > DimensionType_SPARSE_CSR) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:922:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOperator_ADD || e > BuiltinOperator_SEGMENT_SUM) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:1249:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < BuiltinOptions_NONE || e > BuiltinOptions_SegmentSumOptions) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2520:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < Padding_SAME || e > Padding_VALID) return """";
~~^~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2562:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2595:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2625:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2655:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2688:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CombinerType_SUM || e > CombinerType_SQRTN) return """";
~~^~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2718:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/schema/schema_generated.h:2745:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]
if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return """";
~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src\tensorflow\lite\micro\kernels\svdf.cpp:25:0:
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':
C:\Users\saisa\OneDrive\Documents\Arduino\libraries\EloquentTinyML\src/tensorflow/lite/micro/kernels/activation_utils.h:53:1: warning: control reaches end of non-void function [-Wreturn-type]
}
^
Sketch uses 208608 bytes (21%) of program storage space. Maximum is 983040 bytes.
Global variables use 57920 bytes (22%) of dynamic memory, leaving 204224 bytes for local variables. Maximum is 262144 bytes.
"
53303,LSTM expansion,"**System information**
- TensorFlow version (you are using): 1.3
- Are you willing to contribute it (Yes/No): Maybe

**Describe the feature and the current behavior/state.**

Simple LSTMs have limitations. It cannot extract some features, i.e. some mathematical operands, conditional logic, etc. I don't think the LSTM advancements cover this. This paper kind of shows what logic needs to be part of LSTM out of the box: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610074 
Can we improve the LSTM to include ability to figure out some basic logic like mathematical operands and conditional logic?

**Will this change the current api? How?**
Shouldn't.

**Who will benefit with this feature?**
Everyone.

**Any Other info.**
LSTM is brokened at the moment. Pretty rudimentary."
53302,Random seed in tf.keras.initializers.RandomNormal does not work in Tensorflow 2.7.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Setting the seed in tf.keras.initializers.RandomNormal(seed = [some integer]) does not guarantee that the same sequence is generated when constructed with the same seed value.

**Describe the expected behavior**
Setting the seed in tf.keras.initializers.RandomNormal(seed = [some integer]) should guarantee that the same sequence is generated when two initializers are constructed with the same seed value.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): n/a

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Using tensorflow version 2.7.0:
https://colab.research.google.com/drive/1paNe8kBRzwkrZHBi7QF0yhEX8pXO0f1w?usp=sharing

However, using tensorflow 2.6.0, this issue does not occur (note one has to restart the notebook after the pip install in the first cell before running the second cell):
https://colab.research.google.com/drive/10KrXpJtxiaAmnI2vKa61hDNheqjiJyu9?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53301,MLIR backend for XLA,"Hello,  may I ask  a few simple questions on XLA .

1. Is there an open sourced XLA MLIR backend that  one can try ""out of the box"" ?
2. Is there a way to enable/disable various optimizations  that are used in XLA ?
3. What is the best way to figure out  what optimizations are available in XLA ? 
"
53300,tf.stack silently output wrong result with 0-dimension tensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0 and 2.8.0-dev20211203 (nightly)
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Standalone code to reproduce the issue**
```
import tensorflow as tf
x = tf.random.uniform(shape=[0,3])
y = tf.random.uniform(shape=[1,3])
print(tf.stack([x,y]).shape)
```

**Describe the current behavior**
Outputs:
```
(2, 0, 3)
```
Stacking `x` and `y`, and we got an empty tensor! 
I found that this issue occurs in both tf2.7.0 and tf-nightly.

**Describe the expected behavior**
According to the documentation, the stacked tensors should have the same shape. Here the input tensor `x` and `y` don't have the same shape, so an `InvalidArgumentError` error should be raised."
53297,Update https://www.tensorflow.org/install/gpu#install_cuda_with_apt for Ubuntu 21.04,"

## URL(s) with the issue:
https://www.tensorflow.org/install/gpu#install_cuda_with_apt

## Description of issue (what needs changing):

Please add a section for Ubuntu 21.04:

For Ubuntu 21.04 the manual installation instructions are a thing of the past assuming that you have installed your NVIDIA drivers from the Canonical default apt repo .  All one needs to do is:
pip3 install tensorflow
sudo apt install nvidia-cuda-toolkit

This resolves the library error issues: ""Could not load dynamic library 'libcudart.so.11.0'""

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
53296,Cloud TPU VM with v3-32 fails while connecting to the cluster,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 (Cloud TPU VM)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): preinstalled
- TensorFlow version (use command below): 2.6
- Python version: 3.8.10
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: - 
- GPU model and memory: TPU v3-32


**Describe the current behavior**

If specifying TPU local I only connect to the 8 local cores:
```
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))
```

If replacing `'local'` with the TPU name I get the following error:
```
TensorFlow version 2.6.0
Sonnet version 2.0.0
2021-12-03 15:16:13.982669: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
D1203 15:16:18.394708976   58639 ev_posix.cc:173]            Using polling engine: epollex
D1203 15:16:18.394800582   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""grpclb""
D1203 15:16:18.394810386   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""priority_experimental""
D1203 15:16:18.394817121   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""weighted_target_experimental""
D1203 15:16:18.394820536   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""pick_first""
D1203 15:16:18.394823674   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""round_robin""
D1203 15:16:18.394830364   58639 dns_resolver_ares.cc:499]   Using ares dns resolver
D1203 15:16:18.394849444   58639 certificate_provider_registry.cc:33] registering certificate provider factory for ""file_watcher""
D1203 15:16:18.394861188   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""cds_experimental""
D1203 15:16:18.394868067   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_impl_experimental""
D1203 15:16:18.394874855   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_resolver_experimental""
D1203 15:16:18.394879563   58639 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_manager_experimental""
I1203 15:16:18.395006237   58639 server_builder.cc:332]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000
I1203 15:16:18.395095150   58639 socket_utils_common_posix.cc:353] TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter
I1203 15:16:18.433733255   59004 subchannel.cc:1065]         New connected subchannel at 0x815b7a0 for subchannel 0x2918780
2021-12-03 15:16:19.492715: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x8882ad00 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:
2021-12-03 15:16:19.492752: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): TPU, 2a886c8
2021-12-03 15:16:19.492760: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (1): TPU, 2a886c8
2021-12-03 15:16:19.492770: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (2): TPU, 2a886c8
2021-12-03 15:16:19.492781: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (3): TPU, 2a886c8
2021-12-03 15:16:19.492790: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (4): TPU, 2a886c8
2021-12-03 15:16:19.492797: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (5): TPU, 2a886c8
2021-12-03 15:16:19.492804: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (6): TPU, 2a886c8
2021-12-03 15:16:19.492811: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (7): TPU, 2a886c8
2021-12-03 15:16:19.503990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.164.0.8:8470, 1 -> 10.164.0.7:8470, 2 -> 10.164.0.5:8470, 3 -> 10.164.0.6:8470}
2021-12-03 15:16:19.504022: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34945}
2021-12-03 15:16:19.508428: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:489] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1638544579.507078836"",""description"":""Error received from peer ipv4:10.164.0.8:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E1203 15:16:19.508989265   58639 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
https://symbolize.stripped_domain/r/?trace=7f686921518b,7f686921520f,7f66c58688d8,7f66c586ba19,7f66c586d51d,7f66c57a8900,7f66c57a8c85,7f66bfeec9de,7f66bfeecc75,7f66bfee4d11,7f66bfee799c,7f66bfd23903,7f66ad862451,7f66ad85365b,5f5db8,903aff&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f66bbcde000-7f66cf9dd270 
*** SIGABRT received by PID 58639 (TID 58639) on cpu 16 from PID 58639; stack trace: ***
PC: @     0x7f686921518b  (unknown)  raise
    @     0x7f66bb1d41e0        976  (unknown)
    @     0x7f6869215210  (unknown)  (unknown)
    @     0x7f66c58688d9         32  grpc_cq_internal_unref()
    @     0x7f66c586ba1a         32  server_unref()
    @     0x7f66c586d51e        176  grpc_server_destroy
    @     0x7f66c57a8901         96  grpc_impl::Server::~Server()
    @     0x7f66c57a8c86         32  grpc_impl::Server::~Server()
    @     0x7f66bfeec9df        496  tensorflow::GrpcServer::~GrpcServer()
    @     0x7f66bfeecc76         32  tensorflow::GrpcServer::~GrpcServer()
    @     0x7f66bfee4d12       1296  tensorflow::(anonymous namespace)::UpdateContextWithServerDef()
    @     0x7f66bfee799d        720  tensorflow::EagerContextDistributedManager::SetOrUpdateServerDef()
    @     0x7f66bfd23904        176  TFE_ContextSetServerDef
    @     0x7f66ad862452        144  pybind11::cpp_function::initialize<>()::{lambda()#3}::_FUN()
    @     0x7f66ad85365c        720  pybind11::cpp_function::dispatcher()
    @           0x5f5db9  (unknown)  PyCFunction_Call
    @           0x903b00  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7f686921518b,7f66bb1d41df,7f686921520f,7f66c58688d8,7f66c586ba19,7f66c586d51d,7f66c57a8900,7f66c57a8c85,7f66bfeec9de,7f66bfeecc75,7f66bfee4d11,7f66bfee799c,7f66bfd23903,7f66ad862451,7f66ad85365b,5f5db8,903aff&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f66bbcde000-7f66cf9dd270,ca1b7ab241ee28147b3d590cadb5dc1b:7f66ae4d5000-7f66bb507b20 
E1203 15:16:19.708636   58639 coredump_hook.cc:292] RAW: Remote crash data gathering hook invoked.
E1203 15:16:19.708659   58639 coredump_hook.cc:384] RAW: Skipping coredump since rlimit was 0 at process start.
E1203 15:16:19.708672   58639 client.cc:222] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E1203 15:16:19.708680   58639 coredump_hook.cc:447] RAW: Sending fingerprint to remote end.
E1203 15:16:19.708688   58639 coredump_socket.cc:124] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket
E1203 15:16:19.708701   58639 coredump_hook.cc:451] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?
E1203 15:16:19.708709   58639 coredump_hook.cc:525] RAW: Discarding core.
E1203 15:16:19.916116   58639 process_state.cc:771] RAW: Raising signal 6 with default behavior
```

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
"
53295,"SymbolAlreadyExposedError: Symbol random_rotation is already exposed as ('keras.preprocessing.image.random_rotation',)","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- TensorFlow installed from (source or binary): cmd pip
- TensorFlow version: 
                    tensorboard                        2.7.0
                    tensorboard-data-server            0.6.1
                    tensorboard-plugin-wit             1.8.0
                    tensorflow                         2.7.0
                    tensorflow-addons                  0.15.0
                    tensorflow-estimator               2.7.0
                    tensorflow-io                      0.22.0
                    tensorflow-io-gcs-filesystem       0.22.0
                      tf-estimator-nightly               2.8.0.dev2021120309
                      tf-models-official                 2.7.0
                      tf-nightly                         2.8.0.dev20211202
                      tf-slim                            1.1.0
- Python version:  
                    Python 3.9.7
                    conda 4.10.3
- Installed using virtualenv? pip? conda?:              pip         21.2.4

- GPU model and memory:  NO GPU



**Describe the problem**
            
            raise SymbolAlreadyExposedError(
            tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol random_rotation is already exposed as ('keras.preprocessing.image.random_rotation',).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
          VERIFICATION_SCRIPT = os.path.join('models', 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')
          # Verify Installation
          !python {VERIFICATION_SCRIPT}


**Any other info / logs**
                Traceback (most recent call last):
                  File ""G:\object2\models\research\object_detection\builders\model_builder_tf2_test.py"", line 25, in <module>
                    from object_detection.builders import model_builder
                  File ""G:\anaconda3\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\builders\model_builder.py"", line 70, in <module>
                    from object_detection.models import ssd_efficientnet_bifpn_feature_extractor as ssd_efficientnet_bifpn
                  File ""G:\anaconda3\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\models\ssd_efficientnet_bifpn_feature_extractor.py"", line 34, in <module>
                    from official.vision.image_classification.efficientnet import efficientnet_model
                  File ""G:\anaconda3\lib\site-packages\tf_models_official-2.7.0-py3.9.egg\official\vision\image_classification\efficientnet\efficientnet_model.py"", line 37, in <module>
                    from official.vision.image_classification import preprocessing
                  File ""G:\anaconda3\lib\site-packages\tf_models_official-2.7.0-py3.9.egg\official\vision\image_classification\preprocessing.py"", line 25, in <module>
                    from official.vision.image_classification import augment
                  File ""G:\anaconda3\lib\site-packages\tf_models_official-2.7.0-py3.9.egg\official\vision\image_classification\augment.py"", line 31, in <module>
                    from tensorflow.python.keras.layers.preprocessing import image_preprocessing as image_ops
                  File ""G:\anaconda3\lib\site-packages\tensorflow\python\keras\layers\preprocessing\image_preprocessing.py"", line 28, in <module>
                    from tensorflow.python.keras.preprocessing import image as image_preprocessing
                  File ""G:\anaconda3\lib\site-packages\tensorflow\python\keras\preprocessing\__init__.py"", line 22, in <module>
                    from tensorflow.python.keras.preprocessing import image
                  File ""G:\anaconda3\lib\site-packages\tensorflow\python\keras\preprocessing\image.py"", line 1140, in <module>
                    keras_export('keras.preprocessing.image.random_rotation')(random_rotation)
                  File ""G:\anaconda3\lib\site-packages\tensorflow\python\util\tf_export.py"", line 336, in __call__
                    self.set_attr(undecorated_func, api_names_attr, self._names)
                  File ""G:\anaconda3\lib\site-packages\tensorflow\python\util\tf_export.py"", line 351, in set_attr
"
53294,compiled tf.function and tf.data.Iterator error,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-61734-g21f65a888d6 2.7.0-dev20210806
- Python version: Python 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.2
- GPU model and memory: GeForce RTX 2080 Ti / 11019MiB

**Describe the current behavior**
Hitting an error when calling next on a tf.data.Iterator within a compiled (jit_compiled=True) tf.function:

`tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_run_compiled_61: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter [Op:__inference_run_compiled_61]
`
**Describe the expected behavior**

The same code without compilation works fine (does not hit the error above). I am expecting the same behavior.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**

```
import tensorflow as tf

data = tf.data.Dataset.from_tensor_slices(list(range(10)))
data = data.repeat()
iterator = iter(data)

@tf.function(jit_compile=False)
def run(it):
    t = 0
    for _ in range(10):
        x = next(it)
        t += x
    return t

@tf.function(jit_compile=True)
def run_compiled(it):
    t = 0
    for _ in range(10):
        x = next(it)
        t += x
    return t

print(run(iterator))
print(run_compiled(iterator))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Full output of the test above:

`tf.Tensor(45, shape=(), dtype=int32)
Traceback (most recent call last):
  File ""../error.py"", line 24, in <module>
    print(run_compiled(iterator))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 904, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 976, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1965, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 597, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_run_compiled_61: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::ResourceDeleter [Op:__inference_run_compiled_61]`"
53293,TfLiteStatus invoke_status = interpreter.Invoke(); Failed,"### 0. Summary

Help is appreciated!
Details are given below.

To do my first project with TF lite on microcontrollers (TFLM) I follow the Hello World example for TFLM.
I do this with my own network, though. The Structure is

- Input 1x16x16x1 
- Two CNN layers with maxpooling
- Flatten
- Dense layer
- Output 1x2

I encounter the following problem:

In the python world (nearly) everything works.
> Sidenote: It did not work to use converter.inference_input_type = tf.int8 (The quantization scaling seems to expect a uint8 input eventough int8 was specified) Therefore I use uint8.

Also in the Cpp world everything looks fine. Till I do interpreter.Invoke() This creates the follwoing error message in the terminal:
> Input UINT8, output INT8 not supported.
> Node QUANTIZE (number 0) failed to invoke with status 1

However, I still get a prediction that is incorrect.
I wanted to note that I use
converter.inference_input_type = tf.uint8
converter.inference_input_type = tf.uint8

Details follow:

### 1. System information

- Windows 10, use Cygwin to run the make process
- Packages (pip):
	numpy==1.19.2
	pandas==1.2.2
	tensorflow==2.4.0
	tflite==2.4.0
	tensorflow_hub==0.12.0
	opencv-python>=4.5.4
	tqdm==4.62.3
	matplotlib==3.4.3
	scikit-learn>=0.0.0
	pillow>=0.0.0
	scikit-image>=0.0.0
	tensorflow_model_optimization>=0.0.0
	ipykernel==6.4.1
	tensorboard>=0.0.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

## Python-world (model produces reasonable results. Everything seems to be fine here.)
	# imports etc.


	# Load model
	model = tf.keras.models.load_model(""src/mdl/mdl_KERAS_trained"")

	# Define Representative Dataset
	data_obj = data_prep.gesture_data(""../Data/lala"")
	data_obj.load_data()
	data_obj.proc_data_picbased()
	data_obj.split_data()
	x_train = tf.convert_to_tensor(data_obj.x_train, dtype=tf.float32)
	y_train = tf.convert_to_tensor(data_obj.y_train, dtype=tf.float32)

	# Quick Check
	for i in range(12):
		print(model.predict(np.expand_dims(x_train[i, :, :, :], 0)))
		print(y_train[i, :])

	def representative_dataset_train():
		dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(1)
		for data in dataset:
			yield [data]


	# Define converter
	converter = tf.lite.TFLiteConverter.from_keras_model(model)
	converter.optimizations = [tf.lite.Optimize.DEFAULT]
	converter.inference_input_type = tf.uint8
	converter.inference_output_type = tf.uint8
	converter.representative_dataset = representative_dataset_train
	converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
	converter.target_spec.supported_types = [tf.int8]


	# Convert
	tflite_model = converter.convert()

	# Save
	with open(""src/mdl/mdl_tflite.tflite"", ""wb"") as f:
		f.write(tflite_model)


	# Load TFLite
	interpreter = tf.lite.Interpreter(model_path=""src/mdl/mdl_tflite.tflite"")
	interpreter.allocate_tensors()

	# Get input and output tensors.
	input_details = interpreter.get_input_details()
	output_details = interpreter.get_output_details()

	# Scaling to UINT8
	input_quant = input_details[0][""quantization""]
	output_quant = output_details[0][""quantization""]

	x_train8 = np.fix((x_train) / input_quant[0] + input_quant[1])
	y_train8 = y_train

	# Quick check
	print(""prediction"")
	for i in range(20):
		input_data = np.array(
			np.expand_dims(x_train8[i, :, :, :], axis=0), dtype=np.uint8
		)
		interpreter.set_tensor(input_details[0][""index""], input_data)
		interpreter.invoke()
		output_data = interpreter.get_tensor(output_details[0][""index""])
		print(
			output_data, ""   "",
			np.array(output_quant[0]) * (np.array(output_data) - np.array(output_quant[1])), ""   "",
			y_train8[i],
		)




	
	
## Cpp-world: 
```

TF_LITE_MICRO_TESTS_BEGIN

TF_LITE_MICRO_TEST(LoadModelAndPerformInference) {
	// Define the input
	double x[16][16];	// Some input data which are numbers between -1. and 1. Import not shown for clarity

	// Set up logging
	tflite::MicroErrorReporter micro_error_reporter;

	// Model
	const tflite::Model* model = ::tflite::GetModel(g_mdl_model_data);
	if (model->version() != TFLITE_SCHEMA_VERSION) {
	  TF_LITE_REPORT_ERROR(&micro_error_reporter,
	                         ""Model provided is schema version %d not equal ""
	                         ""to supported version %d.\n"",
	                         model->version(), TFLITE_SCHEMA_VERSION);
	}

	// Resolver
	tflite::AllOpsResolver resolver;

	// Arena
	constexpr int kTensorArenaSize = 500*1024;
	uint8_t tensor_arena[kTensorArenaSize];

	// Interpreter
	tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,
	                                       kTensorArenaSize, &micro_error_reporter);

	interpreter.AllocateTensors();

	// Input
	TfLiteTensor* input = interpreter.input(0);

	// Does Input match
	TF_LITE_MICRO_EXPECT_NE(nullptr, input);
	TF_LITE_MICRO_EXPECT_EQ(4, input->dims->size);
	TF_LITE_MICRO_EXPECT_EQ( 1, input->dims->data[0]);
	TF_LITE_MICRO_EXPECT_EQ(16, input->dims->data[1]);
	TF_LITE_MICRO_EXPECT_EQ(16, input->dims->data[2]);
	TF_LITE_MICRO_EXPECT_EQ( 1, input->dims->data[3]);
	TF_LITE_MICRO_EXPECT_EQ(kTfLiteUInt8, input->type);		//kTfLiteUInt8


	// Get quantization parameters
	double input_scale = input->params.scale;
	int input_zero_point = input->params.zero_point;


	// Quantize
	uint8_t x_quantized[256];
	for(int i=0;i<ni;i++){
		for(int j=0;j<nj;j++){
			x_quantized[i*nj+j] = (uint8_t) ((x[i][j]/input_scale) + input_zero_point);
			input->data.uint8[i*nj+j] = x_quantized[i*nj+j];
		}
	}


	// Run the model
	MicroPrintf(""-----invoke-----"");
	TfLiteStatus invoke_status = interpreter.Invoke();
	MicroPrintf(""-----check-----"");
	TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);

	MicroPrintf(""End of File"");


}

TF_LITE_MICRO_TESTS_END
```

### 3. Failure after conversion

Output of make process:
```

	$ make --no-silent --trace -f tensorflow/lite/micro/tools/make/Makefile test_mdl_test
	
	...

	[Some lines of output]
	...
	
	tensorflow/lite/micro/tools/make/gen/windows_x86_64_default/bin/mdl_test '~~~ALL TESTS PASSED~~~' windows
	Testing LoadModelAndPerformInference
	----------
	input_scale 1.0039215*2^-7
	input_zero_point 127
	----------
	-----invoke-----
	Input UINT8, output INT8 not supported.
	Node QUANTIZE (number 0) failed to invoke with status 1
	-----check-----
	kTfLiteOk == invoke_status failed at tensorflow/lite/micro/examples/mdl/mdl_test.cc:200 (0 vs 1)
	output_scale 1.0*2^-8
	output_zero_point 0
	----------
	prediction:   3   4
	prediction:   3   4
	End of File
	0/1 tests passed
	~~~SOME TESTS FAILED~~~
	~~~SOME TESTS FAILED~~~

	make: *** [tensorflow/lite/micro/examples/mdl/Makefile.inc:40: test_mdl_test] Error 1

```"
53292,tf.data.experimental.load does not work on temporary files,"**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): No (I don't have the required knowledge)


**Describe the feature and the current behavior/state.**

When loading a dataset with `tf.data.experimental.load`, the produced dataset will always be a lazily-loaded dataset that doesn't read the files until the dataset object is consumed (e.g. by iterating over it with Python code, or using it to train a model). This poses a problem if the path provided to `load` is a temporary path, since depending on when the dataset is used, the files could be gone when the dataset object attempts to read them.

Consider the following load function:
```python
def load_data(source):
    with tempfile.TemporaryDirectory() as tmpdir:
        prepare_files(source, tmpdir.name)
        return tf.data.experimental.load(tmpdir.name)
```
The dataset returned by this function will be unusable since, as soon as the function returns, the `TemporaryDirectory` context manager will exit and destroy the files. (The `prepare_files` function is just there to represent any logic that could be used to get the files to the temp. dir, such as downloading them over network, extracting them from an archive, or decrypting them.)

Aside from the temporary files issue, there could some other cases in which the developpers would want their datasets to be loaded in memory immediately.

**Will this change the current api? How?**

The feature I propose is to allow callers of `tf.data.experimental.load` to specify whether they want a lazily-loaded dataset, or an eagerly-loaded dataset (that reads the files only once, when `load` is called, and stores the data in memory).

The way I see it, this evolution would add a new optional parameter to `tf.data.experimental.load`. Its default value needs to correspond to lazy loading for backwards compatibility.

This would not pose a big issue since 1. it will be backwards-compatible and 2. only the experimetal API will be modified (and the documentation explicetely says the experimental API may evolve at any time).

**Who will benefit with this feature?**

Anyone who wants to separate their dataset loading logic in a similar function, using a temporary directory or another context manager that destroys the files upon exit. This would be the case when:
- using files downloaded from the Internet
- using encrypted files, e.g. if the data is sensitive or private
- using files compressed externally, to have more control over compression
- using `importlib.resources.as_file` in some cases

**Any Other info.**

The fact that the datasets are lazy-loaded is not documented, it would be nice to add this to the current documentation of `tf.data.experimental.load`. Also on the subject of documentation, once (if) the eager load option is added, the documentation should warn the user about the potentially high memory usage when eager-loading large datasets. In my use case this is not a problem, since 1. the datasets are created with `Dataset.from_tensor_slices` before being saved (so they have already been entirely in memory before), 2. I only use one dataset at a time and 3. I make sure it is garbage-collected as soon as possible.

Here is my current placeholder:
```python
def load_data(source):
    tmppath = tempfile.mkdtemp()
    def deleter():
        shutil.rmtree(tmppath)
    atexit.register(tmppath)
    prepare_files(source, tmppath)
    return tf.data.experimental.load(tmppath)
```
Depending on how many times this function is called during the program's lifetime, this can quickly clutter the disk. It would be nice to have a better solution.

Note that, to fit **my** *specific* use case, it would also work for me to have a variant of `save` and `load` that accept a Python file-like object (and save to/load from a single file). The logic I use in my version of `prepare_files` turns a single file into a directory.
"
53291,Wrong channel ordering in space_to_depth ,"In the example provided for `space_to_depth` function here: https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth

If an input tensor has a larger depth

```python
x = [[[[1, 2, 3], [4, 5, 6]],
      [[7, 8, 9], [10, 11, 12]]]]
```
Then, `space_to_depth` output is: `[ 1  2  3  4  5  6  7  8  9 10 11 12]`, slice along channel dimension (i.e., `x[0, 0, 0, :]`)

While Pytorch `pixel_unshuffle` return: `[ 1  4  7 10  2  5  8 11  3  6  9 12]`

The issue is the inconsistent in Tensorflow's behavior that is when the number of channels is _1_

```python
x= [[[[1], [4]],
       [[7], [10]]]]
```

then, the output is `[ 1  4  7 10]` which is the same as Pytorch. It is expected that when the number of channels is increasing, it should apply the `space_to_depth` to each channel separately, then concatenate the outputs along channel dimension.

However, the way Tensorflow does space_to_depth in the case of multiple channels is that it does select the elements along channel-axis which is wrong.

---

Another example, with the full code, if the input is RGGB Bayer Raw image, then TensorFlow output would be in the wrong order in case the number of channels is greater than 1.

```python
# pip install pytorch==1.10.0
# pip install tensorflow==2.6.1
# pip install einops==0.3.2
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import torch
from torch.nn.functional import pixel_unshuffle
import tensorflow as tf
import numpy as np
from einops import rearrange, repeat

rggb = np.asarray([[1, 2],
                   [2, 3]])

# === Only 1 channel
image = rearrange(rggb, 'h w -> h w 1')
image = repeat(image, 'h w c -> (2 h) (2 w) (1 c)')
print('RGGB image:', image.shape)
print(image[:, :, 0])

img1 = tf.nn.space_to_depth(
    rearrange(image, 'h w c -> 1 h w c'),
    2).numpy()
img2 = pixel_unshuffle(
    rearrange(torch.from_numpy(image), 'h w c -> 1 c h w'),
    2).numpy()

print('Tensorflow:', img1.shape)
print('Pytorch:', img2.shape)

img1 = rearrange(img1, '1 h w c -> h w c')
img2 = rearrange(img2, '1 c h w -> h w c')

print('Tensoflow:')
print(img1[0, 0, :])

print('Pytorch:')
print(img2[0, 0, :])

print('Match:', np.allclose(img1, img2))


# === Repeat to have 2 channels
print()
image = rearrange(rggb, 'h w -> h w 1')
image = repeat(image, 'h w c -> (2 h) (2 w) (2 c)')
print('RGGB image:', image.shape)
print(image[:, :, 0])

img1 = tf.nn.space_to_depth(
    rearrange(image, 'h w c -> 1 h w c'),
    2).numpy()
img2 = pixel_unshuffle(
    rearrange(torch.from_numpy(image), 'h w c -> 1 c h w'),
    2).numpy()

print('Tensorflow:', img1.shape)
print('Pytorch:', img2.shape)

img1 = rearrange(img1, '1 h w c -> h w c')
img2 = rearrange(img2, '1 c h w -> h w c')

print('Tensoflow:')
print(img1[0, 0, :])

print('Pytorch:')
print(img2[0, 0, :])

print('Match:', np.allclose(img1, img2))
```

output

```
RGGB image: (4, 4, 1)
[[1 2 1 2]
 [2 3 2 3]
 [1 2 1 2]
 [2 3 2 3]]
Tensorflow: (1, 2, 2, 4)
Pytorch: (1, 4, 2, 2)
Tensoflow:
[1 2 2 3]
Pytorch:
[1 2 2 3]
Match: True

RGGB image: (4, 4, 2)
[[1 2 1 2]
 [2 3 2 3]
 [1 2 1 2]
 [2 3 2 3]]
Tensorflow: (1, 2, 2, 8)
Pytorch: (1, 8, 2, 2)
Tensoflow:
[1 1 2 2 2 2 3 3]
Pytorch:
[1 2 2 3 1 2 2 3]
Match: False
```"
53289,java.lang.UnsatisfiedLinkError:java.lang.String[] org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(long),"AndroidRuntime: java.lang.UnsatisfiedLinkError: No implementation found for java.lang.String[] org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(long) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_getSignatureKeys and Java_org_tensorflow_lite_NativeInterpreterWrapper_getSignatureKeys__J)
10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(Native Method)
10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.getSignatureKeys(NativeInterpreterWrapper.java:414)
10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.getSignatureKeys(Interpreter.java:313)
10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
10-16 08:37:52.057 32388 32435 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)"
53284,Model not learning when using Dataset.from_generator() instead of Dataset.from_tensor_slices(),"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): Nvidia docker (nvcr.io/nvidia/tensorflow:21.08-tf2-py3)
- TensorFlow version (use command below): 2.5 (default installation) / 2.7 (updated with pip), doesn't work for both
- Python version: 3.8.10
- CUDA/cuDNN version: 11.4

**Describe the current behavior**

I'm trying to train a Pose Recognition model (SimpleBaseline - ResNet50) with the COCO dataset. The labels are loaded with a python function first and are then converted to a tf.dataset and further processed. It works well when using Dataset.from_tensor_slices() for conversion, but the model doesn't learn anything if Dataset.from_generator() is used instead. The later dataset functions are kept the same. Currently the training and evaluation are executed on the same (eval) split of the dataset for debugging. When printing the outputs of the dataset while training they are exactly the same for both approaches. 

```

# Loading labels as python list of dicts:
...
print(labels[0])
{
  'annotids': [183126], 
  'imageid': 425226,
  'raw_keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 142, 309, 1, 177, 320, 2, 191, 398, 2, 237, 317, 2, 233, 426, 2, 306, 233, 2, 92, 452, 2, 123, 468, 2, 0, 0, 0, 251, 469, 2, 0, 0, 0, 162, 551, 2], 
  'keypoints': [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [142, 309, 1], [177, 320, 2], [191, 398, 2], [237, 317, 2], [233, 426, 2], [306, 233, 2], [92, 452, 2], [123, 468, 2], [0, 0, 0], [251, 469, 2], [0, 0, 0], [162, 551, 2]]], 
  'imgpath': '/datasets/coco2017/annotations/../images/val2017/000000425226.jpg',
  'raw_width': 480,
  'raw_height': 640,
  'bbox': [73.35, 206.02, 300.58, 372.5], 
  'score': 1.0
}

# This does work
# Convert to dict-of-lists format and create dataset
labels = {k: [dic[k] for dic in labels] for k in labels[0]}
ds = tf.data.Dataset.from_tensor_slices(labels)

# This doesn't work
output_types = {
    ""annotids"": tf.int32,
    ""imageid"": tf.int32,
    ""raw_keypoints"": tf.float32,
    ""keypoints"": tf.float32,
    ""imgpath"": tf.string,
    ""raw_width"": tf.int32,
    ""raw_height"": tf.int32,
    ""bbox"": tf.float32,
    ""score"": tf.float32,
}
ds = tf.data.Dataset.from_generator(lambda: iter(labels), output_types=output_types)

# Postprocessing like image loading and heatmap generation with dataset.map calls
# They don't change for both approaches
...
```

**Describe the expected behavior**

The model should be able to learn something as well when using from_generator() instead of from_tensor_slices()


**Standalone code to reproduce the issue**
Currently the complete training code is to complex to extract a short standalone example

**Other info / logs**
Training uses Mirrored Strategy on two gpus. It takes about one epoch of training (~6000 images or ~90x2x32 batch examples) that a difference in the loss can be seen clearly. When training for more epochs the loss of the from_generator() approach doesn't decrease further. The heatmaps the network is trying to learn only show a lot of black with some random noise instead of clear gaussian distributions around a point.

**Current workaround**
Use Dataset.from_tensor_slices() instead of Dataset.from_generator()

"
53283,Bug for training GAN using TensorFlow v2.0,"Hi, I run GAN (train as following code) using TensorFlow 2.0.



```python

  @tf.function
  def train_step(self, X, y):
      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

          predictions = self.generator(X, training=True) 

          generated_frame = predictions
          real_frame = tf.cast(y, tf.float32)

          concatenate_inputs = tf.concat([real_frame, generated_frame], axis=0)            
          concatenate_outputs = self.discriminator(concatenate_inputs, training=True)

          score_real, score_generated = tf.split(concatenate_outputs, 2, axis=0)
          discriminator_loss = self.loss_hinge_disc(score_real, score_generated)
          generator_loss = self.generator_loss(real_frame, generated_frame) + 0.05*K.mean(score_generated)

          gen_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)
          disc_gradients = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)
          
          self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))
          self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))
```

But I got the following error and the system kill this task.

```
2021-12-02 20:52:43.795872: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.
2021-12-02 20:52:48.641269: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 8137342976 exceeds 10% of free system memory.
2021-12-02 20:52:59.829117: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-12-02 20:53:00.006395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz
2021-12-02 20:53:00.818017: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/input/_3381' -> 'gradient_tape/model/conv_lst_m2d/while/model/conv_lst_m2d/while_grad/body/_915/gradient_tape/model/conv_lst_m2d/while/gradients/AddN', 'Func/gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/input/_3841' -> 'gradient_tape/model_1/conv_lst_m2d_5/while/model_1/conv_lst_m2d_5/while_grad/body/_1629/gradient_tape/model_1/conv_lst_m2d_5/while/gradients/AddN', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_5', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/convolution_6' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/add_4', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_3', 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_5/while/body/_1451/model_1/conv_lst_m2d_5/while/mul_5', 'Func/gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/input/_3957' -> 'gradient_tape/model_1/conv_lst_m2d_4/while/model_1/conv_lst_m2d_4/while_grad/body/_1819/gradient_tape/model_1/conv_lst_m2d_4/while/gradients/AddN', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_5', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/clip_by_value' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/mul_3', 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/convolution_6' -> 'model_1/conv_lst_m2d_4/while/body/_1273/model_1/conv_lst_m2d_4/while/add_4', 'Func/gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/input/_4073' -> 'gradient_tape/model_1/conv_lst_m2d_3/while/model_1/conv_lst_m2d_3/while_grad/body/_2009/gradient_tape/model_1/conv_lst_m2d_3/while/gradients/AddN', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/convolution_6' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/add_4', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value_2' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_5', 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/clip_by_value' -> 'model_1/conv_lst_m2d_3/while/body/_1095/model_1/conv_lst_m2d_3/while/mul_3', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/convolution_6' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/add_4', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value_2' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_5', 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/clip_by_value' -> 'model/conv_lst_m2d_2/while/body/_357/model/conv_lst_m2d_2/while/mul_3', 'Func/gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/input/_3267' -> 'gradient_tape/model/conv_lst_m2d_1/while/model/conv_lst_m2d_1/while_grad/body/_725/gradient_tape/model/conv_lst_m2d_1/while/gradients/AddN', 'Func/gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/input/_3151' -> 'gradient_tape/model/conv_lst_m2d_2/while/model/conv_lst_m2d_2/while_grad/body/_535/gradient_tape/model/conv_lst_m2d_2/while/gradients/AddN', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_5', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_3', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/convolution_6' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/add_4', 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/clip_by_value_2' -> 'model/conv_lst_m2d_1/while/body/_179/model/conv_lst_m2d_1/while/mul_5', 'Func/model/conv_lst_m2d/while/body/_1/input/_2804' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_2', 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/mul_5' -> 'model/conv_lst_m2d/while/body/_1/model/conv_lst_m2d/while/Identity_4'}.
```

So how can I train GAN using Keras properly?

Thanks!


"
53281,Getting this issue while running my fit method (For CNN),"AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_20164/3579053964.py in <module>
----> 1 history = model.fit(
      2     train_ds,
      3     epochs = EPOCH,
      4     batch_size = BATCH_SIZE,
      5     verbose = 1,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py in autograph_handler(*args, **kwargs)
   1127           except Exception as e:  # pylint:disable=broad-except
   1128             if hasattr(e, ""ag_error_metadata""):
-> 1129               raise e.ag_error_metadata.to_exception(e)
   1130             else:
   1131               raise

AttributeError: in user code:

    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\engine\training.py"", line 946, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\engine\training.py"", line 935, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\engine\training.py"", line 928, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\engine\training.py"", line 846, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 532, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 668, in apply_gradients
        grads_and_vars = self._aggregate_gradients(grads_and_vars)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\optimizer_v2\optimizer_v2.py"", line 484, in _aggregate_gradients
        return self.gradient_aggregator(grads_and_vars)
    File ""C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\optimizer_v2\utils.py"", line 33, in all_reduce_sum_gradients
        if tf.__internal__.distribute.strategy_supports_no_merge_call():

    AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'
"
53280,'_VariantDataset' object has no attribute 'numpy',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes and no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04
- TensorFlow installed from (source or binary): pypi 
- TensorFlow version (use command below): 2.5.0.dev20210108
- Python version:  3.8.5

**Describe the current behavior**
```
for window in win_train_dataset:
  print([item.numpy() for item in window])
<output>:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_877/1363250068.py in <module>
      1 for window in win_train_dataset:
----> 2   print([item.numpy() for item in window])

/tmp/ipykernel_877/1363250068.py in <listcomp>(.0)
      1 for window in win_train_dataset:
----> 2   print([item.numpy() for item in window])

AttributeError: '_VariantDataset' object has no attribute 'numpy'
```

**Describe the expected behavior**
```
for window in win_train_dataset:
  print([item.numpy() for item in window])
<output>:
item#1
item#2
...
item#n
```

**Pseudocode to reproduce the issue**

```
training_dataset = tf.keras.utils.image_dataset_from_directory(training_data_dir,
                                                               shuffle=False,
                                                               label_mode='categorical',
                                                               batch_size=hyperparameters[""BATCH_SIZE""],
                                                               image_size=IMG_SIZE)) 
# Returns a batched dataset, proceed to unbatch it
unb_training_dataset = training_dataset.unbatch()

# Window the unbatched dataset
win_train_dataset = unb_train_dataset.window(3, 
                                    shift=1, 
                                    stride=1,
                                    drop_remainder=True,
                                    )

for window in win_train_dataset:
  print([item.numpy() for item in window])
```
"
53279,Unsupported data type 14 in tensor,"Hi,
I recently converted my custom LSTM model from TensorFlow '2.7.0' to TensorFlow lite so I can implement it on a Android Device  But, when I tried to test it I got this error 'Unsupported data type 14 in tensor'.
Can anyone help me understand this error?

Here is code for converting model to tflite
'''
converter = tf.lite.TFLiteConverter.from_saved_model(TRAIN_MODELS_DIR)
converter.post_training_quantize=True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
'''
"
53278,inaccurate tflite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): 2.7.0
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

```
# h5 model (correct)
import tensorflow as tf
import numpy as np

data = np.ones((1, 320, 160, 6), dtype=np.float32)
model = tf.keras.models.load_model(""model_float32.h5"")
model.predict(data)
```

```
# tflite full integer quantization model (incorrect)
import tensorflow as tf
import numpy as np

interpreter = tf.lite.Interpreter(model_path=""model_quant_uint8.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], np.ones((1, 320, 160, 6), dtype=np.uint8))
interpreter.invoke()

interpreter.get_tensor(output_details[0]['index'])
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

links to models:
https://www.dropbox.com/s/pssd04thvgndo74/model_float32.h5?dl=0
https://www.dropbox.com/s/fpititzzwcsdmi2/model_float32.tflite?dl=0
https://www.dropbox.com/s/k88obxfdb3uowov/model_quant_uint8.tflite?dl=0"
53277,Different results using same code but different CPU type,"**System information**
- Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8

**Describe the current behavior**

When tuning TF models we noted that we were getting different results when using different instance types on AWS. We started removing possible sources of variability until the only difference was the CPU type of the instances. We created a minimal example that trains a simple MLP on a training set and predicts probabilities for a validation set. The following figure compares the predictions obtained with an Intel CPU to those with an AMD CPU:
 
![image](https://user-images.githubusercontent.com/3641872/144308840-b0415669-dcbb-4200-bb55-9815843c3e50.png)


**Describe the expected behavior**

The predicted probabilities should be the same with both CPU types and the points in the above plot should all be on the diagonal.

**Standalone code to reproduce the issue**

Code to reproduce issue: https://github.com/rluethy/tf-cpu-type-diff
"
53276,not able to Install an old version of tensorflow,"How can I install an old version of Tensorflow in conda vertual environnement? I want Tensorflow-gpu 1.4, when I attempt this command:

``` lang-bash
conda install tensorflow-gpu=1.4
```

I see the message in attached picture.


![enter image description here][1]

When I tried `conda create -n tf_1_4 python=3.6 tensorflow-gpu=1.4` and I got this :

``` lang-bash
Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: \
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed

UnsatisfiableError: The following specifications were found to be incompatible with each other:

Output in format: Requested package -> Available versions

Package python conflicts for:
python=3.6
tensorflow-gpu=1.4 -> tensorflow-gpu-base==1.4.1 -> python[version='>=2.7,<2.8.0a0|>=3.5,<3.6.0a0|>=3.6,<3.7.0a0']

The following specifications were found to be incompatible with your system:

 - feature:/linux-64::__glibc==2.26=0
 - feature:|@/linux-64::__glibc==2.26=0

Your installed version is: 2.26
```


  [1]: https://i.stack.imgur.com/nrr6N.png"
53275,Model with both complex and real weights does not work with tf.distribute.MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, Debian Bullseye
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.5.1-13-g386ce34a1c1 2.5.1
- Python version: 3.8.10
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.0
- GPU model and memory: 2x V100 32GB

**Describe the current behavior**
When trying to use `tf.distribute.MirroredStrategy` with a model containing both complex and real (float32) weights, the following error is produced:

```
Traceback (most recent call last):
  File ""/storage/tf_complex64_bug.py"", line 67, in <module>
    model2.fit(data)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1178, in fit
    tmp_logs = self.train_function(iterator)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 763, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:850 train_function  *
        return step_function(self, iterator)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:840 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py:678 _call_for_each_replica
        return mirrored_run.call_for_each_replica(
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica
        return _call_for_each_replica(strategy, fn, args, kwargs)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py:245 _call_for_each_replica
        coord.join(threads)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /root/miniconda3/lib/python3.8/site-packages/six.py:703 reraise
        raise value
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py:238 _call_for_each_replica
        merge_result = threads[0].merge_fn(distribution, *merge_args,
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:148 _all_reduce_sum_fn  **
        return distribution.extended.batch_reduce_to(ds_reduce_util.ReduceOp.SUM,
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2402 batch_reduce_to
        return self._batch_reduce_to(reduce_op, value_destination_pairs, options)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py:767 _batch_reduce_to
        return cross_device_ops.batch_reduce(
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:446 batch_reduce
        return self.batch_reduce_implementation(reduce_op, value_destination_pairs,
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:874 batch_reduce_implementation
        return self._batch_all_reduce(reduce_op,
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:887 _batch_all_reduce
        dense_results = self._do_batch_all_reduce(reduce_op, dense_values)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:910 _do_batch_all_reduce
        device_grad_packs, tensor_packer = _pack_tensors(grouped, self._num_packs)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:820 _pack_tensors
        device_grad_packs = tensor_packer.pack(device_grads)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/cross_device_ops.py:747 pack
        concat_grads = array_ops.concat(flat_grads, 0)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1768 concat
        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1227 concat_v2
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:466 _apply_op_helper
        raise TypeError(""%s that don't all match."" % prefix)

    TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, float32, complex64, complex64, float32, float32] that don't all match.
```

**Describe the expected behavior**
The model should train successfully

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import numpy as np

def complex_uniform_initializer(scale=0.05):
    real_initializer = tf.keras.initializers.RandomUniform(-scale,scale)
    def initializer(shape,dtype):
        if dtype == tf.complex64:
            dtype = tf.float32
        elif dtype == tf.complex128:
            dtype = tf.float64
        real = real_initializer(shape,dtype)
        imag = real_initializer(shape,dtype)
        return tf.dtypes.complex(real,imag)
    return initializer

class ComplexDenseLayer(tf.keras.layers.Layer):

    def __init__(self, out_units, activation=None):
        super().__init__()
        self.out_units = out_units
        self.activation = activation

    def build(self, input_shape):
        inp_units = input_shape[-1]
        initializer = complex_uniform_initializer()
        self.w = self.add_weight(shape=[inp_units, self.out_units],
                                 initializer = initializer,
                                 dtype=tf.complex64, trainable=True)
        self.b = self.add_weight(shape=[self.out_units],
                                 initializer = initializer,
                                 dtype=tf.complex64, trainable=True)

    def call(self,inp):
        x = tf.einsum('bi,ij->bj', inp, self.w)
        x = tf.nn.bias_add(x, self.b)
        return self.activation(x)

    

def model(input_units, intermediate_units, output_units):
    inp = tf.keras.layers.Input((input_units,))
    xreal = tf.keras.layers.Dense(intermediate_units)(inp)
    ximag = tf.keras.layers.Dense(intermediate_units)(inp)
    x = tf.cast(xreal, 'complex64') + 1j*tf.cast(ximag,'complex64')
    x = ComplexDenseLayer(intermediate_units, activation = lambda w: w * tf.math.conj(w))(x)
    x = tf.math.real(x)
    x = tf.keras.layers.Dense(output_units)(x)
    return tf.keras.Model(inp,x) 

nsamples = 100
bsize = 10
ninp,nintermediate,nout = 16,128,16
inp = np.random.rand(nsamples, ninp)
tar = np.random.rand(nsamples, nout)
data = tf.data.Dataset.from_tensor_slices((inp,tar)).batch(bsize)

#Single GPU training works fine
model1 = model(ninp,nintermediate,nout)
model1.summary()
model1.compile(loss='mse', optimizer='adam')
model1.fit(data)

#Distributed training fails
distribute_strategy =  tf.distribute.MirroredStrategy()
with distribute_strategy.scope():
    model2 = model(ninp,nintermediate,nout)
    model2.summary()
    model2.compile(loss='mse', optimizer='adam')
    model2.fit(data)
```
"
53272,Seemingly random TensorBoard logging error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.3
- TensorBoard version: 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: Geforce GTX 1080

**Describe the current behavior**
When running a time neural network and saving data in tensorboard, I randomly get the following error: (By randomly I mean that I have a batch of networks that run one after the other in a batch file and it is never the completely same network to get the error. Mostly GRU or BiGRU networks. Sometimes every network runs without this problem. Sometimes multiple networks are affected.)

2021-11-28 19:11:18.779759: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at summary_kernels.cc:106 : Permission denied: ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2; Permission denied
	Failed to flush 2 events to ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2
	Could not flush events file.
(Full traceback below)

I hope I'm at the right place with this error. If I forgot any important info, please say so.

**Describe the expected behavior**
The run is completed without the error above.

**Other info / logs**:
2021-11-28 19:11:18.779759: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at summary_kernels.cc:106 : Permission denied: ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2; Permission denied
	Failed to flush 2 events to ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2
	Could not flush events file.
Traceback (most recent call last):
  File ""main.py"", line 312, in <module>
    main()
  File ""main.py"", line 241, in main
    validation_data=val_data, callbacks=keras_callbacks)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1137, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 412, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 2179, in on_epoch_end
    self._log_epoch_metrics(epoch, logs)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 2220, in _log_epoch_metrics
    summary_ops_v2.scalar('epoch_' + name, value, step=epoch)
  File ""/usr/lib/python3.6/contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 269, in as_default
    self.flush()
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 286, in flush
    return _flush_fn(writer=self)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 967, in flush
    return gen_summary_ops.flush_summary_writer(resource, name=name)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py"", line 201, in flush_summary_writer
    _ops.raise_from_not_ok_status(e, name)
  File ""/media/compute/homes/mhenning/MA/time-series-on-joints-emg/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.PermissionDeniedError: ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2; Permission denied
	Failed to flush 2 events to ../plots/new_dataset_test/arm_pos_middle/20211128_09_56_BiGRU2l6_t750/logs/train/events.out.tfevents.1638089801.zwinge.2544663.2071.v2
	Could not flush events file. [Op:FlushSummaryWriter]
"
53271,Code completion of Keras Module no longer work since 2.6,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1/
- Python version: 3.8.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:  N/A
- GPU model and memory: N/A

**Describe the current behavior**
Code completion is no longer work for `tf.keras` on the VSCode. since TensorFlow 2.6.
![image](https://user-images.githubusercontent.com/6873761/144241798-4239b2ac-39b4-4c96-98ca-7c89918900e1.png)

**Describe the expected behavior**
Code completion is work for `tf.keras` on the VSCode until TensorFlow 2.5.
![image](https://user-images.githubusercontent.com/6873761/144241133-3a2f04f8-3513-491e-8a43-243666c4d164.png)

A related issue found #52031.
The cause of this problem is lazy import of Keras.
A similar problem exists in TensorFlow Probability.

One solution is to prepare a stub file, but that is a lot of work.
Are you aware that this problem is a VSCode/Pylance problem?

**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no): yes, but I don't have a solution.
- Briefly describe your candidate solution(if contributing): N/A.
"
53270,How to change a saved model input shape in Tensorflow?,"I have a pretrained saved model, input shape is [1, None, None, 3], i want to change model input shape to [1, 320, 320, 3], what should i do?
"
53268,Closing as stale. Please reopen if you'd like to work on this further.,"Closing as stale. Please reopen if you'd like to work on this further.

_Originally posted by @google-ml-butler[bot] in https://github.com/tensorflow/tensorflow/issues/52214#issuecomment-947901218_"
53267,Allowing Input Tensors to be dynamic for Predictions when using the Subclass method for defining a model ,"I am trying to write a Keras Model which implements some complex logic. That logic has been implemented in my call method

As my network layers are convolutional, my model can theoretically predict on input sizes which are different than the size I trained on. Is there any way to specify that for running Predictions on the data.

Here is the skeletal code

     class CustomModel(tf.keras.Model):
         def __init__(self):
              super().__init__()
              self.conv1 = tf.keras.layers.Conv2D(filters=4, kernel_size=4)
              self.conv2= tf.keras.layers.Conv2D(filters=4, kernel_size=4)
      
         def call(inputs):
             image = inputs[0]
             output = self.conv1(image)
             # There is more complex logic here for inputs[1]
             # For sake of simplicity, writing this as output
             another_output = self.conv2(inputs[1])
             combined_output = (output, another_output)
             return combined_output 

And this is the code I am using for running the model

    model = CustomModel()
    model.compile()
    model.fit(x=Train_TF_Dataset)
    
    model.save(filename)
    
    load_model = tf.keras.models.load_model(filename)
    
    model.predict(x=Predict_TF_Dataset)

The train TF Dataset and the predict TF Datasets consist of different sizes which is our usecase.

The training data consisted of a tuple of 28x28x2 images and a second image while the prediction data consists of 512x512x2 along with the corresponding second images. The size of the first input in the tuple can be anything.

Is there some way of specifying that the input shape is (None, None, None, 2) instead of (None, 28, 28, 2). The first value of None is variable by default for the batch size.

The solutions I have seen on stackoverflow just convert it to a functional model which is not possible because of how complex my call method is

Note: 28 and 512 are just numbers. The actual sizes are different. 

Right now when I predict, I will get the following error 

    ValueError: `generator` yielded an element of shape (512, 512, 2) where an element of shape (28, 28, 2) was expected.


**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Will this change the current api? How?** 
No the issue will be present in the current API as well 

**Who will benefit with this feature?**
This will allow people to develop subclass models with variable input sizes for Inference. This is huge use case in many industries 

**Any Other info.**
"
53266,different convergence behavior and performance if including 2nd order gradient in loss function via two slightly different ways,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 20H2, OS build 19042.1348
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): anaconda pip install
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: Python 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: cuda10.0/cudnn11.5
- GPU model and memory: 2080TI

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
In this model a 2nd order gradient is included in loss function as part of regularization term. A significant different convergence behavior is found when calculating 2nd order in two slightly different ways:

with tf.GradientTape(persistent=True) as tape_bnd:
    ...
    tape_bnd.watch(input_bulk_x)
    with tf.GradientTape(persistent=True) as tape_bulk2:
        tape_bulk2.watch(input_bulk_x)
        tape_bulk2.watch(input_bulk_t)
        bulk_pack = tf.concat([input_bulk_x, input_bulk_t], 1)
        prediction_bulk = tf_model(bulk_pack, training=True)
        prediction_bulk_r = prediction_bulk[:,0:1]  (prediction_bulk_r = prediction_bulk[:,0])
        prediction_bulk_i = prediction_bulk[:,1:2]  (prediction_bulk_i = prediction_bulk[:,1])
    ...
    grad_x_r = tape_bulk2.gradient(prediction_bulk_r, input_bulk_x) / (pbin[1] - pbin[0]) * 2
    grad_x_i = tape_bulk2.gradient(prediction_bulk_i, input_bulk_x) / (pbin[1] - pbin[0]) * 2
    del tape_bulk2
...
grad_xx_r = tape_bnd.gradient(grad_x_r, input_bulk_x) / (pbin[1] - pbin[0]) * 2
grad_xx_i = tape_bnd.gradient(grad_x_i, input_bulk_x) / (pbin[1] - pbin[0]) * 2
del tape_bnd

In above code, the following lines:
prediction_bulk_r = prediction_bulk[:,0:1]  (prediction_bulk_r = prediction_bulk[:,0])
prediction_bulk_i = prediction_bulk[:,1:2]  (prediction_bulk_i = prediction_bulk[:,1])
represents two different ways to divide an n by 2 tensor prediction_bulk into two n tensors before calculating 2nd order gradient, 
prediction_bulk_r = prediction_bulk[:,0:1] keeps dimension so prediction_bulk_r is n by 1
prediction_bulk_r = prediction_bulk[:,0] doesn't so prediction_bulk_r  dimension is n

During training process, at initial training step both methods print the same loss, meaning both 2nd order gradient operators return with approximately the same value. But after about 100 steps, for  (prediction_bulk_r = prediction_bulk[:,0]) method the loss no longer decreases, but for prediction_bulk_r = prediction_bulk[:,0:1] method loss decreases smoothly. 

Following are the training outputs for both methods: (loss_loc =lt_loc +lbc_loc +lb_loc )
prediction_bulk_r = prediction_bulk[:,0:1]:
Finished epoch 0 , loss_loc =  0.965272963 , lt_loc =  0.95177114 , lbc_loc =  0.00729189115 , lb_loc =  0.00620991644
Finished epoch 10 , loss_loc =  0.57811451 , lt_loc =  0.569292963 , lbc_loc =  0.000788190926 , lb_loc =  0.00803333055
Finished epoch 20 , loss_loc =  0.454473078 , lt_loc =  0.451767802 , lbc_loc =  0.000194555934 , lb_loc =  0.00251073623
Finished epoch 30 , loss_loc =  0.448864669 , lt_loc =  0.442887783 , lbc_loc =  0.000913872384 , lb_loc =  0.00506302295
Finished epoch 40 , loss_loc =  0.446891814 , lt_loc =  0.441026747 , lbc_loc =  0.000472803076 , lb_loc =  0.00539224083
Finished epoch 50 , loss_loc =  0.441752672 , lt_loc =  0.437939882 , lbc_loc =  0.000413463975 , lb_loc =  0.00339930598
Finished epoch 60 , loss_loc =  0.434724897 , lt_loc =  0.431893885 , lbc_loc =  0.000759351416 , lb_loc =  0.00207163556
Finished epoch 70 , loss_loc =  0.418205 , lt_loc =  0.412783265 , lbc_loc =  0.00237809378 , lb_loc =  0.00304362969
Finished epoch 80 , loss_loc =  0.377492219 , lt_loc =  0.360962123 , lbc_loc =  0.00959464908 , lb_loc =  0.00693544187
Finished epoch 90 , loss_loc =  0.303197145 , lt_loc =  0.260590762 , lbc_loc =  0.0136152282 , lb_loc =  0.0289911665
Finished epoch 100 , loss_loc =  0.239446223 , lt_loc =  0.156737462 , lbc_loc =  0.0188848432 , lb_loc =  0.0638239235
Finished epoch 110 , loss_loc =  0.184993118 , lt_loc =  0.11391528 , lbc_loc =  0.0112376083 , lb_loc =  0.0598402321
Finished epoch 120 , loss_loc =  0.158851862 , lt_loc =  0.105409048 , lbc_loc =  0.00738385599 , lb_loc =  0.0460589677
Finished epoch 130 , loss_loc =  0.14423281 , lt_loc =  0.096214287 , lbc_loc =  0.00382424705 , lb_loc =  0.0441942662
Finished epoch 140 , loss_loc =  0.135551393 , lt_loc =  0.0879743248 , lbc_loc =  0.00334015535 , lb_loc =  0.0442369059
Finished epoch 150 , loss_loc =  0.127870142 , lt_loc =  0.0826394185 , lbc_loc =  0.0032730191 , lb_loc =  0.0419577062
Finished epoch 160 , loss_loc =  0.124033585 , lt_loc =  0.0774272084 , lbc_loc =  0.00544774719 , lb_loc =  0.0411586352
Finished epoch 170 , loss_loc =  0.11641103 , lt_loc =  0.0722029954 , lbc_loc =  0.00310491584 , lb_loc =  0.0411031172
Finished epoch 180 , loss_loc =  0.112142481 , lt_loc =  0.0685963407 , lbc_loc =  0.00335948449 , lb_loc =  0.0401866585
Finished epoch 190 , loss_loc =  0.10562592 , lt_loc =  0.0646096841 , lbc_loc =  0.00248501729 , lb_loc =  0.0385312214
Finished epoch 200 , loss_loc =  0.102625497 , lt_loc =  0.0603574775 , lbc_loc =  0.00585670117 , lb_loc =  0.0364113152
Finished epoch 210 , loss_loc =  0.0917019099 , lt_loc =  0.0549113117 , lbc_loc =  0.00164440088 , lb_loc =  0.0351461954
Finished epoch 220 , loss_loc =  0.086815238 , lt_loc =  0.050563138 , lbc_loc =  0.00135485874 , lb_loc =  0.0348972455
Finished epoch 230 , loss_loc =  0.0829412341 , lt_loc =  0.0474774539 , lbc_loc =  0.00106860022 , lb_loc =  0.0343951844
Finished epoch 240 , loss_loc =  0.0905935 , lt_loc =  0.0465794131 , lbc_loc =  0.0101005193 , lb_loc =  0.0339135714
Finished epoch 250 , loss_loc =  0.0806988478 , lt_loc =  0.0442230441 , lbc_loc =  0.00325619429 , lb_loc =  0.0332196057
Finished epoch 260 , loss_loc =  0.0765827522 , lt_loc =  0.0422764085 , lbc_loc =  0.00129232195 , lb_loc =  0.0330140218
Finished epoch 270 , loss_loc =  0.0742470622 , lt_loc =  0.0410219878 , lbc_loc =  0.000781063805 , lb_loc =  0.0324440114
Finished epoch 280 , loss_loc =  0.0724530667 , lt_loc =  0.0398085974 , lbc_loc =  0.000717243936 , lb_loc =  0.031927228
Finished epoch 290 , loss_loc =  0.0706587136 , lt_loc =  0.0384904146 , lbc_loc =  0.000621825 , lb_loc =  0.0315464735
Finished epoch 300 , loss_loc =  0.0689978376 , lt_loc =  0.0371541791 , lbc_loc =  0.000595943304 , lb_loc =  0.0312477164

prediction_bulk_r = prediction_bulk[:,0]:
Finished epoch 0 , loss_loc =  0.965273 , lt_loc =  0.95177114 , lbc_loc =  0.00729189115 , lb_loc =  0.00620994624
Finished epoch 10 , loss_loc =  0.578066528 , lt_loc =  0.56924212 , lbc_loc =  0.000787865836 , lb_loc =  0.00803656224
Finished epoch 20 , loss_loc =  0.455324501 , lt_loc =  0.451724261 , lbc_loc =  0.000195783374 , lb_loc =  0.00340447319
Finished epoch 30 , loss_loc =  0.450048655 , lt_loc =  0.442762583 , lbc_loc =  0.000911012234 , lb_loc =  0.00637508277
Finished epoch 40 , loss_loc =  0.447754472 , lt_loc =  0.441224217 , lbc_loc =  0.000452358043 , lb_loc =  0.00607788702
Finished epoch 50 , loss_loc =  0.443184465 , lt_loc =  0.438365817 , lbc_loc =  0.00038639488 , lb_loc =  0.0044322731
Finished epoch 60 , loss_loc =  0.43666774 , lt_loc =  0.432844132 , lbc_loc =  0.000703800179 , lb_loc =  0.00311980979
Finished epoch 70 , loss_loc =  0.422279418 , lt_loc =  0.415714264 , lbc_loc =  0.00206818013 , lb_loc =  0.00449697534
Finished epoch 80 , loss_loc =  0.389064491 , lt_loc =  0.371193707 , lbc_loc =  0.00833481457 , lb_loc =  0.00953596272
Finished epoch 90 , loss_loc =  0.328366399 , lt_loc =  0.281947851 , lbc_loc =  0.0159796905 , lb_loc =  0.0304388851
Finished epoch 100 , loss_loc =  0.27213338 , lt_loc =  0.184210449 , lbc_loc =  0.0128458692 , lb_loc =  0.0750770569
Finished epoch 110 , loss_loc =  0.250225097 , lt_loc =  0.15421848 , lbc_loc =  0.00745057128 , lb_loc =  0.0885560513
Finished epoch 120 , loss_loc =  0.243634969 , lt_loc =  0.156038091 , lbc_loc =  0.00717799319 , lb_loc =  0.0804188848
Finished epoch 130 , loss_loc =  0.238658771 , lt_loc =  0.155048981 , lbc_loc =  0.0044888882 , lb_loc =  0.0791209042
Finished epoch 140 , loss_loc =  0.234584287 , lt_loc =  0.149133667 , lbc_loc =  0.00342479628 , lb_loc =  0.082025826
Finished epoch 150 , loss_loc =  0.232386291 , lt_loc =  0.147789568 , lbc_loc =  0.00371110067 , lb_loc =  0.0808856115
Finished epoch 160 , loss_loc =  0.230131447 , lt_loc =  0.147337914 , lbc_loc =  0.00357931526 , lb_loc =  0.0792142153
Finished epoch 170 , loss_loc =  0.228197023 , lt_loc =  0.145577312 , lbc_loc =  0.00342653901 , lb_loc =  0.0791931748
Finished epoch 180 , loss_loc =  0.226216674 , lt_loc =  0.143941388 , lbc_loc =  0.00312073436 , lb_loc =  0.0791545585
Finished epoch 190 , loss_loc =  0.224205345 , lt_loc =  0.142434448 , lbc_loc =  0.00279783597 , lb_loc =  0.0789730474
Finished epoch 200 , loss_loc =  0.222144008 , lt_loc =  0.140951037 , lbc_loc =  0.00249917456 , lb_loc =  0.0786937848
Finished epoch 210 , loss_loc =  0.220653415 , lt_loc =  0.139468715 , lbc_loc =  0.00276256143 , lb_loc =  0.0784221515
Finished epoch 220 , loss_loc =  0.22345908 , lt_loc =  0.139065936 , lbc_loc =  0.00693785353 , lb_loc =  0.0774553
Finished epoch 230 , loss_loc =  0.217616767 , lt_loc =  0.136467338 , lbc_loc =  0.00320873177 , lb_loc =  0.077940695
Finished epoch 240 , loss_loc =  0.214142546 , lt_loc =  0.134901837 , lbc_loc =  0.00151732983 , lb_loc =  0.0777233839
Finished epoch 250 , loss_loc =  0.211947292 , lt_loc =  0.132908329 , lbc_loc =  0.00112436293 , lb_loc =  0.0779145882
Finished epoch 260 , loss_loc =  0.209894076 , lt_loc =  0.131119698 , lbc_loc =  0.000880624109 , lb_loc =  0.0778937489
Finished epoch 270 , loss_loc =  0.207739949 , lt_loc =  0.128978983 , lbc_loc =  0.000651250419 , lb_loc =  0.0781097189
Finished epoch 280 , loss_loc =  0.206072122 , lt_loc =  0.126908109 , lbc_loc =  0.000791691651 , lb_loc =  0.078372322
Finished epoch 290 , loss_loc =  0.204720557 , lt_loc =  0.125317663 , lbc_loc =  0.00118191785 , lb_loc =  0.0782209784
Finished epoch 300 , loss_loc =  0.205160156 , lt_loc =  0.123641305 , lbc_loc =  0.00302041182 , lb_loc =  0.078498438

Another observation is that when using prediction_bulk_r = prediction_bulk[:,0] method, the average training speed per step is way slower than prediction_bulk_r = prediction_bulk[:,0:1] case.

My thought here is since when applying GradientTape.gradient() method to a tensor target, it adds all gradient from all target entries together to produce a gradient tensor with the same shape of source tensor, thus the dimension of tensor prediction_bulk_r (either n by 1 or n) should not matter.

**Describe the expected behavior**
Both methods give the same convergence behavior.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import sys
sys.path.insert(0, '../../Utilities/')

import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np
import matplotlib.pyplot as plt
import scipy.io
from scipy.interpolate import griddata
from pyDOE import lhs
from plotting import newfig, savefig
from mpl_toolkits.mplot3d import Axes3D
import time
import matplotlib.gridspec as gridspec
from mpl_toolkits.axes_grid1 import make_axes_locatable
import pickle

np.random.seed(1234)
tf.random.set_seed(1234)

if __name__ == ""__main__"":
    
    tf.keras.backend.set_floatx('float32')
    
    with open('objs.pkl', 'rb') as f:
        idx_x, idx_t, lhs_res, weights_output, biases_output = pickle.load(f)
    
    
    noise = 0.0     
    
    pos_bound = np.array([-5, 5]).astype(np.float32)
    t_bound = np.array([0, np.pi / 2]).astype(np.float32)
    
    N0 = 50
    N_b = 50
    N_f = 20000
    layers = [2, 100, 100, 100, 100, 2]
    #layers = [2, 100, 100, 2]
    
    data = scipy.io.loadmat('../Data/NLS.mat')
    
    t_val = data[""tt""]
    pos_val = data[""x""]
    func_val = data[""uu""]
    
    #rand_idx_x = np.random.choice(func_val.shape[0], N0, replace=False)
    #rand_idx_t = np.random.choice(func_val.shape[1], N_b, replace=False)
    rand_idx_x = idx_x
    rand_idx_t = idx_t
    
    rand_x = (pos_val[0, rand_idx_x] - pos_bound[0]) / (pos_bound[1] - pos_bound[0]) * 2 - 1
    rand_t = (t_val[0, rand_idx_t] - t_bound[0]) / (t_bound[1] - t_bound[0]) * 2 - 1
    
    #X_f = lhs(2, N_f) #Latin-Hypercube (lhs)
    #X_f[:,0] = X_f[:,0] * 2 - 1
    #X_f[:,1] = X_f[:,1] * 2 - 1
    X_f = lhs_res
    X_f[:,0] = X_f[:,0] * 2 - 1
    X_f[:,1] = X_f[:,1] * 2 - 1
    
    #init tf tensors
    tf_pt_lb = tf.constant([[pos_bound[0]], [t_bound[0]]], dtype=tf.float32)
    tf_pt_ub = tf.constant([[pos_bound[1]], [t_bound[1]]], dtype=tf.float32)
    
    t0_arr_tmp = np.zeros(shape=[2, N0])
    t0_arr_tmp[0, :] = rand_x
    t0_arr_tmp[1, :] = np.zeros(rand_x.shape[0]) - 1
    tf_t0 = tf.convert_to_tensor(np.transpose(t0_arr_tmp), dtype=tf.float32)
    
    tmp_f_val_0 = func_val[rand_idx_x,0]
    tmp2_f_val_0 = np.zeros(shape=[2, N0])
    tmp2_f_val_0[0, :] = np.real(tmp_f_val_0)
    tmp2_f_val_0[1, :] = np.imag(tmp_f_val_0)
    tf_ans_t0 = tf.convert_to_tensor(np.transpose(tmp2_f_val_0), dtype=tf.float32)
    
    xlb_arr_np = np.zeros(N_b) - 1
    xub_arr_np = np.zeros(N_b) + 1
    xb_arr_t = rand_t
    tf_xlb_x_ten = tf.convert_to_tensor(np.expand_dims(xlb_arr_np, axis=1), dtype=tf.float32)
    tf_xub_x_ten = tf.convert_to_tensor(np.expand_dims(xub_arr_np, axis=1), dtype=tf.float32)
    tf_xb_t_ten = tf.convert_to_tensor(np.expand_dims(xb_arr_t, axis=1), dtype=tf.float32)
    
    tf_bulk_px_tensor = tf.convert_to_tensor(np.expand_dims(X_f[:, 0], axis=1), dtype=tf.float32)
    tf_bulk_pt_tensor = tf.convert_to_tensor(np.expand_dims(X_f[:, 1], axis=1), dtype=tf.float32)
    
    
    ##nn layers
    initializer = tf.keras.initializers.GlorotNormal()
    initializer0 = tf.keras.initializers.Zeros()
    tf_model = tf.keras.Sequential()
    for i in range(0, len(layers)):
        if i == 0:
            tf_model.add(tf.keras.Input(shape=(layers[i])))
        elif i == (len(layers)-1):
            tf_model.add(tf.keras.layers.Dense(layers[i], activation=None, kernel_initializer=initializer, bias_initializer=initializer0))
        else:
            tf_model.add(tf.keras.layers.Dense(layers[i], activation='tanh', kernel_initializer=initializer, bias_initializer=initializer0))
    
    optimizer = tf.keras.optimizers.Adam(epsilon=1e-08)
    
    #set weight manually
    for i in range(0, len(layers)-1):
        tf_model.layers[i].set_weights([np.array(weights_output[i]), np.array(biases_output[i].T[:,0])])
    
    
    
    
    @tf.function
    def loss_function(input_t0, label_t0, input_plb, input_pub, input_ptb, input_bulk_x, input_bulk_t, pbin, tbin):

        predictions_t0 = tf_model(input_t0, training=True)
        loss_t0 = tf.math.reduce_mean(tf.math.square(predictions_t0[:,0] - label_t0[:,0]))
        loss_t1 = tf.math.reduce_mean(tf.math.square(predictions_t0[:,1] - label_t0[:,1]))
            
        with tf.GradientTape(persistent=True) as tape_bnd:
            tape_bnd.watch(input_plb)
            lb_input_pack = tf.concat([input_plb, input_ptb], 1)
            prediction_plb = tf_model(lb_input_pack, training=True)
            prediction_plb_r = prediction_plb[:,0]
            prediction_plb_i = prediction_plb[:,1]
            
            tape_bnd.watch(input_pub)
            ub_input_pack = tf.concat([input_pub, input_ptb], 1)
            prediction_pub = tf_model(ub_input_pack, training=True)
            prediction_pub_r = prediction_pub[:,0]
            prediction_pub_i = prediction_pub[:,1]
            
            tape_bnd.watch(input_bulk_x)
            with tf.GradientTape(persistent=True) as tape_bulk2:
                tape_bulk2.watch(input_bulk_x)
                tape_bulk2.watch(input_bulk_t)
                bulk_pack = tf.concat([input_bulk_x, input_bulk_t], 1)
                prediction_bulk = tf_model(bulk_pack, training=True)
                prediction_bulk_r = prediction_bulk[:,0:1]
                prediction_bulk_i = prediction_bulk[:,1:2]
                
            grad_x_r = tape_bulk2.gradient(prediction_bulk_r, input_bulk_x) / (pbin[1] - pbin[0]) * 2
            grad_x_i = tape_bulk2.gradient(prediction_bulk_i, input_bulk_x) / (pbin[1] - pbin[0]) * 2
            grad_t_r = tape_bulk2.gradient(prediction_bulk_r, input_bulk_t) / (tbin[1] - tbin[0]) * 2
            grad_t_i = tape_bulk2.gradient(prediction_bulk_i, input_bulk_t) / (tbin[1] - tbin[0]) * 2
            
            del tape_bulk2
        
        loss_bnd_dl_r = tape_bnd.gradient(prediction_plb_r, input_plb) / (pbin[1] - pbin[0]) * 2
        loss_bnd_du_r = tape_bnd.gradient(prediction_pub_r, input_pub) / (pbin[1] - pbin[0]) * 2
        
        loss_bnd_dl_i = tape_bnd.gradient(prediction_plb_i, input_plb) / (pbin[1] - pbin[0]) * 2
        loss_bnd_du_i = tape_bnd.gradient(prediction_pub_i, input_pub) / (pbin[1] - pbin[0]) * 2
        
        grad_xx_r = tape_bnd.gradient(grad_x_r, input_bulk_x) / (pbin[1] - pbin[0]) * 2
        grad_xx_i = tape_bnd.gradient(grad_x_i, input_bulk_x) / (pbin[1] - pbin[0]) * 2
        
        del tape_bnd
        
        loss_bnd_r = tf.math.reduce_mean(tf.math.square(prediction_plb_r - prediction_pub_r))
        loss_bnd_i = tf.math.reduce_mean(tf.math.square(prediction_plb_i - prediction_pub_i))
        
        loss_bnd_dr = tf.math.reduce_mean(tf.math.square(loss_bnd_dl_r - loss_bnd_du_r))
        loss_bnd_di = tf.math.reduce_mean(tf.math.square(loss_bnd_dl_i - loss_bnd_du_i))
        
        h_mag_sq = prediction_bulk_r**2 + prediction_bulk_i**2
        loss_bulk_r = tf.math.reduce_mean(tf.math.square(-grad_t_i+0.5*grad_xx_r+h_mag_sq*prediction_bulk_r))
        loss_bulk_i = tf.math.reduce_mean(tf.math.square(grad_t_r+0.5*grad_xx_i+h_mag_sq*prediction_bulk_i))
        
        loss_t_sum = loss_t0 + loss_t1
        loss_bnd_sum = loss_bnd_r + loss_bnd_i + loss_bnd_dr + loss_bnd_di
        loss_bulk_sum = loss_bulk_r + loss_bulk_i
        loss_total = loss_t_sum + loss_bnd_sum + loss_bulk_sum
        
        return [loss_total, loss_t_sum, loss_bnd_sum, loss_bulk_sum]
    
    
    @tf.function
    def train_step(input_t0, label_t0, input_plb, input_pub, input_ptb, input_bulk_x, input_bulk_t, optimizer_in, pbin, tbin):
        with tf.GradientTape() as tape:
            loss_total_loc, loss_t_sum_loc, loss_bnd_sum_loc, loss_bulk_sum_loc = loss_function(
                input_t0, label_t0, input_plb, input_pub, input_ptb, input_bulk_x, input_bulk_t, pbin, tbin)
        gradients = tape.gradient(loss_total_loc, tf_model.trainable_variables)
        optimizer_in.apply_gradients(zip(gradients, tf_model.trainable_variables))
        return [loss_total_loc, loss_t_sum_loc, loss_bnd_sum_loc, loss_bulk_sum_loc]
    
    
    def function_factory(model, loss, input_t0, label_t0, input_plb, input_pub, input_ptb, input_bulk_x, input_bulk_t, pbin, tbin):
        # obtain the shapes of all trainable parameters in the model
        shapes = tf.shape_n(model.trainable_variables)
        n_tensors = len(shapes)
        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to
        # prepare required information first
        count = 0
        idx = [] # stitch indices
        part = [] # partition indices
        for i, shape in enumerate(shapes):
            n = np.product(shape)
            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))
            part.extend([i]*n)
            count += n
        part = tf.constant(part)
        
        @tf.function
        def assign_new_model_parameters(params_1d):
            params = tf.dynamic_partition(params_1d, part, n_tensors)
            for i, (shape, param) in enumerate(zip(shapes, params)):
                model.trainable_variables[i].assign(tf.reshape(param, shape))
        
        # now create a function that will be returned by this factory
        @tf.function
        def f(params_1d):
            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters
            with tf.GradientTape() as tape:
                # update the parameters in the model
                assign_new_model_parameters(params_1d)
                # calculate the loss
                loss_value, _, _, _ = loss(input_t0, label_t0, input_plb, input_pub, input_ptb, input_bulk_x, input_bulk_t, pbin, tbin)
                
            # calculate gradients and convert to 1D tf.Tensor
            grads = tape.gradient(loss_value, model.trainable_variables)
            grads = tf.dynamic_stitch(idx, grads)
            
            # print out iteration & loss
            f.iter.assign_add(1)
            tf.print(""TFP Iter:"", f.iter, "", TFP loss:"", loss_value)
            
            # store loss value so we can retrieve later
            tf.py_function(f.history.append, inp=[loss_value], Tout=[])
            
            return loss_value, grads
        
        # store these information as members so we can use them outside the scope
        f.iter = tf.Variable(0)
        f.idx = idx
        f.part = part
        f.shapes = shapes
        f.assign_new_model_parameters = assign_new_model_parameters
        f.history = []
        
        return f
    
    
    
    for epoch in range(50000):
        loss_loc, lt_loc, lbc_loc, lb_loc = train_step(
            tf_t0, tf_ans_t0, tf_xlb_x_ten, tf_xub_x_ten, tf_xb_t_ten, tf_bulk_px_tensor, tf_bulk_pt_tensor, optimizer, pos_bound, t_bound)
        if epoch % 10 == 0:
            tf.print(""Finished epoch"", epoch, "", loss_loc = "", loss_loc, "", lt_loc = "", lt_loc, "", lbc_loc = "", lbc_loc, "", lb_loc = "", lb_loc)
    
    
    
    func = function_factory(tf_model, loss_function, tf_t0, tf_ans_t0, tf_xlb_x_ten, tf_xub_x_ten, tf_xb_t_ten, tf_bulk_px_tensor, tf_bulk_pt_tensor, pos_bound, t_bound)
    init_params_TFP = tf.dynamic_stitch(func.idx, tf_model.trainable_variables)
    results = tfp.optimizer.lbfgs_minimize(
        value_and_gradients_function=func, initial_position=init_params_TFP, max_iterations=50000, 
        tolerance=1.0 * np.finfo(float).eps, num_correction_pairs=50)
    
    


Some comments: 
Since GlorotNormal() method generates initial weights randomly thus the provided code calls some input files related to initial weights and training set choice, in order to ensure the code is with the same initial state for different runs. Please run the packed code attached below.
    
[tf_testcase.zip](https://github.com/tensorflow/tensorflow/files/7631672/tf_testcase.zip)

    
    

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53263,mhlo mhlo.reduce legalize to linalg,"The original issue is described [here](https://github.com/google/iree/issues/7748). mhlo.reduce with variadic operands and different types  can't lower to linalg.

**System information**
- TF Version: master branch

**Describe the current behavior**
cd tensorflow/tensorflow/compiler/mlir/hlo
append below mhlo IR to file tests/hlo-legalize-to-linalg.mlir:
```
func @main_12(%arg0: tensor<128x784xf32>, %arg1: tensor<784x1024xf32>, %arg2: tensor<1024xf32>, %arg3:tensor<1024x1024xf32>, %arg4: tensor<1024xf32>, %arg5: tensor<1024x10xf32>, %arg6: tensor<10xf32>) -> tensor<128xi32> {
  %0 = mhlo.constant dense<0> : tensor<i32>
  %1 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %2 = mhlo.constant dense<0.000000e+00> : tensor<f32>
  %3 = mhlo.constant dense<0.000000e+00> : tensor<128x1024xf32>
  %4 = ""mhlo.dot_general""(%arg0, %arg1) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1],rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x784xf32>,tensor<784x1024xf32>) -> tensor<128x1024xf32>
  %5 = ""mhlo.broadcast_in_dim""(%arg2) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<1024xf32>) ->tensor<1x1024xf32>
  %6 = ""mhlo.broadcast_in_dim""(%5) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<1x1024xf32>) ->tensor<128x1024xf32>
  %7 = mhlo.add %4, %6 : tensor<128x1024xf32>
  %8 = mhlo.maximum %7, %3 : tensor<128x1024xf32>
  %9 = ""mhlo.dot_general""(%8, %arg3) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1],rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x1024xf32>,tensor<1024x1024xf32>) -> tensor<128x1024xf32>
  %10 = ""mhlo.broadcast_in_dim""(%arg4) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<1024xf32>) ->tensor<1x1024xf32>
  %11 = ""mhlo.broadcast_in_dim""(%10) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<1x1024xf32>) ->tensor<128x1024xf32>
  %12 = mhlo.add %9, %11 : tensor<128x1024xf32>
  %13 = mhlo.maximum %12, %3 : tensor<128x1024xf32>
  %14 = ""mhlo.dot_general""(%13, %arg5) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1],rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x1024xf32>,tensor<1024x10xf32>) -> tensor<128x10xf32>
  %15 = ""mhlo.broadcast_in_dim""(%arg6) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<10xf32>) ->tensor<1x10xf32>
  %16 = ""mhlo.broadcast_in_dim""(%15) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<1x10xf32>) ->tensor<128x10xf32>
  %17 = mhlo.add %14, %16 : tensor<128x10xf32>
  %18 = ""mhlo.reduce""(%17, %1) ( {
  ^bb0(%arg7: tensor<f32>, %arg8: tensor<f32>):  // no predecessors
    %31 = mhlo.maximum %arg7, %arg8 : tensor<f32>
    ""mhlo.return""(%31) : (tensor<f32>) -> ()
  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<128x10xf32>, tensor<f32>) -> tensor<128xf32>
  %19 = ""mhlo.broadcast_in_dim""(%18) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<128xf32>) ->tensor<128x1xf32>
  %20 = ""mhlo.broadcast_in_dim""(%19) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<128x1xf32>) ->tensor<128x10xf32>
  %21 = mhlo.subtract %17, %20 : tensor<128x10xf32>
  %22 = ""mhlo.exponential""(%21) : (tensor<128x10xf32>) -> tensor<128x10xf32>
  %23 = ""mhlo.reduce""(%22, %2) ( {
  ^bb0(%arg7: tensor<f32>, %arg8: tensor<f32>):  // no predecessors
    %31 = mhlo.add %arg7, %arg8 : tensor<f32>
    ""mhlo.return""(%31) : (tensor<f32>) -> ()
  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<128x10xf32>, tensor<f32>) -> tensor<128xf32>
  %24 = ""mhlo.broadcast_in_dim""(%23) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<128xf32>) ->tensor<128x1xf32>
  %25 = ""mhlo.log""(%24) : (tensor<128x1xf32>) -> tensor<128x1xf32>
  %26 = ""mhlo.broadcast_in_dim""(%25) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<128x1xf32>) ->tensor<128x10xf32>
  %27 = mhlo.subtract %21, %26 : tensor<128x10xf32>
  %28 = ""mhlo.iota""() {iota_dimension = 0 : i64} : () -> tensor<10xi32>
  %29 = ""mhlo.broadcast_in_dim""(%28) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<10xi32>) ->tensor<128x10xi32>
  %30:2 = ""mhlo.reduce""(%27, %29, %1, %0) ( {
  ^bb0(%arg7: tensor<f32>, %arg8: tensor<i32>, %arg9: tensor<f32>, %arg10: tensor<i32>):  // no predecessors
    %31 = ""mhlo.compare""(%arg7, %arg9) {compare_type = ""FLOAT"", comparison_direction = ""GT""} : (tensor<f32>, tensor<f32>)-> tensor<i1>
    %32 = ""mhlo.compare""(%arg7, %arg7) {compare_type = ""FLOAT"", comparison_direction = ""NE""} : (tensor<f32>, tensor<f32>)-> tensor<i1>
    %33 = mhlo.or %31, %32 : tensor<i1>
    %34 = ""mhlo.compare""(%arg7, %arg9) {compare_type = ""FLOAT"", comparison_direction = ""EQ""} : (tensor<f32>, tensor<f32>)-> tensor<i1>
    %35 = ""mhlo.compare""(%arg8, %arg10) {compare_type = ""SIGNED"", comparison_direction = ""LT""} : (tensor<i32>, tensor<i32>)-> tensor<i1>
    %36 = mhlo.and %34, %35 : tensor<i1>
    %37 = mhlo.or %33, %36 : tensor<i1>
    %38 = ""mhlo.select""(%33, %arg7, %arg9) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32>
    %39 = ""mhlo.select""(%37, %arg8, %arg10) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
    ""mhlo.return""(%38, %39) : (tensor<f32>, tensor<i32>) -> ()
  }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<128x10xf32>, tensor<128x10xi32>, tensor<f32>, tensor<i32>) ->(tensor<128xf32>, tensor<128xi32>)
  return %30#1 : tensor<128xi32>
}
```
and run:
```
./bin/mlir-hlo-opt -hlo-legalize-to-linalg -split-input-file tests/hlo-legalize-to-linalg.mlir
```
result:
```
#map0 = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
#map2 = affine_map<(d0, d1) -> (0, d1)>
#map3 = affine_map<(d0, d1) -> (d0)>
#map4 = affine_map<(d0, d1) -> (d0, 0)>
#map5 = affine_map<(d0) -> (d0)>
#map6 = affine_map<() -> ()>
module  {
  func @main_12(%arg0: tensor<128x784xf32>, %arg1: tensor<784x1024xf32>, %arg2: tensor<1024xf32>, %arg3: tensor<1024x1024xf32>, %arg4: tensor<1024xf32>, %arg5: tensor<1024x10xf32>, %arg6: tensor<10xf32>) -> tensor<128xi32> {
    %cst = arith.constant dense<0> : tensor<i32>
    %cst_0 = arith.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<f32>
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x1024xf32>
    %0 = ""mhlo.dot_general""(%arg0, %arg1) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x784xf32>, tensor<784x1024xf32>) -> tensor<128x1024xf32>
    %1 = linalg.init_tensor [1, 1024] : tensor<1x1024xf32>
    %2 = linalg.generic {indexing_maps = [#map0, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%arg2 : tensor<1024xf32>) outs(%1 : tensor<1x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<1x1024xf32>
    %3 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%2 : tensor<1x1024xf32>) outs(%3 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x1024xf32>
    %5 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %6 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%0, %4 : tensor<128x1024xf32>, tensor<128x1024xf32>) outs(%5 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.addf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x1024xf32>
    %7 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %8 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%6, %cst_2 : tensor<128x1024xf32>, tensor<128x1024xf32>) outs(%7 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.maxf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x1024xf32>
    %9 = ""mhlo.dot_general""(%8, %arg3) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x1024xf32>, tensor<1024x1024xf32>) -> tensor<128x1024xf32>
    %10 = linalg.init_tensor [1, 1024] : tensor<1x1024xf32>
    %11 = linalg.generic {indexing_maps = [#map0, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%arg4 : tensor<1024xf32>) outs(%10 : tensor<1x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<1x1024xf32>
    %12 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %13 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%11 : tensor<1x1024xf32>) outs(%12 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x1024xf32>
    %14 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %15 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%9, %13 : tensor<128x1024xf32>, tensor<128x1024xf32>) outs(%14 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.addf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x1024xf32>
    %16 = linalg.init_tensor [128, 1024] : tensor<128x1024xf32>
    %17 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%15, %cst_2 : tensor<128x1024xf32>, tensor<128x1024xf32>) outs(%16 : tensor<128x1024xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.maxf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x1024xf32>
    %18 = ""mhlo.dot_general""(%17, %arg5) {dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>, precision_config = [""DEFAULT"", ""DEFAULT""]} : (tensor<128x1024xf32>, tensor<1024x10xf32>) -> tensor<128x10xf32>
    %19 = linalg.init_tensor [1, 10] : tensor<1x10xf32>
    %20 = linalg.generic {indexing_maps = [#map0, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%arg6 : tensor<10xf32>) outs(%19 : tensor<1x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<1x10xf32>
    %21 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %22 = linalg.generic {indexing_maps = [#map2, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%20 : tensor<1x10xf32>) outs(%21 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x10xf32>
    %23 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %24 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%18, %22 : tensor<128x10xf32>, tensor<128x10xf32>) outs(%23 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.addf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x10xf32>
    %cst_3 = arith.constant 0xFF800000 : f32
    %25 = linalg.init_tensor [128] : tensor<128xf32>
    %26 = linalg.fill(%cst_3, %25) : f32, tensor<128xf32> -> tensor<128xf32> 
    %27 = linalg.generic {indexing_maps = [#map1, #map3], iterator_types = [""parallel"", ""reduction""]} ins(%24 : tensor<128x10xf32>) outs(%26 : tensor<128xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      %52 = arith.maxf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128xf32>
    %28 = linalg.init_tensor [128, 1] : tensor<128x1xf32>
    %29 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%27 : tensor<128xf32>) outs(%28 : tensor<128x1xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x1xf32>
    %30 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %31 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%29 : tensor<128x1xf32>) outs(%30 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x10xf32>
    %32 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %33 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%24, %31 : tensor<128x10xf32>, tensor<128x10xf32>) outs(%32 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.subf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x10xf32>
    %34 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %35 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%33 : tensor<128x10xf32>) outs(%34 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      %52 = math.exp %arg7 : f32
      linalg.yield %52 : f32
    } -> tensor<128x10xf32>
    %cst_4 = arith.constant 0.000000e+00 : f32
    %36 = linalg.init_tensor [128] : tensor<128xf32>
    %37 = linalg.fill(%cst_4, %36) : f32, tensor<128xf32> -> tensor<128xf32> 
    %38 = linalg.generic {indexing_maps = [#map1, #map3], iterator_types = [""parallel"", ""reduction""]} ins(%35 : tensor<128x10xf32>) outs(%37 : tensor<128xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      %52 = arith.addf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128xf32>
    %39 = linalg.init_tensor [128, 1] : tensor<128x1xf32>
    %40 = linalg.generic {indexing_maps = [#map3, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%38 : tensor<128xf32>) outs(%39 : tensor<128x1xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x1xf32>
    %41 = linalg.init_tensor [128, 1] : tensor<128x1xf32>
    %42 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%40 : tensor<128x1xf32>) outs(%41 : tensor<128x1xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      %52 = math.log %arg7 : f32
      linalg.yield %52 : f32
    } -> tensor<128x1xf32>
    %43 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %44 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%42 : tensor<128x1xf32>) outs(%43 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32):  // no predecessors
      linalg.yield %arg7 : f32
    } -> tensor<128x10xf32>
    %45 = linalg.init_tensor [128, 10] : tensor<128x10xf32>
    %46 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%33, %44 : tensor<128x10xf32>, tensor<128x10xf32>) outs(%45 : tensor<128x10xf32>) {
    ^bb0(%arg7: f32, %arg8: f32, %arg9: f32):  // no predecessors
      %52 = arith.subf %arg7, %arg8 : f32
      linalg.yield %52 : f32
    } -> tensor<128x10xf32>
    %47 = linalg.init_tensor [10] : tensor<10xi32>
    %48 = linalg.generic {indexing_maps = [#map5], iterator_types = [""parallel""]} outs(%47 : tensor<10xi32>) {
    ^bb0(%arg7: i32):  // no predecessors
      %52 = linalg.index 0 : index
      %53 = arith.index_cast %52 : index to i32
      linalg.yield %53 : i32
    } -> tensor<10xi32>
    %49 = linalg.init_tensor [128, 10] : tensor<128x10xi32>
    %50 = linalg.generic {indexing_maps = [#map0, #map1], iterator_types = [""parallel"", ""parallel""]} ins(%48 : tensor<10xi32>) outs(%49 : tensor<128x10xi32>) {
    ^bb0(%arg7: i32, %arg8: i32):  // no predecessors
      linalg.yield %arg7 : i32
    } -> tensor<128x10xi32>
    %51:2 = mhlo.reduce %46, %50, %cst_0, %cst ( {
    ^bb0(%arg7: tensor<f32>, %arg8: tensor<i32>, %arg9: tensor<f32>, %arg10: tensor<i32>):  // no predecessors
      %52 = linalg.init_tensor [] : tensor<i1>
      %53 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%arg7, %arg9 : tensor<f32>, tensor<f32>) outs(%52 : tensor<i1>) {
      ^bb0(%arg11: f32, %arg12: f32, %arg13: i1):  // no predecessors
        %70 = arith.cmpf ogt, %arg11, %arg12 : f32
        linalg.yield %70 : i1
      } -> tensor<i1>
      %54 = linalg.init_tensor [] : tensor<i1>
      %55 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%arg7, %arg7 : tensor<f32>, tensor<f32>) outs(%54 : tensor<i1>) {
      ^bb0(%arg11: f32, %arg12: f32, %arg13: i1):  // no predecessors
        %70 = arith.cmpf une, %arg11, %arg12 : f32
        linalg.yield %70 : i1
      } -> tensor<i1>
      %56 = linalg.init_tensor [] : tensor<i1>
      %57 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%53, %55 : tensor<i1>, tensor<i1>) outs(%56 : tensor<i1>) {
      ^bb0(%arg11: i1, %arg12: i1, %arg13: i1):  // no predecessors
        %70 = arith.ori %arg11, %arg12 : i1
        linalg.yield %70 : i1
      } -> tensor<i1>
      %58 = linalg.init_tensor [] : tensor<i1>
      %59 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%arg7, %arg9 : tensor<f32>, tensor<f32>) outs(%58 : tensor<i1>) {
      ^bb0(%arg11: f32, %arg12: f32, %arg13: i1):  // no predecessors
        %70 = arith.cmpf oeq, %arg11, %arg12 : f32
        linalg.yield %70 : i1
      } -> tensor<i1>
      %60 = linalg.init_tensor [] : tensor<i1>
      %61 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%arg8, %arg10 : tensor<i32>, tensor<i32>) outs(%60 : tensor<i1>) {
      ^bb0(%arg11: i32, %arg12: i32, %arg13: i1):  // no predecessors
        %70 = arith.cmpi slt, %arg11, %arg12 : i32
        linalg.yield %70 : i1
      } -> tensor<i1>
      %62 = linalg.init_tensor [] : tensor<i1>
      %63 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%59, %61 : tensor<i1>, tensor<i1>) outs(%62 : tensor<i1>) {
      ^bb0(%arg11: i1, %arg12: i1, %arg13: i1):  // no predecessors
        %70 = arith.andi %arg11, %arg12 : i1
        linalg.yield %70 : i1
      } -> tensor<i1>
      %64 = linalg.init_tensor [] : tensor<i1>
      %65 = linalg.generic {indexing_maps = [#map6, #map6, #map6], iterator_types = []} ins(%57, %63 : tensor<i1>, tensor<i1>) outs(%64 : tensor<i1>) {
      ^bb0(%arg11: i1, %arg12: i1, %arg13: i1):  // no predecessors
        %70 = arith.ori %arg11, %arg12 : i1
        linalg.yield %70 : i1
      } -> tensor<i1>
      %66 = linalg.init_tensor [] : tensor<f32>
      %67 = linalg.generic {indexing_maps = [#map6, #map6, #map6, #map6], iterator_types = []} ins(%57, %arg7, %arg9 : tensor<i1>, tensor<f32>, tensor<f32>) outs(%66 : tensor<f32>) {
      ^bb0(%arg11: i1, %arg12: f32, %arg13: f32, %arg14: f32):  // no predecessors
        %70 = select %arg11, %arg12, %arg13 : f32
        linalg.yield %70 : f32
      } -> tensor<f32>
      %68 = linalg.init_tensor [] : tensor<i32>
      %69 = linalg.generic {indexing_maps = [#map6, #map6, #map6, #map6], iterator_types = []} ins(%65, %arg8, %arg10 : tensor<i1>, tensor<i32>, tensor<i32>) outs(%68 : tensor<i32>) {
      ^bb0(%arg11: i1, %arg12: i32, %arg13: i32, %arg14: i32):  // no predecessors
        %70 = select %arg11, %arg12, %arg13 : i32
        linalg.yield %70 : i32
      } -> tensor<i32>
      ""mhlo.return""(%67, %69) : (tensor<f32>, tensor<i32>) -> ()
    }) {dimensions = dense<1> : tensor<1xi64>} : (tensor<128x10xf32>, tensor<128x10xi32>, tensor<f32>, tensor<i32>) -> (tensor<128xf32>, tensor<128xi32>)
    return %51#1 : tensor<128xi32>
  }
}


```

**Describe the expected behavior**
```%51:2 = mhlo.reduce %46, %50, %cst_0, %cst``` the reude should conversion to linalg.generic 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):yes
- Briefly describe your candidate solution(if contributing):The original conversion logic only supports the same input types, so simply remove this restriction."
53260,float16 constant readout doesn't match with it's original numpy array,"I tried to create float16 constant from numpy array and read it back, but the returned array doesn't match with it's original numpy array. This seems to be happening only at M1 Macs.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.3.1 on M1 Mac mini
- TensorFlow installed from (source or binary): Binary (tensorflow-macos)
- TensorFlow version (use command below): unknown 2.5.0
- Python version: 3.8.12

**Standalone code to reproduce the issue**

```python
import numpy as np
import tensorflow.compat.v1 as tf

np.random.seed(0)
with tf.Graph().as_default(), tf.Session() as sess:
    x = np.random.sample([1, 1, 1, 1, 768]).astype(np.float16)
    y = tf.constant(x)

    x = np.ones([1, 1, 384, 1, 1], dtype=np.float16)
    y = tf.constant(x, name='97f6762f328ca2b7cceaf2851e0c01a6')
    z = y * 3

    t = sess.graph.get_tensor_by_name('97f6762f328ca2b7cceaf2851e0c01a6:0')
    v = sess.run(t, feed_dict={})

    #print(x.squeeze())
    #print(v.squeeze())
    assert np.array_equiv(x, v)
```

**Describe the current behavior**

```
Traceback (most recent call last):
  File ""test.py"", line 19, in <module>
    assert np.array_equiv(x, v)
AssertionError
```

**Describe the expected behavior**

Two arrays should be equal.
"
53258,_7!Kn3Erz!h2A8n,https://github.com/jerzywozniak099-gmail-com/demo-repository/issues/1#issue-1067799493
53256," 
 ​  ​puts​ ​""I got some JSON: ​#{​push​.​inspect​}​""",https://github.com/tensorflow/tensorflow/issues/53255#issue-1067680018
53255," 
 ​  ​puts​ ​""I got some JSON: ​#{​push​.​inspect​}​""",https://github.com/github/platform-samples/blob/master/hooks/ruby/configuring-your-server/server.rb#L6
53254,tf keras metrics discards imaginary part of complex input and casts to float by taking the real part when passing complex numbers to metric function,"TF 2.4.1

When using a metric that accepts complex numbers, tf keras fit ends up discarding the imaginary part of the complex number and then casts the array to real when inputting y_pred and y_true to a metric function.

It works correctly when used as a loss function.  The casting only occurs when the loss function is used as a metric.

The error is because of casting done in metrics.py on line 609.  See the snippet below from this file.

```
  def update_state(self, y_true, y_pred, sample_weight=None):
    """"""Accumulates metric statistics.

    `y_true` and `y_pred` should have the same shape.

    Args:
      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
      sample_weight: Optional `sample_weight` acts as a
        coefficient for the metric. If a scalar is provided, then the metric is
        simply scaled by the given value. If `sample_weight` is a tensor of size
        `[batch_size]`, then the metric for each sample of the batch is rescaled
        by the corresponding element in the `sample_weight` vector. If the shape
        of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted
        to this shape), then each metric element of `y_pred` is scaled by the
        corresponding value of `sample_weight`. (Note on `dN-1`: all metric
        functions reduce by 1 dimension, usually the last axis (-1)).

    Returns:
      Update op.
    """"""
    y_true = math_ops.cast(y_true, self._dtype)  # THIS IS WRONG!
    y_pred = math_ops.cast(y_pred, self._dtype)
    [y_true, y_pred], sample_weight = \
        metrics_utils.ragged_assert_compatible_and_get_flat_values(
            [y_true, y_pred], sample_weight)
    y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(
        y_pred, y_true)

    ag_fn = autograph.tf_convert(self._fn, ag_ctx.control_status_ctx())
    matches = ag_fn(y_true, y_pred, **self._fn_kwargs)
    return super(MeanMetricWrapper, self).update_state(
        matches, sample_weight=sample_weight)
```

Setting a break point at the start of this function, we see the following vars in debug:
y_true
<tf.Tensor 'IteratorGetNext:3' shape=(None, 256, 256, 1) dtype=complex64>
self._dtype
'float32'

y_true should remain of type complex64 but instead is casted to float32 by ignoring the imaginary part and keeping the real part.

A correct solution will make an additional check for complex data in addition to the system floatx() type and cast accordingly (i.e. if floatx() is float32, complex128 becomes complex64; float64 goes to float32)."
53251,[TFLite] Support for depth-wise 3D convolution,"Hello,

TensorFlow Lite currently supports depth-wise convolutions through ` tf.keras.layers.DepthwiseConv2D` and ` tf.nn.depthwise_conv2d` layers that are converted into TFL `DEPTHWISE_CONV_2D` operators.

Such specific layers aren't available for 3D depth-wise convolutions but the ` tf.keras.layers.Conv3D` layer has a `groups` parameter that can be set to the same number as the input channels to compute the depth-wise 3D convolution. Such parametrized Conv3D layers are unfortunately not convertible in TFL. The example
```python
import tensorflow as tf

model = tf.keras.Sequential(
    [
        tf.keras.layers.InputLayer(
            input_shape=(2, 4, 4, 3), batch_size=1
        ),
        tf.keras.layers.Conv3D(filters=3, kernel_size=(2, 4, 4), groups=3),
    ]
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_quant_model = converter.convert()
```
errors-out in TF 2.7.0 with the following message as the TFL `CONV_3D` operator doesn't support the `groups` parameter (it works if the `groups` parameter is removed or set to 1):
```
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Conv3D
```

Is there any plan to eventually add  ` tf.keras.layers.DepthwiseConv3D` and ` tf.nn.depthwise_conv3d` layers to TF and later on add a  `DEPTHWISE_CONV_3D` operator in TFL? 

We could also detect ` tf.keras.layers.Conv3D` configurations that lead to a depth-wise convolution but interestingly the ` tf.keras.layers.Conv2D` layer with a `groups` parameter equals to the number of input channels isn't converted to a TFL `DEPTHWISE_CONV_2D` and the TFL inference errors-out with:
```
RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (12 != 1)Node number 1 (CONV_2D) failed to prepare.
```

Such cases could be detect for both `Conv2D` and `Conv3D` in the TFL converter so that a `DEPTHWISE_CONV_2/3D` is generated instead.


Thibaut"
53249,Bazel/clang doesn't seem to find the STD libraries on MacOS.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.6.1 Big Sur
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6
- Python version: 3.9
- Installed using virtualenv? pip? conda?: I use a conda environment for the compilers (bazel, clang, mpich)
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Clang 12.0.1, gcc is configured with clang v 13.0.0 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Hello! 
I am trying to install libtensorflow_cc.so and libtensorflow_framework.so using bazel on MacOS 11.6.1. 
I installed bazel 3.7.2 through conda and trying to call the targets but get errors like:

```
/Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/string.h:60:15: fatal error: 'string.h' file not found
#include_next <string.h>
```

I tried to use `xcode-select` to set the path of the standard libraries but it doesn't seem to work. I also checked the 
header files and they are in the correct location. I tried it with python 3.7 and python 3.9 and non of them work. 
I know that there have changes in the std library paths on MacOS distributions and I have been wondering if that caused the problems. 

This is the exact stack I get:

```
In file included from external/com_google_protobuf/src/google/protobuf/compiler/main.cc:33:
In file included from /Users/germp/mambaforge/envs/moose-tf39-conda/include/google/protobuf/compiler/cpp/cpp_generator.h:40:
In file included from /Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/string:511:
In file included from /Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/string_view:179:
In file included from /Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/__string:57:
In file included from /Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/algorithm:651:
In file included from /Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/cstring:60:
/Users/germp/mambaforge/envs/moose-tf39-bazel/bin/../include/c++/v1/string.h:60:15: fatal error: 'string.h' file not found
#include_next <string.h>
```

I have the following packages in my conda environment:
"
53248,Same model trained with different TF versions has much less trainable parameters,"![image](https://user-images.githubusercontent.com/39811082/144064416-e751e7be-9d66-4e94-ae0f-3c495c24ffd2.png)
Hi,
I trained the same architecture with different TF versions 2.2.0 and 2.4.0 and got much less trainable parameters with 2.4.0 according to model.summary report, as you can see in the attached image. What is the reason for such a large difference?"
53247,Quantized model demonstrates bad performance on DSP compared to CPU,"I trained the same binary segmentation model with the same architecture with TF2.2.0 vs TF2.4.0.
I converted the 2.4.0 model to a quantized TFlite and got inferior performance on DSP compared to CPU. 
By inferior performance, I mean that most of the maps are empty / contain very few white pixels, while the maps on the CPU seem as expected. 
The 2.2.0 model (2.2.0) showed similar results on both CPU and DSP.   

What can be the problem?

"
53246,tfhub yamnet model can't perform batch prediction,"I'm trying to use a pretrained model to extract embeddings.

Single observation
```
X = np.random.uniform(-1,1,(10, 1000)) # 10 samples of length 1000
yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(yamnet_model_handle)

yamnet_model(X[0]) # Works, outputs (scores, embeddings, spectrogram)
```

Multiple observations does not work
```
yamnet_model(X) # Doesn't work
hub.KerasLayer(yamnet_model_handle, 
               trainable=False,)(X) #Doesn't work either
````

How can I get the output for all observations in X at once? Is this intended? "
53245,[Feature Request] Make tf.unique operations return mask,"**System information**
- TensorFlow version (you are using): 2.7 (request independent of version)
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
tf.unique (also tf.raw_ops versions) return the indices of first occurrence for each element in a tensor as well as the unique
elements themselves. In addition, I would like the API to return a boolean_mask. The reason is that I need to filter the values of a 
tensor B depending on the unique values of a tensor A. Right now I need to construct the mask from indices which is tedious,
error-prone, not very clean and slower since the result is basically already computed. 

**Will this change the current api? How?**
Yes. tf.raw_ops.Unique* methods accept an additional parameter ""return_mask"" with ""False"" as default. Using ""True"" an additional
output "".mask"" is included in the result.

**Who will benefit with this feature?**
People who would like to filter a set of tensors based on the result of another."
53243,CUDA_ERROR_ILLEGAL_ADDRESS when calling tf.debugging.assert_less using tf.uint8,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): collab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-0-gc256c071bb2 2.7.0
- Python version: 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2
- GPU model and memory: K80 12GB (collab)


**Describe the current behavior**
tf 2.7 throws `CUDA_ERROR_ILLEGAL_ADDRESS` on calling `tf.debugging.assert_less` using `dtype=tf.uint8` tensors
When running with `CUDA_LAUNCH_BLOCKING` = 1 we can see some more information:

running:
```
a = tf.convert_to_tensor((2,), dtype=tf.uint8)
b = tf.convert_to_tensor((4,), dtype=tf.uint8)
tf.debugging.assert_less(
        a,
        b,
        message=""assertion failed"",
    )
```
gives:
`InternalError: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_ILLEGAL_ADDRESS' [Op:Less]`


**Describe the expected behavior**
this worked in tf 2.6, should still work

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1bn9grH_nI76_BtTUvyXVJQt0kZ6TPg0M?usp=sharing


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53242,Perfect shuffle without buffer of size len(dataset),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
The new feature will allow perfect shuffle without using a buffer of size len(dataset). When using tf.data.Dataset object for data pipeline, to faster the data process during model training, we use tf.data.Dataset.cache method. But, with large dataset or/and limited RAM capacity, we cannot use perfect shuffle method because it need the same amount of RAM capacity and can cause an Out of Memory error. 

***Exemple of actual behaviour***

```python
ds = tf.data.Dataset.range(100000)  # large dataset
ds = ds.map(SomeTransformation)
ds = ds.cache() # RAM is 70% use
ds = ds.map(SomeTransformationThatUseRandomBehavior)
ds = ds.shuffle(buffer_size=len(ds))  # (Perfect) shuffle cannot be done because of an OOM Error 

...

model.fit(ds)
```

***Exemple of the new feature***

```python
ds = tf.data.Dataset.range(100000)  # large dataset
ds = ds.map(SomeTransformation)
ds = ds.cache() # RAM is 70% use
ds = ds.map(SomeTransformationThatUseRandomBehavior)
ds = ds.shuffle(buffer_size=None)  # (Perfect) shuffle will not cause an OOM Error because 
                                   # it will no longer need a buffer to work

...

model.fit(ds)
```

***Current solution but not perfect***

```python
ds = tf.data.Dataset.range(100000)  # large dataset
ds = ds.map(SomeTransformation)
ds = ds.cache(""path/to/file"") # RAM is no longer use
ds = ds.map(SomeTransformationThatUseRandomBehavior)
ds = ds.shuffle(buffer_size=len(ds))  # (Perfect) shuffle work but take more time
                                      # to read from a cache file than read from a cache  in RAM

...

model.fit(ds)
```

**Will this change the current api? How?**
tf.data.Dataset.shuffle now allow buffer_size to be set to None

**Who will benefit with this feature?**
 Anyone who are using large datasets and those who are limited with RAM capacity (can only fit once the data in RAM)

**Other Info**
An another shuffle option, if perfect shuffle cannot be done due to some technical constraint, can be to randomly select an element of the dataset. that's mean 2 elements can be draw. "
53241,iOS using TFLiteC API TfLiteInterpreterInvoke() cause memory continues increasing on Metal delegate mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):not really
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 12
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- GPU model and memory: GPU(Metal)

**Describe the current behavior**
I'm using TensorFlowLiteC API to handle video per frame. The model I used is trained by myself. The memory continues increasing while using Metal delegate after called TfLiteInterpreterInvoke() method.  This issue not happens while using CPU mode on iOS, and not happened on Android using GPU.

I'm running this demo code 30 times to simulate a 30 seconds duration video. And invoke the interpreter 30 times in one second to simulate 30 fps. The picture bellow shows memory decrease between the twice play event.

![3491638255212_ pic_hd](https://user-images.githubusercontent.com/3894482/144003649-acddafa6-2c6d-421a-b72d-fe735ff3d339.jpg)

Note: If the video is long enough,  the app will crash!!!

The memory usage using CPU mode:

![3471638254474_ pic_hd](https://user-images.githubusercontent.com/3894482/144006077-c15e9caa-4227-4164-989e-adacf1934f30.jpg)

**Describe the expected behavior**
Hope the memory not increasing continuously while using Metal delegate on iOS. 

**Standalone code to reproduce the issue**
The attachment bellow is the demo project to reproduce the issue. Please execute `pod install`  and then configure the code signing to run on iPhone (not simulator). After launch the app, please press the button `start`. You can see the issue.

PS: I don't think the model is the problem. Because using `mobilenet_v1_1.0_224.tflite` model has the same problem.
[demo.zip](https://github.com/tensorflow/tensorflow/files/7623664/demo.zip)

Any suggestion is appreciate. Thanks. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
there seems no useful log output
"
53240,Adding GPU delegate fails for (mediapipe) Hand landmarks TFLite model in C API,"I am trying to run handlandmark TFLite model in a Visual C++ 2022 project (running it on Windows 10 desktop) and am using TensorFlowLite's C API (I have built tensorflow 2.6.0 from source & am using it's dynamic library dll linked with my project) to locate the hand landmark points, after using the palm detector model. I am using GPU delegate for both palm detector model as well as hand landmark model, downloaded from below mediapipe link:

https://google.github.io/mediapipe/solutions/models.html#hands

However, while adding & using GPU delegate for palm detector model works prefectly, adding GPU delegate for hand landmark model fails (maybe in TfLiteInterpreterOptionsAddDelegate()), as the function TfLiteInterpreterCreate() returns NULL.
Note that if I do not use GPU delegate, the interpreter creation works fine, & I am also able to locate and display the 21 hand landmarks correctly in the image/frame.

There is no error or crash reported, only that debugging indicates that the interpreter creation function returns NULL, if GPU delegate is added to interpreter options.

Below is the code snippet:

```
//string hand_model_name = <path to hand landmark model>.

TfLiteModel *m_hand_landmark_model = TfLiteModelCreateFromFile(hand_model_name.c_str());	
	if (!m_hand_landmark_model)
		return kTfLiteError;

	// set GPU delegate for faster inference in TFLite.
	TfLiteGpuDelegateOptionsV2 gpu_options = TfLiteGpuDelegateOptionsV2Default();
	TfLiteDelegate *m_hand_gpu_delegate = TfLiteGpuDelegateV2Create(&gpu_options);
	TfLiteInterpreterOptions *m_tfl_hand_interpreter_options = TfLiteInterpreterOptionsCreate();
	
	TfLiteInterpreterOptionsAddDelegate(m_tfl_hand_interpreter_options, m_hand_gpu_delegate); // hand interpreter creation fails due to this function failing. If comment this line, below function creates the interpreter as desired.

	// (2) create interpreter.
	TfLiteInterpreter *m_hand_interpreter = TfLiteInterpreterCreate(m_hand_landmark_model, m_tfl_hand_interpreter_options);
	if (!m_hand_interpreter)
		return kTfLiteError;
```

   Can anyone please help me out on why this issue is coming & how to resolve it ? 


Thanks,
Abhijit."
53239,Linker error on TF 2.6.2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 35
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.2
- Python version:3.9.7
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):11.2.1



**Describe the problem**
I run a clean build of tensorflow by running the following command:
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
and the build goes on fine, but at the linking phase of it all I get this error printed out many times over:
```
[bkeys@great-fox tensorflow]$ bazel build --config=dynamic_kernels //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [dynamic_kernels]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=186
INFO: Reading rc options for 'build' from /home/bkeys/Devel/Software/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/bkeys/Devel/Software/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/bkeys/Devel/Software/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python3
INFO: Found applicable config definition build:short_logs in file /home/bkeys/Devel/Software/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/bkeys/Devel/Software/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:dynamic_kernels in file /home/bkeys/Devel/Software/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Found applicable config definition build:linux in file /home/bkeys/Devel/Software/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /home/bkeys/Devel/Software/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/bkeys/.cache/bazel/_bazel_bkeys/db9eb43ef745d6cd6be7b7211ca56ded/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/bkeys/Devel/Software/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/bkeys/Devel/Software/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /home/bkeys/.cache/bazel/_bazel_bkeys/db9eb43ef745d6cd6be7b7211ca56ded/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /home/bkeys/.cache/bazel/_bazel_bkeys/db9eb43ef745d6cd6be7b7211ca56ded/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/bkeys/Devel/Software/tensorflow/tensorflow/python/BUILD:3110:24: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/ppc-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsClose: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsRead: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsWrite: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsTruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsFileSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsFileControl: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsFileControlHint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsDeviceCharacteristics: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsShmLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsOpen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsAccess: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsDlClose: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsSleep: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BeginBenignMalloc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3EndBenignMalloc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MutexAlloc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_mutex_free: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_mutex_enter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_mutex_leave: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MallocSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DbMallocSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_msize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function measureAllocationSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FaultSim: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Atoi64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Atoi64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function putVarint64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function findElementWithHash: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function findElementWithHash: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3HashFind: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixEnterMutex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixLeaveMutex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fileHasMoved: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixFileLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixFileLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dotlockCheckReservedLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixShmRegionPerMap: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixUnmapfile: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixUnfetch: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcacheUnpin: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheDrop: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheMakeClean: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheCleanAll: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheMove: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheMove: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetTreeToList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetTreeToList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetNDeepTree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetNDeepTree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetListToTree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetListToTree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function read32bits: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function read32bits: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function write32bits: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function write32bits: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerUnlockDb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerUnlockDb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_write_changecounter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_write_changecounter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function databaseIsUnmoved: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PagerGet: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walShmBarrier: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerPagecount: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerPagecount: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unlockBtreeMutex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeParseCellPtrNoPayload: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeGetPage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeGetPage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function finalDbSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function finalDbSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function getCellInfo: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function computeCellSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function computeCellSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memIntValue: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeIntegerAffinity: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeChangeP2: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExplainPop: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_vtab_nochange: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_next_stmt: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_next_stmt: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeSorterMerge: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeSorterMerge: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprIsVector: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprTruthValue: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprIsInteger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprToRegister: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function setDoNotMergeFlagOnCopy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnExprCb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTableExprCb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_set_authorizer: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_set_authorizer: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindDbName: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindDbName: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function isDupColumn: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function isDupColumn: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListAssignCursors: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListAssignCursors: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FunctionSearch: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FunctionSearch: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkParentIsModified: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkParentIsModified: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_cancel_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_cancel_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaLocate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaLocate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SetJoinExpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SetJoinExpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unsetJoinExpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unsetJoinExpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function srclistRenumberCursors: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function srclistRenumberCursors: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3TriggerList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3TriggerList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function translateColumnToCopy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function translateColumnToCopy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopOutputAdjust: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopOutputAdjust: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function resolveRemoveWindowsCb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_set_last_insert_rowid: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_set_last_insert_rowid: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_txn_state: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_txn_state: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqliteDefaultBusyCallback: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeInvokeBusyHandler: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_busy_handler: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_busy_handler: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_progress_handler: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_progress_handler: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_trace_v2: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_trace_v2: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_commit_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_commit_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_update_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_update_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_rollback_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_rollback_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_wal_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_wal_hook: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_collation_needed: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_collation_needed: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_collation_needed16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_collation_needed16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_extended_result_codes: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_extended_result_codes: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DbNameToBtree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_db_readonly: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonHexToInt4: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonHexToInt4: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonParseFillInParentage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonParseFillInParentage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonRemoveAllNulls: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonRemoveAllNulls: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachNext: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachNext: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PagerLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PagerLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreePageLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreePageLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheTruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheTruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_truncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_truncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Int64ToText: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GetInt32: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GetInt32: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Atoi: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3AffinityType: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3AffinityType: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CompareAffinity: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IndexAffinityOk: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IndexAffinityOk: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walIndexWriteHdr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walIndexWriteHdr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walEncodeFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walEncodeFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walMerge: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walMerge: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeSwap: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeSwap: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriteBlob: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriteBlob: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlRead: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlRead: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DefaultRowEst: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DefaultRowEst: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walRestartHdr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walRestartHdr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixNextSystemCall: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixNextSystemCall: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixGetSystemCall: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixGetSystemCall: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_value_pointer: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_value_pointer: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pthreadMutexLeave: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pthreadMutexTry: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pthreadMutexEnter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MutexInit: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Strlen30: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InsertBuiltinFuncs: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InsertBuiltinFuncs: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_uri_parameter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_uri_parameter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_uri_key: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_uri_key: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_filename_journal: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_filename_wal: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function appendText: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function appendText: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixGetpagesize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function robust_ftruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function robust_ftruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixGetLastError: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixCheckReservedLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixCheckReservedLock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function posixOpen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function seekAndWriteFd: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function seekAndWriteFd: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixWrite: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixWrite: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixRead: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixRead: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixCurrentTimeInt64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixSleep: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixDlClose: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixDlSym: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixDlOpen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walDecodeFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walDecodeFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function readSuperJournal: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function readSuperJournal: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function binCollFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function binCollFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function estimateIndexWidth: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VListNameToNum: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VListNameToNum: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function patternCompare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function patternCompare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MemFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function computeHMS.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsSync.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walWriteToLog: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walWriteToLog: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walWriteOneFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function walWriteOneFrame: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerSyncHotJournal: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsCurrentTimeInt64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function invokeProfileCallback: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function invokeProfileCallback: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function setDateTimeToCurrent: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function setDateTimeToCurrent: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_mutex_try: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Init: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Shutdown: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetEntrySort: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function rowSetEntrySort: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MallocAlarm: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MallocAlarm: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_free: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3OsCloseFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DbFreeNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_str_reset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RowSetDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function resolveP2Values: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function resolveP2Values: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeDeleteAuxData: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeDeleteAuxData: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CollapseDatabaseArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CollapseDatabaseArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CloseSavepoints: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeMemFinalize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeMemFinalize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeMemClearExternAndSetNull: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeReleaseAndSetInt64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeReleaseAndSetInt64: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function changes: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function total_changes: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function last_insert_rowid: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_result_double: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function out2PrereleaseWithClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeMemClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_result_zeroblob: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_result_zeroblob: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_clear_bindings: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_clear_bindings: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function freeP4Mem: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function freeP4FuncCtx: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function agginfoFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3HashClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3HashClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecDestroy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecDestroy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Free: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Free: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1FreePage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1TruncateUnsafe: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1TruncateUnsafe: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function freeTempSpace: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriterFinish: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriterFinish: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlFreeChunks: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlTruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlTruncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function memjrnlClose: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function releaseAllSavepoints: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function releaseAllSavepoints: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabDisconnect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_free_table: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_free_table: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonParseReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonParseReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonParseFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachCursorReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachClose: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pthreadMutexFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SystemError: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ErrorFinish: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_complete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_complete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function nocaseCollatingFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function nocaseCollatingFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function getSafetyLevel: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function getSafetyLevel: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GetBoolean: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_uri_boolean: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ColumnIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ColumnIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriteVarint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbePmaWriteVarint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GetVarint32: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeRecordUnpack: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeRecordUnpack: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixAccess: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function unixAccess: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecTest: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheSetPageSize.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PcacheSetPageSize.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1EnforceMaxPage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1EnforceMaxPage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Destroy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Destroy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Shrink: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Shrink: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Truncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Truncate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerLockDb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pagerLockDb: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_wait_on_lock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pager_wait_on_lock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3PagerSync: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeLeaveAll: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function btreeLeaveAll: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeLeave: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeLeave: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeClrCopy: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ValueFree: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function releaseMemArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function releaseMemArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function columnMem: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function columnMem: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3JournalOpen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3JournalOpen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VectorFieldSubexpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VectorFieldSubexpr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function heightOfExprList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function heightOfExprList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function heightOfSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function heightOfSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprSetHeight: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprSetHeight: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dupedExprNodeSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dupedExprNodeSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dupedExprSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dupedExprSize: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IsTrueOrFalse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprIdToTrueFalse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprIdToTrueFalse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprNodeIsConstant: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprNodeIsConstant: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprSimplifiedAndOr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprSimplifiedAndOr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IsRowid: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnElistNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnElistNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnIdlistNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnIdlistNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DbIsNamed: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DbIsNamed: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FindIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprIdxCover: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprIdxCover: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IdListDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IdListDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IdListIndex.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3IdListIndex.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabModuleUnref: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabModuleUnref: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabUnlock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabUnlock: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeFreeOpArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeFreeOpArray: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeClearObject: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_stmt_status: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_stmt_status: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabUnlockList: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function callFinaliser: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function callFinaliser: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabSavepoint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabSavepoint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprListUsage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprListUsage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprSelectUsage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprSelectUsage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprUsageNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprUsageNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function termCanDriveIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopClearUnion: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopClearUnion: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function whereLoopClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_busy_timeout: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_busy_timeout: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_wal_autocheckpoint: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_free_filename: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Malloc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Malloc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3HashInsert: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3HashInsert: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MallocZero: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MallocZero: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecCreate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeMergeEngineNew: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function vdbeMergeEngineNew: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1ResizeHash: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1ResizeHash: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Create: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pcache1Create: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecSet: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3BitvecSet: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function addToSavepointBitvecs: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function addToSavepointBitvecs: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function subjournalPageIfRequired: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function subjournalPageIfRequired: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ThreadCreate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ThreadCreate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkTriggerDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkTriggerDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function deleteTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function clearSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function clearSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprDeleteNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprDeleteNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprListDeleteNN: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ParserReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ParserReset: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteReturning: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WindowDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WindowDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function upsertDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function upsertDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FreeIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteColumnNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteColumnNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function cteClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WithDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WithDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteTriggerStep: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteTriggerStep: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabEponymousTableClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SchemaClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SchemaClear: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ResetOneSchema: error: relocation overflow
try relinking with a smaller --stub-group-size
... (about this 1000 times later)
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprCodeTemp: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprVectorRegister: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprVectorRegister: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprCodeVector: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprCodeVector: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprCodeBetween: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprCodeBetween: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function codeEqualityTerm: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function codeEqualityTerm: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprAnalyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function exprAnalyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprAnalyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereExprAnalyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprCodeExprList.isra.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprCodeExprList.isra.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function windowAggStep.isra.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function windowAggStep.isra.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function windowReturnOneRow: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function windowCodeOp: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function windowCodeOp: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function makeSorterRecord: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function makeSorterRecord: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pushOntoSorter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pushOntoSorter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function selectInnerLoop: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function selectInnerLoop: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function updateAccumulator: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function updateAccumulator: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExprIfFalseDup.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateIndexKey: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateIndexKey: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateRowIndexDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateRowIndexDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RefillIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RefillIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function reindexTable.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function reindexTable.part.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function reindexDatabases: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function reindexDatabases: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereBegin: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3WhereBegin: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkScanChildren: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function fkScanChildren: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function multiSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function multiSelect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function updateFromSelect.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function updateFromSelect.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MaterializeView.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3MaterializeView.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ViewGetColumnNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ViewGetColumnNames: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3EndTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3EndTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Parser: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Parser: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RunParser.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RunParser.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameParseSql: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameParseSql: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dropColumnFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function dropColumnFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_declare_vtab: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_declare_vtab: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabConnect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabConnect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachConnect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function jsonEachConnect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3NestedParse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3NestedParse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ClearStatTables: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ClearStatTables: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DropTriggerPtr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DropTriggerPtr: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function destroyRootPage: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function openStatTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function openStatTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function analyzeDatabase: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function analyzeDatabase: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function analyzeTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function analyzeTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTestSchema: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTestSchema: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabFinishParse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VtabFinishParse: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Prepare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Prepare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3LockAndPrepare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3LockAndPrepare: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Prepare16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Prepare16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InitCallback: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InitCallback: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InitOne: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3InitOne: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3VdbeExec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_step: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_step: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabNext: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabNext: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabFilter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function pragmaVtabFilter: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_exec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_exec: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3AnalysisLoad: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3AnalysisLoad: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_get_table: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_get_table: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function execSql: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function execSql: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function execSqlF: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function execSqlF: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RunVacuum: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3RunVacuum: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function blobSeekToRow: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function blobSeekToRow: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_blob_reopen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_blob_reopen: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Init: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Init: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ReadSchema: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ReadSchema: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3StartTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3StartTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3LocateTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3LocateTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3LocateTableItem: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3SrcListLookup: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FkCheck: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FkCheck: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Update.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Update.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3UpsertDoUpdate: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function getRowTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function getRowTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3TriggerColmask: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3TriggerColmask: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CodeRowTriggerDirect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CodeRowTriggerDirect: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FkActions: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3FkActions: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CodeRowTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CodeRowTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateRowDelete: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateConstraintChecks: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3GenerateConstraintChecks: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Insert: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Insert: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteFrom.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DeleteFrom.constprop.0: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_blob_open: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_blob_open: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Pragma: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Pragma: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3DropTable: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CreateIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3CreateIndex: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3AddPrimaryKey: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Analyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3Analyze: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function attachFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function attachFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_table_column_metadata: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_table_column_metadata: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExpandSubquery: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3ExpandSubquery: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function selectExpander: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function selectExpander: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameResolveTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameResolveTrigger: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTableTest: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTableTest: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTableFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameTableFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function renameColumnFunc: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_reset_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_reset_auto_extension: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_complete16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_complete16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function openDatabase: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function openDatabase: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_open16: error: relocation overflow
try relinking with a smaller --stub-group-size
bazel-out/ppc-opt/bin/external/org_sqlite/_objs/org_sqlite/sqlite3.pic.o:sqlite3.c:function sqlite3_open16: error: relocation overflow
try relinking with a smaller --stub-group-size
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 29.392s, Critical Path: 28.37s
INFO: 2 processes: 2 internal.
FAILED: Build did NOT complete successfully
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```git clone https://github.com/tensorflow/tensorflow.git
git checkout origin/c2363d6
./configure
bazel build --config=dynamic_kernels //tensorflow/tools/pip_package:build_pip_package```

"
53238,nasm Error when compilig for other x86_64 architecture.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 21.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.9
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8
- CUDA/cuDNN version: no
- GPU model and memory: no
- Host processor architecture: Coffee Lake  (i7-8750H)
- Destination architecture: Ice Lake


**Describe the problem**
When trying to compile TF for Ice Lake, I got the following error when in the `libjpeg_turbo` dependency:

```
/bin/bash: line 1:  1429 Illegal instruction     (core dumped) bazel-out/k8-opt/bin/external/nasm/nasm -f elf64 -DELF -DPIC -D__x86_64__ -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfig.h)/ -I $(dirname bazel-out/k8-opt/bin/external/libjpeg_turbo/jconfigint.h)/ -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/ -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/ -o $out $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
 - Clone this repository. I am using master branch
 - Set the following env vars:
 ```bash
export BAZEL_VERSION=
export BUILD_AVX2_CONTAINERS=no
export BUILD_AVX_CONTAINERS=no
export BUILD_CLX_CONTAINERS=no
export BUILD_ICX_CLIENT_CONTAINERS=no
export BUILD_ICX_SERVER_CONTAINERS=yes
export BUILD_PY2_CONTAINERS=no
export BUILD_SKX_CONTAINERS=no
export BUILD_SSH=no

export BUILD_TF_BFLOAT16_CONTAINERS=no
export BUILD_TF_V2_CONTAINERS=yes

export ENABLE_DNNL1=no
export ENABLE_GCC8=no
export ENABLE_HOROVOD=no
export ENABLE_SECURE_BUILD=yes

export FINAL_IMAGE_NAME=test/optimized-tensorflow

export OPENMPI_DOWNLOAD_URL=
export OPENMPI_VERSION=
export HOROVOD_VERSION=
export INSTALL_HOROVOD_FROM_COMMIT=no
export IS_NIGHTLY=no
export RELEASE_CONTAINER=no

export TF_BUILD_VERSION_IS_PR=no
export TF_BUILD_VERSION=master

export TF_DOCKER_BUILD_DEVEL_BRANCH_IS_PR=no
export TF_DOCKER_BUILD_DEVEL_BRANCH=master

export TF_REPO=https://github.com/tensorflow/tensorflow

export ROOT_CONTAINER_TAG=latest-python3.9
export ROOT_CONTAINER=tensorflow/build
```
- go to directory `master/tensorflow/tools/ci_build/linux/mkl`
- run: `build-dev-container.sh`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached

[compilation.log](https://github.com/tensorflow/tensorflow/files/7620570/compilation.log)

I also changed the bazel execution line in the dockerfile to limit the number of jobs and provide more info when faling:
```diff
$ git diff Dockerfile.devel-mkl
diff --git a/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl b/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl
index 55f68a73..d1c3a35f 100755
--- a/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl
+++ b/tensorflow/tools/ci_build/linux/mkl/Dockerfile.devel-mkl
@@ -68,7 +68,7 @@ RUN echo ""import /root/.mkl.bazelrc"" >>/root/.bazelrc
 # Install futures>=0.17.1 for Python2.7 compatibility mode
 RUN ${PIP} install future>=0.17.1
 
-RUN bazel --bazelrc=/root/.bazelrc build -c opt \
+RUN bazel --bazelrc=/root/.bazelrc build -c opt --jobs=4 --verbose_failures \
     tensorflow/tools/pip_package:build_pip_package && \
     bazel-bin/tensorflow/tools/pip_package/build_pip_package ""${TF_NIGHTLY_FLAG}"" ""${WHL_DIR}"" && \
     ${PIP} --no-cache-dir install --upgrade ""${WHL_DIR}""/*.whl && \
```

I am not sure if this is the way of doing this kind of compilation of if I need to compile it in a Icelake machine.

"
53237,How to use benchmark,"I want to use [benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) to benchmark my compute graph and its individual operators.
I downloaded a Resnet50 saved_model file from tensorflow hub, then i want to frozen it into a .pb file to use benchmark tool. But i dont't know how to frozen it.
I compiled benchmark tool from the latest source code and it was able to run the [demo](https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip).
I want to know how can I frozen a saved_model into a .pb file which is able to be benchmarked by the tool."
53236,Weight Norm - ConverterError: input resource[0] expected type resource != bool,"Weight Norm - ConverterError: input resource[0] expected type resource != bool
### System information

-   Operating System: **Linux Ubuntu 20.04**
-   TF Version: **2.3, 2.6 or nightly**
-   Python Version: **3.6**


I'm trying to convert to TFLite a model with weight normalization. However, an error is thrown during conversion with both TOCO or MLIR.

Model:
```
def conv_bn_relu(x, filters, kernel_size, strides, name, dilation_rate=(1, 1), use_bn=True, use_relu=True, use_wn=False, use_leakyrelu=False, padding = 'same'):
    #print('conv_bn_relu')
    if use_wn:
        x = WeightNormalization(Conv2D(filters=filters, 
                                       kernel_size=kernel_size, 
                                       strides=strides, 
                                       padding=padding, 
                                       dilation_rate=dilation_rate,
                                       name=name), data_init=False)(x)
    else:
        x = Conv2D(filters=filters, 
                   kernel_size=kernel_size, 
                   strides=strides, 
                   padding=padding, 
                   dilation_rate=dilation_rate,
                   name=name)(x)

    if use_bn:
        x = BatchNormalization(name='%s_bn' % (name))(x)        
    if use_relu:
        x = Activation('relu', name='%s_relu' % (name))(x)
    
    
    if use_leakyrelu:
        x = tf.keras.layers.LeakyReLU(alpha=0.2, name='%s_leakyrelu' % (name))(x)

    return x
```

Conversion snippet:

```
converter = tf.lite.TFLiteConverter.from_keras_model(generator_model)
tflite_model = converter.convert()
open(_MODEL_FILENAME_, ""wb"").write(tflite_model)
```

Error thrown:
```
~/anaconda3/envs/tf23fix/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: input resource[0] expected type resource != bool, the type of functional_3_weight_normalization_cond_assert_equal_1_readvariableop_resource[0]
	In {{node functional_3/weight_normalization/cond/AssignVariableOp}}

```"
53235,error: illegal scale: INF trying Full integer quantization,"**System information**
- OS Platform and Distribution: Ubuntu 18.04.5 
- TensorFlow version: 2.7.0
- Python version: 3.9.5

**Describe the current behavior**
I am trying to quantize a model using the ""Full integer quantization"" guide existing in the ""Post-training quantization guide"". 

If I try to run the Dynamic range quantization the model is quantized correctly. However, when I run the Full integer quantization guide with the dummy dataset I got the following error:
```
Estimated count of arithmetic ops: 134.256 G  ops, equivalently 67.128 G  MACs
fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0
error: illegal scale: INF
Segmentation fault (core dumped)
```
[EDIT]: I put tensorflow version 3.7.0 instead of 2.7.0"
53234,Windows heap corruption caused by upgrading protobuf,"### System information
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
     Windows

- TensorFlow version:
   At https://github.com/tensorflow/tensorflow/pull/52853

- Bazel version (if compiling from source):
  3.7.2
- GCC/Compiler version (if compiling from source):
  MSVC

### Describe the problem
This is a summary of the Windows heap corruption bug discovered when upgrading protobuf in https://github.com/tensorflow/tensorflow/pull/52853#issuecomment-963081770.

In the PR, we tried to upgrade protobuf from 3.9.2 to 3.19.0, but some tf optimizer related tests are failing on Windows with
```
Windows fatal exception: code 0xc0000374
```
The error code indicates a heap corruption. After some investigation, we discovered the problem is caused by:
- How protobuf is linked in TensorFlow on Windows
   On Windows, most of TF C++ code is linked into `_pywrap_tensorflow_internal.pyd` and some extension code is linked into individual pyd files, like `_pywrap_tf_optimizer.pyd`. The later is also linked to the former. However, the protobuf library is statically linked into both dynamic libraries.
- How protobuf works
  Basically, protobuf [deletes some global string in a proto desctructor](https://cs.opensource.google/protobuf/protobuf/+/master:src/google/protobuf/arenastring.h;l=402) after comparing the address of [some global default string](https://cs.opensource.google/protobuf/protobuf/+/master:src/google/protobuf/message_lite.h;l=135). When there are two protobuf runtimes, there are two default strings with different addresses, which caused some memory to be accidentally deleted when mixed together.

 @acozzette also explained why protobuf doesn't work well when linked in multiple places at https://github.com/tensorflow/tensorflow/pull/52853#issuecomment-973138129


### Provide the exact sequence of commands / steps that you executed before running into the problem

To reproduce the original error in a full build:
```
git clone https://github.com/meteorcloudy/tensorflow.git
cd tensorflow
git fetch origin upgrade_protobuf_grpc
configure
bazel test --announce_rc --config=opt tensorflow/python/grappler:memory_optimizer_test --test_arg=MemoryOptimizerSwapTest.testNoSwapping
```

However, the full TF build isn't debuggable, I have constructed [some smaller targets](https://github.com/meteorcloudy/tensorflow/commit/55263135a5b5f71cc254838d6dd97852fb8758a9) to imitate the situation.
To build the minimal reproduce case in a debug build:
```
git clone https://github.com/meteorcloudy/tensorflow.git
cd tensorflow
git fetch origin reproduce_win_heap_corruption_2
configure
bazel run --announce_rc --config=dbg tensorflow/python/grappler:tf_optimizer_wrapper_bin
```

When debugging in Visual Studio, you should be able to see the following:
![image](https://user-images.githubusercontent.com/4171702/143913840-08256bf0-c657-4c74-b5f6-1424b8568b9b.png)

### Possible Solution

To properly solve this problem, we probably need to migrate TF to [cc_shared_library](https://github.com/bazelbuild/rules_cc/blob/main/examples/experimental_cc_shared_library.bzl), which makes dynamic linking more controllable.
"
53233,partially close xla,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): master
- Python version:3.8
- Bazel version (if compiling from source): 2.26
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.1 (clang-1001.0.46.4)
- CUDA/cuDNN version: -
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
v1.12.1-54214-gb51e7cff1aa 2.6.0

**Describe the current behavior**
1. gather in cluster_0
2. dynamic shape in XLA .

**Describe the expected behavior**
1. gather not in cluster_0
2. no dynamic shape in xla .

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): 
python:
```
if flag tf.function(jit_compile=False):
   op mark not in jit.
```
c++ xla jit mark_for_compilation_pass:
```
Status MarkForCompilationPassImpl::FindCompilationCandidates() {
...
for n in graph:
  if n.mark_not_xla:
      continue;
  else:
      add n to Candidates;
...
}
```


**Standalone code to reproduce the issue**
```
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

@tf.function(jit_compile=False,autograph=True)
def no_xla():
  # I don't want to use xla .
  input_a = tf.random.uniform([1024000], dtype=tf.int32, maxval=1024000)
  uniq_a, uniq_idx = tf.unique(input_a)
  uniq_a_shape = tf.shape(uniq_a)
  #uniq_a_shape_print = tf.Print(uniq_a_shape, [uniq_a_shape], ""zz:"", 10, 10)
  source_a = tf.gather(uniq_a, uniq_idx)
  source_a = source_a + source_a
  return source_a

source_a = no_xla()

#  I want to use xla .
res_a_1 = source_a * source_a
res_a_2 = res_a_1 + res_a_1
res_a_3 = res_a_2 + res_a_2
res_a_4 = res_a_3 + res_a_3
res_a_5 = res_a_4 + res_a_4
res_a_6 = res_a_5 + res_a_5


session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True,
                             log_device_placement=True)
#session_config.graph_options.rewrite_options.disable_meta_optimizer=True
session_config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1


with tf.compat.v1.Session(config = session_config) as sess:
    for i in range(16384) :
      _ = sess.run([res_a_6]);
      #print(res)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
2021-11-30 00:14:10.419537: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481] *** Clustering info for graph of size 22
2021-11-30 00:14:10.419579: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1482]  Built 1 clusters, size 14 / 22 (63.64%)
2021-11-30 00:14:10.419585: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1491]   cluster_0 14 / 22 (63.64%)
2021-11-30 00:14:10.419589: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1495]    AddV2: 1 instances
2021-11-30 00:14:10.419593: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1495]    Const: 6 instances
2021-11-30 00:14:10.419596: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1495]    GatherV2: 1 instances # I flag it in not xla, but gather in xla ....
2021-11-30 00:14:10.419600: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1495]    Mul: 6 instances
2021-11-30 00:14:10.419603: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1501]  Unclustered nodes: 8 / 22 (36.36%)
2021-11-30 00:14:10.419607: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1506]   Const: 3 instances
2021-11-30 00:14:10.419656: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1506]   NoOp: 2 instances
2021-11-30 00:14:10.419666: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1506]   RandomUniformInt: 1 instances
2021-11-30 00:14:10.419671: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1506]   Unique: 1 instances
2021-11-30 00:14:10.419674: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1506]   _Retval: 1 instances
2021-11-30 00:14:10.419685: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1561] *** Inter-Cluster edges:
2021-11-30 00:14:10.419689: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1583]  ** Cluster cluster_0
2021-11-30 00:14:10.419693: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1571]   2 incoming edges
2021-11-30 00:14:10.419698: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1573]    [none] StatefulPartitionedCall/Unique # 2
2021-11-30 00:14:10.419702: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1573]    [none] _SOURCE # 1
2021-11-30 00:14:10.419755: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1571]   2 outgoing edges
2021-11-30 00:14:10.419760: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1573]    [none] _SINK # 1
2021-11-30 00:14:10.419764: E tensorflow/compiler/jit/mark_for_compilation_pass.cc:1573]    [none] _retval_add_4_0_0 # 1
```"
53231,Error in lowering tf.Where with iree-import-tf,"I use iree-import-tf tool to convert some model, but failed, error log:
<unknown>:0: error: The following illegal operations still remain:
	tf.Where (count: 1)

"
53230,What is the difference between tpu_driver_client.TpuBackend.create and xla_client.make_tpu_client ？,"Questions in Xla Client Module：
Why TPU has two backend entrance for xla client?
What is the difference between tpu_driver_client and xla_client?
Can I get some ideas of the design in particular?
Thanks a lot."
53229,failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error,"**System information**
- Have I written custom code: below
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.12
- CUDA/cuDNN version: release 10.1, V10.1.243
- GPU model and memory: RTX2060 Mobile 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh):. Log file here [link](https://polimi365-my.sharepoint.com/:t:/g/personal/10607946_polimi_it/ESFGCuewPQRGt8Fv-dFQ61wB8TLOzfl8d1d5AaKyKiQVkg?e=33VUl1)
```Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_os.py"", line 17, in <module>
    platform.linux_distribution(),
AttributeError: module 'platform' has no attribute 'linux_distribution'
2021-11-29 11:17:14.503225: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 5, in <module>
    with tf.Session() as sess:
AttributeError: module 'tensorflow' has no attribute 'Session'
bazel: command not found
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.
```

**Describe the current behavior**
```
>>> import tensorflow as tf
2021-11-29 11:44:55.998697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> tf.config.list_physical_devices(""GPU"")
2021-11-29 11:45:19.987589: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-11-29 11:45:21.698177: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-11-29 11:45:21.698239: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: raffo-OMEN15
2021-11-29 11:45:21.698251: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: raffo-OMEN15
2021-11-29 11:45:21.698392: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.82.0
2021-11-29 11:45:21.698442: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.82.0
2021-11-29 11:45:21.698453: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.82.0
[]
```

I don't know if it can help: the same error happens in the tensorflow docker container. 
"
53228,How to remove NaN in tensor by specific values?,"Hi, I have two tensor, `a` and `b`, which have the same shape. I want to replace NaN value in `a` by corresponding value in `b`. I find `tf.tensor_scatter_nd_update` could replace value for tensor, but I don't know how to generate `indices` where to update. So how can I remove NaN in tensor?

Thanks!
"
53227,how to run tests in the tensorflow/compiler/mlir/hlo/tests,"Hi,
could you provide sime related command case to run the cases in the tensorflow/compiler/mlir/hlo/tests/*.mlir.

thanks"
53226,RecursionError: maximum recursion depth exceeded while calling a Python object,"# 安装 TensorFlow

import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)


but   
D:\Anaconda\envs\tensorflow\python.exe G:/Yolo/deep-learning-for-image-processing-master/deep-learning-for-image-processing-master/tensorflow_classification/Test1_official_demo/train_my.py
Traceback (most recent call last):
  File ""G:/Yolo/deep-learning-for-image-processing-master/deep-learning-for-image-processing-master/tensorflow_classification/Test1_official_demo/train_my.py"", line 3, in <module>
    import tensorflow as tf
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\eager\context.py"", line 32, in <module>
    from tensorflow.core.framework import function_pb2
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\core\framework\function_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""D:\Anaconda\envs\tensorflow\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 158, in register
    if issubclass(subclass, cls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 226, in __subclasscheck__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 207, in __subclasscheck__
    ok = cls.__subclasshook__(subclass)
  File ""D:\Anaconda\envs\tensorflow\lib\typing.py"", line 860, in __extrahook__
    if issubclass(subclass, scls):
  File ""D:\Anaconda\envs\tensorflow\lib\abc.py"", line 197, in __subclasscheck__
    if subclass in cls._abc_cache:
RecursionError: maximum recursion depth exceeded while calling a Python object

Process finished with exit code 1

HELP ME!!!!!!



"
53225,Will benchmark use GPU?,"I'm running bench mark on desktop following the [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark).
Will this command below use GPU?
```sh
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
  --graph=tensorflow_inception_graph.pb \
  --input_layer=""input:0"" \
  --input_layer_shape=""1,224,224,3"" \
  --input_layer_type=""float"" \
  --output_layer=""output:0""
```
If not, how can I run benchmark on GPU?"
53224,[Q&A] Do tf.keras.model convert numpy.ndarray to tensor for every batch?,"I currently use tensorflow 2.5 with GPU Quadro P1000. The tensorflow was built with cudatoolkit and cudnn to activate my GPU

In current, I have a large numpy array (4Gb) with np.uint8 dtype. The model was built using tf.keras.model but at every layer, the argument `dtype`, i use `np.float32` but not `tf.float32`. 

Although when calling `model.fit(ndarray)`, I don't convert to `tf.Tensor` and GPU still worked woth 40 - 50%. 

My question is
(1): Is it possible to set the layers' dtype by `tf.dtype` and use numpy array to directly `model.fit()` or `model.predict()` with active GPU without error, and vice versa?
(2) Does tensorflow, for every batch fed to `tf.keras.Model` automatically convert to the Tensor before training it (with the setting mentioned above), and if so, do I should convert to `tf.Tensor` beforehand or `tf.data.Dataset` to boost performance and input pipeline, in that case, do I need to disable eager execution or do some trick to boost performance? 

At current, the model is trained with 240 - 300 seconds per epoch with ~ 2300 batchs (256 batch size)

Thank you. I love to hear some response

Windows 10 Pro, TF 2.5, Numpy 1.19+  Python 3.8.11"
53223,How can I find map@50 and ap for whole test data on tflite model?,"Hi, 
is there any way to find custom trained tflite model map@50 and ap on whole test dataset?"
53222,Why the eigenvalue output is different for tensorflow and numpy?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Why the eigevectors output is different for tensorflow and numpy?
![image](https://user-images.githubusercontent.com/61866921/143771375-8cb7fc00-1482-4507-a963-6635c405497d.png)


**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53221,"ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1. Shapes are [256,128,2] and [256,128,1].","i got this error : 

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_36/3853691934.py in <module>
      3 
      4 # train model
----> 5 history = model.fit(train_dataset, epochs=1)

/opt/conda/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1182                 _r=1):
   1183               callbacks.on_train_batch_begin(step)
-> 1184               tmp_logs = self.train_function(iterator)
   1185               if data_handler.should_sync:
   1186                 context.async_wait()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    883 
    884       with OptionalXlaContext(self._jit_compile):
--> 885         result = self._call(*args, **kwds)
    886 
    887       new_tracing_count = self.experimental_get_tracing_count()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--> 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    758     self._concrete_stateful_fn = (
    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 760             *args, **kwds))
    761 
    762     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3064       args, kwargs = None, None
   3065     with self._lock:
-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)
   3067     return graph_function
   3068 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3461 
   3462           self._function_cache.missed.add(call_context_key)
-> 3463           graph_function = self._create_graph_function(args, kwargs)
   3464           self._function_cache.primary[cache_key] = graph_function
   3465 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3306             arg_names=arg_names,
   3307             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3308             capture_by_value=self._capture_by_value),
   3309         self._function_attributes,
   3310         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)
   1005         _, original_func = tf_decorator.unwrap(python_func)
   1006 
-> 1007       func_outputs = python_func(*func_args, **func_kwargs)
   1008 
   1009       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    666         # the function a weak reference to itself to avoid a reference cycle.
    667         with OptionalXlaContext(compile_with_xla):
--> 668           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    669         return out
    670 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    992           except Exception as e:  # pylint:disable=broad-except
    993             if hasattr(e, ""ag_error_metadata""):
--> 994               raise e.ag_error_metadata.to_exception(e)
    995             else:
    996               raise

ValueError: in user code:

    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:842 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:835 run_step  **
        outputs = model.train_step(data)
    /opt/conda/lib/python3.7/site-packages/keras/engine/training.py:787 train_step
        y_pred = self(x, training=True)
    /opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py:1037 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:415 call
        inputs, training=training, mask=mask)
    /opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:550 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py:1037 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/keras/layers/merge.py:178 call
        return self._merge_function(inputs)
    /opt/conda/lib/python3.7/site-packages/keras/layers/merge.py:518 _merge_function
        return backend.concatenate(inputs, axis=self.axis)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/keras/backend.py:3069 concatenate
        return tf.concat([to_dense(x) for x in tensors], axis)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1769 concat
        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:1228 concat_v2
        ""ConcatV2"", values=values, axis=axis, name=name)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal
        compute_device)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal
        op_def=op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2042 __init__
        control_input_ops, op_def)
    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op
        raise ValueError(str(e))

    ValueError: Dimension 2 in both shapes must be equal, but are 2 and 1. Shapes are [256,128,2] and [256,128,1]. for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](model/conv2d_transpose/BiasAdd, model/conv2d_3/Relu, model/concatenate/concat/axis)' with input shapes: [256,128,2,32], [256,128,1,32], [] and with computed input tensors: input[2] = <3>.

what should I do to tackle this problem? please.
I'm new in deep learning field and this problem appears in my segmentation model."
53220,Tensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- Windows 10 Home (19043.1348):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX 1080 Ti (11 GB)



**Describe the current behavior**

For some reason I am getting different tensor dimensions when using `gather` in TF 2:

1. The first dimension becomes `None` when I use tensor as an index vector 
2. The first dimension becomes `len(indices)` (as it should) where 'indices' are regular Python list

**This happens only in eager mode (e.g., inside a custom loss function)**

(Same happens when using `boolean_mask`)



**Standalone code to reproduce the issue**

    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Input, Dense, Reshape
    from tensorflow.keras.datasets import mnist
 
    def cutsom_gan_loss_env(model):
       def custom_loss(y_true,y_pred):

        ff = tf.where([True, True, False , False])[:, 0]
        with tf.GradientTape(persistent=True) as tape:
             tf.print(tf.gather(y_true, [0, 1], axis=0).shape) #prints 2,28,28
             tf.print(tf.gather(y_true, ff, axis=0).shape)#prints None,28,28
             tape.watch(y_true)
             yy = model(y_true)
             d_yy = tape.gradient(yy,y_true)
             des_loss = tf.reduce_mean(d_yy)

        return des_loss

    return custom_loss


    def main_():
       n_hidden_units = 5
       num_lay = 3
       kernel_init = keras.initializers.RandomUniform(-0.1, 0.1)
       (x_train, y_train), _ = mnist.load_data()
       x_train = tf.cast(x_train,tf.float32)/255.
       inputs = Input(x_train.shape[1:])
       x = Dense(n_hidden_units,kernel_initializer=kernel_init,  activation='sigmoid' )(inputs)
       for _ in range(num_lay):
           x = Dense(n_hidden_units,kernel_initializer=kernel_init, activation='sigmoid', )(x)

       outputs =Reshape(x_train.shape[1:])(Dense(x_train.shape[1], kernel_initializer=kernel_init, activation='softmax')(x))
       model = Model(inputs=inputs, outputs=outputs)
       model.summary()
       optimizer1 = keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)
       model.compile(loss=cutsom_gan_loss_env(model), optimizer=optimizer1, metrics=None)
       model.fit(x_train,  x_train , batch_size=1000, epochs=1, shuffle=False)


    if __name__=='__main__':
        main_()

"
53218,Tflite c++ inference,"`` have a tflite model for mask detection with a sigmoid layer that outputs values between 0[mask] and 1[no_mask]

I examined the input and output node using netron and here's what I got
![hih](https://user-images.githubusercontent.com/67305499/143722280-44599d76-0038-4bb0-8cbc-1ef86eb0897c.png)
:
I tested the model for inference in python and it works great.
```
`# A simple inference pipline 

import numpy as np
import tensorflow as tf
import cv2


# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""efficient_net.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Rescale to [1,32,32,1].
input_shape = input_details[0]['shape']
img = cv2.imread(""nomask.jpg"")
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
input_data = img_gray[ ..., tf.newaxis]
input_data  =tf.image.resize(input_data, [32,32])
input_data = input_data[ tf.newaxis,...]
input_data = np.array(input_data, dtype=np.float32)


# setting input 
interpreter.set_tensor(input_details[0]['index'], input_data)


interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data[0][0])  `
```

I tried doing the same using c++ but I get either 0 or no output at all
here is my code 
```
#include <iostream>
#include <cstdio>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""
#include ""opencv2/opencv.hpp""

using namespace cv;


#define TFLITE_MINIMAL_CHECK(x)                                  \
    if (!(x))                                                    \
    {                                                            \
        fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
        exit(1);                                                 \
    }

int main(int argc, char* argv[])
{
    if (argc != 2)
    {
        fprintf(stderr, ""minimal <tflite model>\n"");
        return 1;
    }
    const char* filename = argv[1];



    // read image file
    cv::Mat img = cv::imread(""D:\\nomask.png"");

    // convert to float; BGR -> Grayscale
    cv::Mat inputImg;
    img.convertTo(inputImg, CV_32FC1);
    cv::cvtColor(inputImg, inputImg, cv::COLOR_BGR2GRAY);
    // resize image as model input
    cv::resize(inputImg, inputImg, cv::Size(32, 32));
    
    
    // Load model
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(filename);
    TFLITE_MINIMAL_CHECK(model != nullptr);

    // Build the interpreter with the InterpreterBuilder.
    // Note: all Interpreters should be built with the InterpreterBuilder,
    // which allocates memory for the Intrepter and does various set up
    // tasks so that the Interpreter can read the provided model.
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);
    TFLITE_MINIMAL_CHECK(interpreter != nullptr);

    // Allocate tensor buffers.
    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
    

    // Fill input buffers
    // TODO(user): Insert code to fill input tensors.
    // Note: The buffer of the input tensor with index `i` of type T can
    float* input = interpreter->typed_input_tensor<float>(0);
    input = inputImg.ptr<float>(0);
    
    // Run inference
    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
    printf(""\n\n=== Post-invoke Interpreter State ===\n"");
    

     
     
    
    float* output = interpreter->typed_output_tensor<float>(149);
    std::cout << output[0];
  

    return 0;
}
```
please help I have been stuck for 3 days :("
53217,Loading dynamic library cannot find definition of a symbol,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): using google-research/language
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): installed with pip
- TensorFlow version (use command below): git - v2.7.0-rc1-69-gc256c071bb2 , tf - 2.7.0
- Python version: 3.8.10 
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
- GPU model and memory: RTX 3090 , 20GB available

**Describe the current behavior**
The dynamic library file does not have definition/implementation for a symbol
```
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/kevin/pdev/lmrl/language/language/search_agents/muzero/muzero_main.py"", line 25, in <module>
    from language.search_agents.muzero import agent_lib
  File ""/home/kevin/pdev/lmrl/language/language/search_agents/muzero/agent_lib.py"", line 25, in <module>
    from muzero import actor
  File ""/home/kevin/pdev/lmrl/language/muzero/actor.py"", line 28, in <module>
    from seed_rl import grpc
  File ""/home/kevin/pdev/lmrl/language/seed_rl/grpc/__init__.py"", line 21, in <module>
    from seed_rl.grpc.python.ops import *  
  File ""/home/kevin/pdev/lmrl/language/seed_rl/grpc/python/ops.py"", line 25, in <module>
    from seed_rl.grpc.python.ops_wrapper import gen_grpc_ops
  File ""/home/kevin/pdev/lmrl/language/seed_rl/grpc/python/ops_wrapper.py"", line 25, in <module>
    gen_grpc_ops = tf.load_op_library(
  File ""/home/kevin/pdev/lmrl/language/language/search_agents/env/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 59, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/kevin/pdev/lmrl/language/seed_rl/grpc/python/../grpc_cc.so: undefined symbol: _ZN4absl14lts_2020_02_2518container_internal18global_next_sampleE
(env) root@kevin-System-Product-Name:/home/kevin/pdev/lmrl/language# 
```

**Describe the expected behavior**
The libraries load successfully and tensorflow can create the node graph on the gpu

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): fix some dependency?

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

It takes some time but setup google-research/language/language/seach_agents/muzero and run muzero_main.py

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53216,Error after running `python generate2.py --output_dir=/tmp/out`,"**System information**
- OS Platform and Distribution: Windows 10 1809 build: 17763.2300
- Mobile device if the issue happens on mobile device: 
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.6.0
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Got no GPU



**Describe the problem**
Followed [these instructions](https://www.tensorflow.org/community/contribute/docs#build_api_docs), but encountered the following error after running `python generate2.py --output_dir=/tmp/out`:

![q](https://user-images.githubusercontent.com/16363767/143685950-4fd00fd2-4b4a-478e-ac65-2592cc21ecd9.png)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```shell
pip install git+https://github.com/tensorflow/docs

git clone https://github.com/tensorflow/tensorflow
cd tensorflow/tensorflow/tools/docs
python generate2.py --output_dir=/tmp/out
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[tf.log](https://github.com/tensorflow/tensorflow/files/7611906/tf.log)

"
53215,TensorFlow Lite Converter changed output_shapes,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0 (for model training and exporting to saved model) &  2.7.0 (for exporting to TFLite in Google Colab)

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
Trained my model with model_main_tf2.py script before exporting to saved model using the exporter_main_v2.py script. After that I uploaded my saved model to Google Drive before trying to convert the model to TFLite format. Here is the code for TFLite Convert:

```
converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(MODEL_PATH,
    input_arrays=[""serving_default_input_tensor""] ,
    input_shapes={'serving_default_input_tensor':[1,640,640,3]},
    output_arrays=['StatefulPartitionedCall:1', 'StatefulPartitionedCall:2', 'StatefulPartitionedCall:4', 'StatefulPartitionedCall:5'])
converter.allow_custom_ops=False
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""/content/new.tflite"", ""wb"").write(tflite_model)
```

#### Reference for making the custom object detection model:
1)  Reference [Object Detection Tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html): Installation until exporting to saved model

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong: It changed the output shapes of each outputs from this:
```
INFO:tensorflow:output tensors info:
INFO:tensorflow:Tensor's key in saved_model's tensor_map: num_detections
INFO:tensorflow: tensor name: StatefulPartitionedCall:5, shape: (1), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: detection_classes
INFO:tensorflow: tensor name: StatefulPartitionedCall:2, shape: (1, 100), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: detection_boxes
INFO:tensorflow: tensor name: StatefulPartitionedCall:1, shape: (1, 100, 4), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_detection_scores
INFO:tensorflow: tensor name: StatefulPartitionedCall:7, shape: (1, 51150, 101), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: detection_multiclass_scores
INFO:tensorflow: tensor name: StatefulPartitionedCall:3, shape: (1, 100, 101), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: detection_anchor_indices
INFO:tensorflow: tensor name: StatefulPartitionedCall:0, shape: (1, 100), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: detection_scores
INFO:tensorflow: tensor name: StatefulPartitionedCall:4, shape: (1, 100), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: raw_detection_boxes
INFO:tensorflow: tensor name: StatefulPartitionedCall:6, shape: (1, 51150, 4), type: DT_FLOAT
```
to this:
```
print(input_details)
[{'name': 'serving_default_input_tensor', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 
'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 
'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 
'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

print(output_details)
[{'name': 'StatefulPartitionedCall', 'index': 2449, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:1', 'index': 2502, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:2', 'index': 2484, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:3', 'index': 2467, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:4', 'index': 2431, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:5', 'index': 2414, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:6', 'index': 519, 'shape': array([    1, 51150,     4], dtype=int32), 'shape_signature': array([    1, 51150,     4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, 
{'name': 'StatefulPartitionedCall:7', 'index': 527, 'shape': array([    1, 51150,   101], dtype=int32), 'shape_signature': array([    1, 51150,   101], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```
As you can see, the output shapes of StatefulPartitionedCall 0 - 4 changed. Any solution? I've searched around but I haven't seen any problems similar with the problems I have.

### 4. Information
Pre trained models used: ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8

The only things I've changed from the original pipeline was these:
num_classes: 100 (because I have 100 classes)
batch_size: 2
deleted all augmentation steps (flip and crop)
fine_tune_checkpoint=""my_path/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0""
num_steps = 100000
fine_tune_checkpoint_type= ""detection""
use_bfloat16= false
train_input_reader=changed the path
eval_input_reader=changed the path

I'm sorry if my explanation seems confusing, this is the second time I've ever opened an issue in Github. If you need the saved model and tflite, I'll be willing to share it. Please help!"
53214,[Docker] Tensorflow 2.4.4 images are missing from docker hub,"Tensorflow 2.4.4 has been released 25 days ago, but it appears that the docker images have not made their way to docker hub yet.

Would it be possible to investigate the problem and push the images to docker hub?

Thanks!"
53212,@org_tensorflow//tensorflow/lite/toco:toco build error,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: Tensorflow2.6.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):  
- CUDA/cuDNN version: CUDA Version 10.0.130,CUDNN_MAJOR7.4.2
**I am trying to build the tensorflow code, When I execute this command：**

> bazel build -s --cxxopt=""-std=c++14"" -c dbg --cxxopt=-msse4 @org_tensorflow//tensorflow/lite/toco:toco --experimental_repo_remote_exec

**error logs**
ERROR: /home/shiyalun/.cache/bazel/_bazel_shiyalun/648af57cc02a336677529154c4ac62f6/external/org_tensorflow/tensorflow/core/platform/default/BUILD:273:11: Compiling tensorflow/core/platform/default/port.cc [for host] failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
external/org_tensorflow/tensorflow/core/platform/default/port.cc: In function 'tensorflow::port::MemoryInfo tensorflow::port::GetMemoryInfo()':
external/org_tensorflow/tensorflow/core/platform/default/port.cc:360:46: error: could not convert '{9223372036854775807l, 9223372036854775807l}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryInfo'
   MemoryInfo mem_info = {INT64_MAX, INT64_MAX};
                                              ^
external/org_tensorflow/tensorflow/core/platform/default/port.cc: In function 'tensorflow::port::MemoryBandwidthInfo tensorflow::port::GetMemoryBandwidthInfo()':
external/org_tensorflow/tensorflow/core/platform/default/port.cc:373:46: error: could not convert '{9223372036854775807l}' from '<brace-enclosed initializer list>' to 'tensorflow::port::MemoryBandwidthInfo'
   MemoryBandwidthInfo membw_info = {INT64_MAX};
                                              ^
Target @org_tensorflow//tensorflow/lite/toco:toco failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/shiyalun/.cache/bazel/_bazel_shiyalun/648af57cc02a336677529154c4ac62f6/external/org_tensorflow/tensorflow/core/framework/BUILD:1279:31 Middleman _middlemen/@org_Utensorflow_S_Stensorflow_Score_Sframework_Cattr_Uvalue_Uproto_Utext-BazelCppSemantics_build_arch_k8-dbg failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 53 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
INFO: Elapsed time: 349.821s, Critical Path: 309.18s
INFO: 108 processes: 30 internal, 78 linux-sandbox.
FAILED: Build did NOT complete successfully"
53211,Validation split does not take into account the shuffle=True argument of the tuner search method.,"https://github.com/tensorflow/tensorflow/blob/80117da12365720167632761a61e0e32e4db2dcc/tensorflow/python/keras/engine/training.py#L1130

I believe I spotted a mistake in the keras tuner code. While shuffle=True is indeed the default argument, it is not taken into account when creating the train and validation set.

Hope this helps for people to come."
53210,Cannot build via pip on macOS ARM64 M1 Apple Silicon,"I am running macOS 11, python 3.9, and am using ARM64 hardware (Apple Silicon M1). It's a MacBook Air.

Trying to install tensor flow via pip
`pip3.9 install tensorflow-macos==2.6.0`

results in the following error:

```
ERROR: Could not find a version that satisfies the requirement h5py~=3.1.0 (from tensorflow-macos) (from versions: 2.2.1, 2.3.0b1, 2.3.0, 2.3.1, 2.4.0b1, 2.4.0, 2.5.0, 2.6.0, 2.7.0rc2, 2.7.0, 2.7.1, 2.8.0rc1, 2.8.0, 2.9.0rc1, 2.9.0, 2.10.0, 3.0.0rc1, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.3.0, 3.4.0, 3.5.0, 3.6.0)
ERROR: No matching distribution found for h5py~=3.1.0
```

h5py doesn't currently have a wheel available. I can install from source, for example, by installing using home-brew and the following command:
`HDF5_DIR=/opt/homebrew/Cellar/hdf5/1.12.1 pip3.9 install --no-binary=h5py h5py==3.1.0`
This just installs without the binary.

However, there's no way for me to specify this in the tensor flowinstallation via pip, so it won't work.

How can we resolve this issue?"
53209,GPU with OpenGL implementation for Android,"Hi! I am trying to implement processing from the camera using only GPU with OpenGL. I've built libtensorflowlite_gpu_gl.so

I've bound buffer like this:
`const void *model_data =
static_cast<void *>(env->GetDirectBufferAddress(model_buffer));
jlong model_size = env->GetDirectBufferCapacity(model_buffer);

TfLiteModel *model_ = TfLiteModelCreate(model_data, model_size);
TfLiteInterpreterOptions *options_ = TfLiteInterpreterOptionsCreate();
auto *options = new TfLiteGpuDelegateOptions ();
auto delegate_ = TfLiteGpuDelegateCreate(options);
TfLiteGpuDelegateBindBufferToTensor(delegate_, cam_ssbo_id, 0);
TfLiteInterpreterOptionsAddDelegate(options_, delegate_ );

interpreter_ = TfLiteInterpreterCreate(model_, options_);



TfLiteStatus status = TfLiteInterpreterAllocateTensors(interpreter_);`
and then on a new frame, I call
TfLiteStatus status = TfLiteInterpreterInvoke(interpreter_);
It is working. I am receiving the expected result. But it seems like the invoke is working on CPU instead of GPU. It also freezes the main thread (I am calling all of these methods in a separate thread), and the processing is too slow.
I have tried processing on GPU for my model using standard GPU Delegate it was working fine and two times faster. I was expecting that TfLiteInterpreterInvoke(interpreter_) whould work around the same time for both configurations. Can you help me to understand how I should implement processing on GPU correctly?"
53207,Does TF now support neural network inferring on DSP ，using snapdragon 865 equipped with hexagon698,"When i run benchmark on snapdragon 865, it didn't work and said :""Could not create Hexagon delegate: platform may not support delegate or required libraries are missing"". Then i checked my chip information, it was equipped with hexagon698. I read the [document](https://www.tensorflow.org/lite/performance/hexagon_delegate) that tensorflow-lite supports hexagon 690 and others, so I want to know whether it supports 698.
```sh
STARTING!
Log parameter values verbosely: [0]
Num threads: [4]
Graph: [models/resnetv1_50_quant.tflite]
Input layers: [input]
Input shapes: [1,224,224,3]
Enable op profiling: [1]
#threads used for CPU inference: [4]
Use Hexagon: [1]
Hexagon profiling: [1]
Loaded model models/resnetv1_50_quant.tflite
loaded libcdsprpc.so
Could not create Hexagon delegate: platform may not support delegate or required libraries are missing
Tensor # 0 is named input_1 but flags call it input
The input model file size (MB): 26.2023
Initialized session in 65.973ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=6 first=100521 curr=80282 min=79211 max=100521 avg=83420.5 std=7670

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=50 first=78236 curr=79387 min=76850 max=84289 avg=79728.5 std=1611
```"
53205,STFT --> inverse_STFT not reconstructing the original signal when using no window,"Tensorflow is not reconstructing the original signal when applying the STFT followed by the inverse STFT when using no window. The problems arise when the frames of the STFT overlap: It seems like every frame contributes with a weight of 1 regardless of the number of overlapping frames `N = frame_size / frame_step`. As a result, the central part of the signal is `N` times larger than the original. Here is a minimal code to reproduce the error:

```python
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

size = 2048
frame_length = 512
frame_step = 128
waveform = np.sin(np.arange(size) * 1 / 100)

stft = tf.signal.stft(waveform, frame_length, frame_step, window_fn=None)
inverse_stft = tf.signal.inverse_stft(stft, frame_length, frame_step, window_fn=None)

plt.plot(waveform)
plt.plot(inverse_stft)
plt.show()
plt.clf()
```
Broken stft --> istft applied to sinusoidal signal: 
![](https://i.stack.imgur.com/BMjqX.png)

Notice that I'm using no window. If I put the Hann window, the central part works well but the borders are smoothly going to zero, a related but surprisingly different bug that is already documented here: #36616. The implementation of scipy works well under all circumstances.

I'm using Tensorflow 2.6.0 installed with tensorflow-macos on an Apple M1 MacBookPro. "
53204,Keras Nadam optimizer behaves different to `ApplyAdamOp` with `use_nesterov` option,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (but irrelevant)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary (but irrelevant)
- TensorFlow version (use command below): 2.3, 2.7.0, current master
- Python version: 3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

Nadam as implemented in the kernel via the `use_nesterov` option (`tensorflow/tensorflow/core/kernels/training_ops_gpu.cu.cc`) behaves different to `tf.keras.optimizers.Nadam`.

We observed this problem (here: https://github.com/rwth-i6/returnn/issues/766#issuecomment-979216833) where the model converged in TF 1 with the old Nadam and does not anymore with TF 2 with the new Nadam.

Specifically, in the kernel, you have this code:
```cpp
const T mul_factor = (*lr_) * sqrt(static_cast<T>(1.0) - (*beta2_power_)) /
                     (static_cast<T>(1.0) - (*beta1_power_));

...

auto m_i = m[i];
auto g_i = grad[i];
auto v_i = v[i];

m_i += one_minus_beta1 * (g_i - m_i);
v_i += one_minus_beta2 * (g_i * g_i - v_i);
if (use_nesterov) {
  var[i] -= mul_factor * (m_i * beta1 + one_minus_beta1 * g_i) /
            (epsilon + sqrt(v_i));
} else {
  var[i] -= mul_factor * m_i / (epsilon + sqrt(v_i));
}

m[i] = m_i;
v[i] = v_i;
```
And in the Keras Nadam optimizer, we have this code:
```
g_prime = grad / coefficients['one_minus_m_schedule_new']
m_t = (coefficients['beta_1_t'] * m +
       coefficients['one_minus_beta_1_t'] * grad)
m_t = tf.compat.v1.assign(m, m_t, use_locking=self._use_locking)
m_t_prime = m_t / coefficients['one_minus_m_schedule_next']
v_t = (coefficients['beta_2_t'] * v +
       coefficients['one_minus_beta_2_t'] * tf.square(grad))
v_t = tf.compat.v1.assign(v, v_t, use_locking=self._use_locking)
v_t_prime = v_t / coefficients['v_t_prime_denominator']
m_t_bar = (coefficients['one_minus_m_t'] * g_prime +
           coefficients['m_t_1'] * m_t_prime)
var_t = var - coefficients['lr_t'] * m_t_bar / (
    tf.sqrt(v_t_prime) + coefficients['epsilon'])
```

There are differences is the calculation of `m_t` and `v_t`. While mathematically the same, the variant in the kernel is probably numerically more stable.

Another difference is `mul_factor` vs `lr`.

**Describe the expected behavior**

I would expect that Nadam as implemented in the kernel via the `use_nesterov` option (`tensorflow/tensorflow/core/kernels/training_ops_gpu.cu.cc`, available since TF 1, also still in current master), which could be used via `tensorflow.contrib.opt.NadamOptimizer` in TF 1, is conceptually the same as `tf.keras.optimizers.Nadam`.
"
53203,Feature request: tensorflow.keras layer to let us reorder the channels on a specific dimension of a tensor,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): w.7
- Are you willing to contribute it (Yes/No):
- Yes, thou I don't know in how much time I would be able to ship it due to day job.

**Describe the feature and the current behavior/state.**
The feature is a `tf.keras` layer to let us reorder the channels of a specific dimension on during the feedforward operation without having to rely on custom implementation. This would be nice to finetune and deploy models with RGB and BGR without having to change the software that calls the TF Serving API. 

It would be something like
```python
rgb2bgr_layer = tf.keras.layers.ReorderChannel(axis=-1, order[2, 1, 0])
```

**Will this change the current api? How?**
Yes. It would add a new `tf.keras` layer such as the following:

```python
rgb2bgr_layer = tf.keras.layers.ReorderChannel(axis=-1, order[2, 1, 0])
```

**Who will benefit with this feature?**
Everyone who wants to try and deploy different models without changing the software that calls the TF Serving API. 

**Any Other info.**
We can still to something like that-ish with a lambda layer:
```python
tf.keras.layers.Lambda(lambda i: i[..., ::-1])
````

But that just does not seem right. Also, with a native layer things could be done faster on prediction time, which might be a must for some people."
53202,Making tensorflow size smaller using selective registration for MacOs,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Monterey
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not relevant
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0
- Python version: 3.9 
- Installed using virtualenv? pip? conda?: not relevant
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): not relevant
- CUDA/cuDNN version: not relevant
- GPU model and memory: not relevant



**Describe the problem**
I am trying to make TF size smaller on MacOs the size is 950mb but after using the below optional flags the size is reduced to 839mb:
--config=nogcp --config=nonccl --config=noaws --config=nohdfs  --define=with_xla_support=false --define=with_ignite_support=false --define=with_kafka_support=false

1. Is there any more flags that can be used to reduce the size of TF?

2. Since it is still big 839mb I would like to use the selective registration method to get only the operations I use in my model as described in:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py

I followed the steps mentioned in this link but I am not finding equivalent command for MacOs for the following step:
bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" //tensorflow/tools/android/inference_interface:libtensorflow_inference.so 
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a

My questions:
1. Is the selective registration supported for android only? I didn't find a directory named MacOs in //tensorflow/tools/?

2. I need to support both cpu intel and arm so what I should type in --cpu?
I tried --cpu=x86_64 and --cpu=arm64 got:
ERROR: /private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/local_config_cc/BUILD:41:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86_64'

3. What is the equivalent command for MacOs for --crosstool_top=//external:android/crosstool ? 
I tried --crosstool_top=//external:macos/crosstool but got error:
 no such target '//external:macos/crosstool': target 'macos/crosstool' not declared in package 'external' defined by /Users/integrator/tensorflow/WORKSPACE

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. git clone https://github.com/tensorflow/tensorflow.git
2. cd tensorflow
3. git checkout v2.7.0
4. ./configure all is no
5. bazel build tensorflow/python/tools:print_selective_registration_header
6.  bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=/Users/integrator/Downloads/vgg_16.ckpt > tensorflow/core/framework/ops_to_register.h

I used an official trained model from here: http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz
I got runtime warning for this step:

INFO:tensorflow:Loading proto file /Users/integrator/Downloads/vgg_16.ckpt
I1125 15:27:19.769747 8639198720 selective_registration_header_lib.py:84] Loading proto file /Users/integrator/Downloads/vgg_16.ckpt
/Users/integrator/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/selective_registration_header_lib.py:93: RuntimeWarning: Unexpected end-group tag: Not all data was converted
  graph_def = graph_pb2.GraphDef.FromString(file_data)

7. bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" //tensorflow/tools/android/inference_interface:libtensorflow_inference.so 
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a

Got different errors when running the last step three times in sequence:
1. ERROR: /private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/highwayhash/BUILD.bazel:8:11: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':
this rule is missing dependency declarations for the following files included by 'highwayhash/highwayhash/sip_hash.cc':

2. ERROR: /Users/integrator/tensorflow/tensorflow/core/lib/jpeg/BUILD:47:11: undeclared inclusion(s) in rule '//tensorflow/core/lib/jpeg:portable_jpeg_internal':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/lib/jpeg/jpeg_handle.cc':

3. ERROR:/private/var/tmp/_bazel_integrator/0a793f72f57e57f678a00d6fdcdcda5e/external/com_google_absl/absl/hash/BUILD.bazel:29:11: undeclared inclusion(s) in rule '@com_google_absl//absl/hash:hash':
this rule is missing dependency declarations for the following files included by 'com_google_absl/absl/hash/internal/hash.cc':



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53200,Can't use an estimator.BoostedTreesClassifier converted with TFLite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7

### 2. Code

Provide code to help us reproduce your issues using one of the following options:



#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
import pandas as pd
import numpy as np
import pylab as pl
import tensorflow as tf
from IPython.display import clear_output

# features_with_effort is the list of features we want to use in the model, all of them are floats
X_train = df_train[features_with_effort]
X_test = df_test[features_with_effort]
y_train = df_train[target]
y_test = df_test[target]

feature_columns = []
for feat in features_with_effort:
    feature_columns.append(fc.numeric_column(feat,
                                           dtype=tf.float32))

NUM_EXAMPLES = len(y_train)

def make_input_fn(X, y, n_epochs=None, shuffle=True):
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
        if shuffle:
            dataset = dataset.shuffle(NUM_EXAMPLES)
        # For training, cycle thru dataset as many times as need (n_epochs=None).
        dataset = dataset.repeat(n_epochs)
        # In memory training doesn't use batching.
        dataset = dataset.batch(NUM_EXAMPLES)
        return dataset
    return input_fn

# Training and evaluation input functions.
train_input_fn = make_input_fn(X_train, y_train)
eval_input_fn = make_input_fn(X_test, y_test, shuffle=False, n_epochs=1)

# Defining and training the estimator
n_batches = 1
est = tf.estimator.BoostedTreesClassifier(feature_columns,
                                          n_batches_per_layer=n_batches)
est.train(train_input_fn, max_steps=10)

# Eval.
result = est.evaluate(eval_input_fn)
clear_output()
print(pd.Series(result))

# Saving it
dict_columns = {col.key : tf.Variable([], dtype=col.dtype) 
                       for col in feature_columns}
serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(
    dic_catego_columns, default_batch_size=None
)
estimator_dir =""some_dir_name""
path_to_saved_model = est.export_saved_model(estimator_dir,
                      serving_input_receiver_fn=serving_input_receiver_fn)
clear_output()
TFLITE_FILE_PATH = 'boosted_trees.tflite'
converter = tf.lite.TFLiteConverter.from_saved_model(path_to_saved_model)
converter.allow_custom_ops = True
tflite_model = converter.convert()
with open(TFLITE_FILE_PATH, 'wb') as f:
    f.write(tflite_model)

###  trying, and failing to use the tflite model

interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
interpreter.get_tensor(output_details[0][""index""])
```

### 3. Failure after conversion
The conversion seems successful at least there are no errors . When I am trying to use this model with an Interpreter I get this error: 
```

RuntimeError                              Traceback (most recent call last)
<ipython-input-63-8b884bfb46b3> in <module>
      2 
      3 interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)
----> 4 interpreter.allocate_tensors()
      5 
      6 # Get input and output tensors.

~/Work/venvs/tf-env/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    421   def allocate_tensors(self):
    422     self._ensure_safe()
--> 423     return self._interpreter.AllocateTensors()
    424 
    425   def _safe_to_run(self):

RuntimeError: Encountered unresolved custom op: BoostedTreesEnsembleResourceHandleOp.Node number 0 (BoostedTreesEnsembleResourceHandleOp) failed to prepare.
```
 
I dont understand what I am doing wrong or if it is possible.


"
53199,ERROR: tensorflow-2.8.0-cp39-cp39-macosx_11_0_x86_64.whl is not a supported wheel on this platform.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs BigSur 11.1
- Python version: 3.9.9
- Bazel version (if compiling from source): bazel 3.7.2




**Describe the problem**
I decided to install tensorflow from source as I had the AVX2 FMA instruction problem and read that one (the only) way to fix it is to have Tensorflow from source. <br>
I followed the guide on the tensorflow site for installing tensorflow from source. 
I have performed almost all the steps and they have been successful, I miss the last one of the installation of the package. <br>

**Provide the exact sequence of commands / steps that you executed before running into the problem** <br>
This is the command that provides me the error:
` pip install /tmp/tensorflow_pkg/tensorflow-2.8.0-cp39-cp39-macosx_11_0_x86_64.whl`
and this is the ERROR that returned me:
`ERROR: tensorflow-2.8.0-cp39-cp39-macosx_11_0_x86_64.whl is not a supported wheel on this platform.`
<br>
How can I solve this problem?
"
53198,Tested TF Object Detection Models for Jetson Nano,"Hi,

I'm currently working on an AI project using an NVIDIA Jetson Nano (4GB) and TensorFlow 2 where we were planning on using a Faster R-CNN Inception ResNet V2 640x640 model. We tried using TF-TRT to reduce the network, but it seems to be too big to fit in, the vRAM memory is not big enough and using Swap doesn't solve the issue.

We have done several tests and, for the moment, the heaviest network from the TensorFlow Model Zoo we managed to get working is the SSD MobileNet V2 FPNLite 640x640.

I've been searching for a list of networks that have been tested on this device for TF2, but I can't seem to find it. I know of the existence of [this list](https://github.com/NVIDIA-AI-IOT/tf_to_trt_image_classification/#models), but it is for TF1 and doesn't involve the TF2 Model Zoo models.

Is there any chance that a list of working models with their speed, memory usage and mAP tested on a common dataset (COCO or similar) will be developed? This would be specially interesting regarding the different methods available to deploy a model, either via CPU, optimized CPU with TFLite, GPU, optimized GPU with TF-TRT, fully optimized with pure TensorRT or any other possible option that I haven't considered.

Thank you in advance.
"
53197,TFLite GPU - uninitialized value used in if,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.6.2
- Python version: Nope
- Bazel version (if compiling from source): Cmaked
- GCC/Compiler version (if compiling from source): gcc9
- CUDA/cuDNN version: Nope
- GPU model and memory: Any (RTX3090, 1080Ti,...)

Seems to me that at 
https://github.com/tensorflow/tensorflow/blob/08527354a6ad3ee96d02d2581a4dd7742b9c19b9/tensorflow/lite/delegates/gpu/common/tasks/conv_powervr.cc#L392

the conv_params_.z_kernel_is_1 variable may be uninitialized, because some ConvPowerVR::GuessBestParams() do not set the value (e.g. 2D conv version)

Valgrind reports:

==2456592== Conditional jump or move depends on uninitialised value(s)
==2456592== at 0x7839679: std::_Bit_reference::operator=(bool) (stl_bvector.h:92)
==2456592== by 0x7C3A736: std::_Bit_iterator std::__copy_move<false, false, std::random_access_iterator_tag>::__copy_m<bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:342)
==2456592== by 0x7C3A66A: std::_Bit_iterator std::__copy_move_a<false, bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:404)
==2456592== by 0x7C3A5C6: std::_Bit_iterator std::__copy_move_a2<false, bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:440)
==2456592== by 0x7C3A4E8: std::_Bit_iterator std::copy<bool const*, std::_Bit_iterator>(bool const*, bool const*, std::_Bit_iterator) (stl_algobase.h:474)
==2456592== by 0x7C39CF1: void std::vector<bool, std::allocator<bool> >::_M_initialize_range<bool const*>(bool const*, bool const*, std::forward_iterator_tag) (stl_bvector.h:1189)
==2456592== by 0x7C38F47: std::vector<bool, std::allocator<bool> >::vector(std::initializer_list<bool>, std::allocator<bool> const&) (stl_bvector.h:691)
==2456592== by 0x7C29C41: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#3}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const (conv_powervr.cc:392)
==2456592== by 0x7C32AB4: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&) (conv_powervr.cc:713)
==2456592== by 0x7C28198: tflite::gpu::ConvPowerVR::GenerateCode(tflite::gpu::GpuInfo const&) (conv_powervr.cc:249)
==2456592== by 0x7C3830C: tflite::gpu::CreateConvPowerVR(tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const*) (conv_powervr.cc:1449)
==2456592== by 0x7D2801B: tflite::gpu::(anonymous namespace)::SelectConvolutionNVidia(tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const&, tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&) (convolution_selector.cc:76)
==2456592== Uninitialised value was created by a stack allocation
==2456592== at 0x7D27F2B: tflite::gpu::(anonymous namespace)::SelectConvolutionNVidia(tflite::gpu::Convolution2DAttributes const&, tflite::gpu::StrongShape<(tflite::gpu::Layout)10> const&, tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&) (convolution_selector.cc:71)

Can anyone with code path knowledge look into it? Thx. The value might be used in initializer of vector only, still perhaps it would be better to either initialize or not insert into vector if not initialized."
53196,Restoring mixed placement tf.keras.Model attempts to place CPU variable on GPU.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `gcr.io/deeplearning-platform-release/tf2-gpu.2-5`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): `gcr.io/deeplearning-platform-release/tf2-gpu.2-5`
- TensorFlow version (use command below): 2.5
- Python version: 3.7
- CUDA/cuDNN version: `NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.4 `
- GPU model and memory: `V100`

**Describe the current behavior**
1. A model is created where a embedding layer (list of `Variable`s) is placed on CPU and subsequent `Dense` layers are placed according to a given `strategy`.  The embedding layer variables are large and fit on CPU but do not fit on GPU.
2. this model trains (fwd and bwd pass).
3. this model is saved.
3. then an attempt to restore this model is made, but gives the following error, indicating that restoration tries to assign a CPU variable to GPU:
```
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2000000,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator gpu_host_bfc [Op:AssignVariableOp]
```
resulting in OOM on GPU.

**Describe the expected behavior**
Restoration of CPU variables should not go through the `gpu_host_bfc` allocator.

**Standalone code to reproduce the issue**

- I ran the following code on a [8xV100 on GCP with 128 GB in GPU memory and 624 GB in RAM](https://cloud.google.com/compute/docs/gpus#other_available_nvidia_gpu_models)

```

import os

from absl import app, flags, logging
import tensorflow as tf

FLAGS = flags.FLAGS

flags.DEFINE_string('workspace', None, '')


class Embeddings(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    with tf.device('cpu:0'):
      self.table = [
        tf.Variable(name=f'shard_{idx}', initial_value=tf.keras.initializers.glorot_normal()([int(2e6), 256]))
        for idx in range(20)
      ]

  def call(self, inputs):
    with tf.device('cpu:0'):
      looked_up = tf.reduce_sum(tf.nn.embedding_lookup(self.table, inputs), axis=1)
    return looked_up


class M(tf.keras.Model):
  def __init__(self, strategy):
    super().__init__()
    with tf.device('cpu:0'):
      self.embeddings = Embeddings()
    with strategy.scope():
      self.mlp = tf.keras.Sequential([tf.keras.layers.Dense(1)])

  def call(self, inputs):
    inputs = self.embeddings(inputs)
    out = self.mlp(inputs)
    return out


strategy = tf.distribute.MirroredStrategy()

model = M(strategy)
loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
def _replica_loss(labels, logits):
  return tf.nn.compute_average_loss(loss_fn(labels, logits), global_batch_size=10)

with tf.device('cpu:0'):
  emb_optimizer = tf.keras.optimizers.Adam()
with strategy.scope():
  mlp_optimizer = tf.keras.optimizers.Adam()


def split_variables(tv):
  from itertools import filterfalse, tee
  from tensorflow.python.distribute.values import MirroredVariable
  def partition(pred, iterable):
    'Use a predicate to partition entries into false entries and true entries'
    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
    t1, t2 = tee(iterable)
    return list(filterfalse(pred, t1)), list(filter(pred, t2))
  return partition(lambda v: isinstance(v, MirroredVariable), tv)


def save_or_restore(save_dir: str, step: int , **to_save):
 ckpt = tf.train.Checkpoint(**to_save)
 manager = tf.train.CheckpointManager(ckpt, save_dir, max_to_keep=10)
 latest_checkpoint = tf.train.latest_checkpoint(save_dir)
 if latest_checkpoint is not None:
   logging.info(f'Restoring checkpoint: {latest_checkpoint}')
   ckpt.restore(latest_checkpoint)
 else:
   manager.save(checkpoint_number=step)


def _train_step(inputs, labels):
  with tf.GradientTape() as tape:
    logits = model(inputs)
    loss = _replica_loss(labels, logits)

  emb_var, mlp_var = split_variables(model.trainable_variables)
  emb_grad, mlp_grad = tape.gradient(loss, [emb_var, mlp_var])
  mlp_optimizer.apply_gradients(zip(mlp_grad, mlp_var))

  return loss, emb_var, emb_grad

def distribute_step(step_fn):
  @tf.function
  def _step(*step_args):
    loss, emb_var, emb_grad = strategy.run(step_fn, args=step_args)
    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)
    emb_grad = strategy.reduce(tf.distribute.ReduceOp.SUM, emb_grad, axis=None)
    emb_optimizer.apply_gradients(zip(emb_grad, emb_var))
    return loss
  return _step

def main(_argv):
  save_dir = os.path.join(FLAGS.workspace, 'save_dir')

  train_step = distribute_step(_train_step)
  ds = tf.data.Dataset.from_tensors((tf.random.uniform([10, 10], minval=0, maxval=10, dtype=tf.int64), tf.random.uniform([10, 1], minval=0, maxval=1, dtype=tf.int64)))
  ds = strategy.experimental_distribute_dataset(ds)

  for example, label in ds:
    loss = train_step(example, label)
    logging.info(f'loss: {loss}')
    break

  # Save
  logging.info(f'Saving from {save_dir}')
  save_or_restore(
    save_dir=save_dir,
    step=1,
    model=model,
    cpu_optimizer=emb_optimizer,
    gpu_optimizer=mlp_optimizer,
  )

  # Restore
  logging.info(f'Restoring from {save_dir}')
  save_or_restore(
    save_dir=save_dir,
    step=1,
    model=model,
    cpu_optimizer=emb_optimizer,
    gpu_optimizer=mlp_optimizer,
  )


if __name__ == ""__main__"":
  app.run(main)
```

- It creates an embedding table of 20 shards of ~2GB `Variable`s
- **It trains and saves successfully**
- **However, it fails to restore**

with the following attached full logs [repro_save.txt](https://github.com/tensorflow/tensorflow/files/7600948/repro_save.txt)
"
53195,Addition of GradCAM visualization option for output visualization of CNNs,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
GradCAM visualization is a great algorithm to showcase the activation part of an image from a CNN. As described in this paper(https://arxiv.org/pdf/1610.02391.pdf), it can allow users to visualize the issues their model is making in a more interactive way and fix the models.

**Will this change the current api? How?**
Yes, it will change the current API by the addition of a GradCAM visualization function in utils or the addition of a new visualization module itself. Depends on the admins.

**Who will benefit from this feature?**
It will help users who are using CNN to visualize the issue their model is making in a more interactive way by looking at the class activation map of the image and having a look at which part of the image the model gets activated to predict the class. 

**Any Other info.**
No. 
"
53193,programming failure,"a
"
53192,tf.function(jit_compile=True) cannot access int32 weights on GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 20.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
v2.6.0-0-g919f693420e 2.6.0
- Python version:
Python 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
11.3 8.2
- GPU model and memory:
A100 40GB

**Describe the current behavior**

Functions with tf.function(jit_compile=True) cannot access weights of type int32 on the GPU.  It gives error at runtime
```
2021-11-24 15:12:28.148837: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at xla_ops.cc:238 : Invalid argument: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0
```

**Describe the expected behavior**

no error

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
no
- Briefly describe your candidate solution(if contributing):
no solution

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Please just run the following code,

```
import tensorflow as tf
import tensorflow.keras as tk

class Scale(tk.layers.Layer):
    def __init__(self, c, dt):
        super(Scale, self).__init__(name='Scale')
        self.c = self.add_weight(initializer=tf.keras.initializers.Constant(c), dtype=dt)
    @tf.function(jit_compile=True)
    def call(self, x):
        return self.c*x

print('float32:', Scale(2, tf.float32)(tf.ones(4, dtype=tf.float32)))
print('int64:', Scale(2, tf.int64)(tf.ones(4, dtype=tf.int64)))
print('int32:', Scale(2, tf.int32)(tf.ones(4, dtype=tf.int32)))  # This line gives error
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Below is the output from running the above code,
```
$ python issue_int32.py 
2021-11-24 15:16:53.015406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-24 15:16:53.423483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38458 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0
2021-11-24 15:16:53.776487: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x562deb0ee730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-11-24 15:16:53.776535: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2021-11-24 15:16:53.818100: I tensorflow/compiler/jit/xla_compilation_cache.cc:363] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
float32: tf.Tensor([2. 2. 2. 2.], shape=(4,), dtype=float32)
int64: tf.Tensor([2 2 2 2], shape=(4,), dtype=int64)
2021-11-24 15:16:53.865986: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at xla_ops.cc:238 : Invalid argument: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""issue_int32.py"", line 14, in <module>
    print('int32:', Scale(2, tf.int32)(tf.ones(4, dtype=tf.int32)))
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 956, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar2 (defined @ /lus/theta-fs0/software/thetagpu/conda/2021-09-22/mconda3/lib/python3.8/site-packages/keras/engine/base_layer_utils.py:117) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_call_44]
```"
53191,Converted TFlite image segmentation model crashes in Android.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android sdk version 29
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4
- Python version: 3,9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
I have implemented image segmentation model with two stream and was able convert the model into TFlite format. There is no issue when prediction taken using Python API from coverted TFlite. When model in used in Android application using task api library it crashes without any errors in the line below.  It seems there is memory allocation issue when segment method is called. 
```
imageSegmenter.segment(image);
``` 
I have included the link for converted TFlite model below.
https://drive.google.com/drive/folders/1sBRCNNa8c59315wXZ3tPzkINnV_F2o3E?usp=sharing

Android dependancy used.
implementation 'org.tensorflow:tensorflow-lite-task-vision:0.3.0'
"
53190,Importing models directly into Python Variables from model files stored in S3 bucket,"Hello,

I have been thinking from quite some time that if it would have been possible to ***import the ML models directly into python variables*** when the model file is actually not at the local disk where the code is running but rather is ***stored in an S3 bucket***.

As we can load a model file/folder present in the local disk using the tensorflow functions like `tf.keras.models.load_model(""<model_file_name>"")`, is there any possible way to import the model from the model files stored in the S3 bucket?

Because there can be scenarios where we are running some code to import model inside a container and that container has a limited storage bandwidth but we know the model sizes can be extremely large and thus it would become difficult for it to be stored inside a container or a persistent storage.

For, eg. I am able to use the below specified code to import a dataset (CSV File) directly into memory (basically into a python variable) using the ***Pandas*** and the ***Boto3*** library:

```
filename = ""dataset/items_dataset.csv""
s3 = boto3.client('s3',
                  aws_access_key_id = '<AWS_ACCESS_KEY_ID>',
                  aws_secret_access_key = '<AWS_SECRET_ACCESS_KEY>')

obj = s3.get_object(Bucket='<AWS_BUCKET_NAME>', Key=filename)
df = pd.read_csv(io.BytesIO(obj['Body'].read()))
df.head()
```
where `dataset/items_dataset.csv` is the path of the dataset file in the S3 bucket. 
<br>
Thus, I wanted to know whether is it possible to have a similar way of importing the model file stored in the S3 bucket without having the need of downloading it to the local disk before loading it to memory through tensorflow functions.

Also, if there is any way of achieving this, then is there any way to do the vice-versa as well, i.e. *to store the model directly into the S3 bucket?*"
53189,Unit test //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu gives illegal instruction on AARCH64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.8.10
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 11.2.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails as illegal instruction is thrown.

**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --verbose_failures -- //tensorflow/compiler/xla/tests:xla_hlo_profile_test_cpu

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

bazel-bin/tensorflow/compiler/xla/tests/xla_hlo_profile_test_cpu
[==========] Running 2 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 2 tests from HloProfileTest
[ RUN      ] HloProfileTest.ProfileSingleComputation
2021-11-24 17:03:03.560320: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-11-24 17:03:03.560415: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version
2021-11-24 17:03:03.560854: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x3c904d60 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices:
2021-11-24 17:03:03.560885: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Interpreter, <undefined>
Illegal instruction (core dumped)

When running under gdb

Thread 70 ""xla_hlo_profile"" received signal SIGILL, Illegal instruction.
[Switching to Thread 0xfffec6ffcd80 (LWP 420853)]
0x0000fffff7ff8014 in ProfileSingleComputation.5 ()
(gdb) disass
Dump of assembler code for function ProfileSingleComputation.5:
   0x0000fffff7ff8000 <+0>:	str	d12, [sp, #-48]!
   0x0000fffff7ff8004 <+4>:	stp	d11, d10, [sp, #16]
   0x0000fffff7ff8008 <+8>:	stp	d9, d8, [sp, #32]
   0x0000fffff7ff800c <+12>:	mov	x10, xzr
   0x0000fffff7ff8010 <+16>:	ldp	x9, x13, [x3, #8]
=> 0x0000fffff7ff8014 <+20>:	mrs	x8, pmccntr_el0
   0x0000fffff7ff8018 <+24>:	add	x11, x9, #0x20
   0x0000fffff7ff801c <+28>:	ldr	x9, [x3]
   0x0000fffff7ff8020 <+32>:	add	x12, x9, #0x30
   0x0000fffff7ff8024 <+36>:	add	x13, x13, #0x20
   0x0000fffff7ff8028 <+40>:	mov	x14, xzr
   0x0000fffff7ff802c <+44>:	add	x15, x11, x14
   0x0000fffff7ff8030 <+48>:	add	x16, x13, x14
   0x0000fffff7ff8034 <+52>:	ldp	q0, q1, [x15, #-32]
   0x0000fffff7ff8038 <+56>:	ldp	q2, q3, [x16, #-32]
   0x0000fffff7ff803c <+60>:	ldp	q4, q5, [x15]
   0x0000fffff7ff8040 <+64>:	fadd	v0.4s, v0.4s, v2.4s
   0x0000fffff7ff8044 <+68>:	fadd	v1.4s, v1.4s, v3.4s
   0x0000fffff7ff8048 <+72>:	ldp	q2, q3, [x16]
   0x0000fffff7ff804c <+76>:	fadd	v2.4s, v4.4s, v2.4s
   0x0000fffff7ff8050 <+80>:	add	x15, x12, x14
   0x0000fffff7ff8054 <+84>:	stp	q0, q1, [x15, #-48]
   0x0000fffff7ff8058 <+88>:	fadd	v0.4s, v5.4s, v3.4s
   0x0000fffff7ff805c <+92>:	stp	q2, q0, [x15, #-16]
   0x0000fffff7ff8060 <+96>:	add	x14, x14, #0x40
   0x0000fffff7ff8064 <+100>:	cmp	x14, #0x400
   0x0000fffff7ff8068 <+104>:	b.ne	0xfffff7ff802c <ProfileSingleComputation.5+44>  // b.any
   0x0000fffff7ff806c <+108>:	add	x10, x10, #0x1
   0x0000fffff7ff8070 <+112>:	add	x11, x11, #0x400
   0x0000fffff7ff8074 <+116>:	add	x12, x12, #0x400
   0x0000fffff7ff8078 <+120>:	add	x13, x13, #0x400
   0x0000fffff7ff807c <+124>:	cmp	x10, #0x100
   0x0000fffff7ff8080 <+128>:	b.ne	0xfffff7ff8028 <ProfileSingleComputation.5+40>  // b.any
   0x0000fffff7ff8084 <+132>:	mov	x10, xzr
   0x0000fffff7ff8088 <+136>:	mrs	x11, pmccntr_el0
   0x0000fffff7ff808c <+140>:	mov	w12, #0xb717                	// #46871
   0x0000fffff7ff8090 <+144>:	movk	w12, #0x39d1, lsl #16
   0x0000fffff7ff8094 <+148>:	dup	v0.4s, w12
   0x0000fffff7ff8098 <+152>:	mov	w12, #0x25c0                	// #9664
   0x0000fffff7ff809c <+156>:	movk	w12, #0xa59f, lsl #16
   0x0000fffff7ff80a0 <+160>:	dup	v1.4s, w12
   0x0000fffff7ff80a4 <+164>:	mov	w12, #0x337e                	// #13182
   0x0000fffff7ff80a8 <+168>:	movk	w12, #0x2a61, lsl #16
   0x0000fffff7ff80ac <+172>:	dup	v2.4s, w12
   0x0000fffff7ff80b0 <+176>:	mov	w12, #0x37ff                	// #14335
   0x0000fffff7ff80b4 <+180>:	movk	w12, #0xaebd, lsl #16
   0x0000fffff7ff80b8 <+184>:	dup	v3.4s, w12
   0x0000fffff7ff80bc <+188>:	ldr	x12, [x5, #24]
   0x0000fffff7ff80c0 <+192>:	sub	x11, x11, x8
   0x0000fffff7ff80c4 <+196>:	add	x11, x11, x12
   0x0000fffff7ff80c8 <+200>:	str	x11, [x5, #24]
   0x0000fffff7ff80cc <+204>:	mov	w11, #0x41                  	// #65
   0x0000fffff7ff80d0 <+208>:	movk	w11, #0x335c, lsl #16
--Type <RET> for more, q to quit, c to continue without paging--q

So the problem seems to be reading the performance counter register as the illegal instruction flagged is ""mrs	x8, pmccntr_el0"""
53186,TensorFlow Lite Quantization quality parameters.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version 2.6.1:
- Are you willing to contribute it (Yes):

**Describe the feature and the current behavior/state.**

It would be great to have quantization pamrameters / threshold to be exposed in API. Usually quantization algorithms have something like desired similarity between original activations histogram and int8 activations or something like this. I assume TFLite has such parameters buried inside. It would be great to have control over them. Perhaps per layer?

**Will this change the current api? How?**

No api breaks.

**Who will benefit with this feature?**

people who use tflite for int8 inference.

**Any Other info.**

I could look into it, please give me pointers.
"
53185,"Valgrind complains that tflite has ""source and destination overlap in memcpy""","## What I want

Hi thanks for the lib! I want to know whether this is a known thing (e.g. tflite uses a hack and valgrind wrongly thinks it is a bug), or it is a bug. If it is indeed a bug, I can try to provide more details and reproducible samples or demangled stack traces, etc. But if it is a known feature and it is Valgrind who is wrong, then I do not need to spend any more time.

---

<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): tflite C api compled from source
- TensorFlow version (use command below): 2.7.0
- Python version: n/a
- Bazel version (if compiling from source): na
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: na
- GPU model and memory: na

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When using tflite to run inference I see:

```
==26809== Thread 9:
==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)
==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)
==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A9008: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A9586: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)
==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809== 
==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)
==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)
==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A9058: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A9592: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)
==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809== 
==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)
==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)
==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A90AB: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A95F2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)
==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809== 
==26809== Source and destination overlap in memcpy_chk(0x2c30e490, 0x2c30e490, 4)
==26809==    at 0x4C3DE50: __memcpy_chk (vg_replace_strmem.c:1617)
==26809==    by 0x50A8A5F: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A90DB: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A9614: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x7D2F906: __pthread_once_slow (pthread_once.c:116)
==26809==    by 0x50A4EB6: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4A59: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4BD0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A4CE0: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A1FC2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x50A167C: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809==    by 0x4E972A2: ??? (in /media/sf_frontend/tflite_rust/built_artifacts/linux/libtensorflowlite_c.so)
==26809== 
```

**Describe the expected behavior**

should not have error

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): not know...

**Standalone code to reproduce the issue**
n/a

**Other info / logs**
n/a
"
53184,"warning: successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero","When running a simple script from a Tensorflow docker container I get the above mentioned warning:
```
$ sudo docker run --net=host  -it --gpus all tensorflow/tensorflow:2.6.0-gpu /bin/bash
root@gpu-esparig:/# python3
Python 3.6.9 (default, Jan 26 2021, 15:33:00) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
2021-11-24 09:01:58.877869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-24 09:01:58.899255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-24 09:01:58.900051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Num GPUs Available:  1
```
I tried the following:
To get rid of this non-fatal warning use this command:
`for a in /sys/bus/pci/devices/*; do echo 0 | tee -a $a/numa_node; done`

But I get `tee: '/sys/bus/pci/devices/0000:00:09.0/numa_node': Read-only file system` error.

For details go to [this question](https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ) in stackoverflow.

_Originally posted by @nsssayom in https://github.com/tensorflow/tensorflow/issues/42738#issuecomment-922422874_"
53183,How to build tensorflow lite C api for Linux on a MacOS machine?,"Hi thanks for the lib! I need to build tensorflow lite C api for Linux on a MacOS machine. If I do `bazel build -c opt //tensorflow/lite/c:tensorflowlite_c`, I get a `.dylib` that is of mach-o (macos) format instead of a `.so` that is of linux format. Thus I wonder what should I do? Thanks!

I have tried: `--cpu=linux` and its friends (no luck). `--config-linux` (also no use)."
53181,Error showed when called header files using google\protobuf 3.9.2 ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ![image](https://user-images.githubusercontent.com/41325962/143195575-5ea23af8-356e-4ac2-8c33-7f67b825cb2f.png) Traditional Chinese
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): from source 
- TensorFlow version (use command below): r2.5
- Python version: 3.8.11
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): use msvc 2019 build tool
- CUDA/cuDNN version: 11.2 8.1.2
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Try to run following code and got some errors
```
#include <tensorflow/cc/ops/io_ops.h>
#include <tensorflow/cc/ops/parsing_ops.h>
#include <tensorflow/cc/ops/array_ops.h>
#include <tensorflow/cc/ops/math_ops.h>
#include <tensorflow/cc/ops/data_flow_ops.h>


#include <tensorflow/core/public/session.h>
#include <tensorflow/core/protobuf/meta_graph.pb.h>
#include <fstream>

using namespace std;
using namespace tensorflow;
using namespace tensorflow::ops;

int main()
{
	// set up your input paths
	const string pathToGraph = ""D:\\python\\RSNA\\keras2cpp\\keras2tf\\model_tf2_2\\keras_metadata"";
	const string checkpointPath = ""D:\\python\\RSNA\\keras2cpp\\keras2tf\\model_tf2_2\\saved_model"";

	auto session = NewSession(SessionOptions());
	if (session == nullptr)
	{
		throw runtime_error(""Could not create Tensorflow session."");
	}

	Status status;

	// Read in the protobuf graph we exported
	MetaGraphDef graph_def;
	status = ReadBinaryProto(Env::Default(), pathToGraph, &graph_def);
	if (!status.ok())
	{
		throw runtime_error(""Error reading graph definition from "" + pathToGraph + "": "" + status.ToString());
	}

	// Add the graph to the session
	status = session->Create(graph_def.graph_def());
	if (!status.ok())
	{
		throw runtime_error(""Error creating graph: "" + status.ToString());
	}

	// Read weights from the saved checkpoint
	Tensor checkpointPathTensor(DT_STRING, TensorShape());
	checkpointPathTensor.scalar<std::string>()() = checkpointPath;
	status = session->Run({ {graph_def.saver_def().filename_tensor_name(), checkpointPathTensor}, }, {},
		{ graph_def.saver_def().restore_op_name() }, nullptr);
	if (!status.ok())
	{
		throw runtime_error(""Error loading checkpoint from "" + checkpointPath + "": "" + status.ToString());
	}

	cout << 1 << endl;
}
```
error messages
```
Build started...
1>------ Build started: Project: TF_savedmodel, Configuration: Release x64 ------
1>Source.cpp
1>D:\tf_model\TF_savedmodel\include\Eigen\src\Core\arch\Default\Half.h(1,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss
1>D:\tf_model\TF_savedmodel\include\Eigen\src\Core\arch\Default\BFloat16.h(1,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss
1>D:\tf_model\TF_savedmodel\include\Eigen\src\Core\arch\Default\GenericPacketMathFunctions.h(679,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss
1>D:\tf_model\TF_savedmodel\include\Eigen\src\Core\products\GeneralBlockPanelKernel.h(2066,1): warning C4819: The file contains a character that cannot be represented in the current code page (950). Save the file in Unicode format to prevent data loss
1>D:\tf_model\TF_savedmodel\include\unsupported\Eigen\CXX11\src\Tensor\Tensor.h(76,1): warning C4554: '&': check operator precedence for possible error; use parentheses to clarify precedence
1>D:\tf_model\TF_savedmodel\include\unsupported\Eigen\CXX11\src\Tensor\TensorMap.h(43): message : see reference to class template instantiation 'Eigen::Tensor<T,1,1,int>' being compiled
1>        with
1>        [
1>            T=float
1>        ]
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\tensor_types.h(105): message : see reference to class template instantiation 'Eigen::TensorMap<Eigen::Tensor<T,1,1,int>,16,Eigen::MakePointer>' being compiled
1>        with
1>        [
1>            T=float
1>        ]
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(106,109): warning C4003: not enough arguments for function-like macro invocation 'min'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(106,109): error C2589: '(': illegal token on right side of '::'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(106): error C2062: type 'unknown-type' unexpected
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(106,109): error C3805: 'type': unexpected token, expected either '}' or a ','
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(133,123): warning C4003: not enough arguments for function-like macro invocation 'min'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(133,123): error C2589: '(': illegal token on right side of '::'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(133): error C2062: type 'unknown-type' unexpected
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(133,123): error C3805: 'type': unexpected token, expected either '}' or a ','
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(158,94): warning C4003: not enough arguments for function-like macro invocation 'min'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(158,94): error C2589: '(': illegal token on right side of '::'
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(158): error C2062: type 'unknown-type' unexpected
1>D:\tf_model\TF_savedmodel\include\google\protobuf\type.pb.h(158,94): error C3805: 'type': unexpected token, expected either '}' or a ','
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\attr_value.pb.h(687,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\node_def.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(300,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(332,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(587,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(590,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(621,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\function.pb.h(624,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\util\tensor_format.h(502,79): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\util\tensor_format.h(524,71): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\util\tensor_format.h(558,77): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\node_def_util.h(147,20): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\step_stats.pb.h(1180,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\cluster.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\rewriter_config.pb.h(543,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\config.pb.h(1951,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\config.pb.h(3797,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\config.pb.h(3800,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\config.pb.h(3831,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\config.pb.h(3834,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(159,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(166,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(307,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(315,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(730,56): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\framework\op_kernel.h(735,49): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\lib\core\arena.h(46,1): warning C4267: 'argument': conversion from 'size_t' to 'const int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\graph\graph.h(591,34): warning C4244: 'return': conversion from 'const tensorflow::int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\graph\graph.h(596,23): warning C4244: 'return': conversion from 'tensorflow::int64' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\graph\graph.h(632,28): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\graph\graph.h(641,28): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\public\session_options.h(62,2): warning C4091: '__declspec(dllimport)': ignored on left of 'tensorflow::SessionOptions' when no variable is declared
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\struct.pb.h(874,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\saved_object_graph.pb.h(173,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\saved_object_graph.pb.h(381,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(164,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(167,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(479,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(511,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(2470,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>D:\tf_model\TF_savedmodel\include\tensorflow\core\protobuf\meta_graph.pb.h(2502,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
1>Done building project ""TF_savedmodel.vcxproj"" -- FAILED.
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========
```
I'm facing two problem
1. 
![image](https://user-images.githubusercontent.com/41325962/143196623-d6063313-4c2d-4214-afac-05c472cb0f0b.png)
2. 
![image](https://user-images.githubusercontent.com/41325962/143197092-da67e941-2a85-4231-ad70-0abb812af615.png)


I looked at google\protobuf someone facing the similar problem with 2. [here](https://github.com/protocolbuffers/protobuf/issues/6683) in protobuf 3.9.1.  It seems to be solved in protobuf 3.11 version but the latest tensorflow 2.6.2 still require protobuf 3.9.2 version. I think that maybe just I met the error. Or 2. problem just be caused by 1..
Waiting for help. Thank you. 


**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53180,group conv2d can't backprop properly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.7.0-rc1-69-gc256c071bb2 2.7.0

**Describe the current behavior**

I extend tf.keras.layers.Conv2D with the public member function convolution_op() to do group convolution. error occurs at backpropagation. if I change the group number to 1, backpropagation is ok.

**Describe the expected behavior**

extended convolution op should support backpropagation of group convolution.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

class WSConv2D(tf.keras.layers.Conv2D):
  def build(self, input_shape):
    super(WSConv2D, self).build(input_shape);
    self.gain = self.add_weight(shape = (tf.shape(self.kernel)[-1],), dtype = tf.float32, initializer = tf.keras.initializers.Ones()); # self.gain.shape = (cout,)
  def call(self, inputs):
    input_shape = inputs.shape;
    if self._is_causal:  # Apply causal padding to inputs for Conv1D.
      inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs));
    # standardize weight
    mean, var = tf.nn.moments(self.kernel, axes = [0, 1, 2], keepdims = True);
    fan_in = tf.cast(tf.math.reduce_prod(tf.shape(self.kernel)[:-1]), dtype = tf.float32); # fan_in.shape = ()
    kernel = self.gain * (self.kernel - mean) / tf.math.sqrt(tf.math.maximum(var * fan_in, 1e-4));
    # convolution
    outputs = self.convolution_op(inputs, kernel);
    if self.use_bias:
      outputs = outputs + self.bias;
    if self.activation is not None:
      return self.activation(outputs);
    return outputs;

inputs = tf.random.normal(shape = (4,224,224,256));
conv2d = WSConv2D(256, (3,3), groups = 4, padding = 'same');
with tf.GradientTape() as tape:
  outputs = conv2d(inputs);
# NOTE: error occurs here
grads = tape.gradient(outputs, conv2d.trainable_weights);
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```shell
Traceback (most recent call last):
  File ""test.py"", line 27, in <module>
    grads = tape.gradient(outputs, conv2d.trainable_weights);
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 1084, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py"", line 71, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 159, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py"", line 581, in _Conv2DGrad
    gen_nn_ops.conv2d_backprop_input(
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1247, in conv2d_backprop_input
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Computed input depth 256 doesn't match filter input depth 64 [Op:Conv2DBackpropInput]
```"
53179,Tflite LSTM performance drops when running benchmark tool as an app vs compiled binary.,"We've discovered that some of our LSTM models perform unexpectedly ~x2 worse on some Android devices such as the LG Stylo 5. When debugging with the tflite [benchmark tool](https://www.tensorflow.org/lite/performance/measurement), we discovered that this problem only occurs when running tflite inside an APK, and NOT when running the benchmark tool as a compiled binary. It's as if tflite threads scheduling on the cpu is halve. This happens when using even 1 tflite thread and thread affinity doesn't make any difference.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: LG Stylo 5, LG Stylo 6, Samsung Galaxy A21
- TensorFlow installed from (source or binary): r2.7
- TensorFlow version (use command below): r2.7
- Python version: 3.8
- CUDA/cuDNN version: 11.2 / 8.2.1.32
- GPU model and memory: GTX 1080ti, 11GB VRAM


**Reproduce**
Here's a comparison of running the same LSTM model on the LG Stylo 5 (1/2 expected performance running in app) and Samsung Galaxy J3 (no issues):
Samsung Galaxy J3: 4 x 1.4GHz Cortex-A53
LG Stylo 5: 8 x 1.8GHz Cortex-A53
Using only 1 thread, the Stylo 5 should perform 20%+ better than the J3.
The app and compiled binary benchmarks are configured identically (100 warmup steps, 1000 bechmark steps).

```
# Run this to install the benchmark tool, both as a compiled binary, and the app version. Place the LSTM model on the phone.
$ adb push benchmark_model /data/local/tmp
$ adb shell chmod +x /data/local/tmp/benchmark_model
$ adb install -r -d -g android_aarch64_benchmark_model.apk
$ adb push LstmPerfModelTf27Export.tflite /data/local/tmp
```
**Describe the current behavior**
LG Stylo 5:
```
$ adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000
STARTING!
Log parameter values verbosely: [0]
Min num runs: [1000]
Num threads: [1]
Min warmup runs: [100]
Graph: [/data/local/tmp/LstmPerfModelTf27Export.tflite]
#threads used for CPU inference: [1]
Loaded model /data/local/tmp/LstmPerfModelTf27Export.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
The input model file size (MB): 9.46564
Initialized session in 35.59ms.
Running benchmark for at least 100 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=100 first=74230 curr=5768 min=5583 max=74230 avg=6411.35 std=6817

Running benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=1000 first=6186 curr=5358 min=5262 max=6256 avg=5396.75 std=117

Inference timings in us: Init: 35590, First inference: 74230, Warmup (avg): 6411.35, Inference (avg): 5396.75
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=1.57812 overall=10.918

$ adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '""--graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000""'
Stopping: org.tensorflow.lite.benchmark
Starting: Intent { cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras) }

$ adb logcat | grep ""Average inference""
11-23 19:01:42.241 17323 17323 E tflite  : Average inference timings in us: Warmup: 10184.5, Init: 3539, Inference: 11459.1
    Overall max resident set size = 10.2305 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 0.0652695 MB
```
The inference average in benchmark app is doubled!: **11459.1ms** vs **5396.75ms** in benchmark binary.


**Describe the expected behavior**
Here's what happens on most phones, even the much slower Samsung Galaxy J3:
```
$ adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000
STARTING!
Log parameter values verbosely: [0]
Min num runs: [1000]
Num threads: [1]
Min warmup runs: [100]
Graph: [/data/local/tmp/LstmPerfModelTf27Export.tflite]
#threads used for CPU inference: [1]
Loaded model /data/local/tmp/LstmPerfModelTf27Export.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
The input model file size (MB): 9.46564
Initialized session in 5.456ms.
Running benchmark for at least 100 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=100 first=12516 curr=6224 min=6103 max=12516 avg=6375.52 std=651

Running benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=1000 first=7156 curr=6312 min=6099 max=8715 avg=6319.49 std=232

Inference timings in us: Init: 5456, First inference: 12516, Warmup (avg): 6375.52, Inference (avg): 6319.49
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=0.828125 overall=9.99219

$ adb shell am start -S -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity --es args '""--graph=/data/local/tmp/LstmPerfModelTf27Export.tflite --num_threads=1 --warmup_runs=100 --num_runs=1000""'
Stopping: org.tensorflow.lite.benchmark
Starting: Intent { cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras) }

$ adb logcat | grep ""Average inference""
11-23 18:56:24.866 16670 16670 E tflite  : Average inference timings in us: Warmup: 6382.62, Init: 4705, Inference: 6381.92
    Overall max resident set size = 9.83984 MB, total malloc-ed size = 0 MB, in-use allocated/mmapped size = 4095.83 MB
```
Basically no difference between app and compiled binary results: 6381.92ms vs 6319.49ms.

We've only observed this problem for LSTM models (using `tf.keras.layers.LSTM`); no performance decline when benchmarking convolutional models such as [`mobilenet_v1_0.25_224.tflite`](https://www.tensorflow.org/lite/guide/hosted_models).
We've found many phones with this problem: LG Stylo 5 (Qualcomm Snapdragon 450), LG Stylo 6 (MediaTek P35), Samsung Galaxy A21 (MediaTek P35), Samsung Galaxy A12(MediaTek P35).
These phones have much better CPUs than the Samsung Galaxy J3, yet they perform much worse!

Is there any way to make the tflite performance running in apps to be the same as running in the benchmark tool compiled binary?



"
53176,TypeError: '<' not supported between instances of 'WhileBodyFuncGraph' and 'FuncGraph',"Hello,

I have ran tensorflow 2.1 and python 3.8 on a tensorflow project. I got the fp;;



> Start training loop... 0%|          | 0/16153 [00:00<?, ?it/s]2021-11-23 17:42:25.980547: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2) 2021-11-23 17:42:33.055750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll 2021-11-23 17:42:33.387709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll Traceback (most recent call last): File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 350, in _flatten_module leaves = nest.flatten_with_tuple_paths( File ""C:\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py"", line 1425, in flatten_with_tuple_paths flatten(structure, expand_composites=expand_composites))) File ""C:\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py"", line 341, in flatten return _pywrap_utils.Flatten(structure, expand_composites) TypeError: '<' not supported between instances of 'WhileBodyFuncGraph' and 'FuncGraph' 
> 
> The above exception was the direct cause of the following exception: Traceback (most recent call last): File ""code2seq.py"", line 29, in <module> model.train() File ""C:\modelrunner.py"", line 129, in train gradients = tape.gradient(loss, self.model.trainable_variables) File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 175, in trainable_variables return tuple( File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 390, in _flatten_module for subvalue in subvalues: File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 390, in _flatten_module for subvalue in subvalues: File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 390, in _flatten_module for subvalue in subvalues: File ""C:\Anaconda3\lib\site-packages\tensorflow\python\module\module.py"", line 353, in _flatten_module six.raise_from( File ""<string>"", line 3, in raise_from ValueError: Error processing property '_dropout_mask_cache' of <ContextValueCache at 0x2675729b820> 0%|          | 0/16153 [00:10<?, ?it/s]


"
53175,Protobuf header files not copied with -install_headers- bazel target,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: CentOS Linux Release 8.0.1905
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6 release
- Python version: 3.7.8
- Installed using virtualenv? pip? conda?: None, installed libtensorflow_cc.so and libtensorflow_framework.so from source. 
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I would like to compile `libtensorflow_framework.so` and `libtensorflow_cc.so` and link them with my own custom code. 
For this, I first compiled the two targets above using bazel as:

```
bazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:libtensorflow_cc.so --noincompatible_do_not_split_linking_cmdline --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures
```

```
bazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:libtensorflow_framework.so --noincompatible_do_not_split_linking_cmdline --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures
```

However, I still needed the include files so I executed:

```
bazel --output_user_root=/raid/projects/bazel build --jobs=8 --config=opt //tensorflow:install_headers --noincompatible_do_not_split_linking_cmdline --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures
```

which created the required include directory within bazel_bin. At this point, I added the generated include folder to the include path to compile my custom code (which uses tensorflow):

```
-I$(HOME)/projects/tensorflow/bazel-bin/tensorflow/include
```

Then I hit `make` and I got the following error:

```
$(HOME)/projects/tensorflow/bazel-bin/tensorflow/include/tensorflow/core/framework/types.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory
   10 | #include <google/protobuf/port_def.inc>
```

I believe that the inc header files are not copied with the install_headers target. The confusing part is that pip_packages/setup.py seems to have the copy. Maybe it just doesn't have the com_google_protobuf folder in tensorflow at that point. Any suggestions for a workaround? I tried to compile my own version of protobuf (3.11.4) but when I include the header files from those I see conflicting definitions.  

Also, if I install the libraries using pip/conda, I am able to link those libraries with my code. I just want to have an alternative which doesn't require pip/python.
"
53174,training stuck in first epoch at random batch # while using rmsprop as optimizer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10

- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): >= 2.5.0
- Python version:  >= 3.6

- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.2 / 8.1
- GPU model and memory: RTX8000 48GB

**Describe the current behavior**
training stuck at first epoch at random batch # while using rmsprop as optimizer. Same code is working with RTX3090.

**Describe the expected behavior**
continue training till end of the defined epoch


- Do you want to contribute a PR? (yes/no): no


"
53169,filter dataset too slow .,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6
- Are you willing to contribute it (Yes/No): if need I want 


I must have filter dataset, so I must write ```map.filter.batch```, but map batch is too slow than batch map, like:
```
ds = tf.data.TFRecordDataset(""a.txt"")
ds = ds.map()
ds = ds.filter()
ds = ds.batch()
it = ds.get_one_shot_iterator()
gn = it.get_next()
```
so I want:
```
ds = tf.data.TFRecordDataset(""a.txt"")
ds = ds.batch()
ds = ds.map() // parallel batch map
ds = ds.unbatch() // unbatch
ds = ds.filter() // filter
ds = ds.batch() // rebatch 
it = ds.get_one_shot_iterator()
gn = it.get_next()
```

but I get : 
```
Traceback (most recent call last):
  File ""main.py"", line 181, in <module>
    sess.run(train_data_reader(""""));
  File ""main.py"", line 169, in train_data_reader
    train_iterator = train_data_set.make_one_shot_iterator()
AttributeError: 'PrefetchDataset' object has no attribute 'make_one_shot_iterator'
```

I want a best profromance filter dataset ."
53168,Failed precondition: Attempting to use uninitialized,"My code:

```
graph = tf.Graph()
with graph.as_default():
    # input_his: num_samples:216,9,9,144
    input_hsi = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3], 1],
                               name='input_hsi')
    labels_hsi = tf.placeholder(tf.float32, [None, nb_classes], name='labels_hsi')
    is_train = tf.placeholder(tf.bool, shape=[], name='is_train')
    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')
    keep_prob = tf.placeholder(tf.float32, shape=[], name='keep_prob') 

   ##################
    ## # Wrong here ###
    logits, prob = aucn_model.build_model(input_hsi, nb_classes, is_train=is_train, keep_prob=keep_prob) # Wrong here
    #################

    pred = tf.argmax(prob, 1)
    tf.add_to_collection('pred', pred)
```

In function aucn_model.build_model

```
def build_model(input_images, num_output, is_train, keep_prob):
    k = 36
    first_channel = 64
    t_channel = 48

    print(input_images.get_shape())

    x1_0 = first_conv(input_images, channel=first_channel)

    # build spectral blocks
    print('the input shape of spectral blocks is: ', x1_0.get_shape())
    x1, x_1_transit_feature = loop_block(x1_0, channels_per_layer=k, kernel_size=(1, 1, 7), layer_num=2,
                                         is_train=is_train, block_name='spectral_block', loop_num=1)

    # transition layer
    print('the output shape of spectral blocks is: ', x1.get_shape())
    x1 = bn_relu(x1, is_train=is_train)
    tran1_var = conv_var(kernel_size=(1, 1, x1.get_shape()[3]), in_channels=x1.get_shape()[4], out_channels=t_channel,
                         init_method='msra', name='first_transition')
    tran1 = tf.nn.conv3d(x1, tran1_var, [1, 1, 1, 1, 1], padding='VALID')

    tran1 = bn_relu(tran1, is_train=is_train)
    print(tran1.get_shape())
    tran2 = Reshape((tran1.get_shape()[1], tran1.get_shape()[2], tran1.get_shape()[4], 1))(tran1)

    print(tran2.get_shape())
    tran2_var = conv_var(kernel_size=(3, 3, t_channel), in_channels=1, out_channels=first_channel,
                         init_method='msra', name='second_transition')
    x2_0 = tf.nn.conv3d(tran2, tran2_var, [1, 1, 1, 1, 1], padding='VALID')

    print('the input of spatial block:', x2_0.get_shape())
    # build spatial blocks
    x2, x_2_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(3, 1, 1), layer_num=2,
                                         is_train=is_train, block_name='spatial_block_1', loop_num=1)

    print('the output of spatial block:', x2.get_shape())
    x3, x_3_transit_feature = loop_block(x2_0, channels_per_layer=k, kernel_size=(1, 3, 1), layer_num=2,
                                         is_train=is_train, block_name='spatial_block_2', loop_num=1)
    x4 = tf.concat([x2, x3], axis=4)

    # Classifier block

    pool1 = tf.nn.avg_pool3d(x4, ksize=[1, x4.get_shape()[1], x4.get_shape()[2], 1, 1],
                             strides=[1, 1, 1, 1, 1], padding='VALID')
    print(pool1.get_shape())

 ### WRONG  HERE ####
    flatten = tf.layers.flatten(pool1)
# ---when changed to below it works---
    other_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]
    flatten = Reshape((other_dim,))(pool1)
#######################


    #print(tf.__version__)
    #flatten = tf.contrib.layers.flatten(pool1)
    print(flatten.get_shape())
    # flatten = tf.nn.dropout(flatten, keep_prob=keep_prob)
    wfc = tf.get_variable(name='FC_W', shape=[flatten.get_shape()[1], num_output],
                          initializer=tf.contrib.layers.xavier_initializer())
    bfc = tf.get_variable(name='FC_b', initializer=tf.constant(0.0, shape=[num_output]))

    logits = tf.matmul(flatten, wfc) + bfc
    print(logits.get_shape())
    prob = tf.nn.softmax(logits)

    return logits, prob
```
When I change to this code it works
```

flatten = tf.layers.flatten(pool1) # it doesnt work

other_dim = pool1.get_shape()[1]*pool1.get_shape()[2]*pool1.get_shape()[3]*pool1.get_shape()[4]
flatten = Reshape((other_dim,))(pool1) # it works
```
_

**System information**
colab,TF1.15

i just confused that why collapsed in making computational graph informed error ‘uninitialized’,i even not try to excute there."
53166,Unit tests //tensorflow/core/ir/... fail or crash depending on optimization level,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Tests fail with errors like

<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list
    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(""GPU"") assigned_device(""TPU"") {_mlir_device = ""GPU"", some_attribute = ""some attr!""} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)
                     ^
FileCheck error: '<stdin>' is empty.
FileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir


**Describe the expected behavior**

All tests pass

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): Correct improper use of ArrayRef and StringRef

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --copt=-O0 --copt=-ggdb --verbose_failures -- //tensorflow/core/ir/tests:ops.mlir.test

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
================================================================================
==================== Test output for //tensorflow/core/ir/tests:ops.mlir.test:
-- Testing: 1 tests, 1 workers --
FAIL: MLIR tests :: ops.mlir (1 of 1)
******************** TEST 'MLIR tests :: ops.mlir' FAILED ********************
Script:
--
: 'RUN: at line 1';   /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/tfg-opt-no-passes | /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir
--
Exit Code: 2

Command Output (stderr):
--
<stdin>:4:22: error: custom op 'tfg.AddV2' attribute '_mlir_device' occurs more than once in the attribute list
    %AddV2, %ctl_0 = AddV2(%placeholder, %placeholder_1) device(""GPU"") assigned_device(""TPU"") {_mlir_device = ""GPU"", some_attribute = ""some attr!""} : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)
                     ^
FileCheck error: '<stdin>' is empty.
FileCheck command line:  /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/.cache/bazel/_bazel_andrew/c61c5f84d239689cb19a72cfde16be9f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/ir/tests/ops.mlir.test.runfiles/org_tensorflow/tensorflow/core/ir/tests/ops.mlir

--

********************
********************
Failed Tests (1):
  MLIR tests :: ops.mlir


Testing Time: 0.11s
  Failed: 1
================================================================================

"
53165,cannot build tensorflow/lite/java/demo in Android Studio,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- tensorflow: on master `d0af304b7b3cd76a56eae1f5a82910199dd6644d`
- android studio: android-studio-2020.3.1.25-linux

**Describe the problem - Provide the exact sequence of commands / steps that you executed before running into the problem**

I am following this [tutorial](https://www.tensorflow.org/lite/performance/gpu) about **TensorFlow Lite GPU delegate**. And the related video.

when opening `tensorflow/tensorflow/lite/java/demo` in android studio, it tries to build, but fails with:

```
Unable to start the daemon process.

The project uses Gradle 4.4 which is incompatible with Java 11 or newer.

Possible solution:
 - Upgrade Gradle wrapper to 4.8 version and re-import the project
```

I then accept the suggestion (`gradle-wrapper.properties` gets modified) but building still fails with

```
A problem occurred configuring project ':app'.
> java.lang.NullPointerException (no error message)

* Try:
Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Exception is:
org.gradle.api.ProjectConfigurationException: A problem occurred configuring project ':app'.
	at org.gradle.configuration.project.LifecycleProjectEvaluator.addConfigurationFailure(LifecycleProjectEvaluator.java:109)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.onAfterEvaluateFailure(LifecycleProjectEvaluator.java:105)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.notifyAfterEvaluate(LifecycleProjectEvaluator.java:87)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.doConfigure(LifecycleProjectEvaluator.java:72)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.access$100(LifecycleProjectEvaluator.java:37)
	at org.gradle.configuration.project.LifecycleProjectEvaluator$ConfigureProject.run(LifecycleProjectEvaluator.java:125)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:52)
	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:677)
	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:138)
	at org.gradle.execution.TaskPathProjectEvaluator.configure(TaskPathProjectEvaluator.java:35)
	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:62)
	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:41)
	at org.gradle.initialization.DefaultGradleLauncher$ConfigureBuild.run(DefaultGradleLauncher.java:262)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.initialization.DefaultGradleLauncher.configureBuild(DefaultGradleLauncher.java:175)
	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:132)
	at org.gradle.initialization.DefaultGradleLauncher.executeTasks(DefaultGradleLauncher.java:115)
	at org.gradle.internal.invocation.GradleBuildController$1.call(GradleBuildController.java:77)
	at org.gradle.internal.invocation.GradleBuildController$1.call(GradleBuildController.java:74)
	at org.gradle.internal.work.DefaultWorkerLeaseService.withLocks(DefaultWorkerLeaseService.java:152)
	at org.gradle.internal.work.StopShieldingWorkerLeaseService.withLocks(StopShieldingWorkerLeaseService.java:38)
	at org.gradle.internal.invocation.GradleBuildController.doBuild(GradleBuildController.java:96)
	at org.gradle.internal.invocation.GradleBuildController.run(GradleBuildController.java:74)
	at org.gradle.tooling.internal.provider.runner.ClientProvidedPhasedActionRunner.run(ClientProvidedPhasedActionRunner.java:61)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
	at org.gradle.tooling.internal.provider.ValidatingBuildActionRunner.run(ValidatingBuildActionRunner.java:32)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner$3.run(RunAsBuildOperationBuildActionRunner.java:47)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)
	at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:97)
	at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)
	at org.gradle.launcher.exec.RunAsBuildOperationBuildActionRunner.run(RunAsBuildOperationBuildActionRunner.java:43)
	at org.gradle.tooling.internal.provider.SubscribableBuildActionRunner.run(SubscribableBuildActionRunner.java:51)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:50)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter$1.transform(InProcessBuildActionExecuter.java:46)
	at org.gradle.composite.internal.DefaultRootBuildState.run(DefaultRootBuildState.java:65)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46)
	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:32)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:39)
	at org.gradle.launcher.exec.BuildTreeScopeBuildActionExecuter.execute(BuildTreeScopeBuildActionExecuter.java:25)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:80)
	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:53)
	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:62)
	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:34)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:36)
	at org.gradle.tooling.internal.provider.GradleThreadBuildActionExecuter.execute(GradleThreadBuildActionExecuter.java:25)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:43)
	at org.gradle.tooling.internal.provider.ParallelismConfigurationBuildActionExecuter.execute(ParallelismConfigurationBuildActionExecuter.java:29)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:59)
	at org.gradle.tooling.internal.provider.StartParamsValidatingActionExecuter.execute(StartParamsValidatingActionExecuter.java:31)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:59)
	at org.gradle.tooling.internal.provider.SessionFailureReportingActionExecuter.execute(SessionFailureReportingActionExecuter.java:44)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:46)
	at org.gradle.tooling.internal.provider.SetupLoggingActionExecuter.execute(SetupLoggingActionExecuter.java:30)
	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)
	at org.gradle.util.Swapper.swap(Swapper.java:38)
	at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.LogAndCheckHealth.execute(LogAndCheckHealth.java:55)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:62)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:82)
	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:122)
	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)
	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:295)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
Caused by: java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:782)
	at com.google.common.base.Splitter.split(Splitter.java:376)
	at com.android.utils.PathUtils.getClassPathItems(PathUtils.java:84)
	at com.android.build.gradle.internal.transforms.FixStackFramesTransform.<init>(FixStackFramesTransform.java:151)
	at com.android.build.gradle.internal.TaskManager.maybeCreateDesugarTask(TaskManager.java:2425)
	at com.android.build.gradle.internal.TaskManager.createPostCompilationTasks(TaskManager.java:2248)
	at com.android.build.gradle.internal.ApplicationTaskManager.addCompileTask(ApplicationTaskManager.java:295)
	at com.android.build.gradle.internal.ApplicationTaskManager.lambda$createTasksForVariantScope$12(ApplicationTaskManager.java:229)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)
	at com.android.build.gradle.internal.ApplicationTaskManager.createTasksForVariantScope(ApplicationTaskManager.java:225)
	at com.android.build.gradle.internal.VariantManager.createTasksForVariantData(VariantManager.java:530)
	at com.android.build.gradle.internal.VariantManager.lambda$createAndroidTasks$1(VariantManager.java:352)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)
	at com.android.build.gradle.internal.VariantManager.createAndroidTasks(VariantManager.java:348)
	at com.android.build.gradle.BasePlugin.lambda$createAndroidTasks$6(BasePlugin.java:751)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)
	at com.android.build.gradle.BasePlugin.createAndroidTasks(BasePlugin.java:746)
	at com.android.build.gradle.BasePlugin.lambda$null$4(BasePlugin.java:652)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:81)
	at com.android.build.gradle.BasePlugin.lambda$createTasks$5(BasePlugin.java:648)
	at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:91)
	at org.gradle.internal.event.BroadcastDispatch$ActionInvocationHandler.dispatch(BroadcastDispatch.java:80)
	at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)
	at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:230)
	at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:149)
	at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)
	at org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:324)
	at org.gradle.internal.event.BroadcastDispatch$CompositeDispatch.dispatch(BroadcastDispatch.java:234)
	at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:140)
	at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:37)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy36.afterEvaluate(Unknown Source)
	at org.gradle.configuration.project.LifecycleProjectEvaluator$1.execute(LifecycleProjectEvaluator.java:83)
	at org.gradle.configuration.project.LifecycleProjectEvaluator$1.execute(LifecycleProjectEvaluator.java:80)
	at org.gradle.api.internal.project.DefaultProject.stepEvaluationListener(DefaultProject.java:1393)
	at org.gradle.configuration.project.LifecycleProjectEvaluator.notifyAfterEvaluate(LifecycleProjectEvaluator.java:80)
	... 89 more




```"
53164,"Tensorflow build fails with ""Multiple matches are not allowed unless one is unambiguously more specialized"" on linux_s390x","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat 8.1 on s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Tensorflow build
- TensorFlow version: v2.7.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): 8.3
- CUDA/cuDNN version:NA
- GPU model and memory: NA



**Describe the problem**
Tensorflow build fails with ""Multiple matches are not allowed unless one is unambiguously more specialized"" on linux_x390x""

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel --host_jvm_args=""-Xms8g"" --host_jvm_args=""-Xmx8g"" build --define=tensorflow_mkldnn_contraction_kernel=0 --copt=-Wno-maybe-uninitialized  --copt=-mzvector --copt=-funroll-loops --copt=-march=z14 --color=yes --curses=yes -s --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package`

failed with ""Multiple matches are not allowed unless one is unambiguously more specialized"" on linux_x390x""

**Any other info / logs**
```
Building TENSORFLOW.....
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /root/public/git/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/public/git/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /root/public/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/public/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:nogcp in file /root/public/git/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:linux in file /root/public/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/public/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build option --define has changed, discarding analysis cache.
ERROR: /root/public/git/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute ""data"" in //tensorflow/tools/pip_package:licenses:
//tensorflow:linux_s390x
//tensorflow:no_gcp_support
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: /root/public/git/tensorflow/tensorflow/tools/pip_package/BUILD:182:10: Illegal ambiguous match on configurable attribute ""data"" in //tensorflow/tools/pip_package:licenses:
//tensorflow:linux_s390x
//tensorflow:no_gcp_support
Multiple matches are not allowed unless one is unambiguously more specialized.
INFO: Elapsed time: 0.777s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (4 packages loaded, 274 targets co\
nfigured)
Cleaned up the artifacts
Cleaned up the artifacts
```
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53163,"model convert successfully, but invoke fail.","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. [Code](url)
[tf-awake-1109crnn-81-int8-2.6.tflite.zip](https://github.com/tensorflow/tensorflow/files/7585973/tf-awake-1109crnn-81-int8-2.6.tflite.zip)


### 3. Failure after conversion
model convert successfully. but invoke fail.
CONV_2D exception: xa_nnlib_hifi5/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c  
"
53161,tf1.14 tensorboard cannot profile multi batch,"My aim: I use tf1.14 keras to train my model, while I find my gpu utilization rate is very lower, so I want to use tensorboard profile to see which stage cost too much. I follow the wiki from tensorflow set profile_batch to 'str' or 'tuple', it show some error.
my code is below:
`tf.keras.callbacks.TensorBoard(log_dir=FLAGS.get(""model_path"") + ""logs/"", histogram_freq=1, profile_batch='2, 10')`
while it shows error like :
`
File ""/data/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks_v1.py"", line 365, in on_batch_end
    self._total_batches_seen == self._profile_batch - 1):
TypeError: unsupported operand type(s) for -: 'str' and 'int'
`
use tuple like 
`tf.keras.callbacks.TensorBoard(log_dir=FLAGS.get(""model_path"") + ""logs/"", histogram_freq=1, profile_batch=(2, 10))`
it show error like :
`TypeError: unsupported operand type(s) for -: 'tuple' and 'int'`

Can anyone help me?  if I really need to use tf1.14"
53160,MirroredStrategy: Efficient allreduce is not supported for n IndexedSlices,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.4
- Python version: 3.8
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: cuda11.0+cudnn8
- GPU model and memory: V100+32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
With Embedding layer, MirroredStrategy will report a warning: `MirroredStrategy: Efficient allreduce is not supported for n IndexedSlices`.
Trianing speed can scale up with 2-4 GPUs, but as we increase GPU devices, the training speed can not scale up anymore.
Besides, the GPU utilization is quite low if we add more GPUs (can jump from 0 ~ 100%).

I have search the Internet and find some workaround, e.g. #41898. But even if I change MirroredStrategy with MultiWorkerMirroredStrategy, the GPU utilization is quite low and the training time is nearly the same as MirroredStrategy.

**Describe the expected behavior**
In single machine with multiple GPUs, can use MirroredStrategy to scale up training speed with IndexedSlices (Embedding Layer).

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Borrow from @ratovarius
```
import sys
import tensorflow as tf
import numpy as np

def build_model_():

	input_a_size = 20
	input_b_size = 4
	num_classes = 2
	len_embedding = 256

	input_a = tf.keras.layers.Input(shape=(input_a_size,), name='input_a', dtype=np.uint8)
	input_b = tf.keras.layers.Input(shape=(input_b_size,), name='input_b', dtype=np.float32)

	x = tf.keras.layers.Embedding(len_embedding, 100)(input_a)
	x = tf.keras.layers.Conv1D(128, 4, activation='relu')(x)
	x = tf.keras.layers.MaxPooling1D(4)(x)
	x = tf.keras.layers.Flatten()(x)
	branch_a = tf.keras.layers.Dense(64, activation='relu')(x)

	x = tf.keras.layers.Dense(32, activation='relu')(input_b)
	branch_b = tf.keras.layers.Dense(32, activation='relu')(x)

	concat = tf.keras.layers.Concatenate()([
				                            branch_a,
				                            branch_b,
				                           ])

	x = tf.keras.layers.Dense(512, activation = 'relu')(concat)
	output = tf.keras.layers.Dense(num_classes, name='output', activation='softmax')(x)

	model = tf.keras.models.Model(inputs=[
				                          input_a,
				                          input_b,
				                         ],
				                  outputs=[output])

	return model

strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])
with strategy.scope():
    model = build_model_()
    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

y_train = True
y_train = tf.keras.utils.to_categorical(y_train, 2)

dataset = tf.data.Dataset.from_tensors(
    (
        {""input_a"": [[1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.]], 
         ""input_b"": [[1.], [1.], [1.], [1.]],}, 
        {""output"": y_train},
    )
).repeat(1000000).batch(256)

history = model.fit(
    x = dataset,
    epochs=10,
    verbose = 1,
)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53159,Classify issues using more specific labels,"Hi all,

based on the discussion on TF SIG build meeting, raising this issue here.

It would be great if TensorFlow issue tracker would have more specific labels so issues could be classified and browsed only based on some specific criteria. An example of labels that would be very helpful for us:

1. labels specific to dependency issues on Python layer (dependency compatibility issues, underpinning/overpining, ...)
2. labels specific to runtime/buildtime environment issues on software layer ouside of Python libs (python interpreter incompatibilities, CUDA issues, glibc issues, ...)
3. labels specific to runtime/buildtime environment issues on hardware layer (GPU issues, ...)

We would like to consume labeled issues - combining efforts on this front would be very helpful for us (Red Hat) to provide better environments to TensorFlow open-source community.

Thanks in advance for any response."
53157,Outdated documentation for `DataFormatVecPermute`,"This came up while creating a TF-TRT converter for `DataFormatVecPermute` in #52942 
cc @bixia1 

## URL(s) with the issue:

 - Python: https://www.tensorflow.org/api_docs/python/tf/raw_ops/DataFormatVecPermute
 - C++: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/data-format-vec-permute
 - Java: https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/nn/DataFormatVecPermute

## Description of the issue (what needs changing):

- Examples imply that an input of shape `(2, 4)` is valid, but that should be `(4, 2)`.
- The format strings can be of size 4 or 5 (e.g `""NHWC""`, `""NDHWC""`).
- The first dimension of the input shape vector can be `src_format.size()` or `src_format.size() - 2`, in which case it is assumed that non-spatial dimensions are omitted.

As an example, here is a valid Python code that is not covered by the documentation:

```python
import tensorflow as tf
a = tf.constant([1, 2, 3, 4, 5])
print(tf.raw_ops.DataFormatVecPermute(x=a, src_format=""NDHWC"", dst_format=""NCDHW""))
```
Output:
```
tf.Tensor([1 5 2 3 4], shape=(5,), dtype=int32)
```

## Submit a pull request?

I'm planning to submit a PR."
53156,Can't get TF Dataset to work with Keras ImageDataGenerator.flow_from_directory(),"So far I was using a Keras `ImageDataGenerator` with `flow_from_directory()` to train my Keras model with all images from the image class input folders. Now I want to train on multiple GPUs, so it seems I need to use a TensorFlow `Dataset` object.

Thus    I came up with this solution:

```python
keras_model = build_model()
train_datagen = ImageDataGenerator()
training_img_generator = train_datagen.flow_from_directory(
    input_path,
    target_size=(image_size, image_size),
    batch_size=batch_size,
    class_mode=""categorical"",
)
train_dataset = tf.data.Dataset.from_generator(
    lambda: training_img_generator,
    output_types=(tf.float32, tf.float32),
    output_shapes=([None, image_size, image_size, 3], [None, len(image_classes)])
)
# similar for validation_dataset = ...
keras_model.fit(
    train_dataset,
    steps_per_epoch=train_steps_per_epoch,
    epochs=epoch_count,
    validation_data=validation_dataset,
    validation_steps=validation_steps_per_epoch,
)
```

Now this seem to work, the model is trained as usual. However, during training I get the following warning message, when using a mirrored strategy:

> AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset

So I added the following lines between creating the data sets and calling `fit()`:

```python
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
train_dataset.with_options(options)
validation_dataset.with_options(options)
```

However, I still get the same warning.  
This leads me to these two questions:

1. What do I need to do in order to get rid of this warning?
2. **Even more important**: Why is TF not able to split the dataset with the default `AutoShardPolicy.FILE` policy, since I am using thousands of images per class in the input folder?"
53155,"Saving best metrics based on Custom metrics failing (WARNING:tensorflow:Can save best model only with CUSTOM METRICS available, skipping)","I have defined a callback that runs on the epoch end and calculated the metrics. It is working fine in terms of calculating the desired metrics. Below is the function for reference

```
class Metrics(tf.keras.callbacks.Callback):
    def __init__(self, train_tf_data, val_tf_data, model, CLASSES, logs={}, **kwargs):
        super().__init__(**kwargs)
        self.train_tf_data = train_tf_data
        self.val_tf_data = val_tf_data
        self.model = model
        self.CLASSES = CLASSES
        # for train data
        self.train_f1_after_epoch = 0
        self.train_prec_after_epoch = 0
        self.train_recall_after_epoch = 0
        # for val data
        self.val_f1_after_epoch = 0
        self.val_prec_after_epoch = 0
        self.val_recall_after_epoch = 0

    def on_train_begin(self, logs={}):
        self.train_reports = None
        self.val_reports = None
        self.val_f1_after_epoch = 0

    def on_epoch_end(self, epoch, logs={}):
        # for train data
        self.train_reports = test_model(model=self.model, data=self.train_tf_data, 
                                        CLASSES=self.CLASSES)
        self.train_f1_after_epoch = self.train_reports['f1_score']
        self.train_recall_after_epoch = self.train_reports['recall']
        self.train_prec_after_epoch = self.train_reports['precision']

        # for val data
        self.val_reports = test_model(model=self.model, data=self.val_tf_data, 
                                      CLASSES=self.CLASSES)
        self.val_f1_after_epoch = self.val_reports['f1_score']
        self.val_recall_after_epoch = self.val_reports['recall']
        self.val_prec_after_epoch = self.val_reports['precision']

        # saving train results to log dir
        logs[""train_f1_after_epoch""]=self.train_f1_after_epoch
        logs['train_precision_after_epoch'] = self.train_prec_after_epoch
        logs['train_recall_after_epoch'] = self.train_recall_after_epoch
        
        # saving val results to log dir
        logs['val_f1_after_epoch'] = self.val_f1_after_epoch
        logs['val_precision_after_epoch'] = self.val_prec_after_epoch
        logs['val_recall_after_epoch'] = self.val_recall_after_epoch


        print('train_reports_after_epoch', self.train_reports)
        print('val_reports_after_epoch', self.val_reports)

```
```
** .....Some model code .....**
```

## Using this in call back

```
m1 = tf.keras.metrics.CategoricalAccuracy()
m2 = tf.keras.metrics.Recall()
m3 = tf.keras.metrics.Precision()
m4 = Metrics(train_tf_data=train_data, 
             val_tf_data=test_data, model=model, 
             CLASSES=CLASS_NAMES)
optimizers = [
        tfa.optimizers.AdamW(learning_rate=lr * .001 , weight_decay=wd),
        tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)

           ]
optimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]
    
optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)


model.compile(
    optimizer= optimizer,
    loss = 'categorical_crossentropy',
    metrics=[m1, m2, m3],
    )

checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, 
                                                    monitor = 'val_f1_after_epoch',
                                                    save_best_only=True,
                                                    save_weights_only=True,
                                                    mode='max',
                                                    save_freq='epoch',
                                                    verbose=1)
                                                    
checkpoint_cb._supports_tf_logs = False
```

The issue that I am facing is that it is giving me a warning that says

**WARNING:TensorFlow: Can save best model only with val_f1_after_epoch available, skipping**

Upon investigating history I found that metrics is available in the history

```
print(list(history.history.keys()))
['loss',
'categorical_accuracy',
'recall',
'precision',
'val_loss',
'val_categorical_accuracy',
'val_recall',
'val_precision',
'train_f1_after_epoch',
'train_precision_after_epoch',
'train_recall_after_epoch',
'val_f1_after_epoch', #this is the metrics
'val_precision_after_epoch',
'val_recall_after_epoch']

```
I think there is a bug in ModelCheckpoint where is it not looking at the custom metrics and not saving the model.

I am using Tensorflow 2.7 (Also tried this with Tensorflow 2.5)"
53153,tensorflow2.4 can't train 2 model together on 2 different GPU,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10
- TensorFlow installed from (source or binary): pip install 
- TensorFlow version (use command below): tensorflow-gpu==2.4
- Python version: python3 .8
- CUDA/cuDNN version: cuda11.1 cudnn8.0
- GPU model and memory: RTX2080ti * 2, 12G

**Describe the current behavior**
my train code was tensorflow1.13,  use:    
     `import tensorflow.compat.v1 as tf`     
    ` tf.disable_v2_behavior()` 
Now it can train in tensorflow2.4.  

I try to train 2 modeles with different learning_rate and same batchsize on 2 different GPU,   
first can run with  ` os.environ['CUDA_VISIBLE_DEVICES'] = '0'`,  
but secend use `os.environ['CUDA_VISIBLE_DEVICES'] = '1'`, can't run for `out of memory`

`failed to allocate 9.20G (9874664192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory`"
53150,tf.keras.callbacks.TensorBoard(update_freq=#) doesn't work/is buggy on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
__Yes, but its still very simple__
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
__Windows 11 Home__
- TensorFlow installed from (source or binary):
__just did `pip install tensorflow`__
- TensorFlow version (use command below):
__same behavior on 2.6.1 and 2.7.0__
- Python version:
__3.8__
- CUDA/cuDNN version:
__11.2__
- GPU model and memory:
__NVIDIA GeForce RTX 2060, 6144MiB__

**Describe the current behavior**
using `tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=15)` does not correctly log metrics for both training and validation sets every 15 batches, as the docs state it should. Instead, in tensorboard I either see only a plot for the training set and nothing for validation, or most of the time I don't see a plot for either. Additionally, if I do a training run with the `update_freq` argument set, the plots for any previous runs stop appearing in tensorboard when I hit the refresh button. If I then delete the ""bad"" run (the one that had `update_freq` set), my existing plots come back. When I simply remove the `update_freq` argument and just train with `tf.keras.callbacks.TensorBoard(log_dir=log_dir)`, everything works as expected, and I see plots with points at each epoch.

**Describe the expected behavior**
using something like `tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=15)` should produce metrics every 15 batches, and I should be able to view a plot of this in tensorboard, just as I can without specifying to log metrics every `update_freq` batches.

My code:
```python
import tensorflow as tf
import numpy as np
import glob
import datetime
import pandas as pd

num_features = 4000

model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(num_features, 1)),
    tf.keras.layers.Conv1D(filters=7, kernel_size=(256,), activation='swish'),
    tf.keras.layers.Conv1D(filters=1, kernel_size=(2048,), activation=None),
    tf.keras.layers.Flatten()
])

model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.metrics.MeanAbsoluteError()]
)

log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
checkpoint_path = './tf_checkpoints/naive_cnn/{epoch:02d}_{batch:02d}.ckpt'

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq=15)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_freq=100, verbose=1)
train_X = None
train_Y = None

with tf.device('/device:CPU:0'):
    train_X = tf.signal.frame(allair1, frame_length=num_features, frame_step=1000)
    train_Y = tf.signal.frame(label_data[(num_features - 1698):], frame_length=1698, frame_step=1000)
    
train_X = np.expand_dims(train_X, axis=-1)
    
val_X = train_X[:500]
val_Y = train_Y[:500]
train_X = train_X[500:5000]
train_Y = train_Y[500:5000]

history = model.fit(
    x=train_X,
    y=train_Y,
    validation_data=(train_X, train_Y),
    batch_size=100,
    epochs=10,
    callbacks=[cp_callback, tensorboard_callback],
    verbose=1
)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
```
"
53149,dataset as_numpy_iterator() failing when using RaggedTensor,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6 / docker tensorflow:2.5.0-gpu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0 and 2.6.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Calling `as_numpy_iterator()` on a `tf.data.Dataset` having `RaggedTensorSpec` will fail with the error:

```
  File ""/Users/xx/Library/Caches/pypoetry/virtualenvs/sandbox-hasb3I3q-py3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4685, in to_numpy
    numpy = x._numpy()  # pylint: disable=protected-access
AttributeError: 'RaggedTensor' object has no attribute '_numpy'
```
Sample code to reproduce the issue:

```python
import tensorflow as tf

def gen():
    ragged_tensor = tf.ragged.constant([[1, 2], [3]])
    yield 42, ragged_tensor

dataset = tf.data.Dataset.from_generator(gen, output_signature=(
                tf.TensorSpec(shape=(), dtype=tf.int32),
                tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))

iterator = dataset.as_numpy_iterator()
print(iterator.next()) # failing with: AttributeError: 'RaggedTensor' object has no attribute '_numpy'

```
This regression has been brought by this [commit](https://github.com/tensorflow/tensorflow/commit/3e98cbc83e73c834d14137b98da0a26d1d8b7034)

**Describe the expected behavior**

Returning the numpy iterator as for `Tensor` without failing.
OR: providing a workaround or alternative usage to make it work.


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53148,Implementation of  fuzzy inference systems.,"Im working on tensorflow v2.7.0, I am willing to contribute to the same.

Currently there is no implementation of fuzzy inference systems like Sugeno and Tsukamoto networks

This feature request will ideally not change the current api. In fact certain existing tensorflow functionality could be reused.

All Machine learning engineers familiar with Fuzzy Inference Systems and Soft computing will be able to blend in their ideas based on the same to solve a wider range of problems that involves fuzzy logic. This will in fact help in increasing the evaluation and prediction metrics to higher values for domains involving soft computing.

This idea is inspired from the book:  Neuro-fuzzy and soft computing : a computational approach to learning and machine intelligence  (Chuen-Tsai Sun, Eiji Mizutani, and Jyh-Shing Roger Jang)
"
53146,Python TFlite not using multiple threads for float inference with xnnpack,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 64 bit.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pi Zero 2 W
- TensorFlow installed from (source or binary): Source.
- TensorFlow version (use command below): 365a3b68471f158defc3aea79a25fdaa56be4ac8 (i.e. few days old)
- Python version: 3.7
- Bazel version (if compiling from source): cmake 3.16.3
- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

I've trained MobileNet v3 Small using Tensorflow Object Detection API - models [here](https://github.com/tensorflow/tensorflow/files/7576019/models.zip). When I infer with `./linux_aarch64_benchmark_model --graph=models/mobilenetv3_224x224_FLOAT.tflite --num_threads=4 --use_xnnpack=true` I'm happy - around 25ms inference. Running with 1 thread is 57ms, 2 is 35ms, and 3 is 27ms, which is all expected. However, when I run the same tests with the simple script below, I'm getting 56-57ms seconds for any number of threads. Given this is similar to the single threaded performances from the benchmark tool, it'd imply that only a single thread is being used in python. Note that I am seeing `INFO: Created TensorFlow Lite XNNPACK delegate for CPU.` which seems to imply XNNPACK is being used (though I've also seen that log in the benchmark before it later on complaining and not using XNNPACK - I don't see that here, but I can't seem to get more logging.)

Lastly, it's worth noting that if I use a quantized model in Python, it behaves as expected (i.e. faster with more threads, and matches the benchmark tool). Also, while I compiled the pip package with the provided scripts, I tested the wheel from [here](https://wiki.seeedstudio.com/reTerminal_ML_TFLite/), and had the same issue which is at least another data point.

**Describe the expected behavior**

I would expect to see similar behaviour with fp32 inference as with quantized i.e. increased performance with more threads. And I would expect it to match the benchmark tool.

**Standalone code to reproduce the issue**

```python
import time

import numpy as np
from tflite_runtime.interpreter import Interpreter

interpreter = Interpreter(""models/mobilenetv3_224x224_FLOAT.tflite"", num_threads=4)
interpreter.allocate_tensors()
for _ in range(10):
    t0 = time.time()
    interpreter.invoke()
    print(int((time.time() - t0) * 1000))
```

**Other info / logs** NA
"
53145,Bazel command to get dependency graph fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7
- Python version:  3.7
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): MSVS 2019
- CUDA/cuDNN version: 11.5
- GPU model and memory: NVid
[error.log](https://github.com/tensorflow/tensorflow/files/7575460/error.log)
ia Geforce 940MX 



**Describe the problem**
Cannot generate dependency graph text with:
bazel query --notool_deps --noimplicit_deps ""deps(tensorflow:tensorflow_cc)"" --output graph
The following errors might be the problem:
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found


**Provide the exact sequence of commands / steps that you executed before running into the problem**
git clone https://github.com/tensorflow/tensorflow.git .
git checkout r2.7
Added startup --output_user_root=C:/tmp   to bazelrc file 
bazel query --notool_deps --noimplicit_deps ""deps(tensorflow:tensorflow_cc)"" --output graph


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

=========
F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\tensorflow\tf>bazel query --notool_deps --noimplicit_deps ""deps(tensorflow:tensorflow_cc)"" --output graph
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Repository llvm-project instantiated at:
  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/WORKSPACE:15:14: in <toplevel>
  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw --overlay F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:


[error.log](https://github.com/tensorflow/tensorflow/files/7575461/error.log)
"
53144,PyCharm doesn't resolve anything under tensorflow.keras,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: 2.7.0
- Python version: 3.9
- Installed using: Conda
- CUDA/cuDNN version: 11.2/8.1


**Describe the problem**
I've upgraded tensorflow from 2.5 to 2.7 and now PyCharm doesn't resolve anything under tensorflow.keras.

![keras-no-autocompletion](https://user-images.githubusercontent.com/25534110/142740868-4f37d8c6-77ff-4ce4-af24-c48d111e933a.png)

Other modules of tensorflow work, it's only keras that's problematic. I believe this has something to do with the change in TF 2.6 where keras has been split into a separate PIP package.

People have also been reporting this problem to JetBrains (PyCharm developers): https://youtrack.jetbrains.com/issue/PY-50318


**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Create the conda environment:
~ conda create --name tensorflow27
2. Activate it:
~ conda activate tensorflow27
3. Install Python:
~ conda install python=3.9
4. Install TensorFlow:
~ pip install tensorflow
5. Create new project in PyCharm
6. Write this code:
```
import tensorflow as tf
tf.keras.preprocessing.image_dataset_from_directory()
```
7. When you move mouse to image_dataset_from_directory function there will be no autocomplete.
8. Writing _tf.keras._ yields no modules/functions


**Any other info / logs**
/
"
53142,Cannot install tensorflow-gpu==2.3.0rc0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04.5 LTS` (Google Colab)
- TensorFlow version: `tensorflow-gpu 2.3.0rc0`
- Python version: `Python 3.7.12`
- Installed using virtualenv? pip? conda?: `pip`
- CUDA/cuDNN version: CUDA Version: `11.2`. Cuda compilation tools, release `11.1`, `V11.1.105`
- GPU model and memory: `NVIDIA Tesla K80`

**Describe the problem**

I get the following error when trying to install tensorflow-gpu in version `2.3.0rc0`:

```
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.3.0rc0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)
ERROR: No matching distribution found for tensorflow-gpu==2.3.0rc0

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`pip install tensorflow-gpu==2.3.0rc0`"
53141,Whether the padding property in the convolution is valid affects network speed.,"### 1. System information

- OS Platform and Distribution :  Android 11
- TensorFlow installation (pip package or built from source): pip installed
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.2.2
- Inference device: SnapDragon 855 GPU and DSP 
### 2.  After conversion
After post quantized the model, the quantized model runs slower on DSP than fp32 model on GPU. My model's convolution weight is int8, padding property is valid, does valid affect the computational speed of convolution?
I downloaded the mobileNet model from the tensorflow hub and compared it to find that the convolutional property of mobileNet is same.

"
53140,issue about XLA compile MirroredStrategy,"System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): I have tested on Ubuntu 18.04.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla P100

**Describe the current behavior**
When I train my model on multi-gpu with XLA compiling below error is occurred.

```
021-11-20 12:57:08.333476: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-20 12:57:09.302772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15397 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 6.0
2021-11-20 12:57:09.303502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 15397 MB memory:  -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b1:00.0, compute capability: 6.0
2021-11-20 12:57:10.556310: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:241 : INVALID_ARGUMENT: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""/home/sdb/wda/TF2-jit-compile-on-multi-gpu/xla_tf_function_distributed.py"", line 59, in <module>
    train_step_dist(images, labels)
  File ""/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_650]
```

Describe the expected behavior
I want to use XLA  compile MirroredStrategy， because I found that _XLA can now compile MirroredStrategy: the step function passed to`strategy.run` can now be annoted with `jit_compile=True`._ from RELEASE.md from  2.5.0

Standalone code to reproduce the issue:

```
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

# Size of each input image, 28 x 28 pixels
IMAGE_SIZE = 28 * 28
# Number of distinct number labels, [0..9]
NUM_CLASSES = 10
# Number of examples in each training batch (step)
TRAIN_BATCH_SIZE = 100
# Number of training steps to run
TRAIN_STEPS = 1000

# Loads MNIST dataset.
train, test = tf.keras.datasets.mnist.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(train).batch(TRAIN_BATCH_SIZE).repeat()


# Casting from raw data to the required datatypes.
def cast(images, labels):
    images = tf.cast(
        tf.reshape(images, [-1, IMAGE_SIZE]), tf.float32)
    labels = tf.cast(labels, tf.int64)
    return (images, labels)


layer = tf.keras.layers.Dense(NUM_CLASSES)
optimizer = tf.keras.optimizers.Adam()


@tf.function(jit_compile=True)
def compiled_step(images, labels):
    images, labels = cast(images, labels)

    with tf.GradientTape() as tape:
        predicted_labels = layer(images)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=predicted_labels, labels=labels
        ))
    gradients = tape.gradient(loss, layer.trainable_variables)
    return loss, predicted_labels, gradients

@tf.function()
def train_step(images, labels):
    loss, pred, gradients = compiled_step(images, labels)
    optimizer.apply_gradients(zip(gradients, layer.trainable_variables))


strategy = tf.distribute.MirroredStrategy()

@tf.function(jit_compile=True)
def train_step_dist(image, labels):
    strategy.run(train_step, args=(image, labels))


for images, labels in train_ds:
    if optimizer.iterations > TRAIN_STEPS:
        break
    train_step_dist(images, labels)

```"
53139,"Tensorflow datasets - Eurosat data installation issue - ConnectionError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))","Hi,

I have been facing this issue while trying to download Eurosat dataset from Tensorflow datasets API. Is anyone experiencing the same issue since 11/19/2021?

**Code used in Jupyter notebook:**

builder = tfds.builder(""eurosat/all"")

**Error message:**

Downloading and preparing dataset eurosat (1.93 GiB) to /home/jovyan/tensorflow_datasets/eurosat/all/2.0.0...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s]
Extraction completed...: 0 file [00:00, ? file/s]

/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)
    174             conn = connection.create_connection(
--> 175                 (self._dns_host, self.port), self.timeout, **extra_kw
    176             )

/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     95     if err is not None:
---> 96         raise err
     97 

/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     85                 sock.bind(source_address)
---> 86             sock.connect(sa)
     87             return sock

TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    705                 headers=headers,
--> 706                 chunked=chunked,
    707             )

/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    393             else:
--> 394                 conn.request(method, url, **httplib_request_kw)
    395 

/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in request(self, method, url, body, headers)
    238             headers[""User-Agent""] = _get_default_user_agent()
--> 239         super(HTTPConnection, self).request(method, url, body=body, headers=headers)
    240 

/opt/conda/lib/python3.7/http/client.py in request(self, method, url, body, headers, encode_chunked)
   1280         """"""Send a complete request to the server.""""""
-> 1281         self._send_request(method, url, body, headers, encode_chunked)
   1282 

/opt/conda/lib/python3.7/http/client.py in _send_request(self, method, url, body, headers, encode_chunked)
   1326             body = _encode(body, 'body')
-> 1327         self.endheaders(body, encode_chunked=encode_chunked)
   1328 

/opt/conda/lib/python3.7/http/client.py in endheaders(self, message_body, encode_chunked)
   1275             raise CannotSendHeader()
-> 1276         self._send_output(message_body, encode_chunked=encode_chunked)
   1277 

/opt/conda/lib/python3.7/http/client.py in _send_output(self, message_body, encode_chunked)
   1035         del self._buffer[:]
-> 1036         self.send(msg)
   1037 

/opt/conda/lib/python3.7/http/client.py in send(self, data)
    975             if self.auto_open:
--> 976                 self.connect()
    977             else:

/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in connect(self)
    204     def connect(self):
--> 205         conn = self._new_conn()
    206         self._prepare_conn(conn)

/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)
    186             raise NewConnectionError(
--> 187                 self, ""Failed to establish a new connection: %s"" % e
    188             )

NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--> 449                     timeout=timeout
    450                 )

/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    755             retries = retries.increment(
--> 756                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
    757             )

/opt/conda/lib/python3.7/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    573         if new_retry.is_exhausted():
--> 574             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    575 

MaxRetryError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-4-d5e677d3f673> in <module>
     13 print(builder)
     14 # Prepare the data.
---> 15 builder.download_and_prepare()
     16 
     17 

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     50     _check_no_positional(fn, args, ismethod, allowed=allowed)
     51     _check_required(fn, kwargs)
---> 52     return fn(*args, **kwargs)
     53 
     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    332           self._download_and_prepare(
    333               dl_manager=dl_manager,
--> 334               download_config=download_config)
    335 
    336           # NOTE: If modifying the lines below to put additional information in

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in _download_and_prepare(self, dl_manager, download_config)
   1028     super(GeneratorBasedBuilder, self)._download_and_prepare(
   1029         dl_manager=dl_manager,
-> 1030         max_examples_per_split=download_config.max_examples_per_split,
   1031     )
   1032 

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in _download_and_prepare(self, dl_manager, **prepare_split_kwargs)
    869         prepare_split_kwargs)
    870     for split_generator in self._split_generators(
--> 871         dl_manager, **split_generators_kwargs):
    872       if splits_lib.Split.ALL == split_generator.split_info.name:
    873         raise ValueError(

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/image/eurosat.py in _split_generators(self, dl_manager)
    129   def _split_generators(self, dl_manager):
    130     """"""Returns SplitGenerators.""""""
--> 131     path = dl_manager.download_and_extract(self.builder_config.download_url)
    132     path = os.path.join(path, self.builder_config.subdir)
    133     return [

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in download_and_extract(self, url_or_urls)
    372     with self._downloader.tqdm():
    373       with self._extractor.tqdm():
--> 374         return _map_promise(self._download_extract, url_or_urls)
    375 
    376   @property

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in _map_promise(map_fn, all_inputs)
    413   """"""Map the function into each element and resolve the promise.""""""
    414   all_promises = utils.map_nested(map_fn, all_inputs)  # Apply the function
--> 415   res = utils.map_nested(_wait_on_promise, all_promises)
    416   return res

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)
    157         return tuple(mapped)
    158   # Singleton
--> 159   return function(data_struct)
    160 
    161 

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in _wait_on_promise(p)
    397 
    398   def _wait_on_promise(p):
--> 399     return p.get()
    400 
    401 else:

/opt/conda/lib/python3.7/site-packages/promise/promise.py in get(self, timeout)
    510         target = self._target()
    511         self._wait(timeout or DEFAULT_TIMEOUT)
--> 512         return self._target_settled_value(_raise=True)
    513 
    514     def _target_settled_value(self, _raise=False):

/opt/conda/lib/python3.7/site-packages/promise/promise.py in _target_settled_value(self, _raise)
    514     def _target_settled_value(self, _raise=False):
    515         # type: (bool) -> Any
--> 516         return self._target()._settled_value(_raise)
    517 
    518     _value = _reason = _target_settled_value

/opt/conda/lib/python3.7/site-packages/promise/promise.py in _settled_value(self, _raise)
    224             if _raise:
    225                 raise_val = self._fulfillment_handler0
--> 226                 reraise(type(raise_val), raise_val, self._traceback)
    227             return self._fulfillment_handler0
    228 

/opt/conda/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
    717             if value.__traceback__ is not tb:
    718                 raise value.with_traceback(tb)
--> 719             raise value
    720         finally:
    721             value = None

/opt/conda/lib/python3.7/site-packages/promise/promise.py in handle_future_result(future)
    842         # type: (Any) -> None
    843         try:
--> 844             resolve(future.result())
    845         except Exception as e:
    846             tb = exc_info()[2]

/opt/conda/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)
    426                 raise CancelledError()
    427             elif self._state == FINISHED:
--> 428                 return self.__get_result()
    429 
    430             self._condition.wait(timeout)

/opt/conda/lib/python3.7/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--> 384             raise self._exception
    385         else:
    386             return self._result

/opt/conda/lib/python3.7/concurrent/futures/thread.py in run(self)
     55 
     56         try:
---> 57             result = self.fn(*self.args, **self.kwargs)
     58         except BaseException as exc:
     59             self.future.set_exception(exc)

/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/downloader.py in _sync_download(self, url, destination_path)
    229         response = urllib.request.urlopen(request, context=ca_verify['urllib'])
    230     else:
--> 231       response = session.get(url, stream=True)
    232       if response.status_code != 200:
    233         raise DownloadError('Failed to get url %s. HTTP code: %d.' %

/opt/conda/lib/python3.7/site-packages/requests/sessions.py in get(self, url, **kwargs)
    553 
    554         kwargs.setdefault('allow_redirects', True)
--> 555         return self.request('GET', url, **kwargs)
    556 
    557     def options(self, url, **kwargs):

/opt/conda/lib/python3.7/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    540         }
    541         send_kwargs.update(settings)
--> 542         resp = self.send(prep, **send_kwargs)
    543 
    544         return resp

/opt/conda/lib/python3.7/site-packages/requests/sessions.py in send(self, request, **kwargs)
    653 
    654         # Send the request
--> 655         r = adapter.send(request, **kwargs)
    656 
    657         # Total elapsed time of the request (approximately)

/opt/conda/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))"
53138,Benchmark Operator Profiling with GPU,"Thanks for the great work.

The benchmark program on CPU and hexagon delegate shows detailed information on neural net operators.
However, GPU delegate shows only `TfLiteGpuDelegateV2` node.
(I believe this is a squashed operator that runs on the GPU.)

It is hard to see what's going on inside GPU here.
Would it be possible to see the internal of this operator like CPU and Hexagon? 
Or, am I missing anything?

# CPU

```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false &> 01_cpu_noxnn.txt```

```
...
Number of nodes executed: 64
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       34	   266.475	    75.118%	    75.118%	     0.000	       34
	       DEPTHWISE_CONV_2D	       13	    83.740	    23.606%	    98.724%	     0.000	       13
	TFLite_Detection_PostProcess	        1	     3.579	     1.009%	    99.733%	     0.000	        1
	           CONCATENATION	        2	     0.483	     0.136%	    99.869%	     0.000	        2
	                LOGISTIC	        1	     0.233	     0.066%	    99.935%	     0.000	        1
	                 RESHAPE	       13	     0.232	     0.065%	   100.000%	     0.000	       13
```

# GPU
```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_gpu=true --use_xnnpack=false &> 03_gpu_noxnn.txt```

```
...
Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteGpuDelegateV2	        1	    44.928	    90.775%	    90.775%	     0.000	        1
	TFLite_Detection_PostProcess	        1	     4.566	     9.225%	   100.000%	     0.000	        1
```

# Hexagon Delegate
```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false --use_hexagon=true --hexagon_profiling=true &> 05_hexagon.txt```
```
...
Number of nodes executed: 65
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     Supernode_8x8p32to8	       34	  4690.936	    71.401%	    71.401%	     0.000	       42
	DepthwiseSupernode_8x8p32to8	       13	  1551.444	    23.615%	    95.016%	     0.000	       13
	         Requantize_8to8	        1	   113.790	     1.732%	    96.748%	     0.000	        2
	       QuantizedConcat_8	        1	    65.657	     0.999%	    97.747%	     0.000	        1
	   TfLiteHexagonDelegate	        1	    56.703	     0.863%	    98.610%	     0.000	        1
	                 Reshape	       13	    56.144	     0.855%	    99.465%	     0.000	       13
	      QuantizedSigmoid_8	        1	    30.509	     0.464%	    99.929%	     0.000	        1
	TFLite_Detection_PostProcess	        1	     4.651	     0.071%	   100.000%	     0.000	        1
```"
53137,replace private grpc with tensorflow's grpc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary(pip download & conda install)
- TensorFlow version: 2.3.0
- Python version:3.8.11
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am an undergraduate research from the Ohio State University. My group is focusing on optimizing grpc(v1.38.0). Right now, we want to fills our optimized grpc into tensorflow to increase the performance of distributed trainning. However, the problem is we don't know how to modify the configuration to let tensorflow using our optimized grpc. I would very appreciate your help and please let me know if you need further information. Thank you!
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53135,AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip3 installed
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0 (virtualenv)
- Python version: 3.9.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: CPU version

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Prompt with bug report appears:

<WARNING:tensorflow:AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function my_loss at 0x16bb96f70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function my_loss at 0x16bb96f70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2021-11-19 23:56:35.400148: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)>

**Describe the expected behavior**
No bug prompt

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53133,undefined reference to `tensorflow::str_util::EndsWith’,"On  centos 8
TensorFlow 2.6  installed from source.
Python 3.8.2
gcc version 8.4.1 20200928


First:  I build libtensorflow  use cmd: bazel build //tensorflow:libtensorflow_cc.so
second: I build example: tensorflow/tensorflow/examples/label_image/main.cc 

g++ -g  -std=c++14  -DLINUX -fpermissive  -fPIC -DHAVE_INTTYPES_H -DHAVE_NETINET_IN_H   -I/usr/local/include/ -I/opensource/tf/  -I/usr/local/include/eigen3   -I/opensource/tf/third_party -I /opensource/tf/third_party/eigen3  -c src/main.cc -o src/main.o

This is wrong message：
undefined reference to `tensorflow::str_util::EndsWith(absl::string_view, absl::string_view)'

undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'

undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::string_view>)'

however, I run cmd:
nm -Ca libtensorflow_cc.so |grep EndsWith
nm -Ca libtensorflow_framework.so

The function is in the lib. 
I use std=c++11, c++17 ,It doesn't work.
Any help?
Thanks.

 


"
53132,Non-OK-status: GpuLaunchKernel,"I trained a simple autoencoder for high resolution images.
A part of the code:

image_size_x = 1416
image_size_y = 1440
color_channel = 1

batch_size=10
epochs = 50
initial_epoch = 0
data_augmentation = True

verbose = 1

subtract_pixel_mean = True

number_of_workers = 1

input_shape = [image_size_x, image_size_y, 1]

image = Input(shape=input_shape)

def autoenc(input_img):
  #encoder
  x = Conv2D(kernel_size=(1,1), strides=2, filters =16)(input_img)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2D(kernel_size=(2,2), strides=2, filters =32)(x)  
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2D(kernel_size=(2,2), strides=2, filters =64)(x)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2D(kernel_size=(3,3), strides=3, filters =128)(x)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)

  #decoder
  x = Conv2DTranspose(kernel_size=(3,3), strides=3, filters =128)(x)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2DTranspose(kernel_size=(2,2), strides=2, filters =64)(x)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2DTranspose(kernel_size=(2,2), strides=2, filters =32)(x)
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2DTranspose(kernel_size=(1,1), strides=2, filters =16)(x)    
  x = BatchNormalization()(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Conv2DTranspose(kernel_size=(1,1), strides=1, filters =1,activation='sigmoid')(x)
  return x

# Autoencoder
autoencoder = Model(inputs=image, outputs=autoenc(image))
autoencoder.compile(loss='mse', optimizer = 'adam')
autoencoder.summary()

datagen = ImageDataGenerator(
    # set input mean to 0 over the dataset
    featurewise_center=False,
    # set each sample mean to 0
    samplewise_center=False,
    # divide inputs by std of dataset
    featurewise_std_normalization=False,
    # divide each input by its std
    samplewise_std_normalization=False,
    # apply ZCA whitening
    zca_whitening=False,
    # epsilon for ZCA whitening
    zca_epsilon=1e-06,
    # randomly rotate images in the range (deg 0 to 180)
    rotation_range=0,
    # randomly shift images horizontally
    width_shift_range=0,
    # randomly shift images vertically
    height_shift_range=0,
    # set range for random shear
    shear_range=0,
    # set range for random zoom
    zoom_range=0,
    # set range for random channel shifts
    channel_shift_range=0,
    # set mode for filling points outside the input boundaries
    fill_mode='nearest',
    # value used for fill_mode = ""constant""
    cval=0,
    # randomly flip images
    horizontal_flip=False,
    # randomly flip images
    vertical_flip=False,
    # set rescaling factor (applied before any other transformation)
    rescale=1./255,
    # set function that will be applied on each input
    preprocessing_function=None,
    data_format=None,
    # fraction of images reserved for validation (strictly between 0 and 1)
    validation_split=0.2)

image_and_labels=pd.read_csv(csv_path)

train_generator=datagen.flow_from_dataframe(
    dataframe=image_and_labels,
    x_col='images1',
    y_col='images1',
    class_mode='input',
    color_mode='grayscale',
    target_size=(image_size_x, image_size_y),
    batch_size=batch_size)

autoencoder.fit(train_generator, epochs=epochs, initial_epoch=initial_epoch, verbose=verbose, workers=number_of_workers, callbacks=callbacks)


Trained on 1800 images, everything works smoothly. I tried to test it on the SAME train dataset, same batch size.. 
predict = model.predict(train_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)

and it crushes here

2021-11-19 11:47:54.978459: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100
180/180 [==============================] - 61s 318ms/step
2021-11-19 11:48:54.493204: F tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc:151] Non-OK-status: GpuLaunchKernel( concat_fixed_kernel<T, IntType>, config.block_count, config.thread_per_block, 0, gpu_device.stream(), input_ptrs, split_size, static_cast<int>(output->dimension(0)), static_cast<int>(output->dimension(1)), output->data()) status: Internal: invalid configuration argument

Can someone please help me?

Ubuntu 20.04.3 LTS
tensorflow 2.6.0-dev20210614
python 3.8.8
CUDA Version: 11.0
GPU A100-PCIE-40GB
"
53130,//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark fails to build on AARCH64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark fails to build



**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --cxxopt=-Og --cxxopt=-ggdb --verbose_failures -- //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=161
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:
  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium
INFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only
INFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/274f12a44c606ecd20152f3e63c4f186793d9a8c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/0e24cef9fbfb6ac93dd326d458b6bae5183aaa4b.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark (3 packages loaded, 1583 targets configured).
INFO: Found 1 test target...
ERROR: /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/mkl_dnn_v1/BUILD.bazel:166:11: C++ compilation of rule '@mkl_dnn_v1//:mkl_dnn' failed (Exit 1): gcc failed: error executing command 
  (cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.o' -iquoteexternal/mkl_dnn_v1 -iquotebazel-out/aarch64-opt/bin/external/mkl_dnn_v1 -isystem external/mkl_dnn_v1/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -w -DAUTOLOAD_DYNAMIC_KERNELS '-ffp-contract=off' -Og -ggdb '-std=c++14' '-ffp-contract=off' -Og -ggdb -fexceptions -UUSE_MKL -UUSE_CBLAS -DDNNL_ENABLE_MAX_CPU_ISA -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/mkl_dnn_v1/src/common/gemm.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.o)
Execution platform: @local_execution_config_platform//:platform
external/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_sgemm(char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const float*, dnnl::impl::dim_t, const float*, dnnl::impl::dim_t, float, float*, dnnl::impl::dim_t, void*)':
external/mkl_dnn_v1/src/common/gemm.cpp:97:5: error: 'threadpool_utils' has not been declared
   97 |     threadpool_utils::activate_threadpool(
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp:101:5: error: 'threadpool_utils' has not been declared
  101 |     threadpool_utils::deactivate_threadpool();
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_u8s8s32(char, char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const uint8_t*, dnnl::impl::dim_t, uint8_t, const int8_t*, dnnl::impl::dim_t, int8_t, float, int32_t*, dnnl::impl::dim_t, const int32_t*, void*)':
external/mkl_dnn_v1/src/common/gemm.cpp:109:5: error: 'threadpool_utils' has not been declared
  109 |     threadpool_utils::activate_threadpool(
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp:113:5: error: 'threadpool_utils' has not been declared
  113 |     threadpool_utils::deactivate_threadpool();
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_s8s8s32(char, char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const int8_t*, dnnl::impl::dim_t, int8_t, const int8_t*, dnnl::impl::dim_t, int8_t, float, int32_t*, dnnl::impl::dim_t, const int32_t*, void*)':
external/mkl_dnn_v1/src/common/gemm.cpp:121:5: error: 'threadpool_utils' has not been declared
  121 |     threadpool_utils::activate_threadpool(
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp:126:5: error: 'threadpool_utils' has not been declared
  126 |     threadpool_utils::deactivate_threadpool();
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_bf16bf16f32(char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const dnnl::impl::bfloat16_t*, dnnl::impl::dim_t, const dnnl::impl::bfloat16_t*, dnnl::impl::dim_t, float, float*, dnnl::impl::dim_t, void*)':
external/mkl_dnn_v1/src/common/gemm.cpp:134:5: error: 'threadpool_utils' has not been declared
  134 |     threadpool_utils::activate_threadpool(
      |     ^~~~~~~~~~~~~~~~
external/mkl_dnn_v1/src/common/gemm.cpp:138:5: error: 'threadpool_utils' has not been declared
  138 |     threadpool_utils::deactivate_threadpool();
      |     ^~~~~~~~~~~~~~~~
Target //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark failed to build
INFO: Elapsed time: 2.945s, Critical Path: 2.14s
INFO: 71 processes: 57 internal, 14 local.
FAILED: Build did NOT complete successfully
//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark           FAILED TO BUILD

FAILED: Build did NOT complete successfully

"
53129,How can I modify the layer parameters of Bert model at trainabel_variables?,"tensorflow：2.2
os：linux
I want to try to train a 12-layer Robert model and a 6-layer Robert model. In the process of training, I will calculate the average value of some layer parameters of the 12 layer Robert model and all layer parameters of the 6-layer Robert model, so as to see whether this method can distill knowledge。
This is the code for modifying layer parameters：
`bert6 = TFBertModel.from_pretrained('uer/chinese_roberta_L-6_H-768', output_hidden_states=True)
bert12 = TFBertModel.from_pretrained('uer/chinese_roberta_L-12_H-768', output_hidden_states=True)

for var in bert6.trainable_variables:
    if 'attention' in var.name:
        name = ''.join(var.name.split('/')[3:])
        for var2 in bert12.trainable_variables:
            if 'attention' in var2.name:
                name2 = ''.join(var.name.split('/')[3:])
                if name2 == name:
                    print(""start var: "", var)
                    var.numpy = (var + var2) / 2
                    print(""end var: "", var)
                    print(""(var + var2) / 2: "", (var + var2) / 2)`

![1637313249(1)](https://user-images.githubusercontent.com/34124260/142597068-858ecb12-2975-49ee-b3bc-0eb6b3d4afae.png)


However, after checking the status before and after **var** average calculation, I found that it did not update successfully。
So I want to know if there is any way to solve this problem in tensorflow 2.2?
Thank you》
"
53128,@org_tensorflow//tensorflow/lite/toco:toco  build error,"**Hi,
I am trying to build the tensorflow code, When I execute this command：**

> bazel build -s --cxxopt=""-std=c++14"" -c dbg --cxxopt=-msse4 @org_tensorflow//tensorflow/lite/toco:toco  --experimental_repo_remote_exec

**， it gives this error:**
tensorflow/tensorflow/core/platform/default/BUILD:147:11: C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped)
tensorflow/core/platform/default/env_time.cc:26:17: error: use of undeclared identifier 'CLOCK_REALTIME'
  clock_gettime(CLOCK_REALTIME, &ts);
                ^
1 error generated.
Target //tensorflow/lite/toco:toco failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /Users/xm20200921/project/tf260_tflite/kikaime-engine-model-15-so-tf260/dict_maker/tensorflow/tensorflow/core/framework/BUILD:1279:31 C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped)，

Thank you!
"
53127,TFLite Movenet multipose/lightning input error,"Hi, 
I am trying to load and run the Movenet multipose model, the loading of model works fine but when I try to run it gives this error:
`java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input:0) with 3 bytes from a Java Buffer with 491520 bytes.`

As I can understand (and also I looked the model graph in Netron app) the input of the model is `type: uint8[1,1,1,3]` , and this is what the error is also telling, that my converted image doesn't fit the input of the model. So my question is: is this intentionally or the model is not correct ? 

I downloaded model from Tensorflow Hub -> https://tfhub.dev/google/lite-model/movenet/multipose/lightning/tflite/float16/1
And here (in the link above) also is specified that the model accepts 'A frame of video or an image, represented as an `int32 tensor of dynamic shape: 1xHxWx3`'. I handled the dynamic shape tensor as suggested in the link.

Thank you!
"
53124,tf.cast issue in 2.7.0,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.6.1810
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.7.0
-   **Python version**: 3.9.7
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: 11.4.120
-   **GPU model and memory**: NVIDIA Tesla V100 PCIe 32GB
-   **Exact command to reproduce**: tf.cast(tf.ones(1)+65535, tf.uint16)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

tf.cast with dtype=tf.uint16 seems to be treating values as dtype=tf.int16. This was not the case with TensorFlow 2.6.0 - I updated to 2.7.0 yesterday.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
tf.cast(tf.ones(1)+65535, tf.uint16)
<tf.Tensor: shape=(1,), dtype=uint16, numpy=array([32767], dtype=uint16)>
```"
53123,XLA Compilation fails: Non-root tuple types are not handled,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Yes
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.0
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.4.2 / 8.2.4
- GPU model and memory: RTX 2080 Ti 10GB

You can collect some of this information using our environment capture
```
python version: 3.8.10
python branch: 
python build version: ('default', 'Sep 28 2021 16:10:42')
python compiler version: GCC 9.3.0
python implementation: CPython

tf.version.VERSION = 2.7.0
tf.version.GIT_VERSION = v2.7.0-0-gc256c071
tf.version.COMPILER_VERSION = 9.3.0
```

**Describe the current behavior**
When running our custom training loop with `TF_XLA_FLAGS=--tf_xla_auto_jit=2`, the following error is obtained

```
2021-11-19 01:57:34.543515: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:432 : INTERNAL: Non-root tuple types are not handled.
Traceback (most recent call last):
  File ""train.py"", line 140, in <module>
    main(sys.argv[1:])
  File ""train.py"", line 130, in main
    trainer.train(
  File ""/home/bje2lr/venv/cu114/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/bje2lr/venv/cu114/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError:  Non-root tuple types are not handled.
         [[{{node cluster_3_1/xla_compile}}]] [Op:__inference_distributed_train_step_56538]
```

**Describe the expected behavior**

`tf_xla_auto_jit=2` should only jit functions that are supported by XLA.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): -

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


Unfortunatelly, uploading the xla log files did not work.	"
53118,Build from source failing for TF 2.7.0 (with a custom gcc 7.4.0 compiler),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.9 (Nitrogen)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0 (also tried 2.6.0, 2.5.0, 2.3.0)
- Python version: 3.8
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.7.2 
- GCC/Compiler version (if compiling from source): 7.4.0 (also tried 7.3.0, 5.3.0, 6.3.0)
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080ti



**Describe the problem**
I've been trying to install TensorFlow from source for about a month on a cluster in which the native GCC version is old (4.8.0). If I don't build my own GCC version, the installation fails every time because of this issue  #38718 . I've been using this [guide](https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445) to install the TensorFlow from source, but while I don't face the earlier error, the compilation **always** fails with some error related to Eigen, regardless of whichever GCC or TensorFlow version I use.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm BAZEL_CXXOPTS=-std=c++17 ../bazel-3.7.2/output/bazel build  //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures --config=nonccl --config=opt 

**Any other info / logs**

```
ERROR: /global/home/groups/co_noneq/deepmd/tensorflow/tensorflow/core/kernels/BUILD:3535:18: C++ compilation of rule '//tensorflow/core/kernels:cwise_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /global/scratch/users/ansingh/_bazel_ansingh/ccb34797bcab4c95cac85d6d3e6f44f2/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/global/software/sl-7.x86_64/modules/langs/cuda/10.2 \
    GCC_HOST_COMPILER_PATH=/global/home/groups/co_noneq/deepmd/gcc/bin/gcc \
    LD_LIBRARY_PATH=/global/home/groups/co_noneq/deepmd/binutils/lib:/global/home/groups/co_noneq/deepmd/gcc/lib64:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/lib64/stubs:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/lib64:/global/home/users/ansingh/openmm/lib:/global/home/users/ansingh/openmm/lib \
    PATH=/global/home/groups/co_noneq/deepmd/binutils/bin:/global/home/groups/co_noneq/deepmd/gcc/bin:/global/software/sl-7.x86_64/modules/langs/cuda/10.1/bin:/global/home/users/ansingh/miniconda3/bin:/global/home/users/ansingh/miniconda3/condabin:/global/software/sl-7.x86_64/modules/tools/sq/0.1.0/bin:/global/software/sl-7.x86_64/modules/tools/emacs/25.1/bin:/global/software/sl-7.x86_64/modules/tools/vim/7.4/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/global/home/groups/allhands/bin:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate/scripts:/global/home/users/ansingh/bin:/global/home/groups/allhands/bin:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate:/global/home/users/ansingh/moltemplate/moltemplate/moltemplate/scripts \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/global/home/users/ansingh/miniconda3/bin/python3 \
    PYTHON_LIB_PATH=/global/home/users/ansingh/miniconda3/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \
    TF_CUDA_PATHS=/global/software/sl-7.x86_64/modules/cuda/10.1/cudnn/7.6,/global/software/sl-7.x86_64/modules/langs/cuda/10.2 \
    TF_CUDA_VERSION=10 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION='' \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.o' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' '-DHAVE_STRERROR_R=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_DEREGISTER_FRAME=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_POSIX_FALLOCATE=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -iquote external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MathOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/DiscRalPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/LmhloPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/MhloPassIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/transforms_pass_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/DLTIBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/legalize_to_standard_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lower_complex_inc_gen -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/disc_ral_ops_inc_gen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmNeonIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/InstCombineTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/X86VectorIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAttrUtilsGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/EmitCAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/EmitCOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorAttrDefsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCodeGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCommonTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXInfo -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXUtilsAndDesc -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmNeonConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/X86VectorConversionIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CodeGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CommonTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86Info -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86UtilsAndDesc -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/X86DisassemblerInternalHeaders -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem tensorflow/compiler/mlir/hlo/include -isystem bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/include -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/MathToSPIRV -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/MathToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/TosaToLinalg -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToLinalg -isystem external/llvm-project/mlir/lib/Conversion/TosaToSCF -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToSCF -isystem external/llvm-project/mlir/lib/Conversion/TosaToStandard -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToStandard -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/include -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/X86 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_35' '--cuda-gpu-arch=sm_35' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' -DMLIR_GENERATED_CPU_KERNELS_ENABLED -DMLIR_GENERATED_GPU_KERNELS_ENABLED -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/cwise_op_gpu_zeta.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_zeta.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h: In member function 'std::size_t std::hash<float>::operator()(float) const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:22: error: expected ')' before numeric constant
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
                      ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:15: error: invalid operands of types 'float' and 'double(const char*) throw ()' to binary 'operator!='
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
         ~~~~~~^~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:232:64: error: expected ')' before ';' token
  return __val != 0.0f ? std::_Hash_impl::hash(__val) : 0;
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h: In member function 'std::size_t std::hash<double>::operator()(double) const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:22: error: expected ')' before numeric constant
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
                      ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:15: error: invalid operands of types 'double' and 'double(const char*) throw ()' to binary 'operator!='
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
         ~~~~~~^~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/functional_hash.h:244:63: error: expected ')' before ';' token
  return __val != 0.0 ? std::_Hash_impl::hash(__val) : 0;
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: At global scope:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:35: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                   ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:32: error: could not convert 'nan' from 'double(const char*) throw ()' to 'float'
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                               ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:57: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                                         ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:54: error: could not convert 'nan' from 'double(const char*) throw ()' to 'float'
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
                                                     ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:36: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                    ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:33: error: could not convert 'nan' from 'double(const char*) throw ()' to 'double'
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                ~^~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:58: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                                          ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:55: error: could not convert 'nan' from 'double(const char*) throw ()' to 'double'
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
                                                      ~^~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:41: error: expected ')' before numeric constant
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
                                         ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:38: error: could not convert 'nan' from 'double(const char*) throw ()' to 'long double'
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
                                     ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1360:11: error: expected ')' before numeric constant
      long double __i = 0.0L)
           ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1360:8: error: could not convert 'nan' from 'double(const char*) throw ()' to 'long double'
      long double __i = 0.0L)
       ~^~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<float> std::literals::complex_literals::operator""""if(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:28: error: no matching function for call to 'std::complex<float>::complex(<brace-enclosed initializer list>)'
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                            ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<long double>&)
   complex<float>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<double>&)
   complex<float>::complex(const complex<double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note: candidate: constexpr std::complex<float>::complex(float, float)
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note: candidate: constexpr std::complex<float>::complex(std::complex<float>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(const std::complex<float>&)
     struct complex<float>
                   ^~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(std::complex<float>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:26: error: expected primary-expression before '{' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                          ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:26: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:31: error: expected ')' before numeric constant
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                               ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1954:63: error: expected ';' before '}' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<float> std::literals::complex_literals::operator""""if(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:28: error: no matching function for call to 'std::complex<float>::complex(<brace-enclosed initializer list>)'
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                            ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<long double>&)
   complex<float>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1508:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note: candidate: constexpr std::complex<float>::complex(const std::complex<double>&)
   complex<float>::complex(const complex<double>& __z)
           ^~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1504:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note: candidate: constexpr std::complex<float>::complex(float, float)
       _GLIBCXX_CONSTEXPR complex(float __r = 0.0f, float __i = 0.0f)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1060:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note: candidate: constexpr std::complex<float>::complex(std::complex<float>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1058:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(const std::complex<float>&)
     struct complex<float>
                   ^~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note: candidate: constexpr std::complex<float>::complex(std::complex<float>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1053:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:26: error: expected primary-expression before '{' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                          ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:26: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:31: error: expected ')' before numeric constant
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                               ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1958:63: error: expected ';' before '}' token
   { return std::complex<float>{0.0F, static_cast<float>(__num)}; }
                                                               ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<double> std::literals::complex_literals::operator""""i(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:29: error: no matching function for call to 'std::complex<double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                             ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<long double>&)
   complex<double>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note: candidate: constexpr std::complex<double>::complex(double, double)
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note: candidate: constexpr std::complex<double>::complex(std::complex<double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(const std::complex<double>&)
     struct complex<double>
                   ^~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(std::complex<double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:27: error: expected primary-expression before '{' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:27: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:32: error: expected ')' before numeric constant
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1962:64: error: expected ';' before '}' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<double> std::literals::complex_literals::operator""""i(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:29: error: no matching function for call to 'std::complex<double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                             ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<long double>&)
   complex<double>::complex(const complex<long double>& __z)
           ^~~~~~~~~~~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1512:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note: candidate: constexpr std::complex<double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1219:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note: candidate: constexpr std::complex<double>::complex(double, double)
       _GLIBCXX_CONSTEXPR complex(double __r = 0.0, double __i = 0.0)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1209:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note: candidate: constexpr std::complex<double>::complex(std::complex<double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1207:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(const std::complex<double>&)
     struct complex<double>
                   ^~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note: candidate: constexpr std::complex<double>::complex(std::complex<double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1202:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:27: error: expected primary-expression before '{' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:27: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:32: error: expected ')' before numeric constant
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1966:64: error: expected ';' before '}' token
   { return std::complex<double>{0.0, static_cast<double>(__num)}; }
                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<long double> std::literals::complex_literals::operator""""il(long double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:34: error: no matching function for call to 'std::complex<long double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<long double>{0.0L, __num}; }
                                  ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<double>&)
       _GLIBCXX_CONSTEXPR complex(const complex<double>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note: candidate: constexpr std::complex<long double>::complex(long double, long double)
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(const std::complex<long double>&)
     struct complex<long double>
                   ^~~~~~~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:32: error: expected primary-expression before '{' token
   { return std::complex<long double>{0.0L, __num}; }
                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:32: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:37: error: expected ')' before numeric constant
   { return std::complex<long double>{0.0L, __num}; }
                                     ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1970:48: error: expected ';' before '}' token
   { return std::complex<long double>{0.0L, __num}; }
                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex: In function 'constexpr std::complex<long double> std::literals::complex_literals::operator""""il(long long unsigned int)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:34: error: no matching function for call to 'std::complex<long double>::complex(<brace-enclosed initializer list>)'
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                  ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<double>&)
       _GLIBCXX_CONSTEXPR complex(const complex<double>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1373:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note: candidate: constexpr std::complex<long double>::complex(const std::complex<float>&)
       _GLIBCXX_CONSTEXPR complex(const complex<float>& __z)
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1370:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note: candidate: constexpr std::complex<long double>::complex(long double, long double)
       _GLIBCXX_CONSTEXPR complex(long double __r = 0.0L,
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1359:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>::_ComplexT)
       _GLIBCXX_CONSTEXPR complex(_ComplexT __z) : _M_value(__z) { }
           ^~~~~~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1357:11: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(const std::complex<long double>&)
     struct complex<long double>
                   ^~~~~~~~~~~~~        
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note: candidate: constexpr std::complex<long double>::complex(std::complex<long double>&&)
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1352:19: note:   conversion of argument 1 would be ill-formed:
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:32: error: expected primary-expression before '{' token
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:32: error: expected ';' before '{' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:37: error: expected ')' before numeric constant
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                     ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/complex:1974:75: error: expected ';' before '}' token
   { return std::complex<long double>{0.0L, static_cast<long double>(__num)}; }
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits: In static member function 'static constexpr long double std::numeric_limits<long double>::denorm_min()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits:1799:65: error: expected ')' before numeric constant
       denorm_min() _GLIBCXX_USE_NOEXCEPT { return __LDBL_DENORM_MIN__; }
                                                                 ^~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/limits:1799:68: error: cannot convert 'double (*)(const char*) throw ()' to 'long double' in return
       denorm_min() _GLIBCXX_USE_NOEXCEPT { return __LDBL_DENORM_MIN__; }
                                                                    ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h: In function 'Packet Eigen::internal::psqrt_complex(const Packet&)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:80: error: expected ')' before numeric constant
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:63: error: expected primary-expression before '(' token
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:80: error: expected ')' before numeric constant
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
   const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), neg_zero)).v;
                                                                                                   ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:856:99: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:94: error: expected ')' before numeric constant
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:58: error: expected primary-expression before '(' token
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                          ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:94: error: expected ')' before numeric constant
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:103: error: expected ')' before ';' token
   real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
                                                                                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:881:103: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h: In static member function 'static Packet Eigen::internal::pchebevl<Packet, N>::run(Packet, const typename Eigen::internal::unpacket_traits<T>::type*)':
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:54: error: expected ')' before numeric constant
     Packet b1 = pset1<Packet>(static_cast<Scalar>(0.f));
                                                      ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:60: error: expected ')' before ';' token
     Packet b1 = pset1<Packet>(static_cast<Scalar>(0.f));
                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:1653:60: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_i1e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:42: error: expected ')' before numeric constant
     return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);
                                          ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:64: error: expected ')' before ';' token
     return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);
                                                                ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_i1e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:395:42: error: expected ')' before numeric constant
     return pselect(pcmp_lt(x, pset1<T>(0.0)), pnegate(y), y);
                                          ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:395:63: error: expected ')' before ';' token
     return pselect(pcmp_lt(x, pset1<T>(0.0)), pnegate(y), y);
                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:501:42: error: expected ')' before numeric constant
     return pselect(
                                          ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:501:102: error: expected ')' before ';' token
     return pselect(
                                                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:576:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:576:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k0<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:746:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:746:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1e<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1e<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:904:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:904:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_k1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1068:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1068:70: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y0<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                                              ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:74: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                                                                          ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y0<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1461:47: error: expected ')' before numeric constant
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1461:76: error: expected ')' before ';' token
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_j1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1574:46: error: expected ')' before numeric constant
     y_gt_two = pselect(
                                              ^  
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1574:82: error: expected ')' before ';' token
     y_gt_two = pselect(
                                                                                  ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_j1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1662:47: error: expected ')' before numeric constant
     y_gt_five = pselect(
                                               ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1662:84: error: expected ')' before ';' token
     y_gt_five = pselect(
                                                                                    ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y1<T, float>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:46: error: expected ')' before numeric constant
     x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);
                                              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:75: error: expected ')' before ';' token
     x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);
                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In static member function 'static T Eigen::internal::generic_y1<T, double>::run(const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1855:47: error: expected ')' before numeric constant
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1855:76: error: expected ')' before ';' token
     x_le_five = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_five);
                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::digamma_impl_maybe_poly<float>::run(float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:153:21: error: expected ')' before numeric constant
     } else return 0.0f;
                     ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:153:24: error: cannot convert 'double (*)(const char*) throw ()' to 'float' in return
     } else return 0.0f;
                        ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::digamma_impl_maybe_poly<double>::run(double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:176:12: error: expected ')' before numeric constant
     else return 0.0;
            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:176:14: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in return
     else return 0.0;
              ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In function 'T Eigen::internal::flipsign(const T&, const T&)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:46: error: expected ')' before numeric constant
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                              ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:50: error: wrong type argument to unary minus
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                                  ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:494:51: error: expected ')' before ';' token
   const T sign_mask = pset1<T>(Scalar(-0.0));
                                                   ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::machep()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:687:60: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar machep() { assert(false && ""machep not supported for this type""); return 0.0; }
                                                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::big()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:689:57: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar big() { assert(false && ""big not supported for this type""); return 0.0; }
                                                         ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::cephes_helper<Scalar>::biginv()':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:691:60: error: expected ')' before numeric constant
   static EIGEN_STRONG_INLINE Scalar biginv() { assert(false && ""biginv not supported for this type""); return 0.0; }
                                                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:26: error: expected ')' before numeric constant
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
                          ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:61: error: expected ')' before ';' token
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
                                                             ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1394:10: error: 'one' was not declared in this scope
         if( x == one )
          ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1394:10: note: suggested alternative: 'conj'
         if( x == one )
          ^~~
          conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1397:9: error: 'one' was not declared in this scope
         if( x < one )
         ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1397:9: note: suggested alternative: 'conj'
         if( x < one )
         ^~~
         conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1433:22: error: 'one' was not declared in this scope
         s += b*w/(x-one);
                      ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1433:22: note: suggested alternative: 'conj'
         s += b*w/(x-one);
                      ^~~
                      conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1434:12: error: expected primary-expression before '*' token
         s -= half * b;
            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:20: error: expected ')' before numeric constant
         Scalar zero = 0.0, one = 1.0;
                    ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:39: error: expected ')' before ';' token
         Scalar zero = 0.0, one = 1.0;
                                       ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1484:20: error: 'one' was not declared in this scope
         Scalar nplus = n + one;
                    ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1484:20: note: suggested alternative: 'conj'
         Scalar nplus = n + one;
                    ^~~
                    conj
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::betainc_helper<float>::incbps(float, float, float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1782:9: error: expected ')' before numeric constant
     s = 0.0f;
         ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1782:12: error: cannot convert 'double(const char*) throw ()' to 'float' in assignment
     s = 0.0f;
            ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1786:14: error: expected ')' before numeric constant
       if (b == 0.0f) {
              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1786:7: error: invalid operands of types 'float' and 'double(const char*) throw ()' to binary 'operator=='
       if (b == 0.0f) {
       ^~~~~~~~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1789:1: error: expected ')' before 'a'
       a += 1.0f;
 ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static float Eigen::internal::betainc_impl<float>::run(float, float, float)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1805:14: error: expected ')' before numeric constant
     if (a <= 0.0f) return nan;
              ^~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1806:1: error: expected ')' before 'if'
     if (b <= 0.0f) return nan;
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1824:1: error: expected primary-expression before '}' token
   }
 ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::betainc_helper<double>::incbps(double, double, double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1841:9: error: expected ')' before numeric constant
     s = 0.0;
         ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1841:11: error: cannot convert 'double(const char*) throw ()' to 'double' in assignment
     s = 0.0;
           ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In static member function 'static double Eigen::internal::betainc_impl<double>::run(double, double, double)':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1878:16: error: expected ')' before numeric constant
     if (aa <= 0.0 || bb <= 0.0) {
                ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1882:1: error: expected ')' before 'if'
     if ((xx <= 0.0) || (xx >= 1.0)) {
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1889:1: error: expected ')' before 'if'
     if ((bb * xx) <= 1.0 && xx <= 0.95) {
 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1921:13: error: expected ')' before numeric constant
     if (y < 0.0) {
             ^~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1923:3: error: expected ')' before 'else'
     } else {
   ^ ~~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h: In member function 'double std::random_device::entropy() const':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h:1606:14: error: expected ')' before numeric constant
     { return 0.0; }
              ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.h:1606:16: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in return
     { return 0.0; }
                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::poisson_distribution<_IntType>::result_type std::poisson_distribution<_IntType>::operator()(_UniformRandomNumberGenerator&, const std::poisson_distribution<_IntType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1319:18: error: expected ')' before numeric constant
   double __w = 0.0;
                  ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::binomial_distribution<_IntType>::result_type std::binomial_distribution<_IntType>::_M_waiting(_UniformRandomNumberGenerator&, _IntType, double)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1503:20: error: expected ')' before numeric constant
  double __sum = 0.0;
                    ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::normal_distribution<_RealType>::result_type std::normal_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::normal_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1804:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::normal_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::normal_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1851:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:39: error: expected ')' before numeric constant
      while (__r2 > 1.0 || __r2 == 0.0);
                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:44: error: expected ')' before ';' token
      while (__r2 > 1.0 || __r2 == 0.0);
                                            ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:1867:44: error: expected ')' before ';' token
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::gamma_distribution<_RealType>::result_type std::gamma_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::gamma_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2354:19: error: expected ')' before numeric constant
      while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2354:23: error: expected ')' before ';' token
      while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2369:19: error: expected ')' before numeric constant
      while (__u == 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2369:23: error: expected ')' before ';' token
      while (__u == 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::gamma_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::gamma_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2403:19: error: expected ')' before numeric constant
     while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2403:23: error: expected ')' before ';' token
     while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2424:19: error: expected ')' before numeric constant
     while (__v <= 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2424:23: error: expected ')' before ';' token
     while (__v <= 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2435:19: error: expected ')' before numeric constant
        while (__u == 0.0);
                   ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2435:23: error: expected ')' before ';' token
        while (__u == 0.0);
                       ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::discrete_distribution<_IntType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2652:78: error: expected ')' before numeric constant
       const double __sum = std::accumulate(_M_prob.begin(),
                                                                              ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2652:82: error: expected ')' before ';' token
       const double __sum = std::accumulate(_M_prob.begin(),
                                                                                  ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_constant_distribution<_RealType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2805:76: error: expected ')' before numeric constant
       const double __sum = std::accumulate(_M_den.begin(),
                                                                            ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2805:80: error: expected ')' before ';' token
       const double __sum = std::accumulate(_M_den.begin(),
                                                                                ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::piecewise_constant_distribution<_RealType>::result_type std::piecewise_constant_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::piecewise_constant_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2902:71: error: expected ')' before numeric constant
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2902:75: error: expected ')' before ';' token
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_constant_distribution<_RealType>::__generate_impl(_ForwardIterator, _ForwardIterator, _UniformRandomNumberGenerator&, const std::piecewise_constant_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2935:71: error: expected ')' before numeric constant
      const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:2935:75: error: expected ')' before ';' token
      const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'void std::piecewise_linear_distribution<_RealType>::param_type::_M_initialize()':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3030:20: error: expected ')' before numeric constant
       double __sum = 0.0;
                    ^~
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc: In member function 'std::piecewise_linear_distribution<_RealType>::result_type std::piecewise_linear_distribution<_RealType>::operator()(_UniformRandomNumberGenerator&, const std::piecewise_linear_distribution<_RealType>::param_type&)':
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3125:71: error: expected ')' before numeric constant
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                       ^ 
/global/home/groups/co_noneq/deepmd/gcc/lib/gcc/x86_64-pc-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/random.tcc:3125:75: error: expected ')' before ';' token
  const double __pref = __i > 0 ? __param._M_cp[__i - 1] : 0.0;
                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h: In member function 'Eigen::TensorOpCost Eigen::TensorEvaluator<const Eigen::TensorPairReducerOp<ReduceOp, Dims, XprType>, Device>::costPerCoeff(bool) const':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:65: error: expected ')' before numeric constant
     const double compute_cost = 1.0 +
                                                                 ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected ':' before ';' token
     const double compute_cost = 1.0 +
                                                                                                                                                               ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected primary-expression before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorArgMax.h:289:159: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: At global scope:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:595:61: error: expected ')' before numeric constant
   const RealScalar m_sin_PI_div_n_LUT[32] = {
                                                             ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:595:1414: error: expected ')' before '}' token
   const RealScalar m_sin_PI_div_n_LUT[32] = {
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:631:69: error: expected ')' before numeric constant
   const RealScalar m_minus_sin_2_PI_div_n_LUT[32] = {
                                                                     ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:631:1404: error: expected ')' before '}' token
   const RealScalar m_minus_sin_2_PI_div_n_LUT[32] = {
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: In member function 'void Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::butterfly_4(Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::ComplexScalar*)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:26: error: expected primary-expression before '(' token
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                          ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:31: error: expected ')' before numeric constant
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:70: error: expected ')' before ';' token
       tmp[3] = ComplexScalar(0.0, -1.0) * (data[2] - data[3]);
                                                                      ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:441:70: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:26: error: expected primary-expression before '(' token
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                          ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:31: error: expected ')' before numeric constant
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                               ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:69: error: expected ')' before ';' token
       tmp[3] = ComplexScalar(0.0, 1.0) * (data[2] - data[3]);
                                                                     ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:443:69: error: expected ')' before ';' token
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h: In member function 'void Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::butterfly_1D_merge(Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::ComplexScalar*, Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::Index, Eigen::TensorEvaluator<const Eigen::TensorFFTOp<FFT, ArgType, FFTResultType, FFTDir>, Device>::Index)':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:515:28: error: expected ')' before numeric constant
     ComplexScalar w(1.0, 0.0);
                            ^~
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h:515:32: error: expected ')' before ';' token
     ComplexScalar w(1.0, 0.0);
                                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h: In member function 'Eigen::TensorOpCost Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::costPerCoeff(bool) const':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:64: error: expected primary-expression before '(' token
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:82: error: expected ')' before numeric constant
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                                  ^ 
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:251: error: expected ')' before ';' token
     return m_rightImpl.costPerCoeff(vectorized) +
                                                                                                                                                                                                                                                           ^
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:196:251: error: expected ')' before ';' token
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xlogy_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:696:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:699:1: error: expected ')' before 'return'
     return x * numext::log(y);
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:699:26: error: expected ')' before ';' token
     return x * numext::log(y);
                          ^
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xlog1py_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:727:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:730:1: error: expected ')' before 'return'
     return x * numext::log1p(y);
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:730:28: error: expected ')' before ';' token
     return x * numext::log1p(y);
                            ^
./tensorflow/core/kernels/cwise_ops.h: In member function 'Scalar Eigen::internal::xdivy_op<Scalar>::operator()(const Scalar&, const Scalar&) const':
./tensorflow/core/kernels/cwise_ops.h:762:23: error: expected ')' before numeric constant
     if (x == Scalar(0.)) {
                       ^~
./tensorflow/core/kernels/cwise_ops.h:765:1: error: expected ')' before 'return'
     return x / y;
 ^   ~~
./tensorflow/core/kernels/cwise_ops.h:765:13: error: expected ')' before ';' token
     return x / y;
             ^
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar) [with Scalar = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1985:114:   required from 'typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::zeta(const Scalar&, const Scalar&) [with Scalar = float; typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsBFloat16.h:22:86:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:14: error: cannot convert 'double (*)(const char*) throw ()' to 'const float' in initialization
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
              ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar) [with Scalar = float]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1991:119:   required from 'typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::polygamma(const Scalar&, const Scalar&) [with Scalar = float; typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsBFloat16.h:25:91:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:8: error: cannot convert 'double (*)(const char*) throw ()' to 'float' in initialization
         Scalar zero = 0.0, one = 1.0;
        ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::zeta_impl<Scalar>::run(Scalar, Scalar) [with Scalar = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1985:114:   required from 'typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::zeta(const Scalar&, const Scalar&) [with Scalar = double; typename Eigen::internal::zeta_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = double]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/arch/GPU/SpecialFunctions.h:60:34:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1390:14: error: cannot convert 'double (*)(const char*) throw ()' to 'const double' in initialization
         const Scalar zero = 0.0, half = 0.5, one = 1.0;
              ^~~~
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::polygamma_impl<Scalar>::run(Scalar, Scalar) [with Scalar = double]':
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1991:119:   required from 'typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::polygamma(const Scalar&, const Scalar&) [with Scalar = double; typename Eigen::internal::polygamma_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = double]'
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/arch/GPU/SpecialFunctions.h:74:39:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1483:8: error: cannot convert 'double (*)(const char*) throw ()' to 'double' in initialization
         Scalar zero = 0.0, one = 1.0;
        ^~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 866.600s, Critical Path: 166.89s
INFO: 11958 processes: 2116 internal, 9842 local.
FAILED: Build did NOT complete successfully`
```"
53116,Mixed precision doesn't work properly on NVIDIA A10G GPUs,"A large U-Net 3D model configured with mixed precision fails with `No algorithm worked!` (see full `a10g.log` attached) when running inference on a NVIDIA A10G 20GB GPU (compute capability 8.6).

Using `tensorflow/tensorflow:nightly-gpu` Docker image, the error points to an out-of-memory issue (see full log `a10g_tf_nightly.log` attached):
```
No algorithm worked!  Error messages:
  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Allocating 4718624784 bytes exceeds the memory limit of 4294967296 bytes.
  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Allocating 4718624784 bytes exceeds the memory limit of 4294967296 bytes.
         [[{{node model/conv3d_transpose_3/conv3d_transpose}}]] [Op:__inference_predict_function_1150]

```
I'm able to overcome the issue by using full precision instead (i.e by setting `mixed_precision.set_global_policy(""float32"")`.

The same model configured with mixed precision works fine on the previous generation T4 Tesla GPU (compute capability 7.5), which have even less GPU memory - 16GB (see full `t4_tesla.log` attached).


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-1019-aws x86_64)
- TensorFlow installed from (source or binary): Official `tensorflow:latest-gpu` Docker image (`sha256@fc5eb0604722c7bef7b499bb007b3050c4beec5859c2e0d4409d2cca5c14d442`)
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.2.1 / 8.1.0.77-1
- GPU model and memory: A10G (20G) and Tesla T4 (16G).
- NVIDIA driver: 470.82
- `nvidia-smi` outputs for both GPU types provided in attachments.

**Describe the expected behavior**

Mixed precision mode should not exhaust all GPU memory on the newest generation of NVIDIA A10G.


**Standalone code to reproduce the issue**
Steps to reproduce:

1. Start instance with A10G GPU

2. Start interactive Docker container and pass `test.py` (copy from [Colab](https://colab.research.google.com/drive/1Vg8LPNNrYJYAd4qjA_p_GuabzjQdqTwU?usp=sharing))
```
$ docker run --gpus all -v /path/to/test.py:/srv/test.py -it tensorflow/tensorflow:latest-gpu /bin/bash
```
3. Run script
```
python /srv/test.py
```
4. Repeat steps using Tesla T4 (no error obtained)


**Other info / logs**
[a10g.log](https://github.com/tensorflow/tensorflow/files/7565542/a10g.log)
[a10g_tf_nightly.log](https://github.com/tensorflow/tensorflow/files/7565545/a10g_tf_nightly.log)
[t4_tesla.log](https://github.com/tensorflow/tensorflow/files/7565547/t4_tesla.log)

[a10g_nvidia_smi.log](https://github.com/tensorflow/tensorflow/files/7565555/a10g_nvidia_smi.log)
[t4_tesla_nvidia_smi.log](https://github.com/tensorflow/tensorflow/files/7565558/t4_tesla_nvidia_smi.log)"
53115,SSIM index maps,"**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Currently tf.image.ssim() returns a global ssim value but it would be useful to have it return a non globally averaged value as per 
e.g. https://uk.mathworks.com/help/images/ref/ssim.html that would allow the making of ssim index maps

**Will this change the current api? How?**
-

**Who will benefit with this feature?**
anyone doing computer vision

**Any Other info.**
"
53113,TFLite conversion fails when empty weights are involved,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): TF 2.7 and tf-nightly are both affected

### 2. Code
```python
import tensorflow as tf                                                        
                                                                               
class TestModel(tf.keras.models.Model):                                        
                                                                               
  def __init__(self, dims, **kwargs):                                          
    super().__init__(**kwargs)                                                 
    self._dense = tf.keras.layers.Dense(dims)                                  
        
  @tf.function
  def test(self, x):
    return self._dense(x)                                                      
                                                                               
  
# Fails only if dims=0.
test_model = TestModel(dims=0)                                                 
signatures = [
  test_model.test.get_concrete_function(x=tf.TensorSpec([None, 10], tf.float32))  
]                                                                              
    
converter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model)                                                                            
converter.optimizations = [tf.lite.Optimize.DEFAULT]                           
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]         

# Fails with ""ConverterError: Quantize weights transformation failed.""         
# But calling directly test_model.test(tf.random.normal(shape=[1, 10]) works.
tflite_model = converter.convert()
  
interpreter = tf.lite.Interpreter(model_content=tflite_model)                  
result = interpreter.get_signature_runner()(x=tf.random.normal(shape=[1, 10]))
```

### 3. Failure after conversion
Conversion fails when empty tensors are involved. Doing the same manually with an empty matrix variable has the same effect.
`tensorflow.lite.python.convert_phase.ConverterError: Quantize weights transformation failed.`

This does not happen when calling the model method directly, which correctly returns an empty tensor.
It does not fail either if the dense layer weights are not empty by setting `dims > 0`.

For context, I'm not trying to figure out corner cases that fail. I have a multiple signature model where some training time settings can cause some weights to become empty. This is not a problem with regular TF, but this limitation of TFLite is forcing me to introduce additional code in multiple places to work around the issue."
53112,"Misleading behavior on matrix and vector division, which is in contradiction with the documentation in nn.softmax","
**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7.12

**Describe the current behavior**

I found a misleading behavior when using broadcast between matrix and vectors. I will use the example of softmax to demonstrate the issue, which if it's not a bug then the following documentation I believe it is wrong "" https://www.tensorflow.org/api_docs/python/tf/nn/softmax"".

```
# random matrix
A = tf.random.uniform((4,4), maxval=10, dtype=tf.float64)

# softmax by hand, as suggested in: https://www.tensorflow.org/api_docs/python/tf/nn/softmax
A_softmax = tf.exp(A) / tf.reduce_sum(tf.exp(A), axis=-1)
tf.reduce_sum(A_norm, axis=-1) 
# >>> <tf.Tensor: shape=(4,), dtype=float64, numpy=array([0.79724022, 0.34883704, 3.8507741 , 2.37962821])>

A_nn_softmax = tf.nn.softmax(A, axis=-1)
tf.reduce_sum(A_nn_softmax, axis=-1) 
# >>> <tf.Tensor: shape=(4,), dtype=float64, numpy=array([1., 1., 1., 1.])>
```
In the above example, I use the tf.nn.softmax as a baseline and then use the corresponding computation as described in the documentation ( that is also the way I usually implement the softmax operation ). However, as you can see, A_softmax (direct computation) and A_nn_softmax (from the function nn.softmax) gave two different outputs, which according to the documentation should have not happened. I expect the same output for both matrices.

After some time I realise that the problem was related to the column-wise division that has, what I believe to be, a **misleading** behaviour.

```
tf.exp(A) / tf.reduce_sum(tf.exp(A), axis=-1)
```

In the above computation, I am dividing a (4, 4) matrix per (4,) vector, which I expect to perform a column-wise division since it is a column vector. However, it performs a row-wise division. I would like to know if this is an intended behavior and if so, I believe that the documentation of nn.softmax is wrong and should be ""tf.exp(logits) / tf.reduce_sum(tf.expand_dims(tf.exp(logits), axis=-1), axis=-1)"".


**Describe the expected behavior**

I expect that A_softmax equals A_nn_softmax.

To achieve the expected behavior I need to implicitly add an additional dimension to the denominator vector, becoming (4, 1).

In code correspond to:

```
tf.exp(logits) /tf.expand_dims(tf.reduce_sum(tf.exp(logits), axis=-1), axis=-1)
```
In alternative I can also use ""keepdims=True"" when doing the reduce_sum

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1Iv7cw4_gddfaxre5AD_oBQlGih6cJvMM?usp=sharing"
53111,Segmentation fault when passing tf.string tensor arguments in TFLite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): at least TF 2.7 and tf-nightly are affected

### 2. Code

```python
import tensorflow as tf                                                        
                                                                               
class TestModel(tf.keras.models.Model):                                        
  
  def __init__(self, **kwargs):
    super().__init__(**kwargs)                                                 
    self._hash = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            tf.constant(['testing', 'this', 'thing']),                         
            tf.constant([1, 2, 3])),
        default_value=-1)                                                      

  @tf.function
  def test(self, word):
    return self._hash.lookup(word)                                             


test_model = TestModel()                                                       
signatures = [test_model.test.get_concrete_function(tf.TensorSpec([None], tf.string))] 
    
converter = tf.lite.TFLiteConverter.from_concrete_functions(signatures, test_model) 
converter.optimizations = [tf.lite.Optimize.DEFAULT]                           
converter.target_spec.supported_ops = [                                        
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS                                                 
]

tflite_model = converter.convert()                                             
interpreter = tf.lite.Interpreter(model_content=tflite_model)

# Causes segmentation fault. Running test_model.test directly works fine.
result = interpreter.get_signature_runner()(word=tf.constant(['testing', 'that', 'thing']))
```

### 3. Failure after conversion
Conversion raises the following warning but completes.

```
2021-11-18 22:35:48.110926: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1880] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:
Resource ops: HashTableV2, LookupTableFindV2, LookupTableImportV2
Details:
	tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""13"", use_node_name_sharing = false, value_dtype = i32}
	tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<?x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
	tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<3x!tf_type.string>, tensor<3xi32>) -> () : {device = """"}
```

However, trying to run the signature fails with a segmentation fault.

I thought that static hash table lookups were supported in TFLite since many versions ago. Any possible temporary workarounds until this is fixed would be very much appreciated."
53110,Missing wheels for tflite-runtime on PyPI,"Thanks for releasing tflite to PyPI! I have been using the tflite wheels from the coral release previously (https://github.com/google-coral/pycoral/releases), but having tflite on PyPI greatly simplifies usage as a dependency of other python packages.

The PyPI release however doesn't have wheels for all the major platforms as the pycoral release has. For example is `linux_x86_64` missing and there are no wheels for python 3.9.

It would also be really great to have wheels for python 3.10 (which i previously asked for here: https://github.com/google-coral/pycoral/issues/58) in PyPI."
53109, Tensoflow C++ naive build - ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0  (also tried versions up to to 2.7) 
- Python version: 3.9.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc 10.2
- CUDA/cuDNN version: 11.5/8.0
- GPU model and memory: NVidia Geforce 940MX



**Describe the problem**
An error occurred during the fetch of repository 'com_google_protobuf':

**Provide the exact sequence of commands / steps that you executed before running into the problem**
git clone https://github.com/tensorflow/tensorflow.git .
git checkout r2.0
bazel clean
configure
bazel build tensorflow:tensorflow_cc


**Any other info / logs**
F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\tensorflow\tf>bazel build tensorflow:tensorflow_cc
INFO: Reading 'startup' options from c:\users\seth a quarshie\.bazelrc: --output_user_root=F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=126
INFO: Options provided by the client:
  'build' options: --python_path=F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/python/python.exe
INFO: Reading rc options for 'build' from f:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\tensorflow\tf\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --copt=-w --config=v2
INFO: Found applicable config definition build:v2 in file f:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\tensorflow\tf\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Repository com_google_protobuf instantiated at:
  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/WORKSPACE:19:16: in <toplevel>
  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/tensorflow/workspace.bzl:432:20: in tf_repositories
Repository rule tf_http_archive defined at:
  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl:124:34: in <toplevel>
INFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.
 * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz
If the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'com_google_protobuf':
   Traceback (most recent call last):
        File ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl"", line 104, column 25, in _tf_http_archive
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl"", line 71, column 32, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/repo.bzl"", line 52, column 13, in _execute_and_check_ret_code
                fail((""Non-zero return code({1}) when executing '{0}':\n"" + ""Stdout: {2}\n"" +
Error in fail: Non-zero return code(256) when executing 'F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf"" ""-i"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\"" \""-i\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\""""): Access is denied.
 (error: 5)
ERROR: Skipping 'tensorflow:tensorflow_cc': no such package '@com_google_protobuf//': Non-zero return code(256) when executing 'F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf"" ""-i"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\"" \""-i\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\""""): Access is denied.
 (error: 5)
WARNING: Target pattern parsing failed.
ERROR: no such package '@com_google_protobuf//': Non-zero return code(256) when executing 'F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf"" ""-i"" ""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""F:\workdir\units\unit_1_asa\ai\project\c++\ml\dl\software\dl-tool\msys2\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/com_google_protobuf\"" \""-i\"" \""F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/third_party/protobuf/protobuf.patch\""""): Access is denied.
 (error: 5)
INFO: Elapsed time: 3.311s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow




"
53107,Details about tf.keras.layers.Conv2DTranspose params 'padding' and 'output_padding',"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose

## Description of issue (what needs changing):
The documentation tells developers that output_padding has only one restriction that it should be lower that stride param, and it may not be all-inclusive.

### Clear description
Different from PyTorch, Conv2DTranspose implements Keras API, which uses padding params as 'same' or 'valid' instead of concrete int values. That gives developers a false impression that we can give padding cal tasks safely to TF. However, padding param should work with output_padding param, and output_padding cannot be set casually. I check the source code of TF and found that while using padding params as 'same', TF set pad to be kernel_size / 2 defaultly, that means output_padding param shoud be set to stride - 1, or mistakes may occur in some situation. However, the documentation tells developers that output_padding has only one restriction that it should be lower that stride param, so it may not be all-inclusive.

### Suggestions
Actually, setting param 'padding' in 'same' and 'valid' in this layer may be an ill considered decision, but since TF should implement Keras API, may be we have two solutions: One is to tell the developers in the documentation the relation of output_padding with padding param, and adds the implementation theory of same padding in this layer; Or just to adapt the actual pad values to the output_padding param as far as possible, and to warn the developers the risk and give them suggestions.
"
53105,What arguments should I be using for tf.nn.ctc_loss?,"I'm having a lot of trouble converting tf.compat.v1.nn.ctc_loss to tf.nn.ctc_loss (or even tf.v1.ctc_loss_v2 for that matter, which seems to be based on the same thing.) The lack of example usages on https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss makes this rather unclear. Is there any end-to-end example that I can use? Specifically I'm trying to convert 

input_tensor=tf.compat.v1.nn.ctc_loss(
                labels=self.gt_texts,
                inputs=self.ctc_in_3d_tbc,
                sequence_length=self.seq_len,
                ctc_merge_repeated=True,
            )

The above works fine.  My understanding is this is equivalent to 

input_tensor=tf.nn.ctc_loss(
                labels=self.gt_texts,  
                logits=self.ctc_in_3d_tbc,
                label_length=None,  
                logit_length=self.seq_len, 
                blank_index=-1,
            )

but it goesn't seem to be working as well. Can someone confirm that I'm using my arguments properly (especially logit_length - is it supposed to be the same as the previously specified sequence length argument)?"
53104,layers.LocallyConnected2D takes a lot of time to get initialized.,"Tensorflow version: 2.7.0
layers.LocallyConnected2D takes an awful amount of time to build. The time taken to build increases dramatically with increasing size of input. 

> LocallyConnected2D in mode 1 takes 11.0303 seconds which is 43.0397 times Normal Conv2D
> LocallyConnected2D in mode 2 takes 6.6656 seconds which is 26.0089 times Normal Conv2D
> LocallyConnected2D in mode 3 takes 4.1878 seconds which is 16.3407 times Normal Conv2D

Here mode means implementation. 
Sample code to generate the problem can be found in this [notebook](https://colab.research.google.com/drive/1r--vgfn9_6UD-ff7pIYSRfNoZlXQN0gk?usp=sharing)

Moreover the sparse tensor generated in implementation mode 2 uses sparse_mat_mul which is deprecated.
"
53102,TFDS CLI Fails to Install: TFDS Command not Found ,"I am getting the following error when trying to install TFDS CLI. I am running the commands as per the documentation/tutorial on the tensorflow website (https://www.tensorflow.org/datasets/cli). I basically clicked on the open in colab button on the link I attached and tried to run the first cell with. the below listed commands and it failed.

>> !pip install -q tfds-nightly (Executes Successfully)
>> !tfds --version (Throws and Error)

Error Message:
Traceback (most recent call last):
File ""/usr/local/bin/tfds"", line 5, in
from tensorflow_datasets.scripts.cli.main import launch_cli
File ""/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py"", line 40, in
from tensorflow_datasets.scripts.utils import flag_utils
ModuleNotFoundError: No module named 'tensorflow_datasets.scripts.utils'

NOTE:

On inspection of this path --> /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py
I realize that there should exist a similar path --> usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/utils.py with the flag_utils function in it for the ~/cli/main.py script to import/call but it does not exist in the Google Colab directory and I think this is a bug as someone might have deleted it or replcated its functionality elsewhere without realizing that this script would break.

Here is a screenshot of the error. 

![image](https://user-images.githubusercontent.com/35322634/142265312-47476721-f27f-4fd2-b8a2-b5b2c185c693.png)
"
53101,TensorFlow Lite fails to convert LSTM after upgrading from 2.6.2 to 2.7.0.,"### System information

-   Have I written custom code: yes
-   OS Platform and Distribution: Linux Ubuntu 16.04 (TensorFlow official docker images)
-   TensorFlow installed from binary
-   TensorFlow version : 2.7.0
-   Python version: 3.8.10
-   Exact command to reproduce:

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM
tf.version.VERSION
model_in = Input(shape=(800,))
model = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))
converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()

### Describe the problem
Tensorflow fails to convert the above model into TensorFlow lite, opposed to how it worked up to this version.

### Source code / logs
The example below shows how it used to work on version 2.6.0 and how it works now. (it was also working fine for 2.6.2)

docker run -it --rm --name tf36 tensorflow/tensorflow:2.6.0 python
Unable to find image 'tensorflow/tensorflow:2.6.0' locally
2.6.0: Pulling from tensorflow/tensorflow
feac53061382: Already exists
beba0652e867: Already exists
c5060c8118ce: Already exists
bfc0178fb9ad: Already exists
18fb3f957dc0: Already exists
cd5d06d0987e: Already exists
7ed4f7cde30b: Already exists
6bda0595411c: Already exists
Digest: sha256:773d5ce09e4ce003db02740c6a372a8a9f43be2bac23544d8f452bfec5347c53
Status: Downloaded newer image for tensorflow/tensorflow:2.6.0
Python 3.6.9 (default, Jan 26 2021, 15:33:00)
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> from tensorflow.keras.models import Model
>>> from tensorflow.keras.layers import Input, Embedding, LSTM
>>> tf.version.VERSION
'2.6.0'
>>> model_in = Input(shape=(800,))
>>> model = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))
2021-11-17 18:10:51.093603: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2021-11-17 18:10:59.494222: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmpg9eczqrl/assets
INFO:tensorflow:Assets written to: /tmp/tmpg9eczqrl/assets
2021-11-17 18:11:04.174113: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
2021-11-17 18:11:04.174165: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.
2021-11-17 18:11:04.175758: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /tmp/tmpg9eczqrl
2021-11-17 18:11:04.203946: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }
2021-11-17 18:11:04.203996: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpg9eczqrl
2021-11-17 18:11:04.287237: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.
2021-11-17 18:11:04.364667: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: /tmp/tmpg9eczqrl
2021-11-17 18:11:04.412505: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 236769 microseconds.
2021-11-17 18:11:04.589284: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
>>>



docker run -it --rm --name tf37 tensorflow/tensorflow:2.7.0 python
Python 3.8.10 (default, Sep 28 2021, 16:10:42)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> from tensorflow.keras.models import Model
>>> from tensorflow.keras.layers import Input, Embedding, LSTM
>>> tf.version.VERSION
'2.7.0'
>>> model_in = Input(shape=(800,))
>>> model = Model(model_in, LSTM(8)(Embedding(300, 8,)(model_in)))
2021-11-17 18:14:09.592797: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert()
2021-11-17 18:14:18.728660: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmpixyhgbqk/assets
INFO:tensorflow:Assets written to: /tmp/tmpixyhgbqk/assets
2021-11-17 18:14:24.806528: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.
2021-11-17 18:14:24.806615: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.
2021-11-17 18:14:24.808294: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpixyhgbqk
2021-11-17 18:14:24.823489: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }
2021-11-17 18:14:24.823535: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpixyhgbqk
2021-11-17 18:14:24.898508: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.
2021-11-17 18:14:25.002491: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpixyhgbqk
2021-11-17 18:14:25.091938: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 283650 microseconds.
2021-11-17 18:14:25.358785: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(callsite(callsite(callsite(callsite(""TensorArrayV2_1@__inference_standard_lstm_654""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0)))))))) at callsite(""model/lstm/PartitionedCall@__inference__wrapped_model_927""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0))))))))) at callsite(""StatefulPartitionedCall@__inference_signature_wrapper_2825""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0))))))))) at ""StatefulPartitionedCall"")): error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass
loc(callsite(callsite(callsite(callsite(""TensorArrayV2_1@__inference_standard_lstm_654""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0)))))))) at callsite(""model/lstm/PartitionedCall@__inference__wrapped_model_927""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0))))))))) at callsite(""StatefulPartitionedCall@__inference_signature_wrapper_2825""(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1315:0) at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1080:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"":216:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1150:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":1170:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":761:0 at callsite(""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"":775:0 at ""<stdin>"":1:0))))))))) at ""StatefulPartitionedCall"")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n converter._experimental_lower_tensor_list_ops = False
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"", line 775, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"", line 761, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"", line 1170, in convert
    saved_model_convert_result = self._convert_as_saved_model()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"", line 1152, in _convert_as_saved_model
    return super(TFLiteKerasModelConverterV2,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py"", line 945, in convert
    result = _toco_convert_impl(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"", line 223, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py"", line 216, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py"", line 821, in toco_convert_impl
    data = toco_convert_protos(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py"", line 315, in toco_convert_protos
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1315:0: error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1315:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n converter._experimental_lower_tensor_list_ops = False

>>>"
53100,Conflicting behavior with dvc in pytest,"Hello,

I recently came across a conflicting interaction with `dvc` (and possibly other packages presenting the same behavior) regarding the use of `sys.stdout.close()` and `sys.stderr.close()`.
In the codebase of tensorflow this is used here only:
 - https://github.com/tensorflow/tensorflow/blob/d11296a34b8b9ad798d8f273966de83dbb83de83/tensorflow/python/distribute/multi_process_runner.py#L809-L810
while it used there only in the dvc package (I put it for the sake of completeness):
 - https://github.com/iterative/dvc/blob/6c0bc9563c3ac6b104eedf6688dd79a1bc5bf4ba/dvc/daemon.py#L68-L70
 
If I understand correctly, this is done here so that threads may close more quickly as is explained in the docstring:

> We need to explicitly close them since Tensorflow may take a while to exit,
so that the reading threads in the main process can exit more quickly. 

In `dvc` they use it in a method called `_spawn_posix`, so I guess to help them deal with child processes.

I don't mean to enter into the details of the implementation of each package as it seems normal to do that kind of things there, but in the context of `pytest`, as it mocks subprocesses or at least tamper with it, it creates a conflicting resulting in a traceback:

```
Test session starts (platform: linux, Python 3.6.15, pytest 6.0.2, pytest-sugar 0.9.4)
cachedir: .pytest_cache
rootdir: /lib/data-science, configfile: setup.cfg
plugins: cov-3.0.0, sugar-0.9.4, mock-3.6.1
collecting ... 

Results (15.85s):
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 240, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 295, in _main
    config.hook.pytest_collection(session=session)
  File ""/usr/local/lib/python3.6/site-packages/pluggy/hooks.py"", line 286, in __call__
    return self._hookexec(self, self.get_hookimpls(), kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pluggy/manager.py"", line 93, in _hookexec
    return self._inner_hookexec(hook, methods, kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pluggy/manager.py"", line 87, in <lambda>
    firstresult=hook.spec.opts.get(""firstresult"") if hook.spec else False,
  File ""/usr/local/lib/python3.6/site-packages/pluggy/callers.py"", line 208, in _multicall
    return outcome.get_result()
  File ""/usr/local/lib/python3.6/site-packages/pluggy/callers.py"", line 80, in get_result
    raise ex[1].with_traceback(ex[2])
  File ""/usr/local/lib/python3.6/site-packages/pluggy/callers.py"", line 187, in _multicall
    res = hook_impl.function(*args)
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 306, in pytest_collection
    session.perform_collect()
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 516, in perform_collect
    items = self._perform_collect(args, genitems)
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 568, in _perform_collect
    self.items.extend(self.genitems(node))
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 771, in genitems
    yield from self.genitems(subnode)
  File ""/usr/local/lib/python3.6/site-packages/_pytest/main.py"", line 768, in genitems
    rep = collect_one_node(node)
  File ""/usr/local/lib/python3.6/site-packages/_pytest/runner.py"", line 441, in collect_one_node
    rep = ihook.pytest_make_collect_report(collector=collector)  # type: CollectReport
  File ""/usr/local/lib/python3.6/site-packages/pluggy/hooks.py"", line 286, in __call__
    return self._hookexec(self, self.get_hookimpls(), kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pluggy/manager.py"", line 93, in _hookexec
    return self._inner_hookexec(hook, methods, kwargs)
  File ""/usr/local/lib/python3.6/site-packages/pluggy/manager.py"", line 87, in <lambda>
    firstresult=hook.spec.opts.get(""firstresult"") if hook.spec else False,
  File ""/usr/local/lib/python3.6/site-packages/pluggy/callers.py"", line 203, in _multicall
    gen.send(outcome)
  File ""/usr/local/lib/python3.6/site-packages/_pytest/capture.py"", line 740, in pytest_make_collect_report
    out, err = self.read_global_capture()
  File ""/usr/local/lib/python3.6/site-packages/_pytest/capture.py"", line 661, in read_global_capture
    return self._global_capturing.readouterr()
  File ""/usr/local/lib/python3.6/site-packages/_pytest/capture.py"", line 571, in readouterr
    err = self.err.snap()
  File ""/usr/local/lib/python3.6/site-packages/_pytest/capture.py"", line 481, in snap
    self.tmpfile.seek(0)
ValueError: I/O operation on closed file.
```
as any of the two packages that runs after the other tries to close an already close stream.

One workaround I have for this (in case this might interest people that had the same issue) was to tamper with the two objects:
```
def pytest_configure(config):
    with ExitStack() as stack:
        for stream in (sys.stdout, sys.stderr):
            stack.enter_context(patch.object(stream, stream.close.__name__))
        yield
```
put in a `conftest.py` file at the root of the testing folder.

I guess such behavior will also arise whenever two packages make use of these same methods.
I have posted an issue on `dvc` github also:
 - https://github.com/iterative/dvc/issues/6992"
53099,Can't run Hexagonal Delegate with Java API,"I am following this doc: 
https://www.tensorflow.org/lite/performance/hexagon_delegate#add_the_shared_library_to_your_app
I did all 3 steps that are in Java API example (added the dependencies, and added Hexagonal Delegate as option when loading the interpreter), but when I run the interpreter I am getting the error:

`java.lang.UnsupportedOperationException: This Device doesn't support Hexagon DSP execution.`

I tried also all the suggestions that are in the bottom of the documentation provided by tensorflow: 
1. `adb shell cat /proc/cpuinfo | grep Hardware`  returns -> `Hardware        : Qualcomm Technologies, Inc APQ8098` , so I guess we are good here.
2.  I disabled SELinux enforce by running `adb shell setenforce 0` as super user, and when I run `adb shell getenforce` this returns -> `Permissive` , which means that the command worked.

Also another information: `adb shell getprop ro.board.platform` returns -> `msm8998`

I added 3 .so files that I generated by running the script `hexagon_nn_skel.run` ` v1.20.0.1`, in two main folders: `app/src/main/jniLibs/arm64-v8a` and `app/src/main/jniLibs/armeabi-v7a`, now I think that I am missing the part when we load these .so files. Does anybody have any idea ?


Update: 
More information related to issue: 
Device is running on Android 10, Snapdragon 835.
Also I saw that Hexagonal Delegate class is trying to load this native library 'tensorflowlite_hexagon_jni', but I can't find it in my project."
53098,tensorflow allreduce  train problem.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- 18.04ubuntu
- TensorFlow version 2.6.0
- Python version:3.6.8

**Describe the current behavior**
 there are sometimes error, not always, small chance happen, not easy to reproduce.......
only using cpu train. 

2 workers. using tfjob kubeflow operator.
here are the work-0 logs: （note: and each worker restart themself 3times, then reaches backofflimit . job failed..)
```bash


  |   | 2021-11-17 12:30:26 | 20211117
-- | -- | -- | --
  |   |   | 2021-11-17 12:30:26 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:27 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:27 | 2021-11-17 12:30:27,792 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:27 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:27 | 2021-11-17 12:30:27,812 [INFO] Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Cluster is ready.
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,907 [INFO] Cluster is ready.
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,913 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:32 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,914 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 1/3
  |   |   | 2021-11-17 12:30:32 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,915 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 2/3
  |   |   | 2021-11-17 12:30:32 | ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
  |   |   | 2021-11-17 12:30:32 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
  |   |   | 2021-11-17 12:30:32 | :{""created"":""@1637123432.915347775"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1637123432.913304158"",""description"":""Resolver transient failure"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc"",""file_line"":262,""referenced_errors"":[{""created"":""@1637123432.913302221"",""description"":""DNS resolution failed"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc"",""file_line"":202,""grpc_status"":14,""referenced_errors"":[{""created"":""@1637123432.913259195"",""description"":""Name or service not known"",""errno"":-2,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc"",""file_line"":108,""os_error"":""Name or service not known"",""syscall"":""getaddrinfo"",""target_address"":""deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222""}]}]}]}
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,915 [ERROR] Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
  |   |   | 2021-11-17 12:30:32 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
  |   |   | 2021-11-17 12:30:32 | :{""created"":""@1637123432.915347775"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1637123432.913304158"",""description"":""Resolver transient failure"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc"",""file_line"":262,""referenced_errors"":[{""created"":""@1637123432.913302221"",""description"":""DNS resolution failed"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc"",""file_line"":202,""grpc_status"":14,""referenced_errors"":[{""created"":""@1637123432.913259195"",""description"":""Name or service not known"",""errno"":-2,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc"",""file_line"":108,""os_error"":""Name or service not known"",""syscall"":""getaddrinfo"",""target_address"":""deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222""}]}]}]}
  |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
  |   |   | 2021-11-17 12:30:33 | File ""./train_atp_day.py"", line 115, in <module>
  |   |   | 2021-11-17 12:30:33 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
  |   |   | 2021-11-17 12:30:33 | File ""/workspace/script/atp_test_v2/model.py"", line 92, in __init__
  |   |   | 2021-11-17 12:30:33 | super(DeepFM, self).__init__()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 259, in __init__
  |   |   | 2021-11-17 12:30:33 | self._init_batch_counters()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 267, in _init_batch_counters
  |   |   | 2021-11-17 12:30:33 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 270, in __call__
  |   |   | 2021-11-17 12:30:33 | return cls._variable_v2_call(*args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 263, in _variable_v2_call
  |   |   | 2021-11-17 12:30:33 | shape=shape)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 68, in getter
  |   |   | 2021-11-17 12:30:33 | return captured_getter(captured_previous, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 2159, in creator_with_resource_vars
  |   |   | 2021-11-17 12:30:33 | created = self._create_variable(next_creator, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 530, in _create_variable
  |   |   | 2021-11-17 12:30:33 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 308, in create_mirrored_variable
  |   |   | 2021-11-17 12:30:33 | value_list = real_mirrored_creator(**kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 522, in _real_mirrored_creator
  |   |   | 2021-11-17 12:30:33 | v = next_creator(**kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 244, in <lambda>
  |   |   | 2021-11-17 12:30:33 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2675, in default_variable_creator_v2
  |   |   | 2021-11-17 12:30:33 | shape=shape)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 272, in __call__
  |   |   | 2021-11-17 12:30:33 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1607, in __init__
  |   |   | 2021-11-17 12:30:33 | distribute_strategy=distribute_strategy)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1734, in _init_from_args
  |   |   | 2021-11-17 12:30:33 | initial_value = initial_value()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 572, in initial_value_fn
  |   |   | 2021-11-17 12:30:33 | group_size, group_key, collective_instance_key)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py"", line 268, in broadcast_send
  |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 246, in collective_bcast_send
  |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 301, in collective_bcast_send_eager_fallback
  |   |   | 2021-11-17 12:30:33 | attrs=_attrs, ctx=ctx, name=name)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
  |   |   | 2021-11-17 12:30:33 | inputs, attrs, num_outputs)
  |   |   | 2021-11-17 12:30:33 | tensorflow.python.framework.errors_impl.UnavailableError: [_Derived_]Collective ops is aborted by: cluster check alive failed, /job:worker/replica:0/task:1 is down
  |   |   | 2021-11-17 12:30:33 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastSend]
  |   |   | 2021-11-17 12:30:33 | Exception ignored in: <bound method CollectiveAllReduceExtended.__del__ of <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceExtended object at 0x7fc2fa04cf28>>
  |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
2x |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 526, in __del__
  |   |   | 2021-11-17 12:30:33 | AttributeError: 'NoneType' object has no attribute 'info'
2x |   |   | 2021-11-17 12:30:33 | failed
  |   |   | 2021-11-17 12:30:34 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,687 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,707 [INFO] Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:Cluster is ready.
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,081 [INFO] Cluster is ready.
  |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,087 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:37 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,088 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 1/3
  |   |   | 2021-11-17 12:30:37 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,088 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 2/3
  |   |   | 2021-11-17 12:30:37 | ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
  |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
  |   |   | 2021-11-17 12:30:37 | :{""created"":""@1637123437.089088483"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1637123437.086762179"",""description"":""Resolver transient failure"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc"",""file_line"":262,""referenced_errors"":[{""created"":""@1637123437.086759773"",""description"":""DNS resolution failed"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc"",""file_line"":202,""grpc_status"":14,""referenced_errors"":[{""created"":""@1637123437.086713372"",""description"":""Name or service not known"",""errno"":-2,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc"",""file_line"":108,""os_error"":""Name or service not known"",""syscall"":""getaddrinfo"",""target_address"":""deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222""}]}]}]}
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,089 [ERROR] Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
  |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
  |   |   | 2021-11-17 12:30:37 | :{""created"":""@1637123437.089088483"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3941,""referenced_errors"":[{""created"":""@1637123437.086762179"",""description"":""Resolver transient failure"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc"",""file_line"":262,""referenced_errors"":[{""created"":""@1637123437.086759773"",""description"":""DNS resolution failed"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc"",""file_line"":202,""grpc_status"":14,""referenced_errors"":[{""created"":""@1637123437.086713372"",""description"":""Name or service not known"",""errno"":-2,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc"",""file_line"":108,""os_error"":""Name or service not known"",""syscall"":""getaddrinfo"",""target_address"":""deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222""}]}]}]}
  |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
  |   |   | 2021-11-17 12:30:37 | File ""./train_atp_day.py"", line 115, in <module>
  |   |   | 2021-11-17 12:30:37 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
  |   |   | 2021-11-17 12:30:37 | File ""/workspace/script/atp_test_v2/model.py"", line 92, in __init__
  |   |   | 2021-11-17 12:30:37 | super(DeepFM, self).__init__()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 259, in __init__
  |   |   | 2021-11-17 12:30:37 | self._init_batch_counters()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 267, in _init_batch_counters
  |   |   | 2021-11-17 12:30:37 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 270, in __call__
  |   |   | 2021-11-17 12:30:37 | return cls._variable_v2_call(*args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 263, in _variable_v2_call
  |   |   | 2021-11-17 12:30:37 | shape=shape)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 68, in getter
  |   |   | 2021-11-17 12:30:37 | return captured_getter(captured_previous, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 2159, in creator_with_resource_vars
  |   |   | 2021-11-17 12:30:37 | created = self._create_variable(next_creator, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 530, in _create_variable
  |   |   | 2021-11-17 12:30:37 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 308, in create_mirrored_variable
  |   |   | 2021-11-17 12:30:37 | value_list = real_mirrored_creator(**kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 522, in _real_mirrored_creator
  |   |   | 2021-11-17 12:30:37 | v = next_creator(**kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 244, in <lambda>
  |   |   | 2021-11-17 12:30:37 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2675, in default_variable_creator_v2
  |   |   | 2021-11-17 12:30:37 | shape=shape)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 272, in __call__
  |   |   | 2021-11-17 12:30:37 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1607, in __init__
  |   |   | 2021-11-17 12:30:37 | distribute_strategy=distribute_strategy)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1734, in _init_from_args
  |   |   | 2021-11-17 12:30:37 | initial_value = initial_value()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 572, in initial_value_fn
  |   |   | 2021-11-17 12:30:37 | group_size, group_key, collective_instance_key)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py"", line 268, in broadcast_send
  |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 246, in collective_bcast_send
  |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 301, in collective_bcast_send_eager_fallback
  |   |   | 2021-11-17 12:30:37 | attrs=_attrs, ctx=ctx, name=name)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
  |   |   | 2021-11-17 12:30:37 | inputs, attrs, num_outputs)
  |   |   | 2021-11-17 12:30:37 | tensorflow.python.framework.errors_impl.UnavailableError: [_Derived_]Collective ops is aborted by: cluster check alive failed, /job:worker/replica:0/task:1 is down
  |   |   | 2021-11-17 12:30:37 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastSend]
  |   |   | 2021-11-17 12:30:37 | Exception ignored in: <bound method CollectiveAllReduceExtended.__del__ of <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceExtended object at 0x7f56042d7f28>>
  |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
2x |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 526, in __del__
  |   |   | 2021-11-17 12:30:37 | AttributeError: 'NoneType' object has no attribute 'info'
2x |   |   | 2021-11-17 12:30:37 | failed
  |   |   | 2021-11-17 12:30:52 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,287 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
  |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,311 [INFO] Waiting for the cluster, timeout = inf


```

and the  worker-1's log is:

```bash

2021-11-17 12:30:31 | 20211117
-- | --
  |   |   | 2021-11-17 12:30:31 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,903 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,914 [INFO] Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Cluster is ready.
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,924 [INFO] Cluster is ready.
  |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,926 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
  |   |   | 2021-11-17 12:30:33 | File ""./train_atp_day.py"", line 115, in <module>
  |   |   | 2021-11-17 12:30:33 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
  |   |   | 2021-11-17 12:30:33 | File ""/workspace/script/atp_test_v2/model.py"", line 92, in __init__
  |   |   | 2021-11-17 12:30:33 | super(DeepFM, self).__init__()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 259, in __init__
  |   |   | 2021-11-17 12:30:33 | self._init_batch_counters()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 267, in _init_batch_counters
  |   |   | 2021-11-17 12:30:33 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 270, in __call__
  |   |   | 2021-11-17 12:30:33 | return cls._variable_v2_call(*args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 263, in _variable_v2_call
  |   |   | 2021-11-17 12:30:33 | shape=shape)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 68, in getter
  |   |   | 2021-11-17 12:30:33 | return captured_getter(captured_previous, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 2159, in creator_with_resource_vars
  |   |   | 2021-11-17 12:30:33 | created = self._create_variable(next_creator, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 530, in _create_variable
  |   |   | 2021-11-17 12:30:33 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 308, in create_mirrored_variable
  |   |   | 2021-11-17 12:30:33 | value_list = real_mirrored_creator(**kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 522, in _real_mirrored_creator
  |   |   | 2021-11-17 12:30:33 | v = next_creator(**kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 244, in <lambda>
  |   |   | 2021-11-17 12:30:33 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2675, in default_variable_creator_v2
  |   |   | 2021-11-17 12:30:33 | shape=shape)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 272, in __call__
  |   |   | 2021-11-17 12:30:33 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1607, in __init__
  |   |   | 2021-11-17 12:30:33 | distribute_strategy=distribute_strategy)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1734, in _init_from_args
  |   |   | 2021-11-17 12:30:33 | initial_value = initial_value()
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 579, in initial_value_fn
  |   |   | 2021-11-17 12:30:33 | collective_instance_key)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py"", line 345, in broadcast_recv
  |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 59, in collective_bcast_recv
  |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 115, in collective_bcast_recv_eager_fallback
  |   |   | 2021-11-17 12:30:33 | attrs=_attrs, ctx=ctx, name=name)
  |   |   | 2021-11-17 12:30:33 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
  |   |   | 2021-11-17 12:30:33 | inputs, attrs, num_outputs)
  |   |   | 2021-11-17 12:30:33 | tensorflow.python.framework.errors_impl.FailedPreconditionError: [_Derived_]Collective ops is aborted by: group 1 failed to resolve. This normally means the server has restarted
  |   |   | 2021-11-17 12:30:33 | Additional GRPC error information from remote target /job:worker/replica:0/task:0:
  |   |   | 2021-11-17 12:30:33 | :{""created"":""@1637123433.061968995"",""description"":""Error received from peer ipv4:100.91.109.177:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""group 1 failed to resolve. This normally means the server has restarted"",""grpc_status"":9}
  |   |   | 2021-11-17 12:30:33 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastRecv]
2x |   |   | 2021-11-17 12:30:33 | failed
  |   |   | 2021-11-17 12:30:34 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,075 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,086 [INFO] Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:Cluster is ready.
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,098 [INFO] Cluster is ready.
  |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,098 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
  |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
  |   |   | 2021-11-17 12:30:37 | File ""./train_atp_day.py"", line 115, in <module>
  |   |   | 2021-11-17 12:30:37 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
  |   |   | 2021-11-17 12:30:37 | File ""/workspace/script/atp_test_v2/model.py"", line 92, in __init__
  |   |   | 2021-11-17 12:30:37 | super(DeepFM, self).__init__()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 259, in __init__
  |   |   | 2021-11-17 12:30:37 | self._init_batch_counters()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 530, in _method_wrapper
  |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 267, in _init_batch_counters
  |   |   | 2021-11-17 12:30:37 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 270, in __call__
  |   |   | 2021-11-17 12:30:37 | return cls._variable_v2_call(*args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 263, in _variable_v2_call
  |   |   | 2021-11-17 12:30:37 | shape=shape)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 68, in getter
  |   |   | 2021-11-17 12:30:37 | return captured_getter(captured_previous, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 2159, in creator_with_resource_vars
  |   |   | 2021-11-17 12:30:37 | created = self._create_variable(next_creator, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 530, in _create_variable
  |   |   | 2021-11-17 12:30:37 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 308, in create_mirrored_variable
  |   |   | 2021-11-17 12:30:37 | value_list = real_mirrored_creator(**kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 522, in _real_mirrored_creator
  |   |   | 2021-11-17 12:30:37 | v = next_creator(**kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 244, in <lambda>
  |   |   | 2021-11-17 12:30:37 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2675, in default_variable_creator_v2
  |   |   | 2021-11-17 12:30:37 | shape=shape)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py"", line 272, in __call__
  |   |   | 2021-11-17 12:30:37 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1607, in __init__
  |   |   | 2021-11-17 12:30:37 | distribute_strategy=distribute_strategy)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1734, in _init_from_args
  |   |   | 2021-11-17 12:30:37 | initial_value = initial_value()
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 579, in initial_value_fn
  |   |   | 2021-11-17 12:30:37 | collective_instance_key)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py"", line 345, in broadcast_recv
  |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 59, in collective_bcast_recv
  |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py"", line 115, in collective_bcast_recv_eager_fallback
  |   |   | 2021-11-17 12:30:37 | attrs=_attrs, ctx=ctx, name=name)
  |   |   | 2021-11-17 12:30:37 | File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
  |   |   | 2021-11-17 12:30:37 | inputs, attrs, num_outputs)
  |   |   | 2021-11-17 12:30:37 | tensorflow.python.framework.errors_impl.FailedPreconditionError: [_Derived_]Collective ops is aborted by: group 1 failed to resolve. This normally means the server has restarted
  |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:0:
  |   |   | 2021-11-17 12:30:37 | :{""created"":""@1637123437.205532722"",""description"":""Error received from peer ipv4:100.91.109.177:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""group 1 failed to resolve. This normally means the server has restarted"",""grpc_status"":9}
  |   |   | 2021-11-17 12:30:37 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastRecv]
2x |   |   | 2021-11-17 12:30:37 | failed
  |   |   | 2021-11-17 12:30:52 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
  |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,563 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
  |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Waiting for the cluster, timeout = inf
  |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,574 [INFO] Waiting for the cluster, timeout = inf

```

can anyone help to analysis this behavior. 

i have checked  dns log, there not any error log , and all the logs is ok..

i have read some code, i doubt that  ""cluster is ready""  check condition is not very convincing.... ready ,but have problem....

**Describe the expected behavior**
  not happen this type error ..."
53097,Same input of Models loaded on Apple M1 and GCP COS instance have different output vectors.,"**Describe the current behavior**
When giving the same input to the same model loaded in 2 different architectures we expect the output to be ""almost"" identical. In my case I have an encoder model and when I give the same input in the 2 different systems listed below, I find the output vectors to be too different. In particular take a look at the first 6 entries of the output vector in the mentioned scenarios. In both cases inference is done on CPU

**System information 1**
- OS Platform and Distribution: 
      System Version: macOS 11.2.1 (20D75)
      Kernel Version: Darwin 20.3.0
- TensorFlow installed from: source
- TensorFlow version: v1.12.1-44683-gbcaa5ccc43e 2.4.0-rc0
- Python version: Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 15:50:57) [Clang 11.0.1 ] on darwin

output vector: `[2.94306199e-03, -1.54599566e-02,  1.47927711e-02, 4.50096987e-02, -1.71471871e-02,  7.42880329e-02, ...]`

**System information 2**
- OS Platform and Distribution: 
      Linux cff446c83c59 5.10.68+ #1 SMP Sat Nov 6 09:06:00 UTC 2021 x86_64 GNU/Linux
- TensorFlow installed from: source
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: Python 3.7.7 (default, Jun  9 2020, 17:58:51) [GCC 8.3.0] on linux

output vector: `[3.02067166e-03, -1.54666584e-02,  1.48272496e-02, 4.50056419e-02, -1.72036942e-02,  7.43456334e-02, ...]`

**Describe the expected behavior**
I expect the 2 vectors to be ""almost"" equal. The difference shown across the 2 systems is not acceptable.

"
53096,undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.3.0
- Python version:python3.6
- Bazel version (if compiling from source):bazel3.1
- GCC/Compiler version (if compiling from source):7.5
- CUDA/cuDNN version:cuda10.2、cudnn8
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53094,Skipping quantization of tensor X because it has fewer than 1024 elements,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

My model is based on keras [MobilenetV3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py).

```
def build_mode(input_shape, 
			output_bias=None, 
			l1_factor=0, 
			l2_factor=0, 
			dropout_rate=0.2,
			alpha=1.0):
	if l1_factor > 0 and l2_factor == 0:
		kernel_regularizer = regularizers.L1(l1_factor)
	elif l2_factor > 0 and l1_factor == 0:
		kernel_regularizer = regularizers.L2(l2_factor)
	elif l1_factor > 0 and l2_factor > 0:
		kernel_regularizer = regularizers.L1L2(l1_factor, l2_factor)
	else:
		kernel_regularizer = None

	inputs = layers.Input(shape=(input_shape[0], input_shape[1], 3))
	channel_axis = -1
	kernel = 3
	activation = relu
	x = inputs
	x = layers.Conv2D(16, 
					kernel_size=3, 
					strides=(2,2), 
					padding='same', 
					use_bias=False, 
					kernel_regularizer=kernel_regularizer,
					name='Conv')(x)
	x = layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999, name='Conv/BatchNorm')(x)
	x = activation(x)

	# stack
	def depth(d):
		return _depth(d * alpha)

	x = _inverted_res_block(x, 1, depth(16), 3, 2, relu, 0, kernel_regularizer)
	x = _inverted_res_block(x, 72. / 16, depth(24), 3, 2, relu, 1, kernel_regularizer)
	x = _inverted_res_block(x, 88. / 24, depth(24), 3, 1, relu, 2, kernel_regularizer)
	x = _inverted_res_block(x, 4, depth(40), kernel, 2, activation, 3, kernel_regularizer)
	x = _inverted_res_block(x, 6, depth(40), kernel, 1, activation, 4, kernel_regularizer)
	x = _inverted_res_block(x, 6, depth(40), kernel, 1, activation, 5, kernel_regularizer)
	x = _inverted_res_block(x, 6, depth(96), kernel, 2, activation, 8, kernel_regularizer)

	# final
	last_conv_ch = _depth(backend.int_shape(x)[channel_axis]*6)
	last_point_ch = 128#1024

	if alpha > 1.0:
		last_point_ch = _depth(last_point_ch * alpha)

	x = layers.Conv2D(last_conv_ch, kernel_size=1, padding='same', use_bias=False, name='Conv_1')(x)
	x = layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999, name='Conv_1/BatchNorm')(x)
	x = activation(x)
	
	x = layers.AveragePooling2D(7, 1)(x)

	x = layers.Conv2D(last_point_ch, kernel_size=1, padding='same', use_bias=True, name='Conv_2')(x)
	x = activation(x)

	if dropout_rate > 0:
	  x = layers.Dropout(dropout_rate)(x)

	x = layers.Conv2D(2, kernel_size=1, padding='same', name='Logits')(x)
	x = layers.Flatten()(x)
	prob = layers.Softmax()(x) 


	model = tf.keras.Model(inputs=inputs, outputs=prob)

	return model
```

The model is trained with quantization-aware training:

```
import tensorflow_model_optimization as tfmot
import tensorflow as tf

model = build_model()
annotated_model = tf.keras.models.clone_model(model, clone_function=custom_quantization)

with tfmot.quantization.keras.quantize_scope():
	model = tfmot.quantization.keras.quantize_apply(annotated_model)

# standard training pipeline
# ...

```
Finally, the model is quantized:
```
with tfmot.quantization.keras.quantize_scope():
    model = tf.keras.models.load_model(model_path)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(quantized_tflite_model)
```



### 3. Failure after conversion
Some tensors are not quantized. Other custom models that I had trained before are properly quantized.


### 5. (optional) Any other info / logs
**2021-11-17 10:23:37.425715: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/expanded_conv/depthwise/BatchNorm/FusedBatchNormV3;model/expanded_conv/depthwise/depthwise;model/expanded_conv_5/project/Conv2D because it has fewer than 1024 elements (288).
2021-11-17 10:23:37.425738: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/expanded_conv/project/Conv2D because it has fewer than 1024 elements (512).
2021-11-17 10:23:37.425744: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/expanded_conv_1/depthwise/BatchNorm/FusedBatchNormV3;model/expanded_conv_1/depthwise/depthwise;model/expanded_conv_8/project/Conv2D because it has fewer than 1024 elements (648).
2021-11-17 10:23:37.425749: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/expanded_conv_2/depthwise/BatchNorm/FusedBatchNormV3;model/expanded_conv_2/depthwise/depthwise because it has fewer than 1024 elements (792).
2021-11-17 10:23:37.425753: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/expanded_conv_3/depthwise/BatchNorm/FusedBatchNormV3;model/expanded_conv_3/depthwise/depthwise because it has fewer than 1024 elements (864).
2021-11-17 10:23:37.425760: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor model/Logits/Conv2D because it has fewer than 1024 elements (256).**

"
53093,tensorflow-macos cannot train model,"**System information**

- OS Platform and Distribution):macOS 12.1
- TensorFlow installed from (source or binary):tensorflow-macos pip
- TensorFlow version (use command below):2.6
- Python version:3.8
- Bazel version (if compiling from source):
- GPU model and memory: AMD RX6600 8G


**Unable to train model**
```
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
w, h = 28, 28

x_train = x_train.reshape(x_train.shape[0], w, h, 1)
x_test = x_test.reshape(x_test.shape[0], w, h, 1)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```

**Run Information**
```
2021-11-17 16:07:45.954518: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-17 16:07:45.955017: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-11-17 16:07:45.955191: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
Metal device set to: AMD Radeon RX 6600

systemMemory: 16.00 GB
maxCacheSize: 3.99 GB

2021-11-17 16:07:46.368598: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/5
2021-11-17 16:07:46.584915: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-11-17 16:07:46.623 python[4372:114284] -[MPSGraph adamUpdateWithLearningRateTensor:beta1Tensor:beta2Tensor:epsilonTensor:beta1PowerTensor:beta2PowerTensor:valuesTensor:momentumTensor:velocityTensor:gradientTensor:name:]: unrecognized selector sent to instance 0x600007d83720

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```

"
53091,I got problem when I create Interpreter using TF lite,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10+
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.7
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hello, 
I'm trying to make an app using GPU while training with below example code.
https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization

please refer the code above,
with GPUdelegate(https://www.tensorflow.org/lite/performance/gpu_advanced),
I added ""implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'"" in build.gradle of trasfer_api
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'
    // This dependency adds the necessary TF op support.
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'

plus I changed LiteMultipleSignatureModel as
  LiteMultipleSignatureModel(ByteBuffer tfLiteModel, int numClasses) {
    // Initialize interpreter with GPU delegate
    Interpreter.Options options = new Interpreter.Options();
    CompatibilityList compatList = new CompatibilityList();

    if(compatList.isDelegateSupportedOnThisDevice()){
      // if the device has a supported GPU, add the GPU delegate
      GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
      GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
      options.addDelegate(gpuDelegate);
    } else {
      // if the GPU is not supported, run on 4 threads
      options.setNumThreads(4);
    }

    this.interpreter = new Interpreter(tfLiteModel, options);
    this.numClasses = numClasses;
  }

I tried to start the app and I found this device support GPU delegation but it failed with fatal exception as
2021-06-11 09:20:23.894 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Created TensorFlow Lite delegate for GPU.
2021-06-11 09:20:23.899 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Initialized TensorFlow Lite runtime.
2021-06-11 09:20:23.969 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Created TensorFlow Lite delegate for select TF ops.
2021-06-11 09:20:23.972 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: TfLiteFlexDelegate delegate: 0 nodes delegated out of 79 nodes with 0 partitions.
2021-06-11 09:20:23.972 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 2 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 2 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 1 node(s) with delegate (TfLiteFlexDelegate) node, yielding 2 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 4 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.991 8460-8460/org.tensorflow.lite.examples.transfer D/AndroidRuntime: Shutting down VM
2021-06-11 09:20:23.894 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Created TensorFlow Lite delegate for GPU.
2021-06-11 09:20:23.899 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Initialized TensorFlow Lite runtime.
2021-06-11 09:20:23.969 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Created TensorFlow Lite delegate for select TF ops.
2021-06-11 09:20:23.972 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: TfLiteFlexDelegate delegate: 0 nodes delegated out of 79 nodes with 0 partitions.
2021-06-11 09:20:23.972 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 2 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 2 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 1 node(s) with delegate (TfLiteFlexDelegate) node, yielding 2 partitions.
2021-06-11 09:20:23.990 8460-8460/org.tensorflow.lite.examples.transfer I/tflite: Replacing 4 node(s) with delegate (TfLiteFlexDelegate) node, yielding 3 partitions.
2021-06-11 09:20:23.991 8460-8460/org.tensorflow.lite.examples.transfer D/AndroidRuntime: Shutting down VM
2021-06-11 09:20:23.992 8460-8460/org.tensorflow.lite.examples.transfer E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.tensorflow.lite.examples.transfer, PID: 8460
    java.lang.IllegalArgumentException: Internal error: Error applying delegate: 
        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:93)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)
        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:44)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:226)
        at org.tensorflow.lite.examples.transfer.api.LiteMultipleSignatureModel.<init>(LiteMultipleSignatureModel.java:63)
        at org.tensorflow.lite.examples.transfer.api.TransferLearningModel.<init>(TransferLearningModel.java:114)
        at org.tensorflow.lite.examples.transfer.TransferLearningModelWrapper.<init>(TransferLearningModelWrapper.java:46)
        at org.tensorflow.lite.examples.transfer.CameraFragment.onCreate(CameraFragment.java:332)

at the first, this app started to create gpu delegate but it worked as TfLiteFlexDelegate. I guess this is not proper.

I also tried tfLiteModel to set nitivOrder like tfLiteModel.order(ByteOrder.nativeOrder() but got same result.

please help me out to find the way to resolve this problem :)

Thank you


**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53090,Variables don't sync properly between 2 GPUs when SLI is enabled on Windows,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows-10-10.0.19041-SP0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  11.2 / 8.1.0
- GPU model and memory: 2 x NVIDIA GeForce GTX 980 with SLI ON. 4GB memory

**Describe the current behavior**
When SLI is on variables don't sync properly between the 2 GPUs. I first noticed by getting `nan` loss values, but this simple code fails:
```
v = tf.Variable(1.0)
with tf.device('/gpu:1'):
    print(v)
```
Output:
```
2021-11-17 03:03:58.146222: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-17 03:03:58.788548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2691 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2
2021-11-17 03:03:58.791426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2691 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2
<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>
```

Note that turning SLI off makes the issue disappear, but also the model runs slower.

**Describe the expected behavior**
Variables should have the same value regardless of the device.
In the example above, with SLI disabled, the variable has the correct value (1):
```
2021-11-17 03:16:47.448047: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-17 03:16:48.068154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2777 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2
2021-11-17 03:16:48.069778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2777 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 980, pci bus id: 0000:02:00.0, compute capability: 5.2
<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>
```

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf
v = tf.Variable(1.0)
with tf.device('/gpu:1'):
    print(v)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53089,Query regarding contributing,"i have reformatted all files in python directory of tensorflow with black and this is it's link:-https://github.com/Naman23-coder/tensorflow/tree/Naman23-coder ,should i open a pull request?"
53088,tf.distribute.MirroredStrategy makes wrong outputs with tf.keras.layers.Dense,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Linux Ubuntu 18.04
- TensorFlow installed from binary:
- TensorFlow version: 2.6.2
- Python version: 3.8
- CUDA/cuDNN version: cuda 11.3,  8.2.1
- GPU model and memory:  RTX 5000, 2 EA, 16gb

when I'm using mirroredstrategy, the model makes wrong outputs.

![image](https://user-images.githubusercontent.com/9550691/142134915-29371557-c661-4856-9f35-fdb70976ca36.png)

this is logs and the code is like

![image](https://user-images.githubusercontent.com/9550691/142134955-8adc2634-9f9e-483b-b151-065643611097.png)

it's part of music transformer.

the logits changes when I using 2 gpus.
I don't no why it make wrong output shape.

there is no problem with 1 gpu.

I don't no why it works like this."
53087,"tflite with C/C++ on Android, iOS, Linux, MacOS: Is it possible? How to do it?","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): n/a
- TensorFlow version: latest
- Python version: n/a
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**

Hi thanks for the wonderful tf and tflite! I need to use it in Android and iOS apps, and also test my code on MacOS and Linux. Therefore, I need the C++ library of tflite on all these four platforms. I wonder what should I do to compile it?

p.s. I have searched and found related issues, but they seem to be outdated, so I create a new one. I have also read all documents, but get confused since most of them creates libs for java/swift/objc/c/..., instead of C++. I have even seen [doc saying](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c) C++ on ios must use bazel - but I do not know what part should use bazel - is it saying *all my code* must use bazel, or simply compile with bazel?

It is best if I can do it with C++. If not possible, I can accept C.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
n/a

**Any other info / logs**
n/a"
53086,assets/vocab.txt; No such file or directory when loading/saving BERT models from TFhub,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 21.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
Using models https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4 and https://tfhub.dev/google/experts/bert/wiki_books/squad2/2, `tf.saved_model.save` fails with error below.

```
Traceback (most recent call last):
  File ""/home/tensorflow/test_load_save.py"", line 30, in <module>
    tf.saved_model.save(model, model_path)
  File ""/home/tensorflow/.venv_tf27/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py"", line 1280, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""/home/tensorflow/.venv_tf27/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py"", line 1327, in save_and_return_nodes
    builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,
  File ""/home/tensorflow/.venv_tf27/lib/python3.9/site-packages/tensorflow/python/saved_model/builder_impl.py"", line 781, in copy_assets_to_destination_dir
    file_io.copy(asset_source_filepath, asset_destination_filepath)
  File ""/home/tensorflow/.venv_tf27/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py"", line 589, in copy
    copy_v2(oldpath, newpath, overwrite)
  File ""/home/tensorflow/.venv_tf27/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py"", line 583, in copy_v2
    _pywrap_file_io.CopyFile(
tensorflow.python.framework.errors_impl.NotFoundError: /tmp/tmps54xx3u2/assets/vocab.txt; No such file or directory
```


**Describe the expected behavior**
No errors

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
model_name = ""./models/bert_en_uncased_L-12_H-768_A-12_4.tar.gz""

if tarfile.is_tarfile(model_name):
    with tempfile.TemporaryDirectory() as td:
        with tarfile.open(model_name, ""r"") as tar:
            tar.extractall(path=td)
        model = tf.saved_model.load(td)

with tempfile.TemporaryDirectory() as td:
    model_path = os.path.join(td, ""saved_model"")
    tf.saved_model.save(model, model_path)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53083,TensorFlow Lite CMake build is broken (label_image & benchmark_model) on TF 2.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.7.0
- Python version: N/A
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): N/A
- CMake version: 3.19.5
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
The TensorFlow Lite build using CMake is broken in v2.7.0 for generic Arm device (aarch64). 
The problem is caused by this commit https://github.com/tensorflow/tensorflow/commit/feb49693266f444d5d8ce1a439ffbe7ff6e15e8a . First problem is only the bazel build scripts were updated (the CMake does not support the Hexagon delegate yet at all). Secondly the implementation expects the Qualcomm's Hexagon DSP is available on every aarch64 platform, what is not generally true.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Configure the project via CMake: 
```
cmake -DCMAKE_TOOLCHAIN_FILE=${OE_CMAKE_TOOLCHAIN_FILE} ../tensorflow/lite/
```
2. Build the label_image or benchmark_model application: 
```
make -j6 label_image
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
[100%] Linking CXX executable label_image
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `tflite::evaluation::CreateHexagonDelegate(TfLiteHexagonDelegateOptions const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::{lambda(TfLiteDelegate*)#1}::_FUN(TfLiteDelegate*)':
/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:165: undefined reference to `TfLiteHexagonDelegateDelete'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `operator()':
/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:166: undefined reference to `TfLiteHexagonTearDown'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: CMakeFiles/label_image.dir/__/__/tools/evaluation/utils.cc.o: in function `tflite::evaluation::CreateHexagonDelegate(TfLiteHexagonDelegateOptions const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':
/home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:154: undefined reference to `TfLiteHexagonInit'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:156: undefined reference to `TfLiteHexagonInitWithPath'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:161: undefined reference to `TfLiteHexagonTearDown'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:154: undefined reference to `TfLiteHexagonInit'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:156: undefined reference to `TfLiteHexagonInitWithPath'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:159: undefined reference to `TfLiteHexagonDelegateCreate'
/opt/sdk/sysroots/x86_64-pokysdk-linux/usr/libexec/aarch64-poky-linux/gcc/aarch64-poky-linux/10.2.0/real-ld: /home/robo/tensorflow/tensorflow/lite/tools/evaluation/utils.cc:161: undefined reference to `TfLiteHexagonTearDown'
collect2: error: ld returned 1 exit status
examples/label_image/CMakeFiles/label_image.dir/build.make:331: recipe for target 'examples/label_image/label_image' failed
make[3]: *** [examples/label_image/label_image] Error 1
CMakeFiles/Makefile2:4330: recipe for target 'examples/label_image/CMakeFiles/label_image.dir/all' failed
make[2]: *** [examples/label_image/CMakeFiles/label_image.dir/all] Error 2
CMakeFiles/Makefile2:4337: recipe for target 'examples/label_image/CMakeFiles/label_image.dir/rule' failed
make[1]: *** [examples/label_image/CMakeFiles/label_image.dir/rule] Error 2
Makefile:1414: recipe for target 'label_image' failed
make: *** [label_image] Error 2
```
"
53081,tensorflow-gpu-v2.3.0  C++ can not infer on Tesla P4,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.3.0
- Python version: python3.6
- Bazel version (if compiling from source): bazel-3.1.0
- GCC/Compiler version (if compiling from source): gcc version 7.5.0
- CUDA/cuDNN version: CUDA-10.2，cuDNN8
- 
ERROR:
2021-11-16 13:46:36.936240: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
2021-11-16 13:46:36,936 ERROR [EasyEdge] [tf_edge_predictor.cpp:74] 140512694415424 NewSession failed...Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

"
53080,Get an index of each worker in `ParameterServerStrategy `,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
In the current implementation of ParameterServerStrategy, the `ClusterCoordinator.schedule method` assumes workers are equivalent and thus assumes the datasets on different workers are the same except they may be shuffled differently if they contain a `Dataset.shuffle` operation.

But i want to shard the dataset across the worker.
Let's assume that i want to make a dataset from generator function using `tf.data.Dataset.from_generator`. If there's a way to get the index of the environment where the function is running, it's possible for the generator to emit a different result by the task index. 
For now, i couldn't find a way to do it. so I filed this issue. (if there's a way, please let me know)

example code
```
NUM_WORKERS = 10
def example_generator(raw_dataset): 
 worker_idx = tf.??? # the feature i request. [0-9] should be returned as the num_worker = 10
 for i,d in enumerate(examples):
    if (i % NUM_WORKERS) == worker_idx:
      yield d
```

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
53079,tflite::ops::builtin::BuiltinOpResolver resolver is causing crash in a webassembly application opened in Android chrome browser.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): Tensorflow lite (compiled for web assembly)
- TensorFlow version (or GitHub SHA if from source): (2.5.0)

**Standalone code to reproduce the issue** 
```
  void init(const char *api_key)
    {

        cout << ""initilizing the init function""<< endl;
        const char *env = getenv(""ENVIRONMENT"");
        ENV = std::string(env);
        sdk_status = statuscode->running;
        
        cout << ""initilizing step 2""<< endl;

    
        sdk_status = statuscode->success;

        auto landmark_predict_model_ = tflite::FlatBufferModel::BuildFromFile(""data/face_detection_front_128x128_float32.tflite"");
        if(!landmark_predict_model_){
            cout <<  ""Failed to load model "" << endl;
        }else{
                cout <<  ""Model loaded "" << endl;
        }
        cout <<  ""start "" << endl;
        // tflite::MutableOpResolver resolver;
        // RegisterSelectedOps(&resolver);
        tflite::ops::builtin::BuiltinOpResolver resolver;    //this is causing crash. 
        // std::unique_ptr<tflite::Interpreter> landmark_interpreter_;
        // tflite::InterpreterBuilder(*landmark_predict_model_, resolver)(&landmark_interpreter_);
        // landmark_interpreter_->AllocateTensors();
        // landmark_interpreter_->SetAllowFp16PrecisionForFp32(true);
        // landmark_interpreter_->SetNumThreads(4);

        cout << ""initilizing step 3""<< endl;

    }
```

Also, please include a link to a GraphDef or the model if possible.
model file: tflite model from [here](https://drive.google.com/uc?export=download&id=1DOEH2xS_bGVXEsriNJkKE3my62ZH3FUl)

**Any other info / logs**
The crash is happening only in chrome running in mobile phones (tested on Android phone, Processor: snapdragon 710 and above). It is running properly in PC chrome browser. 

"
53078,how to control thread number in Tensorflow1.12?,"I am using [https://github.com/mlcommons/training](mlcommon/training) to train resnet50.
On my device, it is thread limitation.
So I want to control the thread number while trainging.
There are too many threads, As shown in the figure below.
![image](https://user-images.githubusercontent.com/17714376/141889577-41a39540-e6b2-413f-a7d9-2c7125a5c4c0.png)
"
53077,macOS12 tensorflow-metal error,"**System information**

- OS Platform and Distribution : macOS 12.1 intel
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):tensorflow-metal 2.6
- Python version:3.8
- GPU model and memory: AMD Radeon RX 6600

Errors will be reported when training the model
```
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.
```
For example, the following program
```
import tensorflow as tf

print(tf.config.list_physical_devices())

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
print(model.summary())

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2021-11-16 10:59:28.127916: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers
Traceback (most recent call last):
  File ""/Volumes/Workspace/PythonProject/centernet2-tf2/main.py"", line 5, in <module>
    model = tf.keras.models.Sequential([
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/__init__.py"", line 25, in <module>
    from keras import models
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/metrics.py"", line 26, in <module>
    from keras import activations
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/activations.py"", line 20, in <module>
    from keras.layers import advanced_activations
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/layers/__init__.py"", line 23, in <module>
    from keras.engine.input_layer import Input
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/input_layer.py"", line 21, in <module>
    from keras.engine import base_layer
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/engine/base_layer.py"", line 43, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 18, in <module>
    from keras import optimizers
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizers.py"", line 26, in <module>
    from keras.optimizer_v2 import adadelta as adadelta_v2
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/adadelta.py"", line 22, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 36, in <module>
    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py"", line 360, in __init__
    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
  File ""/Users/yurisu/opt/miniconda3/envs/tf-metal-2.6/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py"", line 135, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.
```

These are not the issues with the CPU version. It's OK to do simple things with the GPU, but these errors will occur as soon as you train
```
import tensorflow as tf

print(tf.config.list_physical_devices())

with tf.device('/GPU'):
    a = tf.random.normal(shape=(2,), dtype=tf.float32)
    b = tf.nn.relu(a)
    print(a)

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Metal device set to: AMD Radeon RX 6600

systemMemory: 16.00 GB
maxCacheSize: 3.99 GB

2021-11-16 10:53:19.473055: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-16 10:53:19.473518: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-11-16 10:53:19.473738: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
tf.Tensor([-1.0673397 -1.3157675], shape=(2,), dtype=float32)

```

   


"
53076,Worse performance after converting tf.compat.v1.nn.ctc_loss to tf.nn.ctc_loss,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 1..2
- GPU model and memory: GeForce RTX 3070 8GB

**Describe the current behavior**
I am trying to convert parts of my code from using tf.compat.v1 to pure tf functions. Here is the previous working function, that gives me good performance.

`self.loss = tf.reduce_mean(
            input_tensor=tf.compat.v1.nn.ctc_loss(
                labels=self.gt_texts,
                inputs=self.ctc_in_3d_tbc,
                sequence_length=self.seq_len,
                ctc_merge_repeated=True,
            )
        )
`

This is my attempt to rewrite the function using tf.nn.ctc_loss.

`self.loss = tf.reduce_mean(
            input_tensor=tf.nn.ctc_loss(
                labels=self.gt_texts,  # sparse tensor
                logits=self.ctc_in_3d_tbc,  
                label_length=None,  
                logit_length=self.seq_len,  
                blank_index=-1,
            ))
`
Please let me know if attaching logfiles will be helpful. I figure I must simply be configuring some of the arguments wrong. "
53075,"I want to convert tFLite model into PB save_model, can you help me?  ","I want to convert tFLite model into PB save_model, can you help me?  

After I transform the TFlite model of four output tensors into RKNN model, the data structure and result are completely different. Do you know why?  "
53074,A Node name contains invalid characters error occurs when the PB model is converted to tFLite  ,"During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_change.py"", line 109, in <module>
    test()
  File ""model_change.py"", line 79, in test
    tf.import_graph_def(output_graph_def, name='')
  File ""/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/home/ubuntu/anaconda3/envs/rknn/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py"", line 505, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Node 'model/model/tf.nn.relu6/Relu6;model/model/batch_normalization/FusedBatchNormV3;model/model/batch_normalization_1/FusedBatchNormV3;model/model/depthwise_conv2d/depthwise;model/model/conv2d_9/Conv2D;model/model/conv2d/Conv2D_unfused/conv': Node name contains invalid characters
"
53073,AttributeError: module 'tensorflow' has no attribute 'variable_scope',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):2.7.0
- TensorFlow version (use command below):
- Python version:Python 3.8.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):GCC 10.2.0
- CUDA/cuDNN version:nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Oct_12_20:09:46_PDT_2020
Cuda compilation tools, release 11.1, V11.1.105
Build cuda_11.1.TC455_06.29190527_0
- GPU model and memory:
gpu-2-0.saber
     properties = batch_gpu,gpu-g3,dell,saber,x86_64,nvidia
     status = opsys=linux,uname=Linux gpu-2-0.saber 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64,sessions=274505 349523 380034 397050,nsessions=4,nusers=2,idletime=8805,totmem=101384352kb,availmem=99296204kb,physmem=97190052kb,ncpus=16,loadave=0.00,gres=,netload=40652996470,state=free,varattr= ,cpuclock=Fixed,version=6.1.3,rectime=1637006909,jobs=
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I run my script:

python train.py
Traceback (most recent call last):
  File ""train.py"", line 109, in <module>
    nn = NeuralNetwork(F=args.num_features,           
  File ""/mnt/lustre/anamaria/AI/tensorflow/PhysNet/PhysNet-master/neural_network/NeuralNetwork.py"", line 53, in __init__
    with tf.variable_scope(self.scope):
AttributeError: module 'tensorflow' has no attribute 'variable_scope'


**Describe the expected behavior**
Following previous issues I tried replacing in my code:
this line:
 tf.global_variables_initializer().run()
with this one:
 tf.compat.v1.global_variables_initializer().run()

the error stated above remains the same in both instances.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53072,Tensorflow environment variables breaking python import,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Rocky Linux 8.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Both source and Pip
- TensorFlow version: 2.7.0, 2.4.1
- Python version: 3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 11.4/ cudnn 8.2
- GPU model and memory: Nvidia A100 40 GB

What environment variables does TF look for when being imported in Python? I have 2 installations on my system: 1 is compiled from source (2.4.1) and stored in a user directory. This installation is only for linking to the C++ libraries. The second is installed with pip for use in Python. 

I discovered that by setting the environment variable `TF_LIB_PATH`, the Python installation fails to import Tensorflow with this error:
```
Traceback (most recent call last):
  File ""/home/username/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/username/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb
```

I use various environment variables to set the paths to my source compiled C++ libraries.

But, by not setting the environment variable `TF_LIB_PATH`, I am able to import Tensorflow. Is this expected behavior?
"
53071,Understanding TFLite interpreter output tensor names and ordering,"### 1. System information

OS: Ubuntu 20.04 LTS
TF v2.4.1, built from source

### 2. Code

Can supply code later if necessary. Mostly just wanted ask a quick question to see if this is a known issue with a known solution.

### 3. Failure after conversion

It seems that TFLite model outputs are generically named and not ordered in a predictable way. If my original TF model has outputs like `[""my_output_1"", ""my_output_2""]`, the TFLite model's outputs (e.g. obtained from `interpreter.outputs()`, `interpreter.output_tensor(idx)` will have names like `PartitionedCall:1` and `PartitionedCall:0`. These two outputs will correspond to the two outputs of the TF model, but their order may be permuted.

After searching the API as well as looking googling around a good amount, I was unable to find any information about
* How to tell, other than manually checking shapes and verified expected output values, which output is which.
* If there is a way to induce TFLite to keep the original output tensor names, or at least name the output tensors in a fashion that deterministically links them to their TF counterparts.


"
53070,<spam removed>,<spam removed>
53069,full 8 bit quantization generates unreasonable quant_scale for uint8 on input tensor,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0

### 2. Code

```
def representative_dataset():
    for img in imgs: # img are shape (224,640,3)
        data = np.expand_dims(img, axis=0)
        yield [data.astype(np.float32)]
        
converter = tf.lite.TFLiteConverter.from_keras_model(model) # path to keras model
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8 
#converter.experimental_new_quantizer = False

tflite_quant_model = converter.convert()
open(f""converted_model.tflite"", ""wb"").write(tflite_quant_model)
```
Keras Model: https://cloud.haw-hamburg.de/index.php/s/FPxeczAoaJJgMly (Password: tf)
Tensorflow Lite converted Model: https://cloud.haw-hamburg.de/index.php/s/m9RKnJdOtHg8K3I (Password: tf)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

The conversion succeeds but the quant_scale provided by the 'input_details[""quantization""]' on the input tensor seems wrong. It is set to 0.029... which would map my float32 [0.0-1.0] images to [0-34] (zero_point = 0). That are a lot of details gone on the input, which makes the model perform bad.

When I set the 'experimental_new_quantizer' to 'False', it geneartes 0.0039... for quant_scale and still 0 for zero_point. That would map my images to [0-255], but the invoke fails with an error, that the input tensor lacks data. 

To my knowledge, the quant_scale is calculated by (max-min) / quant_range. quant_range is 255 for uint8. Where does the min max come from? from the representative dataset? I have checked that my representative dataset does not go below 0.0 or above 1.0.
"
53067,Unit test //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on AARCH64,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Test fails with
==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters
[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test
tensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:85: Failure
Value of: _status_or_value7.status().ok()
  Actual: false
Expected: true
INTERNAL: TargetRegistry::lookupTarget failed: No available targets are compatible with triple ""i686-none-android""
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (13 ms)
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (13 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (13 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test

 1 FAILED TEST
================================================================================


**Describe the expected behavior**

Test passes

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing): Add tag to allow test to be excluded on AARCH64 builds

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The test has a hard coded link to an x86 platform triple. But more than that, the test simply does not make sense on AARCH64 as it wants to test the behaviour if there were no vector registers available, but that is never going to be the case on AARCH64. So rather than attempting to fix the test, which would probably bypass any content on AARCH64, just add a tag to the definition to allow it to be excluded.
"
53066,On-Device Training with TensorFlow Lite cannot train on GPU.,"**1.System information**
**1) using colab**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab w/GPU
- TensorFlow version (use command below): 2.7

**2) using local machine**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow version (use command below): 2.7
- Python version:3.8.8
- CUDA/cuDNN version: 11.2 / 8.1
- GPU model and memory: RTX3090

**2. Code**
I use On-Device Training with TensorFlow Lite example.
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb

I used to download a overview.py when I use my local machine.

**3. Current behavior**
When running on Colab with GPU backend, unexpected error occurs when executing the following Cell.
```
restore(checkpoint_path=np.array(""/tmp/model.ckpt"", dtype=np.string_))
```
```
NUM_EPOCHS = 10
BATCH_SIZE = 100
epochs = np.arange(1, NUM_EPOCHS + 1, 1)
losses = np.zeros([NUM_EPOCHS])
m = Model()

for i in range(NUM_EPOCHS):
  for batch_idx in range(len(train_images) // BATCH_SIZE):
    batched_images = train_images[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]
    batched_labels = train_labels[BATCH_SIZE*(batch_idx) : BATCH_SIZE * (batch_idx + 1)]
    result = train(
        x=tf.constant(batched_images, shape=(BATCH_SIZE, IMG_SIZE, IMG_SIZE),
                      dtype=tf.float32),
        y=tf.constant(batched_labels, shape=(BATCH_SIZE, 10), dtype=tf.float32))
  losses[i] = result['loss']
  print('Finished {0} epochs, current loss: {1}'.format(i + 1, losses[i]))

plt.plot(epochs, losses)
plt.show()
```
When running on a local machine with a GPU, Segmentation fault (Core dumped) occurs in the same place as Colab.
```
$ python overview.py 
TensorFlow version: 2.7.0
2021-11-15 22:07:23.390165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.394521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.394982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.395740: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-15 22:07:23.396617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.397092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.397548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.701187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.701686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.702127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:07:23.702560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
2021-11-15 22:07:24.482198: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Finished 10 epochs, current loss: 5.802193641662598
Finished 20 epochs, current loss: 5.773608207702637
Finished 30 epochs, current loss: 5.7503509521484375
Finished 40 epochs, current loss: 5.731655120849609
Finished 50 epochs, current loss: 5.665677070617676
Finished 60 epochs, current loss: 5.549953460693359
Finished 70 epochs, current loss: 3.9862852096557617
Finished 80 epochs, current loss: 3.8360049724578857
Finished 90 epochs, current loss: 3.7696609497070312
Finished 100 epochs, current loss: 3.7227823734283447
2021-11-15 22:09:14.922369: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Importing a function (__inference_internal_grad_fn_421422) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_internal_grad_fn_421450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
2021-11-15 22:09:15.342590: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.
2021-11-15 22:09:15.342614: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.
2021-11-15 22:09:15.342618: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.
2021-11-15 22:09:15.343225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model
2021-11-15 22:09:15.344752: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }
2021-11-15 22:09:15.344766: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model
2021-11-15 22:09:15.354429: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.
2021-11-15 22:09:15.376079: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model
2021-11-15 22:09:15.393471: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 50247 microseconds.
2021-11-15 22:09:15.416095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-11-15 22:09:15.478737: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexBroadcastGradientArgs, FlexReluGrad, FlexRestore, FlexSave
Details:
        tf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = """"}
        tf.ReluGrad(tensor<?x128xf32>, tensor<?x128xf32>) -> (tensor<?x128xf32>) : {device = """"}
        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<10xf32>) : {device = """", preferred_shard = -1 : i64}
        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128x10xf32>) : {device = """", preferred_shard = -1 : i64}
        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<128xf32>) : {device = """", preferred_shard = -1 : i64}
        tf.Restore(tensor<!tf_type.string>, tensor<!tf_type.string>) -> (tensor<784x128xf32>) : {device = """", preferred_shard = -1 : i64}
        tf.Save(tensor<!tf_type.string>, tensor<4x!tf_type.string>, tensor<784x128xf32>, tensor<128xf32>, tensor<128x10xf32>, tensor<10xf32>) -> () : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
INFO: Created TensorFlow Lite delegate for select TF ops.
2021-11-15 22:09:15.503954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:09:15.504198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:09:15.504370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:09:15.504576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:09:15.504741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-15 22:09:15.504886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21492 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.

Segmentation fault (core dumped)
```
**4. Question**
Is it currently possible to do On-Device Training using the GPU?"
53064,TF2 does not load optimizer weights when restoring a saved model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, very little
- OS Platform and Distribution: Ubuntu 20.04 + Colab
- TensorFlow installed from: binary
- TensorFlow version: 2.7.0 (also happens on 2.6 and 2.5)
- Python version: 3.8.10

**Describe the current behavior**
When loading a saved (Keras) model, the optimizer weights are not restored. No error or warning is printed to make the user aware of this. [The documentation](https://www.tensorflow.org/guide/keras/save_and_serialize) also does not mention this. 

**Describe the expected behavior**
The optimizer weights should be restored automatically when loaded a saved model that was saved with its optimizer.

**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
- [Notebook that shows the issue in TF2](https://colab.research.google.com/drive/1WyRoHFnNvoFocqm7jKC1CiH0KzzKh2lk?usp=sharing)
- [Notebook that shows that TF1 does not have the same issue](https://colab.research.google.com/drive/1HDcX9wauIuArJpBlTmfDRlk0Yhr3ngQT?usp=sharing)

**Other info / logs** 
This is a TF2 bug, it does not occur when using TF1 compat mode. It is a very significant bug - it means that models cannot be trained correctly if we want the training to be able to stop and resume. Workarounds such as setting the optimizer weights manually are buggy too - see https://github.com/keras-team/keras/issues/15298.

Issue https://github.com/tensorflow/tensorflow/issues/52346 is related, but not the same - that one is about using checkpoints and getting errors, while here I am using the SavedModel format and not getting any errors (where there should be one).
"
53062,"r2.7 not recognizing old GPU with compute capability 3.0 (GTX 770) at runtime, while everything seems fine during build and installation.","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: 
    No, the only lines I'm trying are 
    ```
    import tensorflow as tf
    print(""TensorFlow version:"", tf.__version__)
    
    tf.config.list_physical_devices('GPU')
    print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
    ```
-   **OS Platform and Distribution**:
    Linux Mint 20.02
-   **TensorFlow installed from**:
    Source
-   **TensorFlow version**:
    r2.7 (last official release branch up to date)
-   **Python version**:
    3.8
-   **Bazel version**:
    3.7.2
-   **GCC/Compiler version**:
    GCC 9.3.0
-   **CUDA/cuDNN version**:
    10.2 / 8.2.4.15 (all samples for both work without a flaw, except for half precision samples obviously failing, cc being 3.0). NVidia Driver Version is 440.33.01
-   **GPU model and memory**:
    Geforce GTX 770 2GB

### Describe the problem
As stated in documentation, after installing Bazel (3.7.2) I ran the `.configure` with the cuda option enabled. It went like this:
```
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is /home/alessandro/tensorflow/bin/python3]: 

Found possible Python library paths:
  /home/alessandro/tensorflow/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/alessandro/tensorflow/lib/python3.8/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.2 in:
    /usr/local/cuda-10.2/targets/x86_64-linux/lib
    /usr/local/cuda-10.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-10.2/targets/x86_64-linux/lib
    /usr/local/cuda-10.2/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.0

WARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.
Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
```

After this, having found a **very** similar [issue](https://github.com/tensorflow/tensorflow/issues/27840), I always add to `.tf_configure`: `build:opt --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0` (even if it seems present as a build `--action_env` option), and the two lines to exclude XLA: `build --define=with_xla_support=false` and `build --action_env TF_ENABLE_XLA=0`.

My `.tf_configure` looks like this:
```
build --action_env PYTHON_BIN_PATH=""/home/alessandro/tensorflow/bin/python3""
build --action_env PYTHON_LIB_PATH=""/home/alessandro/tensorflow/lib/python3.8/site-packages""
build --python_path=""/home/alessandro/tensorflow/bin/python3""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/x86_64-linux-gnu-gcc-9""
build --config=cuda

build --define=with_xla_support=false
build --action_env TF_ENABLE_XLA=0
build:opt --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0

build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-Wno-sign-compare
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
```

Then, I simply run `bazel build //tensorflow/tools/pip_package:build_pip_package` and this goes on without any error or even warnings.

I finally  ran
```./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg```

And even if with warnings, wheel gets built.
```
Mon 15 Nov 2021 11:01:36 AM CET : === Preparing sources in dir: /tmp/tmp.AZIEudvT8k
~/Downloads/tensorflow ~/Downloads/tensorflow
~/Downloads/tensorflow
~/Downloads/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/Downloads/tensorflow
~/Downloads/tensorflow
/tmp/tmp.AZIEudvT8k/tensorflow/include ~/Downloads/tensorflow
~/Downloads/tensorflow
Mon 15 Nov 2021 11:01:52 AM CET : === Building wheel
warning: no files found matching 'README'
warning: no files found matching '*.pyd' under directory '*'
warning: no files found matching '*.pyi' under directory '*'
warning: no files found matching '*.pd' under directory '*'
warning: no files found matching '*.dylib' under directory '*'
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
warning: no files found matching '*.csv' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*' under directory 'tensorflow/include/third_party'
/home/alessandro/tensorflow/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
  warnings.warn(
Mon 15 Nov 2021 11:02:19 AM CET : === Output wheel file is in: /tmp/tensorflow_pkg
```

and `pip install /tmp/tensorflow_pkg/tensorflow-2.7.0-cp38-cp38-linux_x86_64.whl` goes without a flaw:
```
Processing /tmp/tensorflow_pkg/tensorflow-2.7.0-cp38-cp38-linux_x86_64.whl
Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.0)
Requirement already satisfied: libclang>=9.0.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (12.0.0)
Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.41.1)
Requirement already satisfied: typing-extensions>=3.6.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.10.0.2)
Requirement already satisfied: h5py>=2.9.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.5.0)
Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.1.2)
Requirement already satisfied: termcolor>=1.1.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.1.0)
Requirement already satisfied: wrapt>=1.11.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.13.3)
Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)
Requirement already satisfied: google-pasta>=0.1.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.2.0)
Requirement already satisfied: protobuf>=3.9.2 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.19.1)
Requirement already satisfied: tensorboard~=2.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (2.7.0)
Requirement already satisfied: absl-py>=0.4.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.0.0)
Requirement already satisfied: opt-einsum>=2.3.2 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (3.3.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.22.0)
Requirement already satisfied: numpy>=1.14.5 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.21.4)
Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.4.0)
Requirement already satisfied: astunparse>=1.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.6.3)
Requirement already satisfied: six>=1.12.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (1.16.0)
Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorflow==2.7.0) (0.37.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)
Requirement already satisfied: google-auth<3,>=1.6.3 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.3.3)
Requirement already satisfied: setuptools>=41.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (58.3.0)
Requirement already satisfied: werkzeug>=0.11.15 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.0.2)
Requirement already satisfied: requests<3,>=2.21.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.26.0)
Requirement already satisfied: markdown>=2.6.8 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.4)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.0)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)
Requirement already satisfied: rsa<5,>=3.1.4 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.7.2)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.7)
Requirement already satisfied: idna<4,>=2.5 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.3)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /home/alessandro/tensorflow/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.1.1)
Installing collected packages: tensorflow
Successfully installed tensorflow-2.7.0
```

Eventually, I run my python test:
```
import tensorflow as tf
print(""TensorFlow version:"", tf.__version__)

tf.config.list_physical_devices('GPU')
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```

And result is:
```
TensorFlow version: 2.7.0
Num GPUs Available:  0
2021-11-15 11:07:33.547632: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-11-15 11:07:33.547653: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: alessandro-MS-7B79
2021-11-15 11:07:33.547656: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: alessandro-MS-7B79
2021-11-15 11:07:33.547715: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.33.1
2021-11-15 11:07:33.547728: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.33.1
2021-11-15 11:07:33.547732: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.33.1
```

I'm switching to Tensorflow since pyTorch can't seem to handle compute capability 3.0 while Tensorflow can. But can it?"
53061,"can not deploy docker serving, it fails","I have following saved_model dir structure. The saved_model_cli is working correctly with the path but docker command not. Note this is following the book example and apparently docker part of example not working.
test.npy test input feeds 3 instances of mnist 28x28 images to serving.

DIR STRUCT:

root@nonroot-Standard-PC-i440FX-PIIX-1996:~/dev-learn/gpu/tflow/tensorflow/tflow-2nded# tree p297
p297
├── 0001
│   ├── assets
│   ├── saved_model.pb
│   └── variables
│       ├── variables.data-00000-of-00001
│       └── variables.index
├── assets
├── keras_metadata.pb
├── saved_model.pb
└── variables
    ├── variables.data-00000-of-00001
    └── variables.index
CLI:

    saved_model_cli run --dir p297/0001 --tag_set serve --signature_def serving_default --inputs 
flatten_input=test.npy
output of cli (OK)

2021-11-08 15:57:11.458910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-08 15:57:11.460906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31740 MB memory:  -> device: 0, name: Device 738c, pci bus id: 0000:00:07.0
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py:445: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
INFO:tensorflow:Restoring parameters from p297/0001/variables/variables
2021-11-08 15:57:11.961912: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.
2021-11-08 15:57:12.671939: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.
2021-11-08 15:57:12.686081: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.
Result for output key dense_2:
[[3.65022061e-05 2.47262960e-05 6.37578269e-05 2.08125566e-05
  7.02261750e-05 1.18829332e-01 4.30839646e-05 2.72816449e-01
  1.48763061e-02 5.93046188e-01 6.37045162e-07 3.02645799e-06
  3.80635288e-06 4.22269186e-05 6.12226586e-06 4.33605646e-06
  7.58367719e-07 3.06199559e-06 5.42078442e-06 2.38417056e-06
  4.93106018e-06 7.25027712e-06 1.24132812e-05 1.24427579e-05
  1.15528803e-06 4.87520847e-05 1.68714314e-06 8.28819338e-07
  2.23448342e-06 9.11506140e-06]
 [1.64286757e-04 3.39760754e-06 9.66621459e-01 1.61247503e-04
  1.02249524e-02 7.44288286e-07 2.27110237e-02 3.17756710e-10
  1.12835020e-04 3.98790796e-08 4.89366492e-10 1.29820976e-09
  9.63464334e-13 5.25168797e-10 9.53418247e-11 4.82374418e-10
  2.60897762e-11 2.75338996e-12 3.59164387e-09 7.25419169e-11
  5.41757861e-10 8.33503266e-10 1.25494719e-11 4.14474233e-09
  3.54530544e-10 7.37128275e-11 4.25408209e-10 7.22836443e-11
  6.95292546e-10 2.50320233e-11]
 [1.12954825e-04 9.99067128e-01 5.11940962e-05 5.15281979e-04
  2.25681084e-04 7.90114484e-07 2.39875553e-05 1.08763277e-06
  6.77304740e-07 8.83030324e-08 3.44932758e-08 3.36465895e-08
  1.49800372e-09 2.67271858e-08 7.71939810e-08 1.00484840e-07
  5.05017761e-09 4.95800823e-09 1.20519260e-07 1.41827059e-07
  1.35087987e-07 3.95625591e-07 4.27236913e-08 4.71499533e-08
  3.37586954e-08 1.78841599e-08 1.00948716e-08 4.40149028e-10
  2.45303022e-09 2.79508638e-09]]
DOCKER:

    MODEL_NAME=p297
    docker pull tensorflow/serving
    docker run -it --rm -p 8500:8500 -p 8501:8501 \
        -v ""$MODEL_NAME:/models/$MODEL_NAME"" \
        -e MODEL_NAME=$MODEL_NAME \
        tensorflow/serving
output of docker instance (fail):

Using default tag: latest
latest: Pulling from tensorflow/serving
Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb
Status: Image is up to date for tensorflow/serving:latest
docker.io/tensorflow/serving:latest
2021-11-09 00:01:43.627821: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: p297 model_base_path: /models/p297
2021-11-09 00:01:43.628201: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.
2021-11-09 00:01:43.628227: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: p297
2021-11-09 00:01:43.629366: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?
2021-11-09 00:01:44.629551: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?
2021-11-09 00:01:45.629769: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?
^X2021-11-09 00:01:46.629997: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable p297 found under base path /models/p297. Did you forget to name your leaf directory as a number (eg. '/1/')?
I logged onto container using
docker-exec -it <C_ID> /bin/bash
and inspected p297/0001 folder but turns out to be empty."
53060,AlreadyExistsError: File system for s3 already registered,"**System information**
- OS Platform and Distribution ): windows 10 
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.50
- Python version: 3.8
- Installed using: Conda

**Describe the problem**

** Not able to verify the scripts model_builder_tf2_test.py **

VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')
!python {VERIFICATION_SCRIPT}


**tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registeredr**

2021-11-15 00:16:54.612239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-11-15 00:16:54.636703: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""Tensorflow\models\research\object_detection\builders\model_builder_tf2_test.py"", line 25, in <module>
    from object_detection.builders import model_builder
  File ""C:\Users\Sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\builders\model_builder.py"", line 37, in <module>
    from object_detection.meta_architectures import deepmac_meta_arch
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflow\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\meta_architectures\deepmac_meta_arch.py"", line 28, in <module>
    import tensorflow_io as tfio  # pylint:disable=g-import-not-at-top
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\tensorflow_io-0.22.0-py3.8-win-amd64.egg\tensorflow_io\__init__.py"", line 17, in <module>
    from tensorflow_io.python.api import *  # pylint: disable=wildcard-import
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\tensorflow_io-0.22.0-py3.8-win-amd64.egg\tensorflow_io\python\api\__init__.py"", line 19, in <module>
    from tensorflow_io.python.ops.io_dataset import IODataset
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\tensorflow_io-0.22.0-py3.8-win-amd64.egg\tensorflow_io\python\ops\__init__.py"", line 96, in <module>
    plugin_ops = _load_library(""libtensorflow_io_plugins.so"", ""fs"")
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\tensorflow_io-0.22.0-py3.8-win-amd64.egg\tensorflow_io\python\ops\__init__.py"", line 64, in _load_library
    l = load_fn(f)
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow_io-0.22.0-py3.8-win-amd64.egg\tensorflow_io\python\ops\__init__.py"", line 56, in <lambda>
    load_fn = lambda f: tf.experimental.register_filesystem_plugin(f) is None
  File ""C:\Users\sanjay\Anaconda3\envs\tensorflowEnv\lib\site-packages\tensorflow\python\framework\load_library.py"", line 218, in register_filesystem_plugin
    py_tf.TF_RegisterFilesystemPlugin(plugin_location)
**tensorflow.python.framework.errors_impl.AlreadyExistsError: File system for s3 already registered**
"
53058,Move parameter `return_attention_weights` of `BaseAttentionLayer.call` to the constructor ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6
- Are you willing to contribute it (Yes/No): yes


**Describe the feature and the current behavior/state.**
Currently the method compute_output_shape is not implemented for layers derived from `BaseAttentionLayer`. Hence, it's impossible to use these layers with, for example, `TimeDistributed` wrapper (see [issue](https://github.com/keras-team/keras/issues/15515) Attention module not working with TimeDistributed layer).
To fix this, one could use eager mode or implement some workaround. Another way is to implement `compute_output_shape` method for `BaseAttentionLayer`. But `BaseAttentionLayer.call` method has a parameter `return_attention_weights` that adds attention weights tensor to values returned by attention layer, if set. It could change output shape of the layer after build.
Moreover, parameter `return_attention_weights` does not change a state of the layer when provided and is not saved to the object's attributes. So, it's impossible to implement `compute_output_shape` method while `return_attention_weights` belongs to the parameters of the call method. 

My suggestion is to change the API like this:

from 
```python3
class BaseDenseAttention(Layer):
    def __init__(self, causal=False, dropout=0.0,
                 **kwargs):
      ...
    
    def call(self,
             inputs,
             mask=None,
             training=None,
             return_attention_scores=False):
      ...
```

to

```python3
class BaseDenseAttention(Layer):
    def __init__(self, causal=False, dropout=0.0, return_attention_scores=False),  # move the parameter to the constructor
                 **kwargs):
      self.return_attention_scores=return_attention_scores
      ...
    
    def call(self, 
             inputs,
             mask=None,
             training=None,
             return_attention_scores=False):   # <- make it deprecated and add deprecation warning, remove in future versions
      ...

  def compute_output_shape(self, input_shape):
      # this function returns output shape using self.return_attention_scores as a conditional 
      ...

These changes make computation of output shape possible, but keep the possibility to return attention weights, if it's necessary for model debugging and/or analysis. For most use cases, it's enough to place `return_attention_scores` in a constructor of a class. Likewise, in other keras layers the parameters that change output shape of the layer are placed in a constructor of the layer (for example, `return_sequences` in recurrent layers). 
```

Related issues:
- [Unable to create](https://github.com/keras-team/keras/issues/15515) TimeDistributed wrapper for Attention layers
- [Feature request](https://github.com/tensorflow/tensorflow/issues/44127) for the parameter `return_attention_weights` to the `call` method. 

**Will this change the current api? How?**
Parameter return_attention_weights will be moved from call method of BaseAttentionLayer layers to constructor of BaseAttentionLayer. That allows to compute output shape of the attention layers and use them with, for example, TimeDistributed wrapper (that fixes previously mentioned issue).

**Who will benefit with this feature?**
Anyone who would use attention layers.

**Any Other info.**
It could break some of the tutorials/examples. Fixes may be required in the future: [this NMT tutorial](https://github.com/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb) uses `return_attention_scores` parameter.
"
53057,Error during training of EfficientDet-lite0 on GPU using the code model_maker_object_detection.ipynb,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Code: [model_maker_object_detection.ipynb](https://github.com/tensorflow/tensorflow/blob/3beeeff5abbaf24562722c4cbe3af8614346286c/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Pro Ubuntu 18.05
- TensorFlow installed from (source or binary): commands in the notebook (!pip install --upgrade tensorflow==2.5.0)
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.12
- CUDA/cuDNN version: 11.2
- GPU model and memory: P100-PCIE 16GB

## Current behavior
The line:
` 
model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)
`
will run into the following error:

UnknownError: 2 root error(s) found.
  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
	 [[Func/cond_6/then/_3438/input/_6900/_104]]
  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_96848]

Function call stack:
train_function -> train_function


## Expected behavior
It should be able to train on GPU.


## Full Output

Epoch 1/50

UnknownError                              Traceback (most recent call last)
<ipython-input-5-187f39c1697e> in <module>()
----> 1 model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in create(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)
    285     if do_train:
    286       tf.compat.v1.logging.info('Retraining the models...')
--> 287       object_detector.train(train_data, validation_data, epochs, batch_size)
    288     else:
    289       object_detector.create_model()

/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in train(self, train_data, validation_data, epochs, batch_size)
    156       return self.model_spec.train(self.model, train_ds, steps_per_epoch,
    157                                    validation_ds, validation_steps, epochs,
--> 158                                    batch_size, val_json_file)
    159 
    160   def evaluate(self,

/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py in train(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)
    270         callbacks=train_lib.get_callbacks(config.as_dict(), val_dataset),
    271         validation_data=val_dataset,
--> 272         validation_steps=validation_steps)
    273     return model
    274 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1181                 _r=1):
   1182               callbacks.on_train_batch_begin(step)
-> 1183               tmp_logs = self.train_function(iterator)
   1184               if data_handler.should_sync:
   1185                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    948         # Lifting succeeded, so variables are initialized and we can run the
    949         # stateless function.
--> 950         return self._stateless_fn(*args, **kwds)
    951     else:
    952       _, _, _, filtered_flat_args = \

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   3023     return graph_function._call_flat(
-> 3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   3025 
   3026   @property

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1959       # No tape is watching; skip to running the function.
   1960       return self._build_call_outputs(self._inference_function.call(
-> 1961           ctx, args, cancellation_manager=cancellation_manager))
   1962     forward_backward = self._select_forward_and_backward_functions(
   1963         args,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    594               inputs=args,
    595               attrs=attrs,
--> 596               ctx=ctx)
    597         else:
    598           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

UnknownError: 2 root error(s) found.
  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
	 [[Func/cond_6/then/_3438/input/_6900/_104]]
  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/efficientnet-lite0/StatefulPartitionedCall/stem/conv2d/Conv2D}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_96848]

Function call stack:
train_function -> train_function
"
53056,yolov5s-int8.tflite can not be run on the npu of I.MX8M Plus.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04)
- TensorFlow installation (pip package or built from source): pip package 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

Provide code to help us reproduce your issues using one of the following options: python export.py --int8 --weights yolov5s.pt --include tflite



### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

Model can be run on cpu of I.MX8M Plus, but can not be run on npu.

root@imx8mpevk:/usr/bin/tensorflow-lite-2.4.1/examples# ./benchmark_model --graph=/home/2tflite/yolov5s-int8.tflite --use_nnapi=true
 STARTING!
Log parameter values verbosely: [0]
Graph: [/home/yolov5test/yolov5s-int8.tflite]
Use NNAPI: [1]
NNAPI accelerators available: [vsi-npu]
Loaded model /home/yolov5test/yolov5s-int8.tflite
INFO: Created TensorFlow Lite delegate for NNAPI.
WARNING: Operator RESIZE_NEAREST_NEIGHBOR (v3) refused by NNAPI delegate: NNAPI does not support half_pixel_centers == true.
WARNING: Operator RESIZE_NEAREST_NEIGHBOR (v3) refused by NNAPI delegate: NNAPI does not support half_pixel_centers == true.
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 3 delegate kernels.
The input model file size (MB): 7.70178
Initialized session in 13.48ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
ERROR: NN API returned error ANEURALNETWORKS_OP_FAILED at line 4056 while running computation.

ERROR: Node number 284 (TfLiteNnapiDelegate) failed to invoke.

count=1 curr=13456030
	
Benchmarking failed.

"
53055,Current implementation only supports equal length strides in the row and column dimensions.,"When I use layers related  to DepthwiseConv2d operation, such as tf.keras.layers.SeparableConv2d, exception occurs like that: 'Current implementation only supports equal length strides in the row and column dimensions. [Op: DepthwiseConv2dNative]'. It means that i cannot use parameter 'strides' like [1, 2], however, that conflicts with the documentation, which allows 'strides'  with list format without mentioning that elements within the list should be the same.
I'm using TensorFlow 2.0.0, and I guess this bug exisits in 3D scenariothe and further edition.
![1](https://user-images.githubusercontent.com/94270103/141666163-3ba1d244-6efb-49b2-aa49-322d0b4d0889.png)"
53054,Errors in setting up GPU with CUDA and cuDNN,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
TensorFlow installed from (source or binary): pip install tensorflow (in Anaconda)
TensorFlow version: 2.5.0
Python version: 3.8
Installed using virtualenv? pip? conda?: pip install tensorflow (in Anaconda)
CUDA/cuDNN version: CUDA: cuda_11.0.3_451.82_win10 and cuDNN: cudnn-11.0-windows-x64-v8.0.5.39
GPU model and memory: NVIDIA GeForce GTX 1660 Ti with Max-Q Design 6 GB


Dear All,

I have tried several CUDA and cuDNN versions for setting up my GPU without luck. Finally, I have used

**cuda_11.0.3_451.82_win10 and
cudnn-11.0-windows-x64-v8.0.5.39**

in Anaconda (Python 3.8.12). I see there are few positive results and 3 errors  (please see the full output below) as I executed this command:
```
import tensorflow
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```


Error 1:
**2021-11-13 22:15:44.176901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found**

Error 2:
**2021-11-13 22:15:44.191130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found**

Error 3:
**2021-11-13 22:15:44.191174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.** 

I tried to delete EVERYTHING from previous CUDA & cuDNN installations; I wonder if I had missed something!?

Could you please look into the following error messages and give me some suggestoins, please?

I am using Anaconda 3 in a Windows 10 laptop (Lenovo legion 7 slim with NVIDIA GeForce GTX 1660 Ti with Max-Q Design ).

TIA
Sheri

> >>> import tensorflow
> 2021-11-13 22:50:53.719861: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
> >>> from tensorflow.python.client import device_lib
> >>> print(device_lib.list_local_devices())
> 2021-11-13 22:51:32.529087: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-11-13 22:51:32.531546: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
> 2021-11-13 22:51:33.925024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5
> coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s
> 2021-11-13 22:51:33.925223: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
> 2021-11-13 22:51:33.932212: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
> 2021-11-13 22:51:33.932294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
> 2021-11-13 22:51:33.935750: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
> 2021-11-13 22:51:33.936811: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
> 2021-11-13 22:51:33.937346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found
> 2021-11-13 22:51:33.939861: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
> 2021-11-13 22:51:33.940487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
> 2021-11-13 22:51:33.940591: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
> Skipping registering GPU devices...
> 2021-11-13 22:51:34.007417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2021-11-13 22:51:34.007508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
> 2021-11-13 22:51:34.008197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N
> [name: ""/device:CPU:0""
> device_type: ""CPU""
> memory_limit: 268435456
> locality {
> }
> incarnation: 10668262798735033247
> ]
> >>>"
53053,Error when Saving model with data augmentation layer on Tensorflow 2.7 ,"I am getting an error when trying to save a model with data augmentation layers in last tensorflow version (2.7.0).

Here is the code of data augmentation:



> 
    input_shape_rgb = (img_height, img_width, 3)
    data_augmentation_rgb = tf.keras.Sequential(
      [ 
        layers.RandomFlip(""horizontal""),
        layers.RandomFlip(""vertical""),
        layers.RandomRotation(0.5),
        layers.RandomZoom(0.5),
        layers.RandomContrast(0.5),
        RandomColorDistortion(name='random_contrast_brightness/none'),
      ]
    )





Now I build my model like this:

> 
     input_shape = (img_height, img_width, 3)

    model = Sequential([
    layers.Input(input_shape),
    data_augmentation_rgb,
    layers.Rescaling((1./255)),
  
    layers.Conv2D(16, kernel_size, padding=padding, activation='relu', strides=1, 
       data_format='channels_last'),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
  
    layers.Conv2D(32, kernel_size, padding=padding, activation='relu'), # best 4
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
  
    layers.Conv2D(64, kernel_size, padding=padding, activation='relu'), # best 3
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
  
    layers.Conv2D(128, kernel_size, padding=padding, activation='relu'), # best 3
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
  
    layers.Flatten(),
    layers.Dense(128, activation='relu'), # best 1
    layers.Dropout(0.1),
    layers.Dense(128, activation='relu'), # best 1
    layers.Dropout(0.1),
    layers.Dense(64, activation='relu'), # best 1
    layers.Dropout(0.1),
    layers.Dense(num_classes, activation = 'softmax')
     ])
  
     model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=metrics)
     model.summary()



Then after the training is done I just make:

> 
     model.save(""./"")

And I'm getting this error:

> 
      
      ---------------------------------------------------------------------------
      KeyError                                  Traceback (most recent call last)
      <ipython-input-84-87d3f09f8bee> in <module>()
      ----> 1 model.save(""./"")
      
      
      /usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in 
       error_handler(*args, **kwargs)
       65     except Exception as e:  # pylint: disable=broad-except
       66       filtered_tb = _process_traceback_frames(e.__traceback__)
       ---> 67       raise e.with_traceback(filtered_tb) from None
       68     finally:
       69       del filtered_tb
      
       /usr/local/lib/python3.7/dist- 
       packages/tensorflow/python/saved_model/function_serialization.py in 
       serialize_concrete_function(concrete_function, node_ids, coder)
       66   except KeyError:
       67     raise KeyError(
       ---> 68         f""Failed to add concrete function '{concrete_function.name}' to 
       object-""
       69         f""based SavedModel as it captures tensor {capture!r} which is 
       unsupported""
       70         "" or not reachable from root. ""
      
       KeyError: ""Failed to add concrete function 
       'b'__inference_sequential_46_layer_call_fn_662953'' to object-based SavedModel as it 
       captures tensor <tf.Tensor: shape=(), dtype=resource, value=<Resource Tensor>> which 
       is unsupported or not reachable from root. One reason could be that a stateful 
       object or a variable that the function depends on is not assigned to an attribute of 
       the serialized trackable object (see SaveTest.test_captures_unreachable_variable).""

I inspected the reason of getting this error by changing the architecture of my model and I just found that the reason came from the `data_augmentation` layer since the `RandomFlip` and `RandomRotation` and others are changed from `layers.experimental.prepocessing.RandomFlip` to `layers.RandomFlip`, but still the error appears.

"
53052,"Concat tensor of[None, 192] with tensor of [1,128]","How to concatenate tensors of shapes [None, 128] with tensor of [1,128]. Here the first tensor will some data of unknown length and the second tensor is fixed tensor not dependant on data size. The final output should be of shape[None, 328]. This is a part of a neural network concatenation.

I tried 
`c = Concatenate(axis = -1, name = 'DQN_Input')([ a, b])`
but it gives error.
Here a.shape = (None, 192) and b.shape = (1,128) But this does not work. The error is

> ValueError: A Concatenate layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 192), (1, 128)]"
53051,"[TPU, keras preprocessing layer] Some Op must be a compile-time constant.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): google colab
- TensorFlow version (use command below): 2.7
- Python version: google colab
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: TPU issue
- GPU model and memory: TPU issue

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Hi!
TPU error raises especially with Kears preprocessing layers.
I've tried to connect two models, augmentation model that contains preprocessing layer and segmentation model.

```python3
def new_concatenated_model(
    image_input_hw,
    mask_input_hw,
    class_n
):
    seg_model = create_segmentation_model(class_n)
    aug_model = create_augmentation_model(
        image_input_hw, mask_input_hw, class_n)
    
    image_input_shape = list(image_input_hw) + [3]

    @auto_tpu(device=CURRENT_DEVICE) # decorator `auto_tpu` is just context manager.
    def create():
        im = seg_model.input
        model = AugConcatedSegModel(
            inputs=im,
            outputs=seg_model(im),
            augmentation_model=aug_model,
            name='seg_model_train_with_aug'
        )
        return model
    
    model = create()
    return model
```

<br>

`train_step()` function code was mainly came from tensorflow [official tutorial document](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).

```python3
class AugConcatedSegModel(tf.keras.Model):
    def __init__(
        self,
        inputs=None,
        outputs=None,
        augmentation_model=None, 
        **kwargs
    ):
        super().__init__(inputs=inputs, outputs=outputs, **kwargs)
        self.augmentation_model = augmentation_model

    def train_step(self, data):
        im, ma = data
        im, ma = self.augmentation_model((im, ma))

        with tf.GradientTape() as tape:
            ma_pred = self(im, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            loss = self.compiled_loss(ma, ma_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(ma, ma_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}
```

<br>

**Describe the expected behavior**

Expected to train successfully without error.
same code were tested on:
- [x] CPU : No errors
- [x] GPU : No errors
- [x] TPU : Error

You could reproduce this error very fast
https://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing 
`Runtime - Run all`.

+ When I changed `augmentation model` to just plain `Conv2D` layers, the error disappeared.

<br>

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1LhHj1FrkZE9QnFhY-NOO8mn7aiXhZgNh?usp=sharing 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```python3
InvalidArgumentError: 9 root error(s) found.
  (0) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]
	 [[TPUReplicate/_compile/_1646634736830564460/_4]]
  (1) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]
	 [[TPUReplicate/_compile/_1646634736830564460/_4]]
	 [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_47]]
  (2) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2` with op StatelessRandomUniformV2 must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

	 [[{{node sequential_augmentation_model/sequential_augmentation_layers/random_flip/stateless_random_flip_left_right/stateless_random_uniform/StatelessRandomUniformV2}}]]
	 [[TPUReplicate/_compile/_1646634736830564460/_4]]
	 [[tpu_compile_succeeded_assert/_5094882425795608634/_5/_159]]
  (3) INVALID_ARGUMENT: {{function_node __inference_train_function_692915}} Input 0 to node `sequential_augmentation_model/sequential_a ... [truncated]
```"
53050,Hợp nhất thành công một yêu cầu kéo có thể đóng vấn đề này,https://github.com/tensorflow/tensorflow/blob/7e5b561df1f1dbcba9216e662c345eccb46b3048/tensorflow/go/session_test.go#L252
53049,Tensorflow using Intel UHD graphics instead of Nvidia,"While training my model tensorflow is using the Intel UHD graphics instead of Nvidia RTX 2060. But the thing is even if utilization is 0% the RTX memory is being used.
tensorflow = 2.7.0
cudaa = 11.2
cudnn = 8.1
![Screenshot 2021-11-13 091117](https://user-images.githubusercontent.com/69339222/141604087-f1d98ea3-3713-48e6-8c0b-b3a6a39fbdf6.png)"
53047,Android: GPU delegate fails with a YOLOv4 model,"#### System Information

* Custom code: none, using the upstream benchmark
* OS: Android 12
* Device: Google Pixel 4a
* TensorFlow version: nightly release benchmark build (definite version unspecified in the URL)

#### Steps to Reproduce

1. Enable developer options and USB debugging.
2. Execute the script below — it will download the TF benchmark and a model.

Please note that we have an internal YOLOv4 model we cannot share. I’ve found an existing one. However, the result is more or less the same so I’m gonna guess something is up with the model architecture. Also — the model executes fine on CPU / NPU.

Results:

* Expected: no error messages.
* Actual: a lot of error messages — seem to be an error for each model node. See the output below.

```bash
#!/bin/bash
set -eou pipefail


MODEL_FILE_NAME=""model.tflite""

BENCHMARK_PATH=""$(mktemp -d)""
BENCHMARK_FILE_NAME=""tensorflow-benchmark""

DEVICE_PATH=""/data/local/tmp""


echo "":: Fetch benchmark...""
curl \
  --location ""https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model"" \
  --output ""${BENCHMARK_PATH}/${BENCHMARK_FILE_NAME}""

echo "":: Fetch model...""
curl \
  --location ""https://github.com/theAIGuysCode/tensorflow-yolov4-tflite/raw/master/android/app/src/main/assets/yolov4-416-fp32.tflite"" \
  --output ""${MODEL_FILE_NAME}""


echo "":: Move benchmark to the device...""
adb push ""${BENCHMARK_PATH}/${BENCHMARK_FILE_NAME}"" ""${DEVICE_PATH}""
adb shell chmod +x ""${DEVICE_PATH}/${BENCHMARK_FILE_NAME}""

echo "":: Move model to the device...""
adb push ""${MODEL_FILE_NAME}"" ""${DEVICE_PATH}""

echo "":: Run benchmark...""
adb shell taskset f0 ""${DEVICE_PATH}/${BENCHMARK_FILE_NAME}"" \
  --graph=""${DEVICE_PATH}/${MODEL_FILE_NAME}"" \
  --use_gpu=true

echo "":: Remove benchmark...""
adb shell rm ""${DEVICE_PATH}/${BENCHMARK_FILE_NAME}""
rm -rf ""${BENCHMARK_PATH}""

echo "":: Remove model...""
adb shell rm ""${DEVICE_PATH}/${MODEL_FILE_NAME}""
rm -rf ""${MODEL_FILE_NAME}""
```
```
:: Fetch benchmark...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 6029k  100 6029k    0     0  11.8M      0 --:--:-- --:--:-- --:--:-- 11.8M
:: Fetch model...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   196  100   196    0     0    336      0 --:--:-- --:--:-- --:--:--   335
100 23.1M  100 23.1M    0     0  8255k      0  0:00:02  0:00:02 --:--:-- 23.7M
:: Move benchmark to the device...
/var/folders/d8/zmkczjms4jxbtbw24wt7qzbw0000gp/T/tmp.Yg5JHGh1/tens...ark: 1 file pushed, 0 skipped. 99.8 MB/s (6174376 bytes in 0.059s)
:: Move model to the device...
model.tflite: 1 file pushed, 0 skipped. 36.6 MB/s (24279948 bytes in 0.632s)
:: Run benchmark...
STARTING!
Log parameter values verbosely: [0]
Graph: [/data/local/tmp/model.tflite]
Use gpu: [1]
Loaded model /data/local/tmp/model.tflite
INFO: Initialized TensorFlow Lite runtime.
GPU delegate created.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Replacing 144 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.
The input model file size (MB): 24.2799
Initialized session in 2394.17ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
ERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
ERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
ERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
ERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
ERROR: Node number 144 (TfLiteGpuDelegateV2) failed to invoke.
ERROR: TfLiteGpuDelegate Invoke: Given object is not valid
>>> THIS CONTINUES FOR A WHILE <<<
count=852 first=237 curr=175 min=17 max=381 avg=176.995 std=47

Benchmarking failed.
```"
53046,Different variable declaration for `inner_shape` and `outer_shape` in `Slice updates` section of `tensor_scatter_nd_update` ,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update#slice_updates

## Description of issue (what needs changing):

The `inner_shape` and `outer_shape` variables should have the definition of :

`inner_shape = tensor.shape[index_depth:]`
`outer_shape = tensor.shape[:index_depth]`

### Clear description

In the first code block of the section the variables `inner_shape` and `outer_shape` are defined as:

`inner_shape = tensor.shape[:index_depth]`
`outer_shape = tensor.shape[index_depth:]`

In the fourth code block the same variables are defined as:

`inner_shape = tensor.shape[index_depth:]`
`outer_shape = tensor.shape[:index_depth]`

Note that in the two definitions the slices are used inversely for every variable.
However I believe that the definition of the fourth block is the correct one, namely

`inner_shape = tensor.shape[index_depth:]`
`outer_shape = tensor.shape[:index_depth]`


#### Code example

By replicating the example in the `Slice updates` section:

* the `outer_shape` should be 6 
* the `inner_shape` should be 3

that is achieved with the definition

`inner_shape = tensor.shape[index_depth:]`
`outer_shape = tensor.shape[:index_depth]`


Below I provide the code with the corresponding output

```
tensor = tf.zeros([6,3], dtype=tf.int32)
indices = tf.constant([[2],[4]])
num_updates, index_depth = indices.shape.as_list()
print(f""num_updates == {num_updates}, index_depth == {index_depth}"")
print("""")

print(""The outer_shape should be 6 and the inner_shape should be 3"")
print("""")
      
print(""If we use the definition of the first code block"")
outer_shape_1st = tensor.shape[index_depth:]
inner_shape_1st = tensor.shape[:index_depth]
print(f""outer_shape is {outer_shape_1st[0]}, the inner_shape is {inner_shape_1st[0]}"")      
print("""")


print(""If we use the definition of the forth code block"")
outer_shape_4th = tensor.shape[:index_depth]
inner_shape_4th = tensor.shape[index_depth:]
print(f""outer_shape is {outer_shape_4th[0]}, the inner_shape is {inner_shape_4th[0]}"")
```


![image](https://user-images.githubusercontent.com/35838787/141491130-d6f95932-214b-4057-8034-6652845c8894.png)


"
53045,SpaceToDepth and DepthToSpace were not named as expected,"Hello,

In the API, we can give a name for the SpaceToDepth (or DepthToSpace) operator. However, I found that whatever the name I provided, tensorflow always fixes the name of SpaceToDepth as **tf.nn.space_to_depth** (and the same problem for DepthToSpace).

I'm using Windows 10, version 21H1, build 19043.1348 and Tensorflow 2.6

You can easily reproduce the issue with a minimal example as follows:
```
import tensorflow as tf
import json

def sample_network(input_layer):
    s2d = tf.nn.space_to_depth(input_layer, block_size=2, name=""Space2Depth"")
    d2s = tf.nn.depth_to_space(s2d, block_size=2, name=""Depth2Space"")
    return d2s

if __name__ == ""__main__"":
    input_net = tf.keras.Input(shape=(64, 64, 3), dtype=tf.float32)
    output = sample_network(input_net)
    model = tf.keras.Model(inputs=input_net, outputs=output)
    model_json = json.loads(model.to_json())
    with open(""github_issue.json"", ""w"") as f:
        json.dump(model_json, f, indent=2)
```
I would expect the name of these operators are Space2Depth and Depth2Space, but they are not.
![DepthToSpace](https://user-images.githubusercontent.com/12828532/141489907-6f5d4263-abf1-4ac7-b117-e650bcbae8ac.PNG)
![SpaceToDepth](https://user-images.githubusercontent.com/12828532/141489910-c9d24747-fcc6-459a-9899-a939655ed544.PNG)
Thank you for your consideration.
"
53043,lite Benchmark_Model build failed on ARM platform without hexgon,"I'm trying to build benchmark_model with NXP M865 arm dev board, it was successfuly with v2.6.0, but failed with v2.7.0.

I found it was because this patch:
https://github.com/tensorflow/tensorflow/commit/feb49693266f444d5d8ce1a439ffbe7ff6e15e8a

This patch enabled hexgon delegate on all ARM platform, this is not make sense. Please consider rework it make sure it only compiled with platform with qualcomm chip.


"
53042,XNNPACK delegate not enabled in TensorFlow Lite Python Interpreter.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (x64) and Raspberry Pi OS 64bit (raspios_arm64-2021-04-09)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.7.0 and 7b290f9fd9fbf2ac4352b3cbe327e1067e5a3574
- Python version: 3.7.3 (Raspberry Pi OS 64bit)
- Bazel version (if compiling from source): - (Build with CMake)
- GCC/Compiler version (if compiling from source): 8.3.0 (Raspberry Pi OS 64bit)
- CUDA/cuDNN version: - 
- GPU model and memory: -

**Describe the current behavior**
XNNPACK delegate not enabled in TensorFlow Lite Python Interpreter. 
Building with either CMake or Bazel will not take effect.
The following log is not output.
> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
  
  
Delegate lazy initialization was included in the 3d3c6db1ca2d50f6f07722cd800144f8f736167c commit.
For C ++ IF, Interpreter::AllocateTensors calls ApplyLazyDelegateProviders to enable the XNNPACK delegate.
https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/interpreter.cc#L176

However, for Python IF, the XNNPACK delegate is not enabled because ApplyLazyDelegateProviders is not called in InterpreterWrapper::AllocateTensors.
https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L259

**Describe the expected behavior**
The XNNPACK delegate is enabled in the TensorFlow Lite Python Interpreter.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): Yes
- Briefly describe your candidate solution(if contributing): 

**Standalone code to reproduce the issue**

**Other info / logs** Include any logs or source code that would be helpful to
"
53041,TF/MLIR: lhlo to linalg conversion for BroadcastInDimOp - invalid IR,"This is from the latest Tensorflow git repo 33dc8e4b4d4115b308cc55bd99ed5a9cd8b36b4c (Nov 10)

On the input:

```
  func @main(%in: memref<1x40x1xf16>, %out: memref<1x40x8xf16>) {
    ""lmhlo.broadcast_in_dim""(%%in, %out) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (memref<1x40x1xf16>, memref<1x40x8xf16>) -> ()
    return
  }
```

```
$ tf-opt -lhlo-legalize-to-linalg lmhlo.broadcast_in_dim.mlir 
lmhlo.broadcast_in_dim.mlir:7:5: error: 'memref.collapse_shape' op expected collapsed type to be 'memref<1x40xf16>', but got 'memref<1x40xf16, affine_map<(d0, d1) -> (d0 * 40 + d1)>>'
    ""lmhlo.broadcast_in_dim""(%arg3, %arg5) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (memref<1x40x1xf16>, memref<1x40x8xf16>) -> ()
    ^
lmhlo.broadcast_in_dim.mlir:7:5: note: see current operation: %2 = ""memref.collapse_shape""(%arg3) {reassociation = [[0], [1, 2]]} : (memref<1x40x1xf16>) -> memref<1x40xf16, affine_map<(d0, d1) -> (d0 * 40 + d1)>>
```

The IR generated is invalid here. 

Separately, but related to the reshaping being introduced here: it was introduced by this commit:

```
commit 967782ff2a870fbc82e0db99c04eae78dded61d7
Author: Alexander Belyaev <pifon@google.com>
Date:   Fri Jun 5 06:40:28 2020 -0700

    [XLA][MLIR] Insert linalg.reshape when lowering LHLO BroadcastInDimOp.
    
    If size-1 dimensions require expansion, we insert linalg.reshape to get rid of them.
    
    PiperOrigin-RevId: 314918927
    Change-Id: I3e9cd380b93421858343ba299e659c0396ca7bcf
```

The reshaping isn't actually needed here. Was the reason for the reshape to preserve some canonical form for linalg.generic? In that case, adding a pattern on the latter would be a much better separation and placement of concerns without the need to add such code in the lowering pattern.

The rationale for inserting the reshape isn't clear and not summarized on the commit. The above example is just a broadcast via 1x40x1 -> 1x40x8 which can and was easily handled by mapping to `linalg.generic` prior to this commit (without any extra reshapes). The above commit split the converter templated for both hlo and lhlo while adding a large amount of code to introduce the reshape. The `linalg.reshape` itself was subsequently specialized to `collapse/expand` and moved to the memref dialect (`memref.collapse/expand_shape`). After a lowering to loops,  these reshapes on memrefs would get in the way of transformations like affine fusion unless they are eliminated via ""in-placing""/special fusion (which is separately useful to anyway exist); in the absence of such inplacing, they would just get in the way of other downstream transformations like affine fusion for example. For the lmhlo.broadcast_in_dim lowering itself, the pre-existing lowering was simple.

CC: @pifon2a @joker-eph @jpienaar 
"
53040,How do I set up multi capabilities when run the configure file,"gpu ： one gtx 1060
system: linux
source code: tensorflow/r1.15
expect capability: 5.2,6.0,6.1,7.0,7.5

I have only one gpu, but i want to set mutil capabilities  when build libtensorflow.

![企业微信截图_1636692704760](https://user-images.githubusercontent.com/61573829/141411648-b523408a-ca36-42b0-8956-4b87b02c4816.png)
"
53039,How to specify the output layers during model conversion to TFLite in TF2?,"**Describe the problem**
Is there any way to remove the TFLite_Detection_PostProcess in tf2 ssd mobilenet model during conversion to tflite model？In tf1, there is a option to do so, but it seems that in tf2 the option for specifying the output layers is removed. 

In tf1, I can specify the output layers by putting the layer's name in output_array:
input_arrays=[""normalized_input_image_tensor""]
output_arrays=[""raw_outputs/box_encodings"",""raw_outputs/class_predictions""]
input_tensor={""normalized_input_image_tensor"":[1,320,320,1]}

import tensorflow as tf
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('tflite_graph.pb',input_shapes = input_tensor,
input_arrays = input_arrays ,output_arrays = output_arrays)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
open('ssd_mobilenet_v2_tf1.tflite', ""wb"").write(tflite_model)
![with postprocess](https://user-images.githubusercontent.com/57311716/141403792-f97cb470-c8bb-4174-891a-1ffec7b6f402.jpg)


![without postprocess](https://user-images.githubusercontent.com/57311716/141403817-7016ccf9-1249-4eda-ad56-717de818e229.jpg)


"
53038,built-in method __contains__ of dict object at 0xffff8b79a200  on aarch64， ubuntu20.04， tf：v2.5.0， py: v3.8.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution：Ubuntu 20.04):
- Mobile device ：aarch64
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): V2.5.0
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 9.3.0


You can collect some of this information using our environment capture：

Python 3.8.11 (default, Aug  6 2021, 14:51:49) 
[GCC 10.2.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.keras import models,layers
RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 37, in <module>
    from tensorflow.python.client import pywrap_tf_session
  File ""/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py"", line 23, in <module>
    from tensorflow.python._pywrap_tf_session import *
ImportError: SystemError: <built-in method __contains__ of dict object at 0xffff8b79a200> returned a result with an error set
>>> "
53037,The qna model demo does not return answer results,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**Describe the current behavior**
In trying to run the qna model demo for TensorFlow.js I found that the answers always return an empty array. I did not alter any code I ran the out of the box demo direct from the Github page. I left the default information about Nikola Tesla in place and asked the question `Who is Tesla`. In the Chrome console I saw an Array with 0 results.

**Describe the expected behavior**
I expect answers to be returned by the model and output to the browser.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**
The reproducible example exists at the link provided from Github.

https://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html

Thank you."
53034,unit test //tensorflow/compiler/xla/service/cpu/tests:cpu_literal_caching_test fails to build on AARCH64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build fails with
Execution platform: @local_execution_config_platform//:platform
In file included from ./tensorflow/compiler/xla/service/backend.h:31,
                 from ./tensorflow/compiler/xla/tests/hlo_test_base.h:26,
                 from ./tensorflow/compiler/xla/tests/codegen_test_base.h:24,
                 from ./tensorflow/compiler/xla/tests/llvm_irgen_test_base.h:22,
                 from ./tensorflow/compiler/xla/service/cpu/tests/cpu_codegen_test.h:19,
                 from tensorflow/compiler/xla/service/cpu/tests/cpu_literal_caching_test.cc:18:
./tensorflow/compiler/xla/service/transfer_manager.h:37:1: error: expected unqualified-id before 'namespace'
   37 | namespace xla {
      | ^~~~~~~~~
In file included from ./tensorflow/compiler/xla/tests/hlo_test_base.h:26,
                 from ./tensorflow/compiler/xla/tests/codegen_test_base.h:24,
                 from ./tensorflow/compiler/xla/tests/llvm_irgen_test_base.h:22,
                 from ./tensorflow/compiler/xla/service/cpu/tests/cpu_codegen_test.h:19,
                 from tensorflow/compiler/xla/service/cpu/tests/cpu_literal_caching_test.cc:18:
./tensorflow/compiler/xla/service/backend.h:97:3: error: 'TransferManager' does not name a type
   97 |   TransferManager* transfer_manager() const { return transfer_manager_; }
      |   ^~~~~~~~~~~~~~~
./tensorflow/compiler/xla/service/backend.h:164:11: error: 'TransferManager' has not been declared
  164 |           TransferManager* transfer_manager,
      |           ^~~~~~~~~~~~~~~
./tensorflow/compiler/xla/service/backend.h:172:3: error: 'TransferManager' does not name a type
  172 |   TransferManager* transfer_manager_;
      |   ^~~~~~~~~~~~~~~
INFO: Elapsed time: 707.544s, Critical Path: 172.52s
INFO: 4160 processes: 63 internal, 4097 local.
FAILED: Build did NOT complete successfully


**Provide the exact sequence of commands / steps that you executed before running into the problem**

$ bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --cxxopt=-Og --cxxopt=-ggdb --verbose_failures -- //tensorflow/compiler/xla/service/cpu/...

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53033,"The Problem of ""UpSamples2D“ Layer when transfering h5 model file to tflite model file。","### 1. System information

- Windows 10
- pip package
- '2.3.1'

### 2. Code

    print('<-------------------------------------保存为tflite文件:------------------------------------>\n')
    new_model = tf.keras.models.load_model(pbfile)  # path to the SavedModel directory
    new_model.summary()
    converter = tf.lite.TFLiteConverter.from_keras_model(new_model)
    tflite_model = converter.convert()

    # Save the model.
    with open(tflitefile, 'wb') as f:
        f.write(tflite_model)

### 3. Confused of This Code's Result
The Above Code can Successfully converted h5 model to tflite model file, But I'm Confused of the result of tflite model
when ""UpSamples2D“ Layer is showed in h5 model file as following:\
```
       |
      \|/
UpSampling2D
       |
      \|/
```
but is showed in tflite model file as following:
```
               |
              \|/
       |                  |
      \|/                |
    shape           
       |                  |
      \|/                |
  StridedSlice    
       |                  |
      \|/                |
      Mul             
       |                  |
      \|/                \|/
   ResizeNearestNeighbor
               |
              \|/
```
Problem: Why UpSampling2D is divided into two route calculations?
"
53032,Why MutableDenseHashTableV2 in tensorflow 1.10 but gone in 1.15 ?,"There is some questions to ask about the file  tensorflow/core/ops/lookup_ops.cc, I see in v1.10 there is two MutableDenseHashTable, MutableDenseHashTable and MutableDenseHashTableV2, both with the same input and attributes but returning different types of output.

But in the version of 1.15, there is only MutableDenseHashTable with MutableDenseHashTableV2 deleted.

May I ask is there some OP which can substitute MutableDenseHashTableV2 in tensorflow 1.15 ?
"
53031,Unable to install Tensorflow through Pip due to no versions to install from,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro and Linux Ubuntu 20.04 LTS (through WSL2)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto G Stylus (Running on Termux)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.6.2, 2.7.0
- Python version: 3.10.0
- Installed using virtualenv? pip? conda?: Pip and Pipenv
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.4.153
- GPU model and memory: Nvidia Quadro P4000 (8GB VRAM)

**Describe the problem**
When installing tensorflow, it seems like it can't find the selected versions to install from. I've checked for any builds, and on pypi there seems to be existing builds for windows and linux. This also happens with `pip install tensorflow==2.7.0`.

Steps:
1. Install Python 3.10.0 
2. Upgrade pip to the latest version
3. Run `pip install tensorflow`
4. Wait until it returns that it can't find any versions for tensorflow to install from


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

When running `pip install tensorflow`, this is what it returns
```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```"
53030,Tensorflow 2.6 compatibility with previous versions?,"Hi guys, I've trained a model using tensorflow 2.3, keras 2.3.1 and python 3.8. This model seems to be working perfectly fine in 2.4/2.5 but breaks with tf 2.6. I looked through the tensorflow 2.6 , as well as keras 2.6 release notes but couldn't find why this was breaking. 

The model can be loaded in tf.2.6 and the weights are all correct. However during inference, the result is different from that of tf2.3,2.4,2.5. Is there a reason for why this is? 

Model is a transfer learnt efficientnet B3, with a custom top made of dense layers and batch norm. "
53029,Adding images with no bounding box to tflite model maker object detection dataset,"I want to add images without any bounding boxes to the dataset used to train an object detector using tflite model maker. 

According to the [docs](https://cloud.google.com/vision/automl/object-detection/docs/csv-format) I can add 

>one row for each image with no bounding box (such as row 4 below).

```
 TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,
 TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,
 UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3
 TEST,gs://folder/im3.png,,,,,,,,,
```

When doing so I am getting a ValueError ""could not convert string to float"", which is caused by an attempt to cast the None object to a float in the following [line](https://github.com/tensorflow/examples/blob/c930d0af2983ee438dcc005f9373188734f1624f/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py#L337-L338):

```
    xmin, ymin = float(line[3]) * width, float(line[4]) * height
```

How can I properly add such images without bounding boxes? Possibly a more important question; are these empty images even helpful towards retraining the model (i.e. does this give higher precision to the default classifier of None or Empty)?
"
53027,IDLE Time so long period.,"TF version： v2.6, using distributed stragedy.

![image](https://user-images.githubusercontent.com/10629930/141219850-a5cfa6d5-270a-4974-9504-c559cc83b7cf.png)

![image](https://user-images.githubusercontent.com/10629930/141219519-1a3a9680-2bbe-4d3e-818e-d8028fe9ac2b.png)
![image](https://user-images.githubusercontent.com/10629930/141224856-57638193-3d17-4aa2-bc87-ecdc478dc217.png)


![image](https://user-images.githubusercontent.com/10629930/141223727-f40cb68d-cb44-495a-8ec2-4c3e0561ceb3.png)

training deepfm model by using cpu  without gpu train.


input files are tfrecords (15Mb per file)by spark. 

The key  train code like this:

![image](https://user-images.githubusercontent.com/10629930/141219749-49a5a82a-7c59-4605-8695-69c1fbf159b4.png)

why the idle time is so long, and is there some ways to enchance. my problems is that using multimirrored worker can't reduce training time...."
53026,No supported kernel for GPU devices is available (Apple M1 - Metal GPU),"I am trying to replicate the results I obtained from a training I have already completed on both ubuntu and windows (the code works on GPU), and I also tried the code using CPU and it works properly (on MacOS). The problem is when I try to use the GPU. The error message I get is very long (I can paste it all if needed), but I believe that the core part is the following:

> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation loader/GeneratorDataset: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.

Tensorflow version is 2.6.0, and I checked that the GPU is recognized:

> I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
> True

Thanks to anyone that could give hints about how to solve this issue.

"
53025,Issue with conversion of dilated Convolutions #29509 still happening in version 2.6.0,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0

### 2. Code
Standalone code to reproduce the issue - 
```
import numpy as np
import tensorflow as tf
from tensorflow_model_optimization.python.core.quantization.keras import quantize
from tensorflow.python import keras

l = tf.keras.layers

tf.config.run_functions_eagerly(True)

def functional_model():
    """"""Builds an MNIST functional model.""""""
    inp = tf.keras.Input(shape=image_input_shape())
    x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)
    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)
    # TODO(pulkitb): Add BatchNorm when transformations are ready.
    # x = l.BatchNormalization()(x)
    x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)
    x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)
    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)
    x = l.Flatten()(x)
    x = l.Dense(1024, activation='relu')(x)
    x = l.Dropout(0.4)(x)
    out = l.Dense(10, activation='softmax')(x)

    return tf.keras.Model(inp, [out])


def image_input_shape(img_rows=28, img_cols=28):
    if tf.keras.backend.image_data_format() == 'channels_first':
        return 1, img_rows, img_cols
    else:
        return img_rows, img_cols, 1


def preprocessed_data(img_rows=28,
                      img_cols=28,
                      num_classes=10):
    """"""Get data for mnist training and evaluation.""""""
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    if tf.keras.backend.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255

    # convert class vectors to binary class matrices
    y_train = tf.keras.utils.to_categorical(y_train, num_classes)
    y_test = tf.keras.utils.to_categorical(y_test, num_classes)

    return x_train, y_train, x_test, y_test


model = functional_model()
model.summary()
x_train, y_train, x_test, y_test = preprocessed_data()

model.compile(
    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=500)
_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)

print(""Quantizing model"")

quantized_model = quantize.quantize_model(model)
quantized_model.compile(
    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

quantized_model.fit(x_train, y_train, batch_size=500)
_, quantized_model_accuracy = quantized_model.evaluate(
    x_test, y_test, verbose=0)
model.save(""/home/anurag/git/train_data/testOrig.h5"")
quantized_model.save(""/home/anurag/git/train_data/test.h5"")
converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.change_concat_input_ranges = True
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tfliteModel = converter.convert()
with open(""/home/anurag/git/train_data/test.tflite"", 'wb') as outfile:
    outfile.write(tfliteModel)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- The latency in model is high due to the fact that atrous convolution is being broken down to spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d.
- Model produces correct results, but it is slower than expected.

This is the same as #29509. Issue was solved but appears again on newer releases."
53024,Error with higher batch size in stateful LSTM layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.9.5
- CUDA/cuDNN version: 11.2/8.1.1
- GPU model and memory: GTX 1070 Ti

**Describe the current behavior**

When using a stateful LSTM layer with any batch size > 32 I get an error:
```
batch_size = 33
input_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)
lstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)
lstm_input = tf.expand_dims(lstm_input, axis=1)
lstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)
lstm_out, h, c = lstm_layer(lstm_input)
out = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)
```
```
2021-11-10 19:32:33.397959: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]
```
Any batch size <= 32 works fine
Changing the LSTM layer to stateful=False and the Input layer to batch_size=None makes the issue disappear (changing only stateful to False does not)

**Describe the expected behavior**
No difference when changing the batch size.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np


batch_size = 33

input_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)
lstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)
lstm_input = tf.expand_dims(lstm_input, axis=1)
lstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)
lstm_out, h, c = lstm_layer(lstm_input)
out = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)

model = tf.keras.Model(
    inputs=[input_tensor],
    outputs=[out, h, c])
model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())
model.summary()

input_arr = np.random.random(size=(batch_size, 5))
model.predict([input_arr])
```
**Other info / logs** Include any logs or source code that would be helpful to
Executing the above program with any batch size > 32 gives the following error:
```
2021-11-10 19:45:47.216169: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-10 19:45:47.636164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6615 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(33, 5)]                 0         
                                                                 
 dense (Dense)               (33, 16)                  96        
                                                                 
 tf.expand_dims (TFOpLambda)  (33, 1, 16)              0         
                                                                 
 lstm (LSTM)                 [(33, 16),                2112      
                              (33, 16),                          
                              (33, 16)]                          
                                                                 
 dense_1 (Dense)             (33, 1)                   17        
                                                                 
=================================================================
Total params: 2,225
Trainable params: 2,225
Non-trainable params: 0
_________________________________________________________________
2021-11-10 19:45:48.667605: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]
Traceback (most recent call last):
  File ""***\PyCharmCE2021.2\scratches\scratch.py"", line 22, in <module>
    model.predict([input_arr])
  File ""***\venv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""***\venv\lib\site-packages\tensorflow\python\eager\execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:    Invalid input_h shape: [1,33,16] [1,32,16]
	 [[{{node CudnnRNN}}]]
	 [[model/lstm/PartitionedCall]] [Op:__inference_predict_function_1045]

Function call stack:
predict_function -> predict_function -> predict_function


Process finished with exit code 1
```
"
53021,unit test //tensorflow/compiler/xla/tests:local_client_aot_test fails to build on AARCH64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build fails with -
2021-11-10 12:32:55.390296: F tensorflow/compiler/xla/tests/local_client_aot_test_helper.cc:81] unsupported TARGET_CPU: aarch64

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --verbose_failures -- //tensorflow/compiler/xla/tests/...

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

ERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/compiler/xla/tests/BUILD:199:8: Executing genrule //tensorflow/compiler/xla/tests:local_client_aot_test_computation failed (Aborted): bash failed: error executing command 
(cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \

exec env - \

PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \

PYTHON_BIN_PATH=/usr/bin/python3 \

PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \

TF2_BEHAVIOR=1 \

/bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_helper aarch64 > bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_computation.o')
Execution platform: @local_execution_config_platform//:platform
2021-11-10 12:32:55.390296: F tensorflow/compiler/xla/tests/local_client_aot_test_helper.cc:81] unsupported TARGET_CPU: aarch64
/bin/bash: line 1: 729298 Aborted                 (core dumped) bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_helper aarch64 > bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_computation.o
INFO: Elapsed time: 579.863s, Critical Path: 253.00s
INFO: 5482 processes: 2358 internal, 3124 local.
FAILED: Build did NOT complete successfully"
53019,Adding new op  to HLO and add tf to HLO lowering ,"I want to lower tf.leakyrelu to the new HLO op mhlo.leaky_relu. For this,
I created new op in mhlo for leakyrelu in iree/third_party/tensorflow/tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.td
Then in iree/third_party/tensorflow/tensorflow/compiler/mlir/xla/transforms/legalize_tf.cc updated the lowering for tf.leakyrelu to mhlo.leakyrelu 

But when I run bazel build iree_tf_compiler:importer-binaries getting error as 
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/552a1865bc9b933b5dd3d8c3ed93b3cd/external/org_tensorflow/tensorflow/compiler/mlir/xla/BUILD:442:11: C++ compilation of rule '@org_tensorflow//tensorflow/compiler/mlir/xla:mlir_hlo_to_hlo' failed (Exit 1): clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 319 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 319 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from external/org_tensorflow/tensorflow/compiler/mlir/xla/mlir_hlo_to_hlo.cc:1274:
bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/operator_writers.inc:483:26: error: no member named 'LeakyRelu' in namespace 'xla'
  auto xla_result = xla::LeakyRelu(Unwrap(xla_arg_0), Unwrap(xla_arg_1));
                    ~~~~~^
1 error generated.
Target //iree_tf_compiler:importer-binaries failed to build


Which are the files do I need to update if I want to create lowering to new hlo op.
"
53018,Cannot open include file with meta_graph.pb.h after compiling tensorflow c++ from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

----
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
```
        *
        Version	Windows 10 Pro
        Version	21H1
        Install	‎2021/‎8/‎30
        OS build	19043.1319
        Experience	Windows Feature Experience Pack 120.2212.3920.0
        *
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): install from source
- TensorFlow version: 2.5.0
- Python version: 3.8.11
- Installed using virtualenv? pip? conda?: Anaconda
```
        *
        > python
        Python 3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
        > conda -V
        conda 4.10.3
        *
```
- Bazel version (if compiling from source): 3.7.2
```
        *
        > bazel version
        Build label: 3.7.2
        Build target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
        Build time: Thu Dec 17 17:02:17 2020 (1608224537)
        Build timestamp: 1608224537
        Build timestamp as int: 1608224537
        *
```
- GCC/Compiler version (if compiling from source): MSVC 2019
```
        *
        >cl.exe
        Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30136 for x86
        Copyright (C) Microsoft Corporation.  All rights reserved.
        
        usage: cl [ option... ] filename... [ /link linkoption... ]
        *
```
- CUDA/cuDNN version: 11.2.67 / 8.1.1.33
```
        *
        > nvcc --version
        nvcc: NVIDIA (R) Cuda compiler driver
        Copyright (c) 2005-2020 NVIDIA Corporation
        Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
        Cuda compilation tools, release 11.2, V11.2.67
        Build cuda_11.2.r11.2/compiler.29373293_0
        *
```
- GPU model and memory: NVIDIA Geforce RTX 3080 Ti

----
**Describe the problem**
I want to use tensorflow c++ in my project with pretrained customized keras functional api model. I followed some step to convert .h5 model to .pb file. Then I followed [this step](https://stackoverflow.com/questions/59013401/run-tensorflow-model-in-cpp) (answered by @tensorflow Support). But I got an error at first line.
![image](https://user-images.githubusercontent.com/41325962/141092204-8667f227-80fb-4b23-aee0-2a6fa04c3f6a.png)

```
        *
        1>------ Build started: Project: tf_pb, Configuration: Release x64 ------
        1>Source.cpp
        1>D:\tf_model\tf_pb\include\tensorflow\core\protobuf\meta_graph.pb.h(10,10): fatal error C1083: Cannot open include file: 'google/protobuf/port_def.inc': No such file or directory
        1>Done building project ""tf_pb.vcxproj"" -- FAILED.
        *
```
I thought that might be caused by incorrect build process.

----
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Before I installed tensorflow c++ from source, I had already installed (python) tensorflow==2.5.0, tensorflow-gpu==2.5.0, tf-nightly-gpu 20210927 dev.
Steps:
```
        *
        > git clone https://github.com/tensorflow/tensorflow.git
        > cd tensorflow
        > git checkout r2.5
        *
```
```
        *
        > python configure.py
        You have bazel 3.7.2 installed.
        Please specify the location of python. [Default is C:\Users\User\.conda\envs\cuda11\python.exe]:
        
        
        Found possible Python library paths:
          C:\Users\User\.conda\envs\cuda11\lib\site-packages
        Please input the desired Python library path to use.  Default is [C:\Users\User\.conda\envs\cuda11\lib\site-packages]
        
        Do you wish to build TensorFlow with ROCm support? [y/N]: N
        No ROCm support will be enabled for TensorFlow.
        
        Do you wish to build TensorFlow with CUDA support? [y/N]: y
        CUDA support will be enabled for TensorFlow.
        
        Found CUDA 11.2 in:
            C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64
            C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include
        Found cuDNN 8 in:
            C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64
            C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include
        
        
        Please specify a list of comma-separated CUDA compute capabilities you want to build with.
        You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
        Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6
        
        
        Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
        
        
        Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n
        Not overriding eigen strong inline, some compilations could take more than 20 mins.
        
        Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
        Not configuring the WORKSPACE for Android builds.
        
        Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
                --config=mkl            # Build with MKL support.
                --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
                --config=monolithic     # Config for mostly static monolithic build.
                --config=numa           # Build with NUMA support.
                --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
                --config=v2             # Build TensorFlow 2.x instead of 1.x.
        Preconfigured Bazel build configs to DISABLE default on features:
                --config=noaws          # Disable AWS S3 filesystem support.
                --config=nogcp          # Disable GCP support.
                --config=nohdfs         # Disable HDFS support.
                --config=nonccl         # Disable NVIDIA NCCL support.
        *
```
Then compile with this command
```
        *
        bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
        *
```
Waited for about **16 hours compiling time**.
It showed ""successful completed"" after quite a long time.
But when I tried to include the library...
`#include<tensorflow/core/protobuf/meta_graph.pb.h>`
showed
```
        *
        Build started...
        1>------ Build started: Project: tf_pb, Configuration: Release x64 ------
        1>Source.cpp
        1>D:\tf_model\tf_pb\include\tensorflow\core\protobuf\meta_graph.pb.h(10,10): fatal error C1083: Cannot open include file: 'google/protobuf/port_def.inc': No such file or directory
        1>Done building project ""tf_pb.vcxproj"" -- FAILED.
        
        Waiting for help!. Thanks.
        ========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========
        *
```

----
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53017,Interconnect Issue + NaN Error,"When I run python train.py (attached here) it does not give a Y next to the GPU when Interconnect Matrix is checked. Following which it runs the CPU version, and NaN error is attained after a few steps. i have not seen any such issue when training on NVIDIA Tesla K40c (attached as sm25epc.log) or Tesla T4 (attached as nohup.txt
[nohup.txt](https://github.com/tensorflow/tensorflow/files/7510510/nohup.txt)
[sm25epc.log](https://github.com/tensorflow/tensorflow/files/7510523/sm25epc.log)
[train.txt](https://github.com/tensorflow/tensorflow/files/7510530/train.txt)
)

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS

- TensorFlow installed from (source or binary): Anaconda (GPU)
- TensorFlow version: latest version (I'm guessing 2.5 above) I installed using conda create -n tf-gpu or something similar last week
- Python version: 3.9.7
- Installed using virtualenv? pip? conda?: Anaconda (2021.05 release)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0  
- GPU model and memory: A100-SXM4-40GB


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53013,"tflite-model-maker: audio_classifier's DataLoader loads .wav files as int32, resulting in TypeError when training model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (use command below): 2.7.0 (also repeated on 2.6 and 2.5)
- Python version: 3.8

**Describe the current behavior**

Following this tutorial: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_audio_classification.ipynb

When using my own .wav files (16kHz, Mono, float32) as the test and train data, the built in DataLoader function (audio_classifier.DataLoader) loads the arrays as int32's, which leads to this TypeError when I attempt to train the model:

```""TypeError: Input 'y' of 'FloorDiv' Op has type float32 that does not match type int32 of argument 'x'."" ```

This is because the YAMnet model expects float32 as input. However, I can use the exact same .wav files as input into the TF Lite YAMnet, and the model processes an output array (1, 521) as expected - meaning these .wav files are in the correct format. It is only when I attempt to train the model via this transfer learning tutorial that this error appears, which leads me to believe it's a bug with the DataLoader. 

**Describe the expected behavior**

The DataLoader should process .wav files as float32s, and therefore allow model training.

**Standalone code to reproduce the issue**

my wav files: https://storage.googleapis.com/glass_net_wavs_zip/glass_data.zip

```python

from tflite_model_maker import audio_classifier
import os

data_dir = os.getcwd() + '\\glass_data\\sample_data'

spec = audio_classifier.YamNetSpec(
    keep_yamnet_and_custom_heads=False,
    frame_step=0.5 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,
    frame_length=1 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)

train_data = audio_classifier.DataLoader.from_folder(spec, os.path.join(data_dir, 'train'), cache=True)
test_data = audio_classifier.DataLoader.from_folder(spec, os.path.join(data_dir, 'test'), cache=True)
train_data, validation_data = train_data.split(0.8)

# data = audio_classifier.DataLoader.from_folder(spec=spec, data_path=data_dir)
# train_data, rest_data = data.split(0.8)
# validation_data, test_data = rest_data.split(0.5)

batch_size = 16
epochs = 100

print('Training the model')
model = audio_classifier.create(
    train_data,
    spec,
    validation_data,
    batch_size=batch_size,
    epochs=epochs)

model.evaluate(test_data)

```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""<input>"", line 2, in <module>
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 148, in create
    task.train(train_data, validation_data, epochs, batch_size)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 55, in train
    train_ds, _ = self._get_dataset_and_steps(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\audio_classifier.py"", line 42, in _get_dataset_and_steps
    dataset = tf.distribute.get_strategy().distribute_datasets_from_function(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1159, in distribute_datasets_from_function
    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 3574, in _distribute_datasets_from_function
    return dataset_fn(InputContext())
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\custom_model.py"", line 81, in _dataset_fn
    dataset = input_data.gen_dataset(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\data_util\audio_dataloader.py"", line 360, in gen_dataset
    ds = spec.preprocess_ds(ds, is_training=is_training, cache_fn=_cache_fn)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py"", line 523, in preprocess_ds
    ds = ds.map(self._frame, num_parallel_calls=autotune).unbatch()
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1927, in map
    return ParallelMapDataset(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 4522, in __init__
    self._map_func = StructuredFunctionWrapper(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3712, in __init__
    self._function = fn_factory()
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3134, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3100, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3687, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3617, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 763, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\eager\function.py"", line 3971, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\audio_spec.py:497 _frame  *
        clips = tf.signal.frame(
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\signal\shape_ops.py:181 frame
        0, 1 + (length_samples - frame_length) // frame_step)
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1250 binary_op_wrapper
        raise e
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1234 binary_op_wrapper
        return func(x, y, name=name)
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\util\dispatch.py:206 wrapper
        return target(*args, **kwargs)
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\math_ops.py:1526 floordiv
        return gen_math_ops.floor_div(x, y, name=name)
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py:3796 floor_div
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    C:\Users\ghosinsk\anaconda3\envs\GlassNet_v3\lib\site-packages\tensorflow\python\framework\op_def_library.py:555 _apply_op_helper
        raise TypeError(
    TypeError: Input 'y' of 'FloorDiv' Op has type float32 that does not match type int32 of argument 'x'.
```




"
53010,New problem in TF 2.6 that does not appears in 2.3.1 (TypeError: Input must be a SparseTensor.),"Hi, 
I've been migrating our tensorflow images from 2.3 to 2.6. To my surprise, I've found bugs in the code that didn't appear before. The error is as follows: 
```bash
TypeError: in user code:

    <ipython-input-108-26abd43eddb2>:11 parse_function  *
        values = [float(_fill_in_missing(value)) for value in values]
    <ipython-input-110-f4a86837955a>:92 _fill_in_missing  *
        return tf.sparse.to_dense(x, default_value)
    /opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/sparse_ops.py:1711 sparse_tensor_to_dense  **
        sp_input = _convert_to_sparse_tensor(sp_input)
    /opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/sparse_ops.py:71 _convert_to_sparse_tensor
        raise TypeError(""Input must be a SparseTensor."")

    TypeError: Input must be a SparseTensor.
```

The functions that return this error are the following: 
```python
def parse_function(example_proto):
    '''Parse the values from tf examples'''
    feature_spec = tf_transform_output.transformed_feature_spec()
    features = tf.io.parse_single_example(example_proto, feature_spec)
    values = list(features.values())
    values = [float(_fill_in_missing(value)) for value in values]  
    features = tf.stack(values, axis=0)
    return features
def _fill_in_missing(x):
    """"""Replace missing values in a SparseTensor.
    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.
    Args:
    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1
      in the second dimension.
    Returns:
    A rank 1 tensor where missing values of `x` have been filled in.
    """"""
    default_value = '' if x.dtype == tf.string else 0
    return tf.sparse.to_dense(x, default_value)

def get_dataset(path):
    '''Get the dataset and group them into windows'''

    dataset = tf.data.TFRecordDataset(path, compression_type=""GZIP"")
    dataset = dataset.map(parse_function)
    dataset = dataset.window(HISTORY_SIZE, shift=SHIFT, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE))
    dataset = dataset.map(add_mode)
    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
    dataset = dataset.repeat()
    
    return dataset
```

I've tried ignoring the _fill_in_missing function and some new errors appears related to the batch (these errors also don't appear the TF 2.3 version): 
`Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [2,1], [batch]: [1] [Op:IteratorGetNext]`

It's interesting that the previous batches can be used without any problem, but the last batch has a problem and I don't find a way to fix it. 


Older versions (in which the code run without problems):
TF version: 2.3.1
Beam version: 2.28.0
TFMA version: 0.24.3
TFT version: 0.24.1
TFX version: 0.24.0

Current Versions:
tfx-bsl==1.3.0
tensorflow-model-analysis==0.34.1
tensorflow-data-validation==1.3.0
tfx==1.3.2
tensorflow-metadata==1.2.0
ml-metadata==1.3.0
tensorflow-transform==1.3.0
tensorflow==2.6.2
pyarrow==2.0.0
apache-beam==2.32.0
tensorflow-serving-api==2.6.0
tensorflow-estimator==2.6.0"
53009,model.fit() never calls custom metric,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu / OSX
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0, v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.7, 3.9

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Following the [documentation](https://keras.io/api/metrics/) I'm trying to implement a custom metric, however the metric never gets called in the first place. I added sanity checks that should produce errors when the metric is called. You can find the full example in this [notebook][1]

    def my_metric_fn(y_true, y_pred):
        1 / 0
        while True:
            pass
        squared_difference = tf.square(y_true - y_pred)
        return tf.reduce_mean(squared_difference, axis=-1) 

The code runs without a problem after passing the metric

    model.compile(optimizer=opt, metrics=[my_metric_fn])
    history = model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=epochs,
    callbacks=[early_stopping]
    )

What I actually get:

    Epoch 1/100
    59/59 [==============================] - 27s 451ms/step - loss: 16.3928 - my_metric_fn: 0.0000e+00 - val_loss: 16.5252 - val_my_metric_fn: 0.0000e+00
    Epoch 2/100
    59/59 [==============================] - 25s 420ms/step - loss: 16.3508 - my_metric_fn: 0.0000e+00 - val_loss: 16.5316 - val_my_metric_fn: 0.0000e+00
    Epoch 3/100
    59/59 [==============================] - 25s 420ms/step - loss: 16.3420 - my_metric_fn: 0.0000e+00 - val_loss: 16.5372 - val_my_metric_fn: 0.0000e+00
    Epoch 4/100
    59/59 [==============================] - 25s 417ms/step - loss: 16.3365 - my_metric_fn: 0.0000e+00 - val_loss: 16.5287 - val_my_metric_fn: 0.0000e+00
    Epoch 5/100
    59/59 [==============================] - 25s 418ms/step - loss: 16.3251 - my_metric_fn: 0.0000e+00 - val_loss: 16.5271 - val_my_metric_fn: 0.0000e+00


  [1]: https://colab.research.google.com/drive/1YohoIRqUWuvAQvEot5cFXWRfAPxm82cm?usp=sharing

**Describe the expected behavior**

The metric should be called followed by an error.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

I included a notebook earlier

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
53008,MirroredStrategy fails with RaggedTensor,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: YES
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Education N 1903
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: None
-   **TensorFlow installed from (source or binary)**: pip install
-   **TensorFlow version (use command below)**: 2.6
-   **Python version**: 3.9.7
-   **Bazel version (if compiling from source)**: None
-   **GCC/Compiler version (if compiling from source)**: None
-   **CUDA/cuDNN version**:  11.2 / 8.1.0
-   **GPU model and memory**: Nvidia GTX 1080 Ti 11GB
-   **Exact command to reproduce**:

```bash
python name_of_file.py
```

### Describe the problem
The usage of any RaggedTensor in the model to train fails with the MirroredStrategy, whereas is works when there is no strategy. The code described bellow is a toy example, the ragged tensor is just included in the model graph by a tf.print but of course the problem is the same (and original one) when it is included in the calculations for the output of the model.

### Source code / logs

```bash
import tensorflow as tf

@tf.function
def loss(a,b):
    return tf.reduce_mean(tf.abs(a), axis=1)

class FailL(tf.keras.layers.Layer):
    def __init__(self):
        super(FailL, self).__init__()
    
    def call(self, inputs):
        tf.print(tf.ragged.constant([[1],[1,1]]))
        return inputs

class FailM(tf.keras.Model):
    def __init__(self, strategy):
        super(FailM, self).__init__()
        self.strategy = strategy
        if self.strategy is not None:
            with self.strategy.scope():
                self.layer1 = tf.keras.layers.Conv2D(1,[3,3])
                self.layer2 = FailL()
        else:
            self.layer1 = tf.keras.layers.Conv2D(1,[3,3])
            self.layer2 = FailL()
    
    @tf.function
    def call(self, inputs):
        return self.layer2(self.layer1(inputs))
    
    def compile(self):
        if self.strategy is not None:
            with self.strategy.scope():
                super(FailM, self).compile()
                self.loss = loss
                self.optimizer = tf.keras.optimizers.Adam()
        else:
            super(FailM, self).compile()
            self.loss = loss
            self.optimizer = tf.keras.optimizers.Adam()
            
    def train_step(self, data):
        with tf.GradientTape() as tape:
            rag = self.layer2(self.layer1(data))
            loss = self.loss(rag,0)
        grads = tape.gradient(loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {""loss"": loss}
    
    @tf.function
    def distributed_train_step(self, data):
        per_replica_losses = self.strategy.run(self.train_step, args=(data,))
        return {prl: self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[prl], axis=None) for prl in per_replica_losses}
    
    def choose_train_step(self, data):
        if self.strategy is None:
            return self.train_step(data)
        else:
            return self.distributed_train_step(data)

for choose_strat in [None, 
                     tf.distribute.MirroredStrategy(devices=['GPU:0']),
                     ]:
    tf.print('Try with a strategy: ', type(choose_strat))
    model = FailM(choose_strat)
    model.compile()
    res = model.choose_train_step(tf.ones([3,10,10,3]))
    tf.print('Result:', res)
```

And then run it

TRACEBACK::

```bash
Try with a strategy:  <class 'tensorflow.python.distribute.mirrored_strategy.MirroredStrategy'>
Traceback (most recent call last):
  File ""C:\..\trash_test_ragfail.py"", line 74, in <module>
    res = model.choose_train_step(tf.ones([3,10,10,3]))
  File ""C:\..\trash_test_ragfail.py"", line 68, in choose_train_step
    return self.distributed_train_step(data)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\def_function.py"", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\function.py"", line 3039, in __call__
    return graph_function._call_flat(
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\function.py"", line 591, in call
    outputs = execute.execute(
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\tf2_6\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
  (1) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
0 successful operations.
0 derived errors ignored.
         [[{{node test_l_1/StringFormat_1/AsString/map/TensorArrayUnstack/TensorListFromTensor/_18}}]]
         [[Func/test_l_1/StringFormat_1/AsString/map/while/body/_1/input/_59/_32]]
  (1) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
  (1) Invalid argument: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
0 successful operations.
0 derived errors ignored.
         [[{{node test_l_1/StringFormat_1/AsString/map/TensorArrayUnstack/TensorListFromTensor/_18}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_train_step_687]

Function call stack:
distributed_train_step -> distributed_train_step
```"
53007,Issue in import tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53006,py_function isn't working with CompositeTensor (especially TensorArray) tf 2.7.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.6 LTS
- TensorFlow installed from (source or binary): binary (pip install tensorflow)
- TensorFlow version (use command below): .v2.7.0-rc1-69-gc256c071bb2 2.7.0
- Python version: 3.7
- CUDA/cuDNN version: 11.4
- GPU model and memory: RTX 2080 Ti 11Go

**Describe the current behavior**
Hi ! When using `tf.py_function()` with `Tout` a composite Tensor. I get an `ValueError : Attempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff622c6e850>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor` 

**Describe the expected behavior**
I should get a ArrayTensor (or a composite Tensor as specifed in the `tf.py_function` documentation, but i didn't test other composite Tensor). Auto-graph is working but when calling the iterator it fail.

**Standalone code to reproduce the issue**
```python

import tensorflow as tf

dataset = tf.data.Dataset.range(1)


@tf.function
def exemple_tf(sample):
    array = tf.py_function(exemple_py, [sample],
                           Tout=tf.TensorArraySpec(element_shape=[None, 2],
                                                   dtype=tf.dtypes.float32,
                                                   dynamic_size=False,
                                                   infer_shape=True), name=""exemple"")
    return sample


def exemple_py(sample):
    tensor_array = tf.TensorArray(dtype=tf.float32,
                                  size=1,
                                  dynamic_size=False,
                                  clear_after_read=False,
                                  infer_shape=False,
                                  element_shape=[None, 2])
    return tensor_array


dataset = dataset.map(exemple_tf) # Work here

for data in dataset:
    print(data) # Fail here

```

**Other info / logs**
 ```

2021-11-09 14:01:14.983740: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Attempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fab6045b910>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.
Traceback (most recent call last):

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 273, in __call__
    return func(device, token, args)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 151, in __call__
    outputs = self._call(device, args)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 170, in _call
    for (x, dtype) in zip(ret, self._out_dtypes)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 170, in <listcomp>
    for (x, dtype) in zip(ret, self._out_dtypes)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 135, in _convert
    return ops.convert_to_tensor(value, dtype=dtype)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1621, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 347, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 272, in constant
    allow_broadcast=True)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 308, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 106, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)

ValueError: Attempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fab6045b910>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.


Traceback (most recent call last):
  File ""/home/ydupas/PycharmProjects/ExperimentArea/test py_function bug.py"", line 27, in <module>
    for data in dataset:
  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 800, in __next__
    return self._next_internal()
  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 786, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2845, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: Attempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fab6045b910>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.
Traceback (most recent call last):

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 273, in __call__
    return func(device, token, args)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 151, in __call__
    outputs = self._call(device, args)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 170, in _call
    for (x, dtype) in zip(ret, self._out_dtypes)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 170, in <listcomp>
    for (x, dtype) in zip(ret, self._out_dtypes)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 135, in _convert
    return ops.convert_to_tensor(value, dtype=dtype)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped
    return func(*args, **kwargs)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1621, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 347, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 272, in constant
    allow_broadcast=True)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 308, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)

  File ""/home/environment/TensorFlow_270_37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 106, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)

ValueError: Attempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fab6045b910>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.


	 [[{{function_node __inference_exemple_tf_12}}{{node exemple}}]] [Op:IteratorGetNext]

Process finished with exit code 1

```
"
53003,[tflite] any way to extract operator attributes from interpreter,"
**System information**
- python 3.8.11
- TensorFlow 2.5.1
- Are you willing to contribute it: No

**Describe the feature and the current behavior/state.**
With current APIs I can extract operator details with
> ip = tf.lite.Interpreter(model_path=tflite_file)
> ip._get_ops_details()

I could get (for example):

> {'index': 0,
>   'op_name': 'CONV_2D',
>   'inputs': array([0, 4, 2], dtype=int32),
>   'outputs': array([13], dtype=int32)}

I would expect another key ""builtin_option"" in this dict to expose all attributes of this layer contained in flatbuffer, e.g. fused_activation_function, padding, stride_h/w etc.
Currently, I'm parsing a tflite model and automating code preparation for an edge device with a customized DNN accelerator. Those attributes data is important to configure hardware components. Now I'm parsing the tflite model with flatbuffer API to extract those attributes, but the code is bloated and ugly, so I expect that the interpreter._get_ops_details() can include this part.

**Will this change the current api? How?**
The required ""attributes"" key doesn't change the current API.
In `_get_ops_details()` of `interpreter.py`, a new line like
> op_attributes = self._interpreter.NodeAttributes(op_index)

could be added.
**Who will benefit with this feature?**
Anyone who only wants to take the single tflite model as the entry and easily parse both tensors and operators content for their own backend.
**Any Other info.**
I'm still new to tensorflow, if anyone can extract operators in a easy way, please leave comments below :) "
53002,TF2.6 compile error with custom eigen repo,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.6
- Python version: Python 3.6.8 
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): gcc version 6.5.1 20190307
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Hi, guys,
I want to compile tensorflow r2.6 with a custom eigen. But it occurs errors. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
**TF r2.6 is compiled successfully.**
```
#bazel build  //tensorflow/tools/pip_package:build_pip_package
...
INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (430 packages loaded, 26907 targets configured).
INFO: Found 1 target...
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 634.229s, Critical Path: 344.77s
INFO: 7345 processes: 387 internal, 6958 local.
INFO: Build completed successfully, 7345 total actions
```

Based on the same environment, I modified  eigen `workspace.bzl` of TF r26.
```
--- a/third_party/eigen3/workspace.bzl
+++ b/third_party/eigen3/workspace.bzl
@@ -9,13 +9,19 @@ def repo():
     EIGEN_COMMIT = ""12e8d57108c50d8a63605c6eb0144c838c128337""
     EIGEN_SHA256 = ""f689246e342c3955af48d26ce74ac34d21b579a00675c341721a735937919b02""
 
-    tf_http_archive(
-        name = ""eigen_archive"",
-        build_file = ""//third_party/eigen3:eigen_archive.BUILD"",
-        sha256 = EIGEN_SHA256,
-        strip_prefix = ""eigen-{commit}"".format(commit = EIGEN_COMMIT),
-        urls = [
-            ""https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz"".format(commit = EIGEN_COMMIT),
-            ""https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz"".format(commit = EIGEN_COMMIT),
-        ],
-    )
+    native.new_local_repository(
+       name = ""eigen_archive"",
+       path = ""/mnt/eigen"",
+       build_file = ""//third_party/eigen3:eigen_archive.BUILD"",
+    )
```

Then I checkout the eigen repo to the same commit id `12e8d57108c50d8a63605c6eb0144c838c128337`
```
$ cd /mnt/eigen
$ git remote -vvv
origin  https://gitlab.com/libeigen/eigen.git (fetch)
origin  https://gitlab.com/libeigen/eigen.git (push)
$ git checkout -b current 12e8d57108c50d8a63605c6eb0144c838c128337
```
But got an error:
```
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (430 packages loaded, 26907 targets configured).
INFO: Found 1 target...
ERROR: /tensorflow/tensorflow/core/grappler/costs/BUILD:67:11: C++ compilation of rule '//tensorflow/core/grappler/costs:graph_properties' failed (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib64::/usr/lib/python2.7/site-packages/tensorflow \
    PATH=/opt/zookeeper/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/tops/bin:/opt/zookeeper/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/tops/bin:/usr/X11R6/bin:/opt/satools \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/tops/bin/python3 \
    PYTHON_LIB_PATH=/home/tops/lib/python3.6/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.o' -fPIC -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/snappy -iquotebazel-out/k8-opt/bin/external/snappy -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/grappler/costs/graph_properties.cc -o bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/graph_properties/graph_properties.pic.o)
Execution platform: @local_execution_config_platform//:platform
tensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::{anonymous}::DisjointSet<Handle>::Merge(Handle, Handle)':
tensorflow/core/grappler/costs/graph_properties.cc:236:30: error: type/value mismatch at argument 1 in template parameter list for 'template<class> struct std::rank'
   if (x_root->rank < y_root->rank) {
                              ^~~~
tensorflow/core/grappler/costs/graph_properties.cc:236:30: note:   expected a type, got 'y_root->.rank'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 74.775s, Critical Path: 63.70s
INFO: 457 processes: 97 internal, 360 local.
FAILED: Build did NOT complete successfully
```

It really confused me. The error occurs in `tensorflow/core/grappler/costs/graph_properties.cc`.
Any response is welcomed. Thank you.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
53001,Can I init a XlaBuilder using a existing XlaComputation？,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**

It will be useful to modify a existing  HloModule with XlaBuilder Operation Semantics"
53000,Wrong Tensorflow Lite link to hexagon_nn_skel.run v1.21 (actually is pointing to the v1.20.0.1),"Hi, I want to download the hexagon_nn_skel.run v1.21. In the Tensorflow Lite Hexagon guide there are many links to many versions: https://www.tensorflow.org/lite/performance/hexagon_delegate. The problem is that v1.21 is pointing to v1.20. So the question is where can I find the version 1.21 of hexagon_nn_skel.run?
Also, in the guide it is specified that we must use the hexagon_nn libraries with the compatible version of interface library, as stated in bazel config. From bazel config (https://github.com/tensorflow/tensorflow/blob/master/third_party/hexagon/workspace.bzl) I understand that I must use the version 20.0.3 of the interface. So where can I find the version 20.0.3 of the interface library?"
52999,How to specify the output layers during model conversion to TFLite in TF2?,"**Describe the problem**
Is there any way to remove the TFLite_Detection_PostProcess in tf2 ssd mobilenet model during conversion to tflite model？In tf1, there is a option to do so, but it seems that in tf2 the option for specifying the output layers is removed. 

In tf1, I can specify the output layers by putting the name in output_array:
input_arrays=[""normalized_input_image_tensor""]
output_arrays=[""raw_outputs/box_encodings"",""raw_outputs/class_predictions""]
input_tensor={""normalized_input_image_tensor"":[1,320,320,1]}

import tensorflow as tf
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('tflite_graph.pb',input_shapes = input_tensor,
input_arrays = input_arrays ,output_arrays = output_arrays)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
open('ssd_mobilenet_v2_tf1.tflite', ""wb"").write(tflite_model)
"
52994,Unable to use tensorflow after installing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I try to install tensorflow and it just says all of the requirements are already satisfied but it throws an error everytime I try to use tensorflow.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**

C:\Users\josep\Videos>pip3 install --user --upgrade tensorflow
Requirement already up-to-date: tensorflow in c:\users\josep\anaconda3\lib\site-packages (2.7.0)
Requirement already satisfied, skipping upgrade: absl-py>=0.4.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (0.15.0)
Requirement already satisfied, skipping upgrade: flatbuffers<3.0,>=1.12 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.12)
Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.12.1)
Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.41.1)
Requirement already satisfied, skipping upgrade: gast<0.5.0,>=0.2.1 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (0.4.0)
Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.32.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (0.37.0)
Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (3.1.0)
Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (2.7.0)
Requirement already satisfied, skipping upgrade: keras<2.8,>=2.7.0rc0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (2.7.0)
Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.21.4)
Requirement already satisfied, skipping upgrade: libclang>=9.0.1 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (12.0.0)
Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (0.2.0)
Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (3.19.1)
Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.21.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (0.21.0)
Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.16.0)
Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (3.3.0)
Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.6 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (3.7.4.3)
Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.6.3)
Requirement already satisfied, skipping upgrade: tensorboard~=2.6 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (2.7.0)
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.1 in c:\users\josep\anaconda3\lib\site-packages (from tensorflow) (1.1.2)
Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)
Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (58.5.3)
Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)
Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)
Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)
Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)
Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)
Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\users\josep\anaconda3\lib\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)
Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\users\josep\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)
Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\users\josep\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)
Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\users\josep\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)
Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\users\josep\anaconda3\lib\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)
Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\users\josep\anaconda3\lib\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)
Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= ""3.6"" in c:\users\josep\anaconda3\lib\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)
Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\users\josep\anaconda3\lib\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)
Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\users\josep\anaconda3\lib\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)
Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\users\josep\anaconda3\lib\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)
Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\users\josep\anaconda3\lib\site-packages (from rsa<5,>=3.1.4; python_version >= ""3.6""->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)


C:\Users\josep\Videos>python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
Traceback (most recent call last):
  File ""C:\Users\josep\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\josep\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\josep\anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\josep\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 79, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\josep\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
"
52992,Providing OS thread ids when tracing,"**System information**

 - Debian 9
 - python 3.5
 - TensorFlow 2.0.0

**Are you willing to contribute it (Yes/No):**

Yes

**Describe the feature and the current behavior/state.**

Currently, tracing only reports internal ids for threads:

```json
{
    ""ph"": ""t"",
    ""cat"": ""DataFlow"",
    ""name"": ""bert/encoder/layer_0/output/dense/bias"",
    ""pid"": 1,
    ""tid"": 5,
    ""ts"": 1635482148893406,
    ""id"": 1
}
```

It would helpful to be able to also report the OS thread id. Unfortunately, `threading.enumerate` does not appear to report these threads. Examining the runtime from the OS also does not have enough information to identify which is the executing thread.

**Will this change the current api? How?**

I am not quite sure of all the implications but I am imagining a boolean argument to enable collection of the ids:

```python
# hook for estimator
hook = tf.estimator.ProfilerHook(save_secs=1, output_dir=output, os_ids=True)

# tensorboard callback
callback = tf.keras.callbacks.TensorBoard(log_dir = logs,
                                                 histogram_freq = 1,
                                                 profile_batch = '500,520',
                                                 os_ids=True)
```

In addition, downstream concurrent code such as threads, pools, etc would need to changed to grab the id, probably with `getppid`. Finally, the tracing data structures need to be updated to include this field. The new trace output could look something like:

```json
{
    ""ph"": ""t"",
    ""cat"": ""DataFlow"",
    ""name"": ""bert/encoder/layer_0/output/dense/bias"",
    ""pid"": 1,
    ""tid"": 5,
    ""os_pid"": 120311,
    ""ts"": 1635482148893406,
    ""id"": 1
}
```

I am not 100% clear on all the consequences but I am very willing to dig into this once I know where to look.

**Who will benefit with this feature?**

This would allow additional context to be attached to the trace. For example, linux reports detailed task information through the `/proc` system, such as the executing cpu. This can then be attached to data that is broken down by cpu.
"
52989,Error in converting tensorflow model hdf5 to onnx format,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows on jupyter notebook
- TensorFlow version: 2.6.0
- Python version: 3.8

**Describe the problem**
I didn't reach to convert my hdf5 model to onnx format with tf2onnx.convert.
But, i tested the second method
**Provide the exact sequence of commands / steps that you executed before running into the problem**
tf2onnx error with the command: 
python -m tf2onnx.convert  --saved-model saved_model --inputs input_1:0 --outputs gaze_output:0 --output mobilenet.onnx
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-08 17:01:09,696 - WARNING - '--tag' not specified for saved_model. Using --tag serve
Traceback (most recent call last):
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tf2onnx\convert.py"", line 617, in <module>
    main()
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tf2onnx\convert.py"", line 228, in main
    use_graph_names=args.use_graph_names)
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tf2onnx\tf_loader.py"", line 612, in from_saved_model
    tag, signatures, concrete_function, large_model, use_graph_names)
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tf2onnx\tf_loader.py"", line 549, in _from_saved_model_v2
    imported = tf.saved_model.load(model_path, tags=tag)  # pylint: disable=no-value-for-parameter
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 864, in load
    result = load_internal(export_dir, tags, options)[""root""]
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 903, in load_internal
    ckpt_options, options, filters)
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 162, in __init__
    self._load_all()
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 259, in _load_all
    self._load_nodes()
  File ""D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tensorflow\python\saved_model\load.py"", line 448, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'

SECOND method:
loaded_model = tensorflow.keras.models.load_model('my_eye_tracking_model.h5')
onnx_model = tf2onnx.convert.from_keras(loaded_model)
WARNING:tensorflow:From D:\MyPrograms\anaconda3\envs\GazeEnv\lib\site-packages\tf2onnx\tf_loader.py:703: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`

But the onnx_model has the architecture with input and output

import onnx
onnx.save_model(onnx_model, ""test.onnx"")

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52988,TensorFlow 2.7 does not detect CUDA installed through conda,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory: GTX 2080Ti

**Describe the current behavior**

After installing cuda/cudnn through conda (`conda install cudatoolkit=11.2 cudnn=8.1`), TensorFlow 2.7 reports that it cannot find the cuda libraries.

```
2021-11-08 14:49:16.412959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-08 14:49:16.413006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-11-08 14:49:22.640508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.640617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.640698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.640776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.640853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.640941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.641022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.641099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-11-08 14:49:22.641120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and se
tup the required libraries for your platform.
```

Installing TensorFlow 2.6 (or earlier) in the same environment, with the same cuda/cudnn installation, doesn't show any problem, it detects the libraries and GPU support works as expected.

The problem can be worked around by manually adding the conda lib directory to `LD_LIBRARY_PATH` (`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib`). However, obviously this is not ideal, as it needs to be repeated/adjusted for every new conda environment. It would be better if TensorFlow just detected the conda installed libraries, as it did in TensorFlow < 2.7.

**Describe the expected behavior**

TensorFlow should detect cuda/cudnn libraries installed through `conda`, as it did in TensorFlow<2.7.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```
conda create -n tmp python=3.8
conda activate tmp
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1
pip install ""tensorflow==2.7.0""
python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""  # displays []
LD_LIBRARY_PATH=LD_LIBRARY_PATH:$CONDA_PREFIX/lib python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""  # displays [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
pip install ""tensorflow<2.7.0""
python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""  # displays [[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]]
```"
52987,Function _implements is missing when create a new object,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):debian10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.7.0
- Python version:3.9.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```_implements``` is missing in new objects.
**Describe the expected behavior**
```_implements``` exists.
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf

experimental_implements = "" "".join(['name: ""addons:MaxUnpooling2D""'])
@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32), tf.TensorSpec(shape=[], dtype=tf.float32)], experimental_implements = experimental_implements)
def test(a, b):
    return a+b



class Test(tf.Module):
    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32), tf.TensorSpec(shape=[], dtype=tf.float32)], experimental_implements = experimental_implements)
    def test(self, a, b):
        return a+b

model = Test()
model2 = Test()
model.test = test
assert model.test._implements == test._implements
assert model2.test._implements == Test.test._implements
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52986, Custom training with tf.distribute.Strategy fails with BatchNorm,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): using Docker image `tensorflow/tensorflow:2.5.0-gpu`
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: Python 3.8.0
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory:

```bash
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |
| 23%   43C    P0    69W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A |
| 23%   43C    P0    69W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 00000000:82:00.0 Off |                  N/A |
| 23%   37C    P0    57W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  TITAN X (Pascal)    Off  | 00000000:83:00.0 Off |                  N/A |
| 23%   42C    P0    67W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I tried adapting [this tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) on custom training loop with tf.distribute.Strategy, but I am facing an issue. I get the following exception if one of the replicas receives an empty batch:

```bash
AssertionError: in user code:

    /tmp/ipykernel_21821/3562653524.py:98 distributed_train_step  *
        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:678 _call_for_each_replica
        return mirrored_run.call_for_each_replica(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica
        return _call_for_each_replica(strategy, fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:245 _call_for_each_replica
        coord.join(threads)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /usr/local/lib/python3.8/dist-packages/six.py:703 reraise
        raise value
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:220 _call_for_each_replica
        merge_args = distribute_utils.regroup(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:62 regroup
        regrouped_tuple = tuple(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:63 <genexpr>
        regroup(tuple(v[i] for v in values), wrap_class, always_wrap)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:61 regroup
        assert len(v) == len(v0)

    AssertionError: 
```

I think this issue happens when one of of the GPU receives an empty per-replica batch. I tried reproducing this bug with the minimal example provided [here](https://www.tensorflow.org/tutorials/distribute/custom_training), and it didn't happen. I suppose it's because this issue happens only if our model uses **BatchNorm**. So, I adapted this minimal example by using a ResNet (provided in tf.keras.applications) as a model. My full code is available below.

**Describe the expected behavior**

This issue shouldn't happen. Also, I suppose this issue is raised because the **BatchNorm** statistics are computed locally, which can be a problem because each device would use different statistics.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

I couldn't reproduce this issue directly on Colab because it requires at least 2 GPUs:

```python
# Import TensorFlow
import tensorflow as tf

# Helper libraries
import numpy as np
import os

print(tf.__version__)

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), _ = fashion_mnist.load_data()

# Adding a dimension to the array -> new shape == (28, 28, 1)
# We are doing this because the first layer in our model is a convolutional
# layer and it requires a 4D input (batch_size, height, width, channels).
# batch_size dimension will be added later on.
train_images = train_images[..., None]

# Getting the images in [0, 1] range.
train_images = train_images / np.float32(255)

# Padding images because ResNet requires a miniaml shape of (32, 32)
padded_train_images = np.concatenate([
    np.zeros((len(train_images), 2, 28, 1)), 
    train_images, 
    np.zeros((len(train_images), 2, 28, 1))
], axis=1)
padded_train_images = np.concatenate([
    np.zeros((len(train_images), 32, 2, 1)), 
    padded_train_images, 
    np.zeros((len(train_images), 32, 2, 1))
], axis=2)

# If the list of devices is not specified in the
# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.
strategy = tf.distribute.MirroredStrategy()

print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))

BUFFER_SIZE = len(train_images)

BATCH_SIZE_PER_REPLICA = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

EPOCHS = 10

# We keep only the first images, so that the last GPU receives an empty batch
padded_train_images = padded_train_images[:strategy.num_replicas_in_sync-1]
train_labels = train_labels[:strategy.num_replicas_in_sync-1]

train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) 
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)

def create_model():
  inputs = tf.keras.Input((32, 32, 1))
  preprocessed = tf.keras.layers.Conv2D(3, (1, 1))(inputs) # ResNet requires 3 channels
  features = tf.keras.applications.ResNet50V2(
      include_top=False, 
      input_tensor=preprocessed, 
      pooling=""avg"", weights=None).output
  logits = tf.keras.layers.Dense(10)(features)
  return tf.keras.Model(inputs, features)

with strategy.scope():
  # Set reduction to `none` so we can do the reduction afterwards and divide by
  # global batch size.
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True,
      reduction=tf.keras.losses.Reduction.NONE)
  def compute_loss(labels, predictions):
    per_example_loss = loss_object(labels, predictions)
    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)

# model, optimizer, and checkpoint must be created under `strategy.scope`.
with strategy.scope():
  model = create_model()

  optimizer = tf.keras.optimizers.Adam()

  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

def train_step(inputs):
  images, labels = inputs

  with tf.GradientTape() as tape:
    predictions = model(images, training=True)
    loss = compute_loss(labels, predictions)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return loss 

# `run` replicates the provided computation and runs it
# with the distributed input.
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

for epoch in range(EPOCHS):
  # TRAIN LOOP
  total_loss = 0.0
  num_batches = 0
  for x in train_dist_dataset:
    total_loss += distributed_train_step(x)
    num_batches += 1
  train_loss = total_loss / num_batches

  print(f""Epoch {epoch+1}, Loss: {train_loss}"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Here is the full trace of the execution of the code above on 4 GPUs:

```bash
2021-11-08 14:37:06.516502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0

2.5.0
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.

2021-11-08 14:37:14.416516: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-11-08 14:37:14.609629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:14.610983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:14.612266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: 
pciBusID: 0000:82:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:14.613980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: 
pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:14.614027: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-11-08 14:37:14.619791: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-11-08 14:37:14.619850: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-11-08 14:37:14.620958: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-11-08 14:37:14.621767: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-11-08 14:37:14.622819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2021-11-08 14:37:14.623705: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-11-08 14:37:14.623948: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-11-08 14:37:14.651130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3
2021-11-08 14:37:14.652061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-08 14:37:15.287138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:15.289012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:15.290747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: 
pciBusID: 0000:82:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:15.292413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: 
pciBusID: 0000:83:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2021-11-08 14:37:15.303710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3
2021-11-08 14:37:15.303805: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-11-08 14:37:17.190690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-11-08 14:37:17.190759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 2 3 
2021-11-08 14:37:17.190771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y N N 
2021-11-08 14:37:17.190779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N N N 
2021-11-08 14:37:17.190786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 2:   N N N Y 
2021-11-08 14:37:17.190793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 3:   N N Y N 
2021-11-08 14:37:17.198780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11436 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)
2021-11-08 14:37:17.200356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11436 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)
2021-11-08 14:37:17.202384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11436 MB memory) -> physical GPU (device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0, compute capability: 6.1)
2021-11-08 14:37:17.203758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11436 MB memory) -> physical GPU (device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0, compute capability: 6.1)

INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
Number of devices: 4
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

2021-11-08 14:37:18.079248: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_DOUBLE
      type: DT_UINT8
    }
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 32
        }
        dim {
          size: 32
        }
        dim {
          size: 1
        }
      }
      shape {
      }
    }
  }
}

2021-11-08 14:37:21.185304: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-11-08 14:37:21.205000: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199965000 Hz

WARNING:tensorflow:Gradients do not exist for variables ['conv2_block1_preact_bn/gamma:0', 'conv2_block1_preact_bn/beta:0', 'conv2_block1_1_bn/gamma:0', 'conv2_block1_1_bn/beta:0', 'conv2_block1_2_bn/gamma:0', 'conv2_block1_2_bn/beta:0', 'conv2_block2_preact_bn/gamma:0', 'conv2_block2_preact_bn/beta:0', 'conv2_block2_1_bn/gamma:0', 'conv2_block2_1_bn/beta:0', 'conv2_block2_2_bn/gamma:0', 'conv2_block2_2_bn/beta:0', 'conv2_block3_preact_bn/gamma:0', 'conv2_block3_preact_bn/beta:0', 'conv2_block3_1_bn/gamma:0', 'conv2_block3_1_bn/beta:0', 'conv2_block3_2_bn/gamma:0', 'conv2_block3_2_bn/beta:0', 'conv3_block1_preact_bn/gamma:0', 'conv3_block1_preact_bn/beta:0', 'conv3_block1_1_bn/gamma:0', 'conv3_block1_1_bn/beta:0', 'conv3_block1_2_bn/gamma:0', 'conv3_block1_2_bn/beta:0', 'conv3_block2_preact_bn/gamma:0', 'conv3_block2_preact_bn/beta:0', 'conv3_block2_1_bn/gamma:0', 'conv3_block2_1_bn/beta:0', 'conv3_block2_2_bn/gamma:0', 'conv3_block2_2_bn/beta:0', 'conv3_block3_preact_bn/gamma:0', 'conv3_block3_preact_bn/beta:0', 'conv3_block3_1_bn/gamma:0', 'conv3_block3_1_bn/beta:0', 'conv3_block3_2_bn/gamma:0', 'conv3_block3_2_bn/beta:0', 'conv3_block4_preact_bn/gamma:0', 'conv3_block4_preact_bn/beta:0', 'conv3_block4_1_bn/gamma:0', 'conv3_block4_1_bn/beta:0', 'conv3_block4_2_bn/gamma:0', 'conv3_block4_2_bn/beta:0', 'conv4_block1_preact_bn/gamma:0', 'conv4_block1_preact_bn/beta:0', 'conv4_block1_1_bn/gamma:0', 'conv4_block1_1_bn/beta:0', 'conv4_block1_2_bn/gamma:0', 'conv4_block1_2_bn/beta:0', 'conv4_block2_preact_bn/gamma:0', 'conv4_block2_preact_bn/beta:0', 'conv4_block2_1_bn/gamma:0', 'conv4_block2_1_bn/beta:0', 'conv4_block2_2_bn/gamma:0', 'conv4_block2_2_bn/beta:0', 'conv4_block3_preact_bn/gamma:0', 'conv4_block3_preact_bn/beta:0', 'conv4_block3_1_bn/gamma:0', 'conv4_block3_1_bn/beta:0', 'conv4_block3_2_bn/gamma:0', 'conv4_block3_2_bn/beta:0', 'conv4_block4_preact_bn/gamma:0', 'conv4_block4_preact_bn/beta:0', 'conv4_block4_1_bn/gamma:0', 'conv4_block4_1_bn/beta:0', 'conv4_block4_2_bn/gamma:0', 'conv4_block4_2_bn/beta:0', 'conv4_block5_preact_bn/gamma:0', 'conv4_block5_preact_bn/beta:0', 'conv4_block5_1_bn/gamma:0', 'conv4_block5_1_bn/beta:0', 'conv4_block5_2_bn/gamma:0', 'conv4_block5_2_bn/beta:0', 'conv4_block6_preact_bn/gamma:0', 'conv4_block6_preact_bn/beta:0', 'conv4_block6_1_bn/gamma:0', 'conv4_block6_1_bn/beta:0', 'conv4_block6_2_bn/gamma:0', 'conv4_block6_2_bn/beta:0', 'conv5_block1_preact_bn/gamma:0', 'conv5_block1_preact_bn/beta:0', 'conv5_block1_1_bn/gamma:0', 'conv5_block1_1_bn/beta:0', 'conv5_block1_2_bn/gamma:0', 'conv5_block1_2_bn/beta:0', 'conv5_block2_preact_bn/gamma:0', 'conv5_block2_preact_bn/beta:0', 'conv5_block2_1_bn/gamma:0', 'conv5_block2_1_bn/beta:0', 'conv5_block2_2_bn/gamma:0', 'conv5_block2_2_bn/beta:0', 'conv5_block3_preact_bn/gamma:0', 'conv5_block3_preact_bn/beta:0', 'conv5_block3_1_bn/gamma:0', 'conv5_block3_1_bn/beta:0', 'conv5_block3_2_bn/gamma:0', 'conv5_block3_2_bn/beta:0', 'post_bn/gamma:0', 'post_bn/beta:0'] when minimizing the loss.
INFO:tensorflow:Error reported to Coordinator: 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py"", line 220, in _call_for_each_replica
    merge_args = distribute_utils.regroup(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 62, in regroup
    regrouped_tuple = tuple(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 63, in <genexpr>
    regroup(tuple(v[i] for v in values), wrap_class, always_wrap)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py"", line 61, in regroup
    assert len(v) == len(v0)
AssertionError

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
/tmp/ipykernel_21821/3562653524.py in <module>
    105   num_batches = 0
    106   for x in train_dist_dataset:
--> 107     total_loss += distributed_train_step(x)
    108     num_batches += 1
    109   train_loss = total_loss / num_batches

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--> 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    761     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    762     self._concrete_stateful_fn = (
--> 763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    764             *args, **kwds))
    765 

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-> 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3277     arg_names = base_arg_names + missing_arg_names
   3278     graph_function = ConcreteFunction(
-> 3279         func_graph_module.func_graph_from_py_func(
   3280             self._name,
   3281             self._python_function,

/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--> 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, ""ag_error_metadata""):
--> 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

AssertionError: in user code:

    /tmp/ipykernel_21821/3562653524.py:98 distributed_train_step  *
        per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:678 _call_for_each_replica
        return mirrored_run.call_for_each_replica(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica
        return _call_for_each_replica(strategy, fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:245 _call_for_each_replica
        coord.join(threads)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:389 join
        six.reraise(*self._exc_info_to_raise)
    /usr/local/lib/python3.8/dist-packages/six.py:703 reraise
        raise value
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception
        yield
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py:220 _call_for_each_replica
        merge_args = distribute_utils.regroup(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:62 regroup
        regrouped_tuple = tuple(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:63 <genexpr>
        regroup(tuple(v[i] for v in values), wrap_class, always_wrap)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_utils.py:61 regroup
        assert len(v) == len(v0)

    AssertionError: 
```
"
52985,Possible Bug on Building From Source (crosstool_wrapper_driver_is_not_gcc),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18:04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5.x, 2.6.x, 2.7.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: Docker (nvidia/cuda:11.4.2-cudnn8-runtime-ubuntu18.04)
- Bazel version (if compiling from source): 3.7.2- (@non-git)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: CUDA 11.4.2 & cuDNN 8.2.4
- GPU model and memory: Both RTX 3060 12GB (C.C. 8.6) & NVIDIA A100 40GB PCIe (C.C. 8.0)



**Describe the problem**
Hi. I am having problems while trying to build various versions of the TensorFlow from source. **I have successfully built TF v2.6.0 on NVIDIA A100 40GB PCIe server** but I can not build any other version. I tried with all the versions between 2.5.x and 2.7.0. The main problem is something related to `crosstool_wrapper_driver_is_not_gcc` error. However, I can not find any proper solution to this. I have tried the solution provided on the following link without any luck: #13481. Which was:

```
sh -c ""echo '/usr/local/cuda-11.4/lib64' >> /etc/ld.so.conf.d/nvidia.conf""
ldconfig
```
None of the following CUDA/cuDNN/TensorRT combination works. I am using TensorRT 8.0.3 deb file installation and NVIDIA claims `versions 10.2, 11.0 update 1, 11.1 update 1, 11.2 update 2, 11.3 update 1, and 11.4 update 2 are supported.` in the docs:

- CUDA 11.4.2 | cuDNN 8.2.4 | TensorRT 8.0.3 --> For TF v2.7.0 
- CUDA 11.3.1 | cuDNN 8.2.0 | TensorRT 8.0.3 --> For TF v2.5.x and v2.6.x
- CUDA 11.2.2 | cuDNN 8.1.1 | TensorRT 8.0.3 --> For TF v2.5.x

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The list of inputs for the `./configure`:
```
'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6,8.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda
```
And here is the Bazel build command:
`bazel build -j 10 -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. This 

Last of all here is my full error output:
```
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6,8.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /root/tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /root/tensorflow/WORKSPACE:23:14: in <toplevel>
  /root/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnn-frontend/archive/73210a930333eaf66b42b01693bce7b70719c354.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/llvm-project/llvm/BUILD.bazel:673:11: C++ compilation of rule '@llvm-project//llvm:Core' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/usr/local/cuda-11.3/bin/nvcc:/root/bazel-3.7.2/output:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=8.6,8.0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/Core/TypeFinder.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/Core/TypeFinder.o' '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' '-DHAVE_STRERROR_R=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_DEREGISTER_FRAME=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_POSIX_FALLOCATE=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -c external/llvm-project/llvm/lib/IR/TypeFinder.cpp -o bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/Core/TypeFinder.o)
Execution platform: @local_execution_config_platform//:platform
In file included from external/llvm-project/llvm/include/llvm/ADT/Hashing.h:55:0,
                 from external/llvm-project/llvm/include/llvm/ADT/DenseMapInfo.h:16,
                 from external/llvm-project/llvm/include/llvm/ADT/DenseMap.h:16,
                 from external/llvm-project/llvm/include/llvm/ADT/DenseSet.h:16,
                 from external/llvm-project/llvm/include/llvm/IR/TypeFinder.h:16,
                 from external/llvm-project/llvm/lib/IR/TypeFinder.cpp:13:
/usr/include/c++/7/tuple: In instantiation of 'class std::tuple<llvm::DISubroutineType*, llvm::TempMDNodeDeleter>':
/usr/include/c++/7/bits/unique_ptr.h:152:27:   required from 'class std::__uniq_ptr_impl<llvm::DISubroutineType, llvm::TempMDNodeDeleter>'
/usr/include/c++/7/bits/unique_ptr.h:163:33:   required from 'class std::unique_ptr<llvm::DISubroutineType, llvm::TempMDNodeDeleter>'
external/llvm-project/llvm/include/llvm/IR/DebugInfoMetadata.h:1307:42:   required from here
/usr/include/c++/7/tuple:1128:2: internal compiler error: Segmentation fault
  tuple(allocator_arg_t __tag, const _Alloc& __a,
  ^~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 824.851s, Critical Path: 58.83s
INFO: 3373 processes: 785 internal, 2588 local.
FAILED: Build did NOT complete successfully

```
"
52983,tensorflow-estimatior==2.0.0 not available so can't fix known issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution Linux Ubuntu 20.04.2 LTS
- Installed using mamba
- TensorFlow version: 2.0.0
- Python version: 2.7.0

**Describe the problem**
Default  tensorflow-estimator  mismatches tensorflow as in https://stackoverflow.com/questions/66022256/modulenotfounderror-no-module-named-tensorflow-core-estimator-for-tensorflow
but for 2.0.0 can't be installed direclty as in https://stackoverflow.com/a/66027093/7607734
(it worked for tf 2.1.0)
also https://stackoverflow.com/a/66027135/7607734 gives nothing.
So seem to be unresolved for 2.0.0


**Provide the exact sequence of commands / steps that you executed before running into the problem**
1) 
mamba create --name py3_7_keras_ocr  python=3.7.0 keras-ocr anaconda
2) 
conda activate py3_7_keras_ocr
3)
 conda list | grep tensorflow
4)
mamba install tensorflow-estimatior==2.0.0
or
conda install tensorflow-estimatior==2.0.0

**Any other info / logs**
3)
> conda list | grep tensorflow

```
tensorflow                2.0.0           mkl_py37h66b46cc_0  
tensorflow-base           2.0.0           mkl_py37h9204916_0  
tensorflow-estimator      2.6.0              pyh7b7c402_0  
```

4)
> conda install tensorflow-estimatior==2.0.0
```
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - tensorflow-estimatior==2.0.0
Current channels:

  - https://conda.anaconda.org/conda-forge/linux-64
  - https://conda.anaconda.org/conda-forge/noarch
  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch
```

"
52982,"Problem with calling the same object twice in the ""call"" part","My model is this:

```
class DenseModel(Model):
    def __init__(self):
        super(DenseModel, self).__init__(name='class_dense_model')
        self.dense_1 = layers.Dense(1)
        self.dense_64 = layers.Dense(64, activation=tf.nn.relu)
        self.dense_100 = layers.Dense(100,activation=tf.nn.relu)
        self.dense_200 = layers.Dense(200,activation=tf.nn.relu)
        self.dense_200_2 = layers.Dense(200, activation=tf.nn.relu)

    def call(self, input_tensor, training=False, **kwargs):
        out_1 = self.dense_64(input_tensor)
        out_2 = self.dense_100(out_1)
        out_3 = self.dense_200(out_2)
        out_4 = self.dense_200(out_3)
        return self.dense_1(out_4)

    def build_graph(self, shape):
        x = tf.keras.layers.Input(shape=shape)
        return Model(inputs=[x], outputs=self.call(x))
```

Then I build it like this 

```
model = DenseModel()
model.build_graph(INPUT_SHAPE)
```

This gives me the following error:

""ValueError: Dimensions must be equal, but are 200 and 100 for '{{node dense_5/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Placeholder, dense_5/MatMul/ReadVariableOp)' with input shapes: [?,200], [100,200].""

When I change ```out_4 = self.dense_200(out_3)``` into ```out_4 = self.dense_200_2(out_3)``` the error is gone. I realized that the error is caused when using the same object twice in the ""call"" part. I checked the documentation from this link https://www.tensorflow.org/guide/keras/custom_layers_and_models

I saw that you build two objects from the same class and the same parameters in __init__ part. Why? I mean, why do we have problem with calling the same object twice in the ""call"" part? What if I want to call one object 100 times in for loop? I mean, I just don't understand why I get this error when it shouldn't be the error.


"
52981,Pip install tensorflow==2.7.0 expects cuda 11.0 instead 11.2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>
Well, it seems to fall between a bug and an installation issue :P

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip 
- TensorFlow version: 2.6.1 upgrading to 2.7 via pip
- Python version:  3.8.12
- Installed using virtualenv? pip? conda?: conda for env incl cudatoolkit and cudnn, pip for tf 2.6.1 and 2.7.0
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.2.2               he111cf0_8    nvidia; 8.1.0.77             h90431f1_0    conda-forge
- GPU model and memory: k80



**Describe the problem**
Just set up a running TF 2.6.1 using pip in a conda env last week following the requirements as state in https://www.tensorflow.org/install/gpu. When I upgraded to 2.7 via pip the installed package suddenly required cuda 11.0 and thus fails to load/use the gpu. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
conda update --force conda
conda create -n tf2 python=3.8
conda activate tf2
conda install cudatoolkit==11.3
conda install cudatoolkit==11.3.1
conda install cudnn
conda install pandas numpy scikit-learn
conda install cupti
conda remove cudnn
conda remove cupti
conda remove cudatoolkit
conda install -c nvidia cudatoolkit==11.2
conda install -c nvidia cudatoolkit==11.2.2
conda list -n tf2
conda install -c nvidia cudnn=8.1.0
conda install -c conda-forge cudnn=8.1.0
pip install --upgrade tensorflow     # this was under 2.6.1; needs ==2.6.1 now
conda install matplotlib seaborn
nvidia-smi
conda install -c conda-forge notebook jupyter ipykernel jupyter_contrib_nbextensions
jupyter notebook --generate-config
vi .jupyter/jupyter_notebook_config.py
jupyter notebook password
vi .jupyter/jupyter_notebook_config.py
sudo
sudo print
sudo systemctl start jupyter.service
sudo systemctl stop jupyter.service
sudo systemctl start jupyter.service
pip install tensorflow-datasets
<em>this was my working 2.6.1 install with gpu available</em> Then I proceeded to upgrade to 2.7 once it became available:
pip install -U tensorflow
<em> 
This then leads to the new TF looking for Cuda 11.0!
Installing collected packages: tensorflow-estimator, keras, tensorflow
  Attempting uninstall: tensorflow-estimator
    Found existing installation: tensorflow-estimator 2.6.0
    Uninstalling tensorflow-estimator-2.6.0:
      Successfully uninstalled tensorflow-estimator-2.6.0
  Attempting uninstall: keras
    Found existing installation: keras 2.6.0
    Uninstalling keras-2.6.0:
      Successfully uninstalled keras-2.6.0
  Attempting uninstall: tensorflow
    Found existing installation: tensorflow 2.6.1
    Uninstalling tensorflow-2.6.1:
      Successfully uninstalled tensorflow-2.6.1
Successfully installed keras-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0

>>> import tensorflow as tf
2021-11-08 09:33:10.345741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-08 09:33:10.345774: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
# packages in environment at /home/student/miniconda3/envs/tf2:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main
_openmp_mutex             4.5                       1_gnu
absl-py                   0.12.0                   pypi_0    pypi
argon2-cffi               20.1.0           py38h27cfd23_1
astunparse                1.6.3                    pypi_0    pypi
async_generator           1.10                       py_0    conda-forge
attrs                     21.2.0             pyhd8ed1ab_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge
blas                      1.0                         mkl
bleach                    4.1.0              pyhd8ed1ab_0    conda-forge
bottleneck                1.3.2            py38heb32a55_1
brotli                    1.0.9                he6710b0_2
ca-certificates           2021.10.8            ha878542_0    conda-forge
cachetools                4.2.4                    pypi_0    pypi
certifi                   2021.10.8        py38h578d9bd_1    conda-forge
cffi                      1.14.6           py38ha65f79e_0    conda-forge
charset-normalizer        2.0.7                    pypi_0    pypi
clang                     5.0                      pypi_0    pypi
cudatoolkit               11.2.2               he111cf0_8    nvidia
cudnn                     8.1.0.77             h90431f1_0    conda-forge
cycler                    0.10.0                   py38_0
dbus                      1.13.18              hb2f20db_0
debugpy                   1.4.1            py38h709712a_0    conda-forge
decorator                 5.1.0              pyhd8ed1ab_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
dill                      0.3.4                    pypi_0    pypi
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
expat                     2.4.1                h2531618_2
flatbuffers               1.12                     pypi_0    pypi
fontconfig                2.13.1               h6c09931_0
fonttools                 4.25.0             pyhd3eb1b0_0
freetype                  2.11.0               h70c0345_0
future                    0.18.2                   pypi_0    pypi
gast                      0.4.0                    pypi_0    pypi
giflib                    5.2.1                h7b6447c_0
glib                      2.69.1               h5202010_0
google-auth               2.3.3                    pypi_0    pypi
google-auth-oauthlib      0.4.6                    pypi_0    pypi
google-pasta              0.2.0                    pypi_0    pypi
googleapis-common-protos  1.53.0                   pypi_0    pypi
grpcio                    1.41.1                   pypi_0    pypi
gst-plugins-base          1.14.0               h8213a91_2
gstreamer                 1.14.0               h28cd5cc_2
h5py                      3.1.0                    pypi_0    pypi
icu                       58.2                 he6710b0_3
idna                      3.3                      pypi_0    pypi
importlib-metadata        4.8.1            py38h578d9bd_1    conda-forge
importlib-resources       5.4.0                    pypi_0    pypi
intel-openmp              2021.4.0          h06a4308_3561
ipykernel                 6.4.2            py38he5a9106_0    conda-forge
ipython                   7.29.0           py38he5a9106_1    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
ipywidgets                7.6.5              pyhd8ed1ab_0    conda-forge
jedi                      0.18.0           py38h578d9bd_3    conda-forge
jinja2                    3.0.2              pyhd8ed1ab_0    conda-forge
joblib                    1.1.0              pyhd3eb1b0_0
jpeg                      9d                   h7f8727e_0
jsonschema                4.1.2              pyhd8ed1ab_0    conda-forge
jupyter                   1.0.0            py38h578d9bd_6    conda-forge
jupyter_client            7.0.6              pyhd8ed1ab_0    conda-forge
jupyter_console           6.4.0              pyhd8ed1ab_0    conda-forge
jupyter_contrib_core      0.3.3                      py_2    conda-forge
jupyter_contrib_nbextensions 0.5.1              pyhd8ed1ab_2    conda-forge
jupyter_core              4.9.1            py38h578d9bd_0    conda-forge
jupyter_highlight_selected_word 0.2.0           py38h578d9bd_1002    conda-forge
jupyter_latex_envs        1.4.6           pyhd8ed1ab_1002    conda-forge
jupyter_nbextensions_configurator 0.4.1            py38h578d9bd_2    conda-forge
jupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge
jupyterlab_widgets        1.0.2              pyhd8ed1ab_0    conda-forge
keras                     2.6.0                    pypi_0    pypi
keras-preprocessing       1.1.2                    pypi_0    pypi
kiwisolver                1.3.1            py38h2531618_0
lcms2                     2.12                 h3be6417_0
ld_impl_linux-64          2.35.1               h7274673_9
libclang                  12.0.0                   pypi_0    pypi
libffi                    3.3                  he6710b0_2
libgcc-ng                 9.3.0               h5101ec6_17
libgfortran-ng            7.5.0               ha8ba4b0_17
libgfortran4              7.5.0               ha8ba4b0_17
libgomp                   9.3.0               h5101ec6_17
libpng                    1.6.37               hbc83047_0
libsodium                 1.0.18               h36c2ea0_1    conda-forge
libstdcxx-ng              9.3.0               hd4cf53a_17
libtiff                   4.2.0                h85742a9_0
libuuid                   1.0.3                h7f8727e_2
libwebp                   1.2.0                h89dd481_0
libwebp-base              1.2.0                h27cfd23_0
libxcb                    1.14                 h7b6447c_0
libxml2                   2.9.12               h03d6c58_0
libxslt                   1.1.34               hc22bd24_0
lxml                      4.6.3            py38hf1fe3a4_0    conda-forge
lz4-c                     1.9.3                h295c915_1
markdown                  3.3.4                    pypi_0    pypi
markupsafe                2.0.1            py38h497a2fe_0    conda-forge
matplotlib                3.4.3            py38h06a4308_0
matplotlib-base           3.4.3            py38hbbc1b5f_0
matplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge
mistune                   0.8.4           py38h497a2fe_1004    conda-forge
mkl                       2021.4.0           h06a4308_640
mkl-service               2.4.0            py38h7f8727e_0
mkl_fft                   1.3.1            py38hd3c417c_0
mkl_random                1.2.2            py38h51133e4_0
munkres                   1.1.4                      py_0
nbclient                  0.5.4              pyhd8ed1ab_0    conda-forge
nbconvert                 6.2.0            py38h578d9bd_0    conda-forge
nbformat                  5.1.3              pyhd8ed1ab_0    conda-forge
ncurses                   6.3                  h7f8727e_0
nest-asyncio              1.5.1              pyhd8ed1ab_0    conda-forge
notebook                  6.4.5              pyha770c72_0    conda-forge
numexpr                   2.7.3            py38h22e1b3c_1
numpy                     1.19.5                   pypi_0    pypi
oauthlib                  3.1.1                    pypi_0    pypi
olefile                   0.46               pyhd3eb1b0_0
openssl                   1.1.1l               h7f8727e_0
opt-einsum                3.3.0                    pypi_0    pypi
packaging                 21.0               pyhd8ed1ab_0    conda-forge
pandas                    1.3.4            py38h8c16a72_0
pandoc                    2.16                 h7f98852_0    conda-forge
pandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge
parso                     0.8.2              pyhd8ed1ab_0    conda-forge
pcre                      8.45                 h295c915_0
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    8.4.0            py38h5aabda8_0
pip                       21.2.4           py38h06a4308_0
prometheus_client         0.12.0             pyhd8ed1ab_0    conda-forge
promise                   2.3                      pypi_0    pypi
prompt-toolkit            3.0.21             pyha770c72_0    conda-forge
prompt_toolkit            3.0.21               hd8ed1ab_0    conda-forge
protobuf                  3.19.1                   pypi_0    pypi
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pycparser                 2.20               pyh9f0ad1d_2    conda-forge
pygments                  2.10.0             pyhd8ed1ab_0    conda-forge
pyparsing                 3.0.4              pyhd3eb1b0_0
pyqt                      5.9.2            py38h05f1152_4
pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
python                    3.8.12               h12debd9_0
python-dateutil           2.8.2              pyhd3eb1b0_0
python_abi                3.8                      2_cp38    conda-forge
pytz                      2021.3             pyhd3eb1b0_0
pyyaml                    5.4.1            py38h497a2fe_0    conda-forge
pyzmq                     19.0.2           py38ha71036d_2    conda-forge
qt                        5.9.7                h5867ecd_1
qtconsole                 5.1.1              pyhd8ed1ab_0    conda-forge
qtpy                      1.11.2             pyhd8ed1ab_0    conda-forge
readline                  8.1                  h27cfd23_0
requests                  2.26.0                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
scikit-learn              1.0.1            py38h51133e4_0
scipy                     1.7.1            py38h292c36d_2
seaborn                   0.11.2             pyhd3eb1b0_0
send2trash                1.8.0              pyhd8ed1ab_0    conda-forge
setuptools                58.0.4           py38h06a4308_0
sip                       4.19.13          py38he6710b0_0
six                       1.15.0                   pypi_0    pypi
sqlite                    3.36.0               hc218d9a_0
tensorboard               2.7.0                    pypi_0    pypi
tensorboard-data-server   0.6.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
tensorflow                2.6.1                    pypi_0    pypi
tensorflow-datasets       4.4.0                    pypi_0    pypi
tensorflow-estimator      2.6.0                    pypi_0    pypi
tensorflow-io-gcs-filesystem 0.21.0                   pypi_0    pypi
tensorflow-metadata       1.4.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
terminado                 0.12.1           py38h578d9bd_0    conda-forge
testpath                  0.5.0              pyhd8ed1ab_0    conda-forge
threadpoolctl             2.2.0              pyh0d69192_0
tk                        8.6.11               h1ccaba5_0
tornado                   6.1              py38h27cfd23_0
tqdm                      4.62.3                   pypi_0    pypi
traitlets                 5.1.1              pyhd8ed1ab_0    conda-forge
typing-extensions         3.7.4.3                  pypi_0    pypi
urllib3                   1.26.7                   pypi_0    pypi
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
werkzeug                  2.0.2                    pypi_0    pypi
wheel                     0.37.0             pyhd3eb1b0_1
widgetsnbextension        3.5.2            py38h578d9bd_0    conda-forge
wrapt                     1.12.1                   pypi_0    pypi
xz                        5.2.5                h7b6447c_0
yaml                      0.2.5                h516909a_0    conda-forge
zeromq                    4.3.4                h9c3ff4c_0    conda-forge
zipp                      3.6.0              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11               h7b6447c_3
zstd                      1.4.9                haebb681_0

"
52980,win11 gtx960M tensorflow2.5~ tensorflow2.7 CUDA out of memory,"win11 gtx960M tensorflow2.5~ tensorflow2.7 report errors : CUDA out of memory ,but win10 gtx960M tensorflow2.5~ tensorflow2.7 can run normally"
52979,TFLite static library generated is not working,"
The generated static library through CMake sources as per tflite documentation is not working.

**System information**
- Debian 10 (e.g., Linux Ubuntu 16.04):
- TensorFlow Lite installed from CMake (source or binary): Source
- TensorFlow version 2.4.1

**Describe the current behavior**
Currently, the static library when generated through the bash script present in tensorflow/lite/tools/make/build_lib.sh is working.
But when generated as per documentation through CMake is not working.

**Describe the expected behavior**
Need the working static library generated through CMake.
Provide us the working CMakeLists file.
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
1)Use the steps of cmake build to generate static library on host.
2)Run build_lib.sh and generate static library.

There are differences between two libraries and only second one is working.
Need a proper CMakeLists file to generate static library,
.
"
52978,"Problems with saving and loading stock TF models (Resnet50, MobileNetV3, etc.) starting from TF 2.5 ","System: Ubuntu 20, Python 3.9.5, TF 2.5 (or 2.6)

This code: 

base_model = MobileNetV3Large(); 
base_model.save('test')

Generates a warning: ""generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument."" 

I ignored this warning, trained a model and observed that when saving and loading the model back, it is losing accuracy at which it was saved.

Expected behavior: The stock models should save and load without warnings or problems."
52977,OCR example app crashes with new image,"I'm not certain if this is the correct issue type, I'm using the documentation type since the [examples repo](https://github.com/tensorflow/examples) instructed me to do so (specifically the ""To file an issue"" link in the README).

## URL(s) with the issue:
https://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L200

## Description of issue (what needs changing):
I tried modifying the OCR example app by using a different image and it resulted in a crash with an odd error message stating `E/cv::error(): OpenCV(4.5.3) Error: Assertion failed (m.dims >= 2) in Mat, file /home/quickbirdstudios/opencv/releases/opencv-4.5.3/modules/core/src/matrix.cpp, line 751`.
After some debugging I found the error was caused by [line 200 of OCRModelExecutor.kt](https://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L200) when `detectedConfidences` is empty. I believe there should be some check here to prevent passing an empty list and instead taking some other action. Alternatively, the error should be caught and a useful message should be providing explaining the error instead of printing the confusing message from OpenCV.

I also tried lowering the threshold on [line 164](https://github.com/tensorflow/examples/blob/03f796596c9ca9d3d42e5cb43a726b8e220c73b2/lite/examples/optical_character_recognition/android/app/src/main/java/org/tensorflow/lite/examples/ocr/OCRModelExecutor.kt#L164) to 0.1. This resulted in a different error message about `Native Mat has unexpected type or size`, but at least the app didn't crash."
52976, Crash with tensorflow lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):

**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.


I am using this model

        model = Sequential()        
        model.add(Conv2D(32, (3, 3), input_shape=inputShape))
        model.add(Activation('relu'))
        model.add(Conv2D(32, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.5))

        model.add(Conv2D(64, (3, 3)))
        model.add(Activation('relu'))
        model.add(Conv2D(64, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.5))
        
        model.add(Flatten())
        model.add(Dense(768))#1024))#256))
        model.add(Activation('relu'))
        model.add(Dropout(0.5))
        model.add(Dense(nbClasses))
        model.add(Activation('softmax'))



then I convert it like this : 

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]



And on Android with with tensorflowlite 2.6. 0 I got a  crash because

    java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""wcsnrtombs"" referenced by ""libtensorflowlite_jni.so""...

Then I switched to tensorflowlite 2.7.0-rc0
And I got this crash


java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""getpagesize"" referenced by ""libtensorflowlite_jni.so""...


I'll now try without this : 
[tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
52975,Need clarification and assistance on this one (Friday with Leanne),"Need clarification and assistance on this one (Friday with Leanne)

_Originally posted by @ZCorso in https://github.com/WorldEES/Q/issues/5#issuecomment-351091481_

__Originally posted by @jessecantu in https://github.com/jessecantu/overriddes/issues/1__"
52974,Cannot build from source and illegal instruction on import tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04

- TensorFlow installed from (source or binary): Source and Binary

- TensorFlow version: 2.6.

- Python version: 3.8

- Installed using virtualenv? pip? conda?: Using virtualenv to install by pip, and build from source


- Bazel version (if compiling from source): 4.2.1

- GCC/Compiler version (if compiling from source): 9.3.0

- CUDA/cuDNN version: 11.4

- GPU model and memory: EVGA RTX 3060



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

When using pip to install, attempting to import tensorflow produces an ""Illegal instruction(core dumped)"" error.

When using Bazel to build from source using the command, `bazel build --jobs=2 --local_ram_resources=2048 --config=opt config=cuda //tensorflow/tools/pip_package:build_pip_package` I come back hours later to find the screen frozen and the computer unresponsive.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Currently unable to copy and paste logs from terminal because my computer freezes during the build.
"
52973,Release pre-built wheels for manylinux2014_aarch64 and macosx_arm64,"**System information**
- TensorFlow version (you are using): 2.6
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

You can't `pip install tensorflow` natively on an M1 Mac, since there are no pre-built wheels for macosx_arm64 or manylinux2014_aarch64 (for a native Linux container or VM on M1). 

PyTorch [has provided these pre-built binaries](https://pypi.org/project/torch/#files) for several versions now. It would be great if TF was similarly easy to install on the latest generation of Macs. 

(I know that the separate PyPI package tensorflow-macos exists, but it isn't a portable solution and is not documented.)

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Any user of the latest and future generations of Macs."
52972,Cannot run TensorFlow 2.7 in Docker on M1 (Apple Silicon),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.0 Monterey
- TensorFlow installed from (source or binary): pre-built binary (Docker and pip)
- TensorFlow version (use command below): 2.7
- Python version: 3.8.10

**Describe the current behavior**

Our team needs to run TensorFlow as part of a larger application in Docker. However, this doesn't seem possible on an M1 Mac.

For example, if I use the default TF Docker image (for x86-64 only, an ARM64 image is not available): 

```
> docker run -it tensorflow/tensorflow /bin/bash
> python -c ""import tensorflow""
The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.
qemu: uncaught target signal 6 (Aborted) - core dumped
Aborted
```

I get the same error when installing from pip on an x86-64 Linux container:

```
> docker run -it --platform=linux/amd64 python:3.8-buster /bin/bash
> pip install --upgrade pip && pip install tensorflow
> python -c ""import tensorflow""
The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.
qemu: uncaught target signal 6 (Aborted) - core dumped
Aborted
```

Ostensibly, this is because the pre-built TensorFlow [requires the CPU](https://www.tensorflow.org/install/pip#hardware-requirements) to support AVX instructions, but this is not supported by Docker / QEMU when emulating an x86-64 container on M1.  

**Describe the expected behavior**

There should be a way to run TensorFlow in Docker on M1! (Without building from source.)

Every other ML/DS library works with on Docker on M1: PyTorch, Scikit-Learn, Numpy, Scipy, etc. 

**Standalone code to reproduce the issue**

See code snippets above."
52968,`tf.svd` fails during GradientTape mode,"**System information**
- OS Platform and Distribution: macOS v12.0.1
- TensorFlow installed from: pypi
- TensorFlow version: v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.9
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Hi, I'm trying to contract a network with multiple tensors and using singular value decomposition during contraction to simplify the contraction process. Whilst this works perfectly when I'm not taking any gradient, it fails once gradient tape starts to watch the tensors (I'm not sure why this is related). Below I wrote my simple contraction function and the function that I'm taking `svd`:

```python
import tensorflow as tf

@tf.function
def contraction_step(network, max_singular_values: int):

    bottom = network[-1]
    uppper = network[-2]

    def contract_up_down(up,dn):
        shu = tf.shape(up)
        shd = tf.shape(dn)
        c = tf.einsum(""ijkxlm,nkpyqr->injpxylqmr"", up, dn)
        return tf.reshape(c, (
            shu[0]*shd[0], shu[1], shd[2], shu[-3], shd[-3], shu[-2]*shd[-2], shu[-1]*shd[-1]
        ))

    new = []
    multiplier = tf.eye(tf.shape(bottom[-1])[-1]*tf.shape(uppper[-1])[-1], dtype=uppper[-1].dtype)
    for ix in reversed(range(len(bottom))):
        tensor = contract_up_down(uppper[ix], bottom[ix])
        t = tf.einsum(""ludpxor,ij->ludpxoj"",tensor, multiplier)
        u, s, vh = svd(t, 1, max_singular_values = max_singular_values)
        multiplier = tf.tensordot(u, s/tf.norm(s), axes=(-1,0))
        new.insert(0, vh)
    new[-1] = tf.tensordot(new[-1], multiplier, axes=(-1,0))

    return network[:-2] + [new]

def svd(tensor,pivot,max_singular_values = None,cutoff = 0.0):
    left_dims = tf.shape(tensor)[:pivot]
    right_dims = tf.shape(tensor)[pivot:]
    tensor = tf.reshape(tensor, (tf.reduce_prod(left_dims), tf.reduce_prod(right_dims)))

    s, u, v = tf.linalg.svd(tensor)

    s_shape = tf.math.count_nonzero(
        tf.cast(s >= cutoff, dtype = tf.int32), dtype = tf.int32
    )
    if max_singular_values is None:
        max_singular_values = s_shape
    else:
        max_singular_values = tf.cast(tf.constant(max_singular_values), dtype = tf.int32)
    num_sing_vals_keep = tf.maximum(
        tf.minimum(max_singular_values, s_shape), tf.constant(1, dtype = tf.int32)
    )

    s = tf.slice(s, [0], [num_sing_vals_keep])
    u = tf.slice(u, [0, 0], [tf.shape(u)[0], num_sing_vals_keep])
    v = tf.slice(v, [0, 0], [tf.shape(v)[0], num_sing_vals_keep])

    vh = tf.linalg.adjoint(v)

    dim_s = tf.shape(s)[0]  # must use tf.shape (not s.shape) to compile
    u = tf.reshape(u, tf.concat([left_dims, [dim_s]], axis = -1))
    vh = tf.reshape(vh, tf.concat([[dim_s], right_dims], axis = -1))

    return u, tf.linalg.diag(s), vh
```
These functions work perfectly while using standalone:
```
upper = [tf.random.uniform((5,3,3,2,1,5), dtype=tf.float64) for _ in range(5)]
lower = [tf.random.uniform((5,3,3,2,1,5), dtype=tf.float64) for _ in range(5)]

contracted = contraction_step([upper, lower], 2)[0]
print(f""shapes: {', '.join([str(x.shape) for x in contracted])}"")

# shapes: (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2), (2, 3, 3, 2, 2, 1, 2)
```
However, with the gradient, I get the following error:
```
with tf.GradientTape() as tape:
    tape.watch(upper + lower)
    contracted = contraction_step([upper, lower], 2)[0]

NotImplementedError: SVD gradient has not been implemented for input with unknown inner matrix shape.
```
It seems like for some reason during gradient mode TensorFlow loses the shape information of the tensors. Note that I get the same error when I set the tensors as `tf.Variable` instead of watching them manually. Any help would be highly appreciated!

The same error has been reproduced in [Colab as well](https://colab.research.google.com/drive/1Cg5D-qZz8hIlY0bNxU-2WmuBpyDo8wGs?usp=sharing).

Thanks
"
52965,"<unknown> shape of Tensor, as_list() is not defined on a unknown TensorShape","Good afternoon, 

I am trying to implement a pipeline using TFX and Tensorflow. For this, I use 3 accelerometer measurements to predict the activity that is being performed at that moment (6 classes). To do the processing, I have to transform the dataset to sequences, and for that I make use of .windows function. The code used to perform this processing is the following: 

```python
HISTORY_SIZE = 80
BATCH_SIZE = 32
SHIFT = 40

def parse_function(example_proto):
    '''Parse the values from tf examples'''
    feature_spec = tf_transform_output.transformed_feature_spec()
    features = tf.io.parse_single_example(example_proto, feature_spec)
    values = list(features.values())
    values = [float(_fill_in_missing(value)) for value in values]    
    features = tf.stack(values, axis=0)
    return features

def add_mode(features):
    '''Calculate mode of activity for the current history size of elements'''
    # Removes dimensions of size 1 from the shape of a tensor.
    features = tf.squeeze(features)
    
    # Finds unique elements in a 1-D tensor.
        # This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x. 
        # This operation also returns a tensor idx the same size as x that contains the index of each value of x in the unique output y. 
        # Finally, it returns a third tensor count that contains the count of each element of y in x
    unique, idx, count = tf.unique_with_counts(features[:,0])
    
    # Computes tf.math.maximum of elements across dimensions of a tensor.
    max_occurrences = tf.reduce_max(count)
    
    # Returns the truth value of (x == y) element-wise.
    max_cond = tf.equal(count, max_occurrences)
    
    # Gather slices from params axis axis according to indice
    max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))

    #Features (X) are all features except activity (x-acc, y-acc, z-acc)
    #Target(Y) is the mode of activity values of all rows in this window
    return (features[:,1:], max_numbers)

def get_dataset(path):
    '''Get the dataset and group them into windows'''
    dataset = tf.data.TFRecordDataset(path, compression_type=""GZIP"")
    dataset = dataset.map(parse_function)
    dataset = dataset.window(HISTORY_SIZE, shift=SHIFT, drop_remainder=True)dataset
    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE))
    dataset = dataset.map(add_mode)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.repeat()
    return dataset

def _fill_in_missing(x):
    """"""Replace missing values in a SparseTensor.
    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.
    Args:
    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1
      in the second dimension.
    Returns:
    A rank 1 tensor where missing values of `x` have been filled in.
    """"""
    default_value = '' if x.dtype == tf.string else 0
    # Todo fix
    return x`
```

Once this is done, I get the following tensors as outputs: 
Features (x-acc, y-acc, z-acc):

(32, 80, 3)
tf.Tensor(
[[[0.6224036  0.77866393 0.526603  ]
  [0.61964923 0.7690456  0.5003369 ]
  [0.48020944 0.96107006 0.5791352 ]
  ...
  [0.5549216  0.89545894 0.5957243 ]
  [0.63135535 0.85801584 0.6734857 ]
  [0.5490686  0.50144786 0.5694582 ]]

 [[0.2963554  0.9882078  0.5528691 ]
  [0.41926903 0.38533998 0.41773698]
  [0.49363694 0.79515266 0.30230445]
  ...
  [0.57041496 0.9882078  0.69595015]
  [0.5394283  0.3602634  0.43259805]
  [0.5752351  0.9294669  0.63961625]]

 [[0.57213646 0.67526615 0.5044842 ]
  [0.615862   0.41144708 0.4481503 ]
  [0.47814363 0.30220947 0.30403247]
  ...
  [0.33112925 0.9514518  0.3537998 ]
  [0.6468486  0.7594272  0.6831627 ]
  [0.48124233 0.33999607 0.27119985]]

 ...

 [[0.5742022  0.7168314  0.50724906]
  [0.61000896 0.754618   0.4702691 ]
  [0.5122289  0.83843553 0.51519793]
  ...
  [0.5294437  0.6666783  0.5093227 ]
  [0.5683492  0.743969   0.48201975]
  [0.5036216  0.6886632  0.4954984 ]]

 [[0.505343   0.84427524 0.53351516]
  [0.505343   0.6299222  0.51899964]
  [0.5294437  0.9236271  0.6054013 ]
  ...
  [0.5810881  0.7515263  0.645146  ]
  [0.5645619  0.8047711  0.52003646]
  [0.563529   0.837405   0.5003369 ]]

 [[0.6175835  0.8944283  0.5587444 ]
  [0.50431013 0.84324473 0.5238381 ]
  [0.5101631  0.7343506  0.516926  ]
  ...
  [0.54700285 0.76114476 0.534552  ]
  [0.55285585 0.6879761  0.50241053]
  [0.49466982 0.6886632  0.5093227 ]]], shape=(32, 80, 3), dtype=float32)

Target (activity):

tf.Tensor(
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.
 2. 2. 2. 2. 2. 2. 2. 2.], shape=(32,), dtype=float32)
(32,)


When I want to get the information from this tensor, I get the following result: <RepeatDataset shapes: (< unknown>, < unknown>), types: (tf.float32, tf.float32)>

If I want to train a simple model using these sequences, I get the following error, and I don't know how to fix it: ValueError: as_list() is not defined on an unknown TensorShape.

The code of the model is below: 
```python
model = Sequential()
model.add(LSTM(units=200, input_shape=(HISTORY_SIZE,3),name=""LSTM_1""))
model.add(Dropout(0.15))
model.add(Dense(units=20, activation='relu',name=""DENSE_1""))
model.add(Dropout(0.15))
model.add(Dense(units=10, activation='relu'))
model.add(Dense(6, activation='softmax',name=""OUTPUT""))
adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)
model.compile(loss= 'CategoricalCrossentropy' , optimizer=adam, metrics=['accuracy']) #, weighted_metrics=METRICS)
model.fit(train_dataset, epochs=500, steps_per_epoch=20)`
```

Is there any way to solve it, I have seen some solution in the case of images, but not in the case of sequences. 


Versions: 
tfx-bsl==1.3.0
tensorflow-model-analysis==0.34.1
tensorflow-data-validation==1.3.0
tfx==1.3.2
tensorflow-metadata==1.2.0
ml-metadata==1.3.0
tensorflow-transform==1.3.0
tensorflow==2.6.2
pyarrow==2.0.0
apache-beam==2.32.0
tensorflow-serving-api==2.6.0
tensorflow-estimator==2.6.0
"
52964,Build did NOT complete successfully,"Hi,

My system is,

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Computational Node
- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow tensorflow
- TensorFlow version: Install TensorFlow 2
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Building from source 
- Bazel version (if compiling from source): bazel 3.1.0.
- GCC/Compiler version (if compiling from source): Tried versions 7,8,9, and 10
- CUDA/cuDNN version: None
- GPU model and memory: None

After following the steps bellow;

**cd /some/workspace
git clone https://github.com/tensorflow/tensorflow tensorflow
cd tensorflow
git checkout r2.4
./configure--> to special options were chosen
bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so**

I get this log of errors:

**```
INFO: Options provided by the client: Inherited 'common' options: --isatty=1 --terminal_columns=237 INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc: Inherited 'common' options: --experimental_repo_remote_exec INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc: 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2 INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.tf_configure.bazelrc: 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.6 --action_env PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages --python_path=/usr/bin/python3.6 --config=xla --action_env TF_CONFIGURE_IOS=0 INFO: Found applicable config definition build:short_logs in file /home/u211355/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:v2 in file /home/u211355/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:xla in file /home/u211355/tensorflow/.bazelrc: --define=with_xla_support=true INFO: Found applicable config definition build:linux in file /home/u211355/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels INFO: Found applicable config definition build:dynamic_kernels in file /home/u211355/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400"" DEBUG: Repository io_bazel_rules_go instantiated at: no stack (--record_rule_instantiation_callstack not enabled) Repository rule git_repository defined at: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel> INFO: Build options --action_env, --define, and --host_copt have changed, discarding analysis cache. WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found INFO: Analyzed target //tensorflow:libtensorflow_cc.so (211 packages loaded, 20042 targets configured). INFO: Found 1 target... ERROR: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/com_google_protobuf/BUILD:110:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \ exec env - \ LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \ PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \ PWD=/proc/self/cwd \ /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o) Execution platform: @local_execution_config_platform//:platform gcc: error: unrecognized command line option '-std=c++14' Target //tensorflow:libtensorflow_cc.so failed to build ERROR: /home/u211355/tensorflow/tensorflow/BUILD:820:1 C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \ exec env - \ LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \ PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \ PWD=/proc/self/cwd \ /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o) Execution platform: @local_execution_config_platform//:platform INFO: Elapsed time: 55.118s, Critical Path: 0.02s INFO: 0 processes. FAILED: Build did NOT complete successfully
I would appreciate your help with this error. I have also tried different versions of GCC from 7-10 and that also didn't solve the error.
```**

"
52960,Make line wrapping and precision of tf.print configurable.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): not relevant
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
tf.print() wraps past the 4th element for me. Output from printing an (N, 7, 7), where N=2 in this case, looks like this:

```
[[[ 5.62430954e+00 -3.49580956e+00 -8.04360168e+02 -5.40295654e+02
   -8.65279853e-01 -1.06588173e+00 -1.18294978e+00]
  [-3.49580956e+00  7.82608986e+00  4.73972961e+02  1.18741919e+03
    1.61477351e+00  1.88551021e+00  1.97545743e+00]
  [-8.04360168e+02  4.73972961e+02  1.31743172e+05  7.44966484e+04
    2.14444519e+02  2.22681152e+02  1.98635208e+02]
  [-5.40295654e+02  1.18741919e+03  7.44966484e+04  1.97479641e+05
    2.52062241e+02  2.85624451e+02  2.64948578e+02]
  [-8.65279853e-01  1.61477351e+00  2.14444519e+02  2.52062241e+02
    3.18258524e+00  2.82087207e+00  2.33711910e+00]
  [-1.06588173e+00  1.88551021e+00  2.22681152e+02  2.85624451e+02
    2.82087207e+00  2.64690948e+00  2.28564334e+00]
  [-1.18294978e+00  1.97545743e+00  1.98635208e+02  2.64948578e+02
    2.33711910e+00  2.28564334e+00  2.24237132e+00]]

 [[ 6.57234287e+00 -4.52555466e+00 -1.04999585e+03 -7.79698792e+02
   -1.47998118e+00 -2.11049843e+00 -4.34581661e+00]
  [-4.52555466e+00  8.80841637e+00  7.03340576e+02  1.52776123e+03
    2.77018666e+00  3.56193233e+00  6.32034349e+00]
  [-1.04999585e+03  7.03340576e+02  1.78551625e+05  1.23507805e+05
    4.00629730e+01  1.89105148e+02  5.56380432e+02]
  [-7.79698792e+02  1.52776123e+03  1.23507805e+05  2.79413969e+05
    3.76035431e+02  5.24331360e+02  1.04614160e+03]
  [-1.47998118e+00  2.77018666e+00  4.00629730e+01  3.76035431e+02
    1.25411425e+01  1.00502367e+01  9.19474983e+00]
  [-2.11049843e+00  3.56193233e+00  1.89105148e+02  5.24331360e+02
    1.00502367e+01  9.68517876e+00  1.09725800e+01]
  [-4.34581661e+00  6.32034349e+00  5.56380432e+02  1.04614160e+03
    9.19474983e+00  1.09725800e+01  1.55766268e+01]]]
```

I work a lot with these fixed-sized matrices (eg: 7x7, or 9x9). Debugging those is hard with line wrapping.
I'd like to get them all 7 elements of the matrix row on one line, at least. Additionally, I would fancy having less precision, as that is not that informative to debug.

**Will this change the current api? How?**
Either have like numpy a `set_print_options()`, or have extra arguments to tf.print along the lines of `line_length=None` (where None means ""no wrapping at all"") and `format=""%12.7e""` where I could specify for example `format=""%10.4f""`.
I am slightly in favor of the custom options per `tf.print()` call to allow granular control.

**Who will benefit with this feature?**
Most likely anyone debugging stuff with matrices.
"
52959,data.Dataset.from_generator  Report Error in Distributed Training,"the message is ""Cannot assign a device for operation PyFunc: {{node PyFunc}} was explicitly assigned to /job:localhost/replica:0/task:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.""  but my codes run in TF 2.2.0,  the training workers fine.    

my demo code test.py is :
```python
import tensorflow.compat.v1 as tf
import time
import random
from tensorflow.python.framework import ops
import logging
import numpy as np
tf.disable_v2_behavior()

tf.app.flags.DEFINE_string(""ps_hosts"", """", ""One of 'ps', 'worker'"")
tf.app.flags.DEFINE_string(""worker_hosts"", """", ""One of 'ps', 'worker'"")
tf.app.flags.DEFINE_string(""job_name"", ""worker"", ""One of 'ps', 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
tf.app.flags.DEFINE_string(""buckets"", None, ""oss buckets"")
tf.app.flags.DEFINE_integer(""interval"", 60, ""interval"")
tf.app.flags.DEFINE_string(""protocol"", ""grpc"", ""interval"")

FLAGS = tf.app.flags.FLAGS

def data_generator():
    dataset = np.array(range(1000))
    for d in dataset:
        yield d
def train(task_index, cluster, is_chief, target, buckets):
  available_worker_device = ""/job:worker/task:%d"" % (task_index)
  with tf.device(
      tf.train.replica_device_setter(worker_device=available_worker_device,
                                     cluster=cluster)):
    dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32), (tf.TensorShape([])))
    dataset = dataset.repeat(3)
    dataset = dataset.batch(4)
    iterator = dataset.make_one_shot_iterator()
    one_element = iterator.get_next()

    local_step = 1
    config = tf.ConfigProto(inter_op_parallelism_threads=7)
    with tf.train.MonitoredTrainingSession(master=target,
                                           is_chief=is_chief,
                                           hooks=[],
                                           save_checkpoint_secs=60,
                                           checkpoint_dir=buckets,
                                           config=config) as mon_sess:
      while True:
        l = mon_sess.run(one_element)
        local_step += 1
        if local_step % 100 == 0:
          logging.info(l)


def main(argv):
  print(""job name = %s"" % FLAGS.job_name)
  print(""task index = %d"" % FLAGS.task_index)
  is_chief = FLAGS.task_index == 0

  ps_spec = FLAGS.ps_hosts.split("","")
  worker_spec = FLAGS.worker_hosts.split("","")
  cluster = tf.train.ClusterSpec({""ps"": ps_spec, ""worker"": worker_spec})

  config = tf.ConfigProto(inter_op_parallelism_threads=8)
  server = tf.distribute.Server(cluster,
                                job_name=FLAGS.job_name,
                                protocol=FLAGS.protocol,
                                task_index=FLAGS.task_index,
                                config=config)

  # join the ps server
  if FLAGS.job_name == ""ps"":
    server.join()
  # start the training
  print(""start trainig"")
  print(FLAGS.buckets)
  train(task_index=FLAGS.task_index,
        cluster=cluster,
        is_chief=is_chief,
        target=server.target,
        buckets=FLAGS.buckets)
if __name__ == ""__main__"":
  tf.app.run()
```

my start command is below:
python test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=ps
python test.py --ps_hosts=127.0.0.1:9200 --worker_hosts=127.0.0.1:9100 --task_index=0 --job_name=worker

could you please tell me what's wrong with my code? my code works fine in TF2.2.0。
hope for your answer, thanks!"
52958,[Help] How to feed the RaggedTensor as inputs in Graph execution ?,
52957,TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.,"Hi I am still getting the 'TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.' even after applying the fix mentioned previously.
Any suggestions?
"
52956,Change the input layer from a saved TF model,"Is there any way to load a Tensorflow model, change the input layer and save again the new model? (I would like to convert a model from NHWC vs NCHW) "
52955,tf.signal.stft has dynamic-sized tensors causing the tflite delegate to not work,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **iPhone12**
- TensorFlow installed from (source or binary): **PIP**
- TensorFlow version (use command below): **2.6**
- Python version: **3.9**

**Describe the current behavior**

```
I tried adding the tf.signal.stft operation to the model and exported it to the tflite model for testing,
found that the XNNPack delegate will report the following error message: ""
Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors."",
I think it's bad for performance.
```

**Describe the expected behavior**
```
Expect tf.signal.stft to support use in delegate, because it is very useful for audio processing.
```

**Standalone code to reproduce the issue**
```
import tensorflow as tf

class STFT(tf.keras.layers.Layer):
    def __init__(self, fft_size, hop_size, **kwargs):
        super(STFT, self).__init__(**kwargs)
        self._fft_size = fft_size
        self._hop_size = hop_size

    def call(self, inputs):
        return tf.signal.stft(inputs, 
                              frame_length=self._fft_size,
                              frame_step=self._hop_size,
                              pad_end=True)

model = tf.keras.models.Sequential([
        tf.keras.layers.Input(shape=(160000)),
        STFT(1024, 500)
    ])

model.save(saved_model_path, save_format='tf')
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
tflite_model = converter.convert()
open(tflite_path, ""wb"").write(tflite_model_path)
```"
52954,TFLite int8 quantization failure in TensorFlow 2.7.0,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): v2.7.0

### 2. Code

The MWE is provided in the following colab notebook:
https://colab.research.google.com/drive/1K-Hwq6LEfjFr-4dSLEI4dhQd6pCR0K-F?usp=sharing

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The log is attached as below:
```
INFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assets
INFO:tensorflow:Assets written to: /tmp/tmp0cb_yd9c/assets
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-15-674e2748ac1c> in <module>()
     16 converter.inference_output_type = tf.uint8  # or tf.int8
     17 converter.experimental_new_quantizer = False
---> 18 tflite_model = converter.convert()

10 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/schema_py_generated.py in Pack(self, builder)
   5704             for i in reversed(range(len(self.operatorCodes))):
   5705                 builder.PrependUOffsetTRelative(operatorCodeslist[i])
-> 5706             operatorCodes = builder.EndVector(len(self.operatorCodes))
   5707         if self.subgraphs is not None:
   5708             subgraphslist = []

TypeError: EndVector() takes 1 positional argument but 2 were given
```

This error only occurs with `flabbuffers==2.0`. I could convert an int8 quantization model with `tensorflow==2.7.0` and `flatbuffers==1.12`. This problem is related to the PR https://github.com/tensorflow/tensorflow/pull/51504. In the 
 issue https://github.com/google/flatbuffers/issues/6858, it is discussed that the number of `EndVector()`'s arguments changed from two to one with the flatbuffers version changing from `1.12` to `2.0`."
52953,Missing tensorflow 2.6.2 Windows wheels,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow version: 2.6.2
- Python version: 3.8
- Installed using: poetry (pip)

After running poetry (1.1.11) on a project, the dependency resolution picks tensorflow version 2.6.2 but when it tries to install the package it fails because tensorflow 2.6.2 is missing the Windows wheels (https://pypi.org/project/tensorflow/2.6.2/#files).

Windows wheels are available for versions 2.6.0 (https://pypi.org/project/tensorflow/2.6.0/#files), 2.6.1 (https://pypi.org/project/tensorflow/2.6.1/#files) and 2.7.0 (https://pypi.org/project/tensorflow/2.7.0/#files). 
"
52952,"""tf.print()"" breaks the tensor value order in OrderedDict","**System information**
- OS Platform and Distribution: macOS Big Sur 11.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version: The issue occurs in both TF v1.15.2 and TF v2.7.
- Python version: Python 3.6.5 with TF v1.15.2, or Python 3.8.3 with TF v2.7

**Issue Description & Repro Code**

I am using `tf.print()` to log the intermediate values evaluated from a OrderedDict containing the string-tensor pair, but the evaluation result shows that the tensor order is messed up. The following python script `try_tf_print.py` is an example.

```
from collections import OrderedDict

import tensorflow as tf

tf.compat.v1.disable_eager_execution()


def build_tf_print_ops_with_dict() -> None:
    a = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 1])
    b = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 3])
    c = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 2])
    d = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 5])
    p_dict = OrderedDict([
        (""foo"", a),
        (""bar"", b),
        (""baz"", c),
        (""qux"", d)
    ])
    tf_print_ops = tf.print(p_dict)
    sess = tf.compat.v1.Session()
    sess.run(tf_print_ops, feed_dict={
        a: [[1]],
        b: [[2, 4, 6]],
        c: [[0.2, 0.2]],
        d: [[100, 200, 300, 400, 500]]
    })


build_tf_print_ops_with_dict()
```
I'd expect tf.print() evaluation result to be as `OrderedDict([('foo', [[1]]), ('bar', [[2 4 6]]), ('baz', [[0.2 0.2]]), ('qux', [[100 200 300 400 500]])])`, while what I got was `OrderedDict([('foo', [[2 4 6]]), ('bar', [[0.2 0.2]]), ('baz', [[1]]), ('qux', [[100 200 300 400 500]])])`, in which the tensor value order is broken -- It is supposed to follow the OrderedDict order `(""foo"", a), (""bar"", b), (""baz"", c), (""qux"", d)`, but the order it gets is `(""foo"", b), (""bar"", c), (""baz"": a), (""qux"", d)`. The issue could be 100% reproduced by running above script `try_tf_print.py`.


"
52951,Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker image 
- TensorFlow version: 2.6.1
- Keras version: Getting the above error when import
- Python version: Python 3.6
- Installed using virtualenv? pip? conda?: pip (Docker image)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory: Nvidia A100 40GB 



**Describe the problem**
Looks like this issue , https://github.com/keras-team/keras/issues/15579
Is it the right one ? because after applying the fix mentioned in above link ""pip install tensorflow==2.6.2"" , getting below error
```
root@6a893d98bbb1:/app/project# python3 download_process.py --help
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Traceback (most recent call last):
  File ""download_process.py"", line 13, in <module>
    from utils import *
  File ""/app/project/utils.py"", line 4, in <module>
    from object_detection.inputs import train_input
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py"", line 26, in <module>
    from object_detection.builders import model_builder
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 29, in <module>
    from object_detection.builders import matcher_builder
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/matcher_builder.py"", line 23, in <module>
    from object_detection.matchers import bipartite_matcher  # pylint: disable=g-import-not-at-top
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/matchers/bipartite_matcher.py"", line 20, in <module>
    from tensorflow.contrib.image.python.ops import image_ops
ModuleNotFoundError: No module named 'tensorflow.contrib'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Build Image by running below dockerfile

```
FROM tensorflow/tensorflow:2.3.0-gpu

RUN apt-get update --fix-missing && apt-get install -y \
    ffmpeg \
    git \
    git-core \
    g++ \
    pkg-config \
    python3-pip \
    unzip \
    vim \
    wget \
    zip \
    zlib1g-dev

WORKDIR /app

COPY requirements.txt .
RUN pip3 install -r requirements.txt
RUN pip3 install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI

ENV TF_CPP_MIN_LOG_LEVEL=2

RUN wget https://github.com/protocolbuffers/protobuf/releases/download/v3.13.0/protoc-3.13.0-linux-x86_64.zip && \
    unzip protoc-3.13.0-linux-x86_64.zip -d /app/protobuf/

ENV PATH ""$PATH:/app/protobuf/bin""

RUN git clone https://github.com/tensorflow/models.git && \
    cd /app/models/research/ && \
    protoc object_detection/protos/*.proto --python_out=. && \
    cp object_detection/packages/tf2/setup.py . &&\
    python -m pip install .
```

- Spawned container and inside it trying to run a pre-processing script. Below are packages used in it
   
```
import argparse
import io
import os
import subprocess

import ray
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
#import tensorflow as tf
from PIL import Image
from psutil import cpu_count
tf.disable_v2_behavior()
from utils import *
from object_detection.utils import dataset_util, label_map_util
```

- Error Log Output
```
2021-11-04 10:25:18.325660: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers
Traceback (most recent call last):
  File ""download_process.py"", line 13, in <module>
    from utils import *
  File ""/app/project/utils.py"", line 4, in <module>
    from object_detection.inputs import train_input
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/inputs.py"", line 26, in <module>
    from object_detection.builders import model_builder
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 25, in <module>
    from object_detection.builders import box_predictor_builder
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/box_predictor_builder.py"", line 20, in <module>
    from object_detection.predictors import convolutional_box_predictor
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/predictors/convolutional_box_predictor.py"", line 26, in <module>
    from object_detection.core import box_predictor
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/core/box_predictor.py"", line 137, in <module>
    class KerasBoxPredictor(tf.keras.layers.Layer):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/usr/local/lib/python3.6/dist-packages/keras/__init__.py"", line 25, in <module>
    from keras import models
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""/usr/local/lib/python3.6/dist-packages/keras/metrics.py"", line 26, in <module>
    from keras import activations
  File ""/usr/local/lib/python3.6/dist-packages/keras/activations.py"", line 20, in <module>
    from keras.layers import advanced_activations
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py"", line 23, in <module>
    from keras.engine.input_layer import Input
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py"", line 21, in <module>
    from keras.engine import base_layer
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 43, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""/usr/local/lib/python3.6/dist-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 18, in <module>
    from keras import optimizers
  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizers.py"", line 26, in <module>
    from keras.optimizer_v2 import adadelta as adadelta_v2
  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/adadelta.py"", line 22, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/optimizer_v2.py"", line 37, in <module>
    ""/tensorflow/api/keras/optimizers"", ""keras optimizer usage"", ""method"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py"", line 361, in __init__
    len(labels), name, description, *labels)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py"", line 135, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.

```



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
52947,Build did NOT complete successfully,"Hi,

I'm using python Python 3.6.8 on CentOS 7, my bazel version is bazel 3.1.0. Using ""git checkout r2.4"" and trying to make the C++ interface I get the following error:

`INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=237
INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/u211355/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.6 --action_env PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages --python_path=/usr/bin/python3.6 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/u211355/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/u211355/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/u211355/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:linux in file /home/u211355/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/u211355/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
INFO: Build options --action_env, --define, and --host_copt have changed, discarding analysis cache.
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (211 packages loaded, 20042 targets configured).
INFO: Found 1 target...
ERROR: /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/external/com_google_protobuf/BUILD:110:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command
  (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \
    PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o)
Execution platform: @local_execution_config_platform//:platform
gcc: error: unrecognized command line option '-std=c++14'
Target //tensorflow:libtensorflow_cc.so failed to build
ERROR: /home/u211355/tensorflow/tensorflow/BUILD:820:1 C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): gcc failed: error executing command
  (cd /home/u211355/.cache/bazel/_bazel_u211355/4bc589a879d91108e775a9cd349c5c7c/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib \
    PATH=/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-7/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-8/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-9/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/rh/devtoolset-10/root/usr/bin:/home/u211355/xcrysden-1.5.60-bin-semishared:/home/u211355/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/.local/bin:/home/u211355/bin:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util:/home/u211355/xcrysden-1.5.60-bin-semishared/scripts:/home/u211355/xcrysden-1.5.60-bin-semishared/util \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -g0 -w -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 55.118s, Critical Path: 0.02s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
`
I would appreciate your help with this error. I have also tried different versions of GCC from 7-10 and that also didn't solve the error.

Thanks
Amir"
52945,AutoGraph could not transform function map_fn,"**System information**
- I have written custom code:
- OS Platform and Distribution: Ubuntu 20.04.3 LTS
- TensorFlow installed from: conda install tensorflow-gpu
- TensorFlow version: 2.4.1
- Python version: 3.8.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

n_examples=10
n_features=10

examples = np.random.rand(n_features,n_examples)

with tf.io.TFRecordWriter('float-examples.tfrecord') as tfrecord:
    for idx in range(examples.shape[0]):
        label = [idx]
        feature = examples[idx]
        features = {
            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label)),
            'feature': tf.train.Feature(float_list=tf.train.FloatList(value=feature))
        }
        example = tf.train.Example(features=tf.train.Features(feature=features))
        tfrecord.write(example.SerializeToString())

def map_fn(serialized_example):
    feature = {
        'label': tf.io.FixedLenFeature([1], tf.int64),
        'feature': tf.io.FixedLenFeature([n_features], tf.float32)
    }
    example = tf.io.parse_single_example(serialized_example, feature)
    return example['feature'], example['label']

dataset = tf.data.TFRecordDataset('float-examples.tfrecord')
dataset = dataset.map(map_fn)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Warning**
```
WARNING:tensorflow:AutoGraph could not transform <function map_fn at 0x7f8277779a60> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function map_fn at 0x7f8277779a60> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

2021-11-04 15:13:10.525042: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-11-04 15:13:10.525665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-11-04 15:13:10.587434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.588392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5
coreClock: 1.185GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 245.91GiB/s
2021-11-04 15:13:10.588467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-11-04 15:13:10.610012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-11-04 15:13:10.610112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-11-04 15:13:10.623555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-11-04 15:13:10.626966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-11-04 15:13:10.649055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-11-04 15:13:10.652900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-11-04 15:13:10.657991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-11-04 15:13:10.658218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.658977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.659536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-11-04 15:13:10.660407: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-04 15:13:10.662399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.662721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 with Max-Q Design computeCapability: 7.5
coreClock: 1.185GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 245.91GiB/s
2021-11-04 15:13:10.662743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-11-04 15:13:10.662758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-11-04 15:13:10.662765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-11-04 15:13:10.662772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-11-04 15:13:10.662779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-11-04 15:13:10.662786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-11-04 15:13:10.662794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-11-04 15:13:10.662801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-11-04 15:13:10.662834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.663109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:10.663354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-11-04 15:13:10.663616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-11-04 15:13:11.325157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-11-04 15:13:11.325186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-11-04 15:13:11.325191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-11-04 15:13:11.325667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:11.326063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:11.326339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-04 15:13:11.326585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4771 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)
2021-11-04 15:13:11.327677: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
```"
52944,tf.print() with XLA compilation,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

Hello,

Does anyone know whether tf.print (or an equivalent workaround) can be made to work with XLA compilation? I get an error saying that XLA does not recognize the ""printV2"" operation if I have a tf.print statement inside a function decorated with @tf.function(jit_compile=True).

If this functionality does not exist, would like to request that it be added as a new feature!

Thanks!

**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
tf.print() does not seem to work when used within functions that are compiled with XLA ( decorated with @tf.function(jit_compile=True)

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Users of XLA-compiled tensorflow code. 

**Any Other info.**
"
52941,tf.keras.backend.set_floatx does not change the default dtype in tensor initialisation,"I would like to change the default float precision for initialising tensors and found the [tf.keras.backend.set_floatx](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx) function in the documentation. 
When I use this function, `tf.keras.backend.floatx()` shows the configured float type but unexpectedly this type does not affect the type of created tensors:
```python3
python3 -c 'import tensorflow as tf; tf.keras.backend.set_floatx(""float64""); print(tf.keras.backend.floatx(), tf.constant(1.0))'
```
The command outputs `float64 tf.Tensor(1.0, shape=(), dtype=float32)` when I execute it.
I have tested it with intel_tensorflow 2.6.0, tensorflow 2.6.0 from conda-forge, tensorflow-gpu 2.4.1 from conda-forge and tensorflow 2.6.1 from pip3. The type of the created tensor never matched the configured default type.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I didn't execute an official example script.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux and Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not applicable
- TensorFlow installed from (source or binary): conda-forge and pip3
- TensorFlow version (use command below): intel_tensorflow 2.6.0, tensorflow 2.6.0 from conda-forge, tensorflow-gpu 2.4.1 from conda-forge and tensorflow 2.6.1 from pip3
- Python version: 3.9.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA does not yet work for me
- GPU model and memory: no nvidia GPU and Nvidia GeForce RTX 3080

**Describe the current behavior**

The default float precision is used when creating a tensor.


**Describe the expected behavior**

If I don't explicitly specify a dtype argument, the dtype is float32 and not the current keras backend floatx.


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```python3
python3 -c 'import tensorflow as tf; tf.keras.backend.set_floatx(""float64""); print(tf.keras.backend.floatx(), tf.constant(1.0))'
```

**Other info / logs**"
52938,session config conflict with server_def_,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version:3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.1 (clang-1001.0.46.4)
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
```zsh: command not found: v1.12.1-54214-gb51e7cff1aa```

code:
```
config0 = tf.ConfigProto(intra_op_parallelism_threads=0)
config1 = tf.ConfigProto(intra_op_parallelism_threads=1)
server = tf.train.Server.create_local_server(config=config0)
sess0 = tf.Session(server.target, config=config0)
sess1 = tf.Session(server.target, config=config1)
```
**Describe the current behavior**
sess1 have config0
**Describe the expected behavior**
sess1 have config1

**[Contributing](https://www.tensorflow.org/community/contribute)**
- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
-     https://github.com/tensorflow/tensorflow/pull/52939

session config in server to session: https://github.com/tensorflow/tensorflow/issues/8982 
background:
1. We use two sessions to run two graphs in two thread.
2. two graph share a resource-mgr, because we hope save a checkpoint ."
52937,tf.keras import raises an AlreadyExistsError with keras 2.7,"
Hello there :wave:

I encountered a CI problem with a build job today that wasn't happening yesterday. So I checked the difference in terms of dependency and the only difference was keras. So I inspected the traceback and ended up tracking the import from keras that causes trouble. I already reported this to the keras team in https://github.com/keras-team/keras/issues/15585 but I figured it might be of importance to you folks considering this impacts several imports from tensorflow itself!

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.4.100 & cuDNN 8.2.2 
- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design

**Describe the current behavior**

Running the standalone code throws an `AlreadyExistsError`

**Describe the expected behavior**

Not raising any error.

- Do you want to contribute a PR? (yes/no): happy to do so, but I'm not sure how to solve this
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```python
from tensorflow.keras.utils import img_to_array
```

**Other info / logs** Include any logs or source code that would be helpful to

```
---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
<ipython-input-1-a2ab22b98110> in <module>
----> 1 from tensorflow.keras.utils import img_to_array

~/miniconda3/lib/python3.8/site-packages/keras/api/_v2/keras/__init__.py in <module>
      8 import sys as _sys
      9 
---> 10 from keras import __version__
     11 from keras.api._v2.keras import __internal__
     12 from keras.api._v2.keras import activations

~/miniconda3/lib/python3.8/site-packages/keras/__init__.py in <module>
     23 
     24 # See b/110718070#comment18 for more details about this import.
---> 25 from keras import models
     26 
     27 from keras.engine.input_layer import Input

~/miniconda3/lib/python3.8/site-packages/keras/models.py in <module>
     18 import tensorflow.compat.v2 as tf
     19 from keras import backend
---> 20 from keras import metrics as metrics_module
     21 from keras import optimizer_v1
     22 from keras.engine import functional

~/miniconda3/lib/python3.8/site-packages/keras/metrics.py in <module>
     24 
     25 import numpy as np
---> 26 from keras import activations
     27 from keras import backend
     28 from keras.engine import base_layer

~/miniconda3/lib/python3.8/site-packages/keras/activations.py in <module>
     18 
     19 from keras import backend
---> 20 from keras.layers import advanced_activations
     21 from keras.utils.generic_utils import deserialize_keras_object
     22 from keras.utils.generic_utils import serialize_keras_object

~/miniconda3/lib/python3.8/site-packages/keras/layers/__init__.py in <module>
     21 
     22 # Generic layers.
---> 23 from keras.engine.input_layer import Input
     24 from keras.engine.input_layer import InputLayer
     25 from keras.engine.input_spec import InputSpec

~/miniconda3/lib/python3.8/site-packages/keras/engine/input_layer.py in <module>
     19 from keras import backend
     20 from keras.distribute import distributed_training_utils
---> 21 from keras.engine import base_layer
     22 from keras.engine import keras_tensor
     23 from keras.engine import node as node_module

~/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in <module>
     41 from keras.engine import node as node_module
     42 from keras.mixed_precision import autocast_variable
---> 43 from keras.mixed_precision import loss_scale_optimizer
     44 from keras.mixed_precision import policy
     45 from keras.saving.saved_model import layer_serialization

~/miniconda3/lib/python3.8/site-packages/keras/mixed_precision/loss_scale_optimizer.py in <module>
     16 
     17 from keras import backend
---> 18 from keras import optimizers
     19 from keras.mixed_precision import loss_scale as keras_loss_scale_module
     20 from keras.optimizer_v2 import optimizer_v2

~/miniconda3/lib/python3.8/site-packages/keras/optimizers.py in <module>
     24 from keras.optimizer_v1 import Optimizer
     25 from keras.optimizer_v1 import TFOptimizer
---> 26 from keras.optimizer_v2 import adadelta as adadelta_v2
     27 from keras.optimizer_v2 import adagrad as adagrad_v2
     28 from keras.optimizer_v2 import adam as adam_v2

~/miniconda3/lib/python3.8/site-packages/keras/optimizer_v2/adadelta.py in <module>
     20 import numpy as np
     21 from keras import backend_config
---> 22 from keras.optimizer_v2 import optimizer_v2
     23 from tensorflow.python.util.tf_export import keras_export
     24 

~/miniconda3/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py in <module>
     34 
     35 
---> 36 keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(
     37     ""/tensorflow/api/keras/optimizers"", ""keras optimizer usage"", ""method"")
     38 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, name, description, *labels)
    358       *labels: The label list of the new metric.
    359     """"""
--> 360     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
    361                                     len(labels), name, description, *labels)
    362 

~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, metric_name, metric_methods, label_length, *args)
    133           self._metric_name, len(self._metric_methods)))
    134 
--> 135     self._metric = self._metric_methods[self._label_length].create(*args)
    136 
    137   def __del__(self):

AlreadyExistsError: Another metric with the same name already exists.
```
"
52936,@mihaimaruseac I'm working in colab and  changed `tf.gfile.GFile` to `tf.io.gfile.GFile` but still get this error,"@mihaimaruseac I'm working in colab and  changed `tf.gfile.GFile` to `tf.io.gfile.GFile` but still get this error
`AttributeError: module 'tensorflow' has no attribute 'gfile'
`

_Originally posted by @mitramir55 in https://github.com/tensorflow/tensorflow/issues/31315#issuecomment-643277101_"
52935,TFlite model always have fixed dimensions output,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):all the platform:(win10 ubuntu android)
- TensorFlow installed from (source or binary):(colab default)
- TensorFlow version:2.6.0(colab default)

I try to add some postprocess layers after [a Keras model](https://drive.google.com/file/d/1yKLW5F7-6x9ylH65MgVxH5N5oQJgK8wq/view?usp=sharing) which can detect faces in an image, then I convert it to Savedmodel format and the savedmodel also works well.After that I convert it to[ a tflite model](https://drive.google.com/file/d/1i1fLQTQGA7V8tcihyWhldoIswOaEe0tq/view?usp=sharing), but no matter what image I input, the tflite model alway has a fixed dimensions and it equals to a parameter [**max_output_size**,17] which I set in  
`out_boxes = tf.image.non_max_suppression(box_tlbr, scores, max_output_size=5,
                                                 score_threshold=0.5, iou_threshold=0.3)`
eg:
![no_face](https://user-images.githubusercontent.com/32239722/140266840-bab3021e-548f-4b28-8079-5d8902cf5c57.PNG)
tflite model output:
`[[ 4.5394583e+00  3.3936653e+00  2.9001554e+01  2.9001198e+01
   3.2894442e+00 -1.5875316e+00  9.0569210e+00 -7.6022291e-01
   5.5141363e+00  2.7957358e+00  3.5612986e+00  8.0328922e+00
  -3.7986336e+00  9.2154026e-02  1.3225850e+01  2.9794626e+00
   4.5634112e-03]
 [ 4.5394583e+00  3.3936653e+00  2.9001554e+01  2.9001198e+01
   3.2894442e+00 -1.5875316e+00  9.0569210e+00 -7.6022291e-01
   5.5141363e+00  2.7957358e+00  3.5612986e+00  8.0328922e+00
  -3.7986336e+00  9.2154026e-02  1.3225850e+01  2.9794626e+00
   4.5634112e-03]
 [ 4.5394583e+00  3.3936653e+00  2.9001554e+01  2.9001198e+01
   3.2894442e+00 -1.5875316e+00  9.0569210e+00 -7.6022291e-01
   5.5141363e+00  2.7957358e+00  3.5612986e+00  8.0328922e+00
  -3.7986336e+00  9.2154026e-02  1.3225850e+01  2.9794626e+00
   4.5634112e-03]
 [ 4.5394583e+00  3.3936653e+00  2.9001554e+01  2.9001198e+01
   3.2894442e+00 -1.5875316e+00  9.0569210e+00 -7.6022291e-01
   5.5141363e+00  2.7957358e+00  3.5612986e+00  8.0328922e+00
  -3.7986336e+00  9.2154026e-02  1.3225850e+01  2.9794626e+00
   4.5634112e-03]
 [ 4.5394583e+00  3.3936653e+00  2.9001554e+01  2.9001198e+01
   3.2894442e+00 -1.5875316e+00  9.0569210e+00 -7.6022291e-01
   5.5141363e+00  2.7957358e+00  3.5612986e+00  8.0328922e+00
  -3.7986336e+00  9.2154026e-02  1.3225850e+01  2.9794626e+00
   4.5634112e-03]]`
and 
![friends](https://user-images.githubusercontent.com/32239722/140267008-b24ebde3-66b3-431c-b9bb-31a1fbd36e9d.jpg)
tflite model output:
`[[ 26.30391     45.815998    15.318579    15.318455    25.36796
   41.83495     29.585167    45.42548     25.27378     46.95403
   23.45119     49.55304     22.601065    40.5007      32.286842
   48.044865     0.7468504 ]
 [ 78.23201     51.20295     19.179186    19.179142    78.84692
   47.8986      83.48271     50.48215     80.6107      52.646446
   78.44246     54.880447    72.91944     45.85292     83.785065
   51.694183     0.6747569 ]
 [ 53.54354     47.331875    19.37531     19.375237    50.628742
   43.143784    55.891808    45.867424    51.231808    47.359013
   50.78139     50.537304    49.22387     43.62208     60.94362
   48.629707     0.6197063 ]
 [101.9684      49.856655    19.639782    19.639744    98.39309
   46.63186    104.05909     46.658813   100.23142     49.016006
  100.64084     52.422737    96.56004     47.981262   109.495316
   47.874664     0.5907868 ]
 [ 43.294758    48.23123     18.912622    18.91256     41.17029
   45.106667    45.666157    46.91061     41.867565    48.94021
   41.34953     51.683       38.825493    43.775955    50.385445
   47.55902      0.57340574]]`
The last column means the face confidence, I know I can filter some boxes,  just out of curiosity，  does it have to be a fixed dimension output or it is some bug in tflite's nms






Codes I use for add some layers:
```
blazeface_tf_model = tf.keras.models.load_model(r""/content/blazeface_tf.h5"")
class FaceDetetor(layers.Layer):
    def __init__(self):
        super(FaceDetetor, self).__init__()
        self.classifier = blazeface_tf_model


    def call(self, inputs):
        input_tensor = (inputs / 127.5) - 1
        temp_tensor = self.classifier(input_tensor)
        
        reshape_tensor = tf.reshape(temp_tensor, [-1, temp_tensor.shape[2]])
        final_boxes = tf.slice(reshape_tensor, [0, 0], [-1, 4])
        
        temp_boxex_1 = tf.slice(final_boxes, [0, 0], [-1, 2])  # 中心 y_x
        temp_boxex_2 = tf.slice(final_boxes, [0, 2], [-1, 2])  # w_h
        temp_boxex_2_1 = temp_boxex_2 / 2
        
        ts_sub1 = tf.subtract(temp_boxex_1, temp_boxex_2_1, name=None)
        temp_clip = tf.clip_by_value(ts_sub1, 0, 100000000)
        
        ts_add1 = tf.add(temp_boxex_1, temp_boxex_2_1, name=None)
        
        yx1_columns = tf.unstack(temp_clip, axis=-1)
        xy1_columns = tf.stack([yx1_columns[1], yx1_columns[0]], axis=-1)
        yx2_columns = tf.unstack(ts_add1, axis=-1)
        xy2_columns = tf.stack([yx2_columns[1], yx2_columns[0]], axis=-1)
        box_tlbr = tf.concat([xy1_columns, xy2_columns], 1)
        #### xywh_to_tlbr ####
        raw_scores = tf.slice(reshape_tensor, [0, temp_tensor.shape[2] - 1], [-1, 1])
        scores = tf.reshape(raw_scores, [-1])
        out_boxes = tf.image.non_max_suppression(box_tlbr, scores, max_output_size=5,
                                                 score_threshold=0.5, iou_threshold=0.3)
       
        rows = tf.gather(reshape_tensor, out_boxes, axis=0)
        final_boxes = tf.slice(rows, [0, 0], [-1, temp_tensor.shape[2] - 1])
        if len(final_boxes.shape) == 1:
            final_boxes = tf.expand_dims(final_boxes, axis=0)
        orig_points = final_boxes * 128
        final_scores = tf.gather(raw_scores, out_boxes, axis=0)
        final_result = tf.concat([orig_points, final_scores], 1)
        return final_result
max_output_size = 5
score_threshold = 0.75
iou_threshold = 0.5
inputs_0 = keras.Input(batch_shape=((1, 128, 128, 3)), dtype=tf.float32, name=""input_images"")
# inputs_1 = keras.Input(shape=(1), dtype=tf.int32, name=""max_output_size"")
# inputs_2 = keras.Input(shape=(1), dtype=tf.float32, name=""score_threshold"")
# inputs_3 = keras.Input(shape=(1), dtype=tf.float32, name=""iou_threshold"")
outputs = FaceDetetor()(inputs_0)

model_folder_path = r""/content/face_detector_inputs3""
model = keras.Model(inputs_0, outputs=outputs)
model.save(model_folder_path, save_format='tf')
print(""saved model!"")
```

Codes I use for savedmodel convert to tflite:
```
converter = tf.lite.TFLiteConverter.from_saved_model(model_folder_path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_types = [tf.float16]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
    ]
tflite_model = converter.convert()
output_tflite_path_float = r'/content/face_detector_float16.tflite'
with open(output_tflite_path_float, 'wb') as f:
    f.write(tflite_model)
```

**Standalone code to reproduce the issue** 
For more  details: [colab ](https://colab.research.google.com/drive/12fIvXNJvrtgeUvgfwVbUwISQdWStmStz?usp=sharing)

"
52934,[TF-TRT] - TRTEngineOP do not show output shape,"TRTEngineOP node attribute `_output_shapes` returns an empty list. We need to fix that.

@bixia1 @tfeher  FYI"
52933,Failing to Quantize for Edge TPU. Requires tf.float32 type for conversion but tf.uint8 for Edge,"I am attempting to quantize a model post creation for use on a coral product. This requires me to quantize the model, unfortunately quantization throws a significant error. Below is the architecture of my model and associated Colab notebook showing the error. In short, the model successfully converts to a tflite when I allow the input and output to be tf.float32, which renders a TPU useless, but when I code the model to adjust the input and output to tf.uint8 I get a ValueError. 

I have spoken with Coral extensively and they have referred me here. I really hope you can help out!



 1. System information

- Windows 10
- TensorFlow installation Anaconda
- TensorFlow library: 2.6.0

 2. Code

https://colab.research.google.com/drive/1CdjguiaWf-M5VXv9mqmfcbJXrGt7m-xt?usp=sharing


Error log

Log from Edge TPU compiler when input output is allowed to be tf.float32

Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 472 ms.

Input model: IVUS_v1.tflite
Input size: 133.24MiB
Output model: IVUS_v1_edgetpu.tflite
Output size: 133.23MiB
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 0
Total number of operations: 81
Operation log: IVUS_v1_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 81

Operator                       Count      Status

CONV_2D                        28         Operation is working on an unsupported data type
TRANSPOSE_CONV                 7          Operation is working on an unsupported data type
LOGISTIC                       1          Operation is working on an unsupported data type
FULLY_CONNECTED                1          Operation is working on an unsupported data type
ADD                            1          Operation is working on an unsupported data type
PRELU                          28         Operation is working on an unsupported data type
MAX_POOL_2D                    7          Operation is working on an unsupported data type
CONCATENATION                  7          Operation is working on an unsupported data type
MUL                            1          Operation is working on an unsupported data type
Compilation child process completed within timeout period.
Compilation succeeded!"
52923,Restoring a model with frozen TimeDistributed layers onto an unfrozen model does not work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H1
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.6.0-0-g919f693420e 2.6.0
- Python version: 3.7.12
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
The following (minimal) implementation of the model and weight saving and loading fails with the timedistributed layer but works without it.

```
import os
from tensorflow.keras.layers import Dense, Input, TimeDistributed
from tensorflow.keras.activations import sigmoid
from tensorflow.keras.metrics import BinaryAccuracy
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import MobileNetV2

def get_model(working_dir, finetune):

    base_model = MobileNetV2(weights='imagenet',
                             include_top=False,
                             alpha=1.4,
                             input_shape=(224, 224, 3),
                             pooling='avg')

    inp = Input((10, 224, 224, 3))
    x = TimeDistributed(base_model)(inp)

    # Comment out the previous two lines
    # and uncomment these next two lines to see the expected behavior

    # inp = base_model.input
    # x = base_model.output

    predictions = Dense(1, activation=sigmoid)(x)

    model = Model(inputs=inp, outputs=predictions)

    if not finetune:
        for layer in base_model.layers:
            layer.trainable = False
        learning_rate = 0.001
    else:
        saved_model_path = os.path.join(working_dir, ""saved_model.h5"")
        model.load_weights(saved_model_path)
        learning_rate = 0.00001

    binacc = BinaryAccuracy(name=""binary_accuracy"")
    adam = Adam(lr=learning_rate, epsilon=1e-09, clipnorm=0.001)

    model.compile(optimizer=adam,
                    loss='binary_crossentropy',
                    metrics=[binacc])
    model.summary()

    return model

working = ""/content/test""

model = get_model(working, False)
model.save(os.path.join(working, ""saved_model.h5""))
model = get_model(working, True)
```

I'm trying to finetune a model that I'm working on. I'm initially creating a model with a pretrained model connected to some new layers. The pretrained model is frozen for the first round of training and the model is saved. I then create the same model with the pretrained model unfrozen and try to restore the weights using model.load_weights. This results in the following error on my local pc running tensorflow 2.4.0.

""ValueError: Cannot assign to variable expanded_conv_depthwise/depthwise_kernel:0 due to variable shape (3, 3, 48, 1) and value shape (48,) are incompatible"" and 

On Google Colab running tensorflow 2.6.0, I get

""ValueError: axes don't match array""

**Describe the expected behavior**

Weights should be restored correctly.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1X92pRi4yqNhOO0l_AAbpbSpaGidwytlP
"
52922,TensorFlow 2.6 installs Keras 2.7,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: 2.6.1
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv + pip

**Describe the problem**

TensorFlow 2.6 installs Keras 2.7, but it should install Keras 2.6 instead.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
pip install tensorflow==2.6.*
```

**Any other info / logs**

See `keras-2.7.0` in the list of installed packages:

> `Successfully installed absl-py-0.15.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 clang-5.0 dataclasses-0.8 flatbuffers-1.12 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.1 h5py-3.1.0 idna-3.3 importlib-metadata-4.8.1 keras-2.7.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.1 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.7 werkzeug-2.0.2 wrapt-1.12.1 zipp-3.6.0`
"
52921,"ValueError: input resource[0] expected type resource != float, the type of embeddings_sharded_0[0]","Hi,
I'm going to optimize my model and convert it to FP16. When using trt_conver, this error raises:
```
ValueError: input resource[0] expected type resource != float, the type of embeddings_sharded_0[0]
        In {{node EncoderTransformer/EmbeddingLookup/EmbeddingLookupUnique/embedding_lookup/Gather}}
```"
52920,Bazel build tensorflow error - C++ compilation of rule,"**System information**
- OS Platform and Distribution: Debian buster Raspbian 10 
- TensorFlow installed from source
- TensorFlow version: 2.6.0
- Python version: 3.7.3
- Bazel version: 3.7.2 
- GCC/Compiler version: 8.3.0
- CUDA/cuDNN version: No CUDA
- GPU model and memory: No GPU


I am trying to build tensorflow using bazel (3.7.2). But when I run:
`bazel build //tensorflow/tools/pip_package:build_pip_package`

I got this error after ~13 hours running.

```
ERROR: /home/pi/tensorflow/tensorflow/compiler/xla/service/llvm_ir/BUILD:64:11: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:llvm_util' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 141 argument(s) skipped)
In file included from external/com_google_absl/absl/base/internal/endian.h:29,
                 from external/com_google_absl/absl/strings/cord.h:73,
                 from ./tensorflow/core/platform/default/cord.h:22,
                 from ./tensorflow/core/platform/cord.h:25,
                 from ./tensorflow/core/platform/tstring.h:24,
                 from ./tensorflow/core/platform/types.h:23,
                 from ./tensorflow/core/platform/logging.h:20,
                 from ./tensorflow/core/platform/status.h:25,
                 from ./tensorflow/core/lib/core/status.h:19,
                 from ./tensorflow/compiler/xla/status.h:19,
                 from ./tensorflow/compiler/xla/array.h:33,
                 from ./tensorflow/compiler/xla/array2d.h:29,
                 from ./tensorflow/compiler/xla/literal.h:32,
                 from ./tensorflow/compiler/xla/service/llvm_ir/llvm_util.h:35,
                 from tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:16:
external/com_google_absl/absl/base/casts.h: In instantiation of 'Dest absl::lts_20210324::bit_cast(const Source&) [with Dest = long long int; Source = void (*)(const char*, long long int); typename std::enable_if<(! absl::lts_20210324::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':
tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:382:76:   required from here
external/com_google_absl/absl/base/casts.h:176:30: error: static assertion failed: Source and destination types should have equal sizes.
   static_assert(sizeof(Dest) == sizeof(Source),
                 ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
external/com_google_absl/absl/base/casts.h: In instantiation of 'Dest absl::lts_20210324::bit_cast(const Source&) [with Dest = long long int; Source = const char*; typename std::enable_if<(! absl::lts_20210324::internal_casts::is_bitcastable<Dest, Source>::value), int>::type <anonymous> = 0]':
tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:384:55:   required from here
external/com_google_absl/absl/base/casts.h:176:30: error: static assertion failed: Source and destination types should have equal sizes.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 48965.078s, Critical Path: 1635.76s
INFO: 7815 processes: 954 internal, 6861 local.
FAILED: Build did NOT complete successfully
```

"
52917,TensorFlow,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
52916,"Missing libtensorflow builds for 2.6.1, 2.5.2 and 2.4.4","There don't appear to be prebuilt binaries for libtensorflow (https://www.tensorflow.org/install/lang_c) for 2.6.1, 2.5.2 and 2.4.4.

These binaries exist for earlier point releases for those minor versions on https://storage.googleapis.com/tensorflow/libtensorflow/, but none exist for the above point releases.

https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.6.1.tar.gz is one of the missing files.

See #48550 for a similar occurrence.

Would it be possible to push those binaries/trigger CI builds to generate them?

Thanks!"
52915,Installing tensorflow in python,"<em>hi, i'm trying to install tensorflow for python but i get two errors installing it, and i google but every solution dosn't work</em>

### **CODE**
`pip3 install tensorflow`

output: 
```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

### **2° TRIAL**
`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.2.0-py2-none-any.whl`

output
```
Defaulting to user installation because normal site-packages is not writeable
ERROR: tensorflow-1.2.0-py2-none-any.whl is not a supported wheel on this platform.

```


**_IT DOESN'T WORK_**
what shall i do??

"
52914,"NNAPI on Kirin 980 NPU. Model won't run on NPU, libc : Access denied finding property ""ro.hardware.chipname""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 11.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei P30 Pro
- TensorFlow installed from (source or binary):  android_aarch64_benchmark_tool
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I have been experiencing NNAPI producing longer inference times on my device than with standard CPU delegate, so i ran benchmark binary with tensorflow prebuilt NNAPI models. The debug from logcat shows that it get stuck with the following error

```
11-02 14:18:11.933 29254 29254 E libc    : Access denied finding property ""ro.hardware.chipname""
11-02 14:18:11.928 29254 29254 W benchmark_model: type=1400 audit(0.0:752014): avc: denied { read } for pid=29254 name=""u:object_r:vendor_default_prop:s0"" dev=""tmpfs"" ino=15480 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0
```

I have tested this on Huawei P30 Pro and Huawei Nova 5T with the same results. 
I downloaded mobilenet_v1_1.0_224_quant.tflite NNAPI prebuilt model from here https://www.tensorflow.org/lite/performance/nnapi#use_supported_models_and_ops

Then i pushed to the device and invoked the benchmarking tool

 .downloaded android aarch64 benchmark tool as per instructions and invoked the model like so
```
adb shell /data/local/tmp/benchmark_model \
  --num_threads=4 \
  --graph=/data/local/tmp/mobilenet_v1_1.0_224_quant.tflite \
  --warmup_runs=1 \
  --num_runs=50 --use_nnapi=1 --verbose=1 --disable_nnapi_cpu=0 --nnapi_accelerator_name=""ipuadaptor""
```

I enabled verbose nnapi log using this command

```adb shell setprop debug.nn.vlog 1```

```
11-02 14:18:11.894 29254 29254 D skia    : HME SkHmeDecFunction1 constructor ok
11-02 14:18:11.894 29254 29254 I HiTraceC: entered LogRegisterGetIdFun
11-02 14:18:11.894 29254 29254 I HiTraceC: entered HiTraceInit
11-02 14:18:11.896 29254 29254 I Manager : DeviceManager::DeviceManager
11-02 14:18:11.896 29254 29254 I Manager : findAvailableDevices
11-02 14:18:11.898 29254 29254 I Manager : Found interface ipuadaptor
11-02 14:18:11.903  1029 15537 I aiserver: IPUNNAdaptor getCapabilities_1_1
11-02 14:18:11.903  1029 15537 I aiserver: AiModelMngrService getCapabilities_1_1
11-02 14:18:11.903  1029 15537 I aiserver: AiModelMngrService getCapabilities
11-02 14:18:11.904  1029 15537 I hcs     : AndroidNNMLUExecutor::GetCapabilities Get device capabilities success.
11-02 14:18:11.904  1029 15537 I AndroidNN: AnnHcsService::GetCapabilities(169)::""Get capabilities done!""
11-02 14:18:11.904 29254 29254 I Manager : Capab {.relaxedFloat32toFloat16PerformanceScalar = {.execTime = 0.470000, .powerUsage = 0.660000}, .relaxedFloat32toFloat16PerformanceTensor = {.execTime = 0.470000, .powerUsage = 0.660000}, .operandPerformance = [7]{{.type = FLOAT32, .info = {.execTime = 0.470000, .powerUsage = 0.660000}}, {.type = INT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = UINT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = TENSOR_FLOAT32, .info = {.execTime = 0.470000, .powerUsage = 0.660000}}, {.type = TENSOR_INT32, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = OEM, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}, {.type = TENSOR_OEM_BYTE, .info = {.execTime = 0.470000, .powerUsage = 0.250000}}}}
11-02 14:18:11.905 29254 29254 I tflite  : Initialized TensorFlow Lite runtime.
11-02 14:18:11.905 29254 29254 I tflite  : Created TensorFlow Lite delegate for NNAPI.
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 1 offset 2405268 size 864
11-02 14:18:11.906 29254 29254 I TypeManager: TypeManager::TypeManager
11-02 14:18:11.906 29254 29254 I TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.
11-02 14:18:11.906 29254 29254 I TypeManager: NNAPI Vendor extensions enabled: 0
11-02 14:18:11.906 29254 29254 I Memory  : add()
11-02 14:18:11.906 29254 29254 I Memory  : It's new
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 2 offset 4219296 size 128
11-02 14:18:11.906 29254 29254 I Memory  : add()
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 3 size 4
11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 0
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 4 size 4
11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 4
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 5 size 4
11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 8
11-02 14:18:11.906 29254 29254 I ModelBuilder: setOperandValue for operand 6 size 4
11-02 14:18:11.906 29254 29254 I ModelBuilder: Copied small value to offset 12
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 8 offset 2400848 size 288
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 9 offset 4219436 size 128
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 10 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 16
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 11 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 20
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 12 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 24
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 13 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 28
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 14 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 32
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 16 offset 3995772 size 2048
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 17 offset 4219576 size 256
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 18 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 36
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 19 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 40
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 20 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 44
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 21 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 48
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 23 offset 2400260 size 576
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 24 offset 4219844 size 256
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 25 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 52
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 26 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 56
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 27 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 60
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 28 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 64
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 29 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 68
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 31 offset 3997840 size 8192
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 32 offset 4220112 size 512
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 33 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 72
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 34 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 76
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 35 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 80
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 36 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 84
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 38 offset 2397036 size 1152
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 39 offset 4220636 size 512
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 40 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 88
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 41 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 92
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 42 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 96
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 43 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 100
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 44 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 104
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 46 offset 2376020 size 16384
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 47 offset 3431192 size 512
11-02 14:18:11.907 29254 29254 I Memory  : add()
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 48 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 108
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 49 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 112
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 50 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 116
11-02 14:18:11.907 29254 29254 I ModelBuilder: setOperandValue for operand 51 size 4
11-02 14:18:11.907 29254 29254 I ModelBuilder: Copied small value to offset 120
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 53 offset 4217608 size 1152
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 54 offset 4218772 size 512
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 55 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 124
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 56 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 128
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 57 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 132
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 58 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 136
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 59 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 140
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 61 offset 3962980 size 32768
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 62 offset 4221160 size 1024
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 63 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 144
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 64 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 148
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 65 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 152
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 66 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 156
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 68 offset 3436360 size 2304
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 69 offset 4222196 size 1024
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 70 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 160
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 71 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 164
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 72 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 168
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 73 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 172
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 74 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 176
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 76 offset 4141756 size 65536
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 77 offset 4223232 size 1024
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 78 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 180
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 79 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 184
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 80 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 188
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 81 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 192
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 83 offset 2369080 size 2304
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 84 offset 4207316 size 1024
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 85 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 196
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 86 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 200
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 87 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 204
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 88 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 208
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 89 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 212
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 91 offset 4006048 size 131072
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 92 offset 4224268 size 2048
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 93 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 216
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 94 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 220
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 95 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 224
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 96 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 228
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 98 offset 2102304 size 4608
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 99 offset 4226328 size 2048
11-02 14:18:11.908 29254 29254 I Memory  : add()
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 100 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 232
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 101 size 4
11-02 14:18:11.908 29254 29254 I ModelBuilder: Copied small value to offset 236
11-02 14:18:11.908 29254 29254 I ModelBuilder: setOperandValue for operand 102 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 240
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 103 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 244
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 104 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 248
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 106 offset 2106924 size 262144
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 107 offset 2403208 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 108 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 252
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 109 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 256
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 110 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 260
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 111 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 264
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 113 offset 2097684 size 4608
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 114 offset 4228388 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 115 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 268
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 116 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 272
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 117 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 276
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 118 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 280
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 119 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 284
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 121 offset 1835524 size 262144
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 122 offset 2401148 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 123 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 288
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 124 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 292
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 125 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 296
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 126 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 300
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 128 offset 2392416 size 4608
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 129 offset 4230448 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 130 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 304
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 131 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 308
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 132 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 312
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 133 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 316
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 134 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 320
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 136 offset 1573368 size 262144
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 137 offset 4232508 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 138 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 324
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 139 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 328
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 140 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 332
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 141 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 336
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 143 offset 4137136 size 4608
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 144 offset 4234568 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 145 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 340
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 146 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 344
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 147 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 348
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 148 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 352
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 149 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 356
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 151 offset 460 size 262144
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 152 offset 2398200 size 2048
11-02 14:18:11.909 29254 29254 I Memory  : add()
11-02 14:18:11.909 29254 29254 I ModelBuilder: setOperandValue for operand 153 size 4
11-02 14:18:11.909 29254 29254 I ModelBuilder: Copied small value to offset 360
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 154 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 364
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 155 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 368
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 156 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 372
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 158 offset 3431740 size 4608
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 159 offset 4236628 size 2048
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 160 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 376
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 161 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 380
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 162 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 384
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 163 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 388
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 164 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 392
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 166 offset 1311208 size 262144
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 167 offset 4238688 size 2048
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 168 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 396
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 169 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 400
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 170 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 404
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 171 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 408
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 173 offset 2371400 size 4608
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 174 offset 4240748 size 2048
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 175 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 412
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 176 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 416
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 177 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 420
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 178 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 424
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 179 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 428
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 181 offset 3438680 size 524288
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 182 offset 4242808 size 4096
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 183 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 432
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 184 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 436
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 185 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 440
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 186 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 444
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 188 offset 4208356 size 9216
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 189 offset 4246916 size 4096
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 190 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 448
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 191 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 452
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 192 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 456
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 193 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 460
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 194 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 464
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 196 offset 262616 size 1048576
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 197 offset 4251024 size 4096
11-02 14:18:11.910 29254 29254 I Memory  : add()
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 198 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 468
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 199 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 472
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 200 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 476
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 201 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 480
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 203 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 484
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 204 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 488
11-02 14:18:11.910 29254 29254 I ModelBuilder: setOperandValue for operand 205 size 4
11-02 14:18:11.910 29254 29254 I ModelBuilder: Copied small value to offset 492
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 206 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 496
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 207 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 500
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 208 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 504
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 210 offset 2406152 size 1025024
11-02 14:18:11.911 29254 29254 I Memory  : add()
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 211 offset 4255132 size 4004
11-02 14:18:11.911 29254 29254 I Memory  : add()
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 212 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 508
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 213 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 512
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 214 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 516
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 215 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 520
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValueFromMemory for operand 217 offset 4259148 size 8
11-02 14:18:11.911 29254 29254 I Memory  : add()
11-02 14:18:11.911 29254 29254 I ModelBuilder: setOperandValue for operand 219 size 4
11-02 14:18:11.911 29254 29254 I ModelBuilder: Copied small value to offset 524
11-02 14:18:11.911 29254 29254 I ModelBuilder: copyLargeValuesToSharedMemory has 0 values.
11-02 14:18:11.912 29254 29254 I GraphDump: // ModelBuilder::finish
11-02 14:18:11.912 29254 29254 I GraphDump: digraph {
11-02 14:18:11.912 29254 29254 I GraphDump:     d0 [style=filled fillcolor=black fontcolor=white label=""0\nTQ8A(1x224x224x3)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d1 [label=""1: REF\nTQ8A(32x3x3x3)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d2 [label=""2: REF\nTI32(32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d3 [label=""3: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d4 [label=""4: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d5 [label=""5: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d6 [label=""6: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d7 [label=""7\nTQ8A(1x112x112x32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d8 [label=""8: REF\nTQ8A(1x3x3x32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d9 [label=""9: REF\nTI32(32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d10 [label=""10: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d11 [label=""11: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d12 [label=""12: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d13 [label=""13: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d14 [label=""14: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d15 [label=""15\nTQ8A(1x112x112x32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d16 [label=""16: REF\nTQ8A(64x1x1x32)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d17 [label=""17: REF\nTI32(64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d18 [label=""18: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d19 [label=""19: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d20 [label=""20: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d21 [label=""21: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d22 [label=""22\nTQ8A(1x112x112x64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d23 [label=""23: REF\nTQ8A(1x3x3x64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d24 [label=""24: REF\nTI32(64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d25 [label=""25: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d26 [label=""26: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d27 [label=""27: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d28 [label=""28: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d29 [label=""29: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d30 [label=""30\nTQ8A(1x56x56x64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d31 [label=""31: REF\nTQ8A(128x1x1x64)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d32 [label=""32: REF\nTI32(128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d33 [label=""33: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d34 [label=""34: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d35 [label=""35: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d36 [label=""36: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d37 [label=""37\nTQ8A(1x56x56x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d38 [label=""38: REF\nTQ8A(1x3x3x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d39 [label=""39: REF\nTI32(128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d40 [label=""40: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d41 [label=""41: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d42 [label=""42: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d43 [label=""43: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d44 [label=""44: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d45 [label=""45\nTQ8A(1x56x56x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d46 [label=""46: REF\nTQ8A(128x1x1x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d47 [label=""47: REF\nTI32(128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d48 [label=""48: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d49 [label=""49: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d50 [label=""50: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d51 [label=""51: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d52 [label=""52\nTQ8A(1x56x56x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d53 [label=""53: REF\nTQ8A(1x3x3x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d54 [label=""54: REF\nTI32(128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d55 [label=""55: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d56 [label=""56: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d57 [label=""57: COPY\nI32 = 2""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d58 [label=""58: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d59 [label=""59: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d60 [label=""60\nTQ8A(1x28x28x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d61 [label=""61: REF\nTQ8A(256x1x1x128)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d62 [label=""62: REF\nTI32(256)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d63 [label=""63: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d64 [label=""64: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d65 [label=""65: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d66 [label=""66: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d67 [label=""67\nTQ8A(1x28x28x256)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d68 [label=""68: REF\nTQ8A(1x3x3x256)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d69 [label=""69: REF\nTI32(256)""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d70 [label=""70: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d71 [label=""71: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d72 [label=""72: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d73 [label=""73: COPY\nI32 = 1""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d74 [label=""74: COPY\nI32 = 0""]
11-02 14:18:11.912 29254 29254 I GraphDump:     d75 [label=""75\nTQ8A(1x28x28x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d76 [label=""76: REF\nTQ8A(256x1x1x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d77 [label=""77: REF\nTI32(256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d78 [label=""78: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d79 [label=""79: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d80 [label=""80: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d81 [label=""81: COPY\nI32 = 0""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d82 [label=""82\nTQ8A(1x28x28x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d83 [label=""83: REF\nTQ8A(1x3x3x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d84 [label=""84: REF\nTI32(256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d85 [label=""85: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d86 [label=""86: COPY\nI32 = 2""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d87 [label=""87: COPY\nI32 = 2""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d88 [label=""88: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d89 [label=""89: COPY\nI32 = 0""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d90 [label=""90\nTQ8A(1x14x14x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d91 [label=""91: REF\nTQ8A(512x1x1x256)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d92 [label=""92: REF\nTI32(512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d93 [label=""93: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d94 [label=""94: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d95 [label=""95: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d96 [label=""96: COPY\nI32 = 0""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d97 [label=""97\nTQ8A(1x14x14x512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d98 [label=""98: REF\nTQ8A(1x3x3x512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d99 [label=""99: REF\nTI32(512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d100 [label=""100: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d101 [label=""101: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d102 [label=""102: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d103 [label=""103: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d104 [label=""104: COPY\nI32 = 0""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d105 [label=""105\nTQ8A(1x14x14x512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d106 [label=""106: REF\nTQ8A(512x1x1x512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d107 [label=""107: REF\nTI32(512)""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d108 [label=""108: COPY\nI32 = 1""]
11-02 14:18:11.913 29254 29254 I GraphDump:     d109 [label=""109: COPY\nI32 = 1""]
11-02 14:18:11.922  1029 15537 I aiserver: IPUNNAdaptor getSupportedOperations_1_1
11-02 14:18:11.922  1029 15537 I aiserver: AiModelMngrService getSupportedOperations_1_1
11-02 14:18:11.923  1029 15537 E aiserver: AiModelMngrServic::getSupportedOperations_1_1 not relaxed Model.
11-02 14:18:11.927  2375  2470 I OllieMsgCenter: publishEvt what:2135162884
11-02 14:18:11.933 29254 29254 E libc    : Access denied finding property ""ro.hardware.chipname""
11-02 14:18:11.928 29254 29254 W benchmark_model: type=1400 audit(0.0:752014): avc: denied { read } for pid=29254 name=""u:object_r:vendor_default_prop:s0"" dev=""tmpfs"" ino=15480 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0
```

To confirm that NPU exists and is available

```
robert@robert:~/Downloads$ adb shell lshal | grep neural
Warning: Skipping ""android.frameworks.cameraservice.service@2.0::ICameraService/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.frameworks.displayservice@1.0::IDisplayService/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.frameworks.schedulerservice@1.0::ISchedulingPolicyService/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.frameworks.sensorservice@1.0::ISensorManager/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.frameworks.stats@1.0::IStats/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.audio.effect@5.0::IEffectsFactory/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.audio@5.0::IDevicesFactory/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.biometrics.fingerprint@2.1::IBiometricsFingerprint/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.bluetooth.audio@2.0::IBluetoothAudioProvidersFactory/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.bluetooth@1.0::IBluetoothHci/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.camera.provider@2.4::ICameraProvider/legacy/0"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.gatekeeper@1.0::IGatekeeper/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.gnss@1.0::IGnss/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.gnss@1.1::IGnss/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.gnss@2.0::IGnss/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.graphics.composer@2.1::IComposer/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.graphics.composer@2.2::IComposer/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.health@2.0::IHealth/backup"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.health@2.0::IHealth/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.ir@1.0::IConsumerIr/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.keymaster@3.0::IKeymasterDevice/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.light@2.0::ILight/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.media.c2@1.0::IComponentStore/software"": no information for PID 1021, are you root?
Warning: Skipping ""android.hardware.memtrack@1.0::IMemtrack/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.nfc@1.0::INfc/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.nfc@1.1::INfc/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.nfc@1.2::INfc/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.power.stats@1.0::IPowerStats/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.power@1.0::IPower/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio.config@1.0::IRadioConfig/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio.config@1.1::IRadioConfig/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio@1.0::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio@1.1::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio@1.2::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio@1.3::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.radio@1.4::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.secure_element@1.0::ISecureElement/SIM1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.sensors@1.0::ISensors/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.soundtrigger@2.0::ISoundTriggerHw/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.thermal@1.0::IThermal/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.usb@1.0::IUsb/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.vibrator@1.0::IVibrator/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi.supplicant@1.0::ISupplicant/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi.supplicant@1.1::ISupplicant/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi.supplicant@1.2::ISupplicant/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi@1.0::IWifi/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi@1.1::IWifi/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi@1.2::IWifi/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hardware.wifi@1.3::IWifi/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/SIM1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/ashmem"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/backup"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/eid"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hiaiserver"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hiaiserver_modelmanager"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hiaiserver_v2"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/huawei.camera.cfgsvr"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/huawei.cameraresource.service"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/huaweiantitheft"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/huaweisigntool"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hwfactoryinterface_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hwhiview_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hwsched"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hwstp"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/hwvoiceid"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/ipuadaptor"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/legacy/0"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/perfgenius"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/perfpolicy"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/rildi"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/software"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/uniperf"": cannot be fetched from service manager (null)
Warning: Skipping ""android.hidl.base@1.0::IBase/virtualcamera.streamchange"": cannot be fetched from service manager (null)
Warning: Skipping ""android.system.net.netd@1.0::INetd/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.system.net.netd@1.1::INetd/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.system.suspend@1.0::ISystemSuspend/default"": cannot be fetched from service manager (null)
Warning: Skipping ""android.system.wifi.keystore@1.0::IKeystore/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.activity_recognition@1.0::IActivityRecognition/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.activity_recognition@1.1::IActivityRecognition/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.audioremote@1.0::IAudioRemoteConnect/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.biometrics.fingerprint@2.1::IExtBiometricsFingerprint/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.biometrics.fingerprint@2.2::IExtBiometricsFingerprint/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.biometrics.hwfacerecognize@2.0::IBiometricsFaceRecognize/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.biometrics.hwsecurefacerecognize@2.0::ISecureBiometricsFaceRecognize/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.biometrics.hwvoiceid@2.0::IBiometricsVoiceId/hwvoiceid"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.bluetooth@1.0::IHwBluetoothHciExt/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.bluetooth@1.1::IHwBluetoothHciExt/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.camResource@1.0::IHwCameraResourceService/huawei.cameraresource.service"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.camResource@1.1::IHwCameraResourceService/huawei.cameraresource.service"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.camResource@1.2::IHwCameraResourceService/huawei.cameraresource.service"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.camResource@1.3::IHwCameraResourceService/huawei.cameraresource.service"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.cfgsvr@1.0::IHwCamCfgSvr/huawei.camera.cfgsvr"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.cfgsvr@1.1::IHwCamCfgSvr/huawei.camera.cfgsvr"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.factory@1.0::ICameraFactory/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.camera.vircamera@1.0::IVirCameraChannel/virtualcamera.streamchange"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.dolby.dms@1.0::IDms/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.dubai@1.0::IDubaiManager/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.dubai@1.1::IDubaiManager/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.eid@1.0::IEid/eid"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.eid@1.1::IEid/eid"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fm@1.0::IFmControl/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.0::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.1::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.2::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.3::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.4::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.5::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.fusd@1.6::IFusdLbs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.gnss@2.0::IHWGnss/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.graphics.hwcinterface@1.0::IHwcInterface/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.graphics.hwcinterface@1.1::IHwcInterface/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.graphics.mediacomm@2.0::IMediaComm/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.graphics.mediacomm@2.1::IMediaComm/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hinetmanager@1.0::IHinetmanagerDevice/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hisiradio@1.0::IHisiRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hisiradio@1.1::IHisiRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hisiradio@1.2::IHisiRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hisiradio@1.3::IHisiRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hivrar@2.0::IHiVRAR/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hivrar@2.1::IHiVRAR/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.huaweiantitheft@1.0::IHuaweiAntiTheft/huaweiantitheft"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.huaweisigntool@1.0::ISignTool/huaweisigntool"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwdisplay.displayengine@1.0::IDisplayEngineWrapper/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwdisplay.displayengine@1.1::IDisplayEngineWrapper/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwdisplay.displayengine@1.2::IDisplayEngineWrapper/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwdisplay@1.0::IDisplay/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwfactoryinterface@1.0::IHwFactoryInterface/hwfactoryinterface_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwfactoryinterface@1.1::IHwFactoryInterface/hwfactoryinterface_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwfs@1.0::IHwfs/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwhiview@1.0::IHwHiviewInterface/hwhiview_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwhiview@1.1::IHwHiviewInterface/hwhiview_hal"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwsched@1.0::ISched/hwsched"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwstp@1.0::IHwStp/hwstp"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwstp@1.1::IHwStp/hwstp"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwvibrator@1.0::IHWVibrator/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.hwvibrator@1.1::IHWVibrator/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.iawareperf@1.0::IUniPerf/uniperf"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.iawareperfpolicy@1.0::IPerfPolicy/perfpolicy"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.iked@1.0::IIkedDevice/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.jpegdec@1.0::IJpegDecode/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.libteec@3.0::ILibteecGlobal/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.light@2.0::ILight/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.modemchr@1.0::IModemchrDevice/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.motion@1.0::IMotion/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.nfc@1.0::IHWNfc/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.perfgenius@2.0::IPerfGenius/perfgenius"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.power@1.0::IHWPower/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.radio.chr@1.0::IRadioChr/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.radio.deprecated@1.0::IOemHook/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.radio.ims@2.0::IRadioIms/rildi"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.radio@2.0::IRadio/slot1"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.sensors@1.0::ISensors/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.sensors@1.1::ISensors/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.tp@1.0::ITouchscreen/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.wifi.supplicant@3.0::ISupplicant/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.wifi@1.0::IHwWifiExt/default"": cannot be fetched from service manager (null)
Warning: Skipping ""vendor.huawei.hardware.wifi@1.1::IHwWifiExt/default"": cannot be fetched from service manager (null)
DM,FC Y android.hardware.neuralnetworks@1.0::IDevice/ipuadaptor                                               N/A        N/A    
DM,FC Y android.hardware.neuralnetworks@1.1::IDevice/ipuadaptor                                               N/A        N/A    
```

"
52912,[feature request & discuss] observe tensor execution efficiency by eBPF tracepoints.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):

NONE

**Describe the feature and the current behavior/state.**

Have you guys ever think about expose tensor processing efficiency in large scale distributed training leveraging eBPF? The traced metrics can be consumed by other components, e.g. resource scheduler and infers the execution quality of each worker and find straggler worker (one of thousands scenarios, just a casual thinking)

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
52910,Build broken by incorrectly dropped comma,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 8.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: git HEAD
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

Build breaks with -
error in tensorflow_aarch64 setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Parse error at ""'< 2.8'"": Expected stringEnd

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

See https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-nightly/156/console

Problem commit is https://github.com/tensorflow/tensorflow/commit/3e842b053711f2d81cd775037875a8d744d79ed5
"
52908,WARNING : tensorflow:AutoGraph could not transform,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.6.0
- Python version: 3.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cudnn-11.4-windows-x64-v8.2.4.15 and cuda_11.4.2_471.41_win10
- GPU model and memory: N/A
-PyCharm details: PyCharm 2021.3 EAP (Community Edition)



**Describe the problem**
The issue occurs when creating an executable in PyCharm using TensorFlow. When running the program in the PyCharm IDE, no errors occur, but when the executable runs, this error and warning occurs:

2021-11-02 00:26:13.629314: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error

WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015EDA772670> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function Model.make_train_function.<locals>.train_function at 0x0000015EDA772670>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
4333/4352 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9866WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015EDA8F0A60> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x0000015EDA8F0A60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Created a project in Python using TensorFlow. Converted the .py file to an executable using: pyinstaller --onefile xxxxxxx.py
When the executable runs, the warning/error occurs.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[TensorFlowBugReport.txt](https://github.com/tensorflow/tensorflow/files/7458622/TensorFlowBugReport.txt)
[TensorFlowPythonCode.txt](https://github.com/tensorflow/tensorflow/files/7458624/TensorFlowPythonCode.txt)

"
52907,tensor lost shape info after tf.math.unsorted_segment_*,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: Python 3.8.5
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 11.1/n/a
- GPU model and memory: GeForce GTX 1070 8117MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import numpy as np


class M1(tf.keras.Model):
    def __init__(self, **kwargs):
        super(M1, self).__init__(**kwargs)
        self.fc = tf.keras.layers.Dense(units=1)

    @tf.function
    def call(self, inputs, training=None):
        x, y, z = inputs
        x = tf.math.unsorted_segment_sum(x, tf.squeeze(y), z)
        return self.fc(x)


class M2(tf.keras.Model):
    def __init__(self, **kwargs):
        super(M2, self).__init__(**kwargs)
        self.fc = tf.keras.layers.Dense(units=1)

    @tf.function
    def call(self, inputs, training=None):
        x, y, z = inputs
        x = tf.math.segment_sum(x, tf.squeeze(y))
        return self.fc(x)


def gen():
    for _ in range(1024):
        offset = np.random.randint(1, 10, size=1024)
        y = np.repeat(np.arange(1024), offset)
        z = 1024
        x = np.random.rand(offset.sum(), 32)
        yield (x, y, z), np.random.rand(1024)


ds = tf.data.Dataset.from_generator(
    gen,
    output_signature=((tf.TensorSpec(shape=(None, 32), dtype=tf.float64),
                      tf.TensorSpec(shape=(None, ), dtype=tf.int64),
                      tf.TensorSpec(shape=[], dtype=tf.int32)),
                       tf.TensorSpec(shape=(None, ), dtype=tf.float64))
)



m1, m2 = M1(), M2()
m1.compile(loss=tf.keras.losses.MeanSquaredError(),
           optimizer=tf.keras.optimizers.Adagrad())
m2.compile(loss=tf.keras.losses.MeanSquaredError(),
           optimizer=tf.keras.optimizers.Adagrad())
# tf.math.segment_* is OK
m2.fit(ds, epochs=8)
# tf.math.unsorted_segment_* FAILED
m1.fit(ds, epochs=8)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""/tmp/pycharm_project_124/python/test/dev.py"", line 54, in <module>
    m1.fit(ds, epochs=8)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/training.py"", line 1184, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 759, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3066, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3463, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3298, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1007, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 668, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 994, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *
        return step_function(self, iterator)
    /tmp/pycharm_project_124/python/test/dev.py:14 call  *
        return self.fc(x)
    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:1030 __call__  **
        self._maybe_build(inputs)
    /usr/local/miniconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:2659 _maybe_build
        self.build(input_shapes)  # pylint:disable=not-callable
    /usr/local/miniconda3/lib/python3.8/site-packages/keras/layers/core.py:1175 build
        raise ValueError('The last dimension of the inputs to `Dense` '

    ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.


Process finished with exit code 1
"
52905,tf.repeat fails with jit_compile when repeats is a tensor!,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**: 
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):compiled from source
- TensorFlow version (use command below):2.5.6
- Python version:3.8
- Bazel version (if compiling from source):bazel 4.2.1
- GCC/Compiler version (if compiling from source):9.3.0
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.repeat fails if the repeats argument is a tensor
**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np


####
#### test repeat within jit_compile
####

repeats = tf.constant([10, 5, 1])

@tf.function(jit_compile = True)
def repeat_vector(x):
    return tf.repeat(x, repeats, axis=-1)

x = [1., 2., 3.] 
repeat_vector(x)
```
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
2021-11-01 19:00:56.491404: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x221bf60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-11-01 19:00:56.491446: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version
2021-11-01 19:00:56.496213: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at xla_ops.cc:247 : Invalid argument: Detected unsupported operations when trying to compile graph __inference_repeat_vector_58[_XlaMustCompile=true,config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000"",executor_type=""""] on XLA_CPU_JIT: Where (No registered 'Where' OpKernel for XLA_CPU_JIT devices compatible with node {{node Repeat/boolean_mask/Where}}){{node Repeat/boolean_mask/Where}}
The op is created at: 
File ""test_repeat.py"", line 20, in <module>
  repeat_vector(x)
File ""test_repeat.py"", line 14, in repeat_vector
  return tf.repeat(x, repeats, axis=-1)
Traceback (most recent call last):
  File ""test_repeat.py"", line 20, in <module>
    repeat_vector(x)
  File ""/home/mabba/tf_cpu_source/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 885, in __call__
    result = self._call(*args, **kwds)
  File ""/home/mabba/tf_cpu_source/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 956, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/home/mabba/tf_cpu_source/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/mabba/tf_cpu_source/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 591, in call
    outputs = execute.execute(
  File ""/home/mabba/tf_cpu_source/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_repeat_vector_58[_XlaMustCompile=true,config_proto=""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000"",executor_type=""""] on XLA_CPU_JIT: Where (No registered 'Where' OpKernel for XLA_CPU_JIT devices compatible with node {{node Repeat/boolean_mask/Where}}){{node Repeat/boolean_mask/Where}}
The op is created at: 
File ""test_repeat.py"", line 20, in <module>
  repeat_vector(x)
File ""test_repeat.py"", line 14, in repeat_vector
  return tf.repeat(x, repeats, axis=-1) [Op:__inference_repeat_vector_58]

```
"
52897,Illegal instruction (core dumped),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04.5 LTS x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:2.6.0
- Python version:3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version:
- GPU model and memory: VMware SVGA II Adapter, 16035MiB


![Screenshot at 2021-11-01 20-31-51](https://user-images.githubusercontent.com/55286282/139698557-1530fd25-fc97-4aa1-bb44-9a240c6f9009.png)
![Screenshot at 2021-11-01 20-55-10](https://user-images.githubusercontent.com/55286282/139698566-a21d34f8-e85d-479e-a0f2-77bdc87e6c95.png)
![Screenshot at 2021-11-01 20-56-47](https://user-images.githubusercontent.com/55286282/139698570-675ead7b-48d0-4a72-a939-a34c16db435b.png)

**Describe the problem**
i am using latest tensorflow and i am getting error. kindly help me to fix this issue.
i saw some fixes they said me to use tensorflow version 1.5 but i have to use tensorflow version 2 is there any other ways to fix it?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf
"
52896,tf.linalg.triangular_solve different behaviour on CPU vs GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- https://colab.research.google.com/drive/1uTUzEQXDR6JvCkFQyS-3n2pAUKAH-TLk?usp=sharing#scrollTo=BcSb0SDQ5nI0
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (also Colab)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.3 (also Colab)
- Python version: 3.7.10

**Describe the current behavior**
When trying to apply tf.linalg.triangular_solve on an invalid input matrix (in my case I included np.nan in the matrix) - different behaviour is observed when running on CPU vs GPU. On GPU - the functions returns NaNs, whereas on CPU an InvalidArgumentError occurs

**Describe the expected behavior**
Consistent behaviour between CPU & GPU

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1uTUzEQXDR6JvCkFQyS-3n2pAUKAH-TLk?usp=sharing#scrollTo=BcSb0SDQ5nI0


"
52895,How to install tensorflow to ARM32 Windows platform? Is there any solution for this problem?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- ARM32 Windows10
- GPU model and memory:16G



**Describe the problem**
ARM base Windows system can't install tensorflow?
Is there any solution for this?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Can't install tensorflow

**Any other info / logs**
Can't install tensorflow on ARM32  Windows10"
52894,Bug: SomeTimes Coredumped using tfjob,"hello, iam using tfjob to train keras model.

most of times, they work fine. But some times, it will crash  after train and savemodel.

our partial train code is here:

```python
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=""/workspace/model/fit_logs/"", histogram_freq=1)

    model.fit(
        dataset,
        epochs=epochs,
        verbose=2#,
#         validation_data=test_dataset#,
#         callbacks = [tensorboard_callback]
#         callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)]
    )

    # checkpoint_path = '/'.join((args.checkpoint_path, save_day, save_hour))
    # if TASK_INDEX == 0:
    #    checkpoint_path = checkpoint_path
    # else:
        # Save to a path that is unique across workers.
    #    checkpoint_path = checkpoint_path + '/worker_tmp_' + str(TASK_INDEX)

    inputs = tf.keras.layers.Input(shape=(input_length,), dtype=tf.int64, name='input')
    outs = model(inputs)
    mymodel = tf.keras.Model(inputs, outs)
    mymodel.save(checkpoint_path)

    if TASK_INDEX == 0:
        # tf2onnx
        try:
            onnx_args = OnnxArgs()
            onnx_args.saved_model = checkpoint_path
            onnx_args.output = checkpoint_path + '/deepfm.onnx'
            onnx_args.tag = 'serve'


            parse2onnx(onnx_args)
            logging.info(""Success"")
            logging.info(onnx_args.saved_model)
        except Exception as e:
            logging.error(""Failed convert"")
            logging.error(str(e))
```
And the log is:
![image](https://user-images.githubusercontent.com/10629930/139662542-31221f80-a0de-4c76-b4f5-cfd017612de4.png)

I get the coredumpe file;

it show this:
![image](https://user-images.githubusercontent.com/10629930/139662656-6a26ea88-2d49-4472-a1b3-e5b07d801654.png)


the operator log is :

i don't know the reason and don't know whether it is a tensorflow bug...
![image](https://user-images.githubusercontent.com/10629930/139662899-d21133db-6b1c-4567-ac5d-6811f41f00f4.png)

Any help will be appreciated... Thanks alot 
"
52893,Tensorflow2.4 matmul returns different result(CPU)  comparing  numpy & torch matmul,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.9

**Describe the current behavior**
```
>>> tf.matmul(inputs, kernel)
<tf.Tensor: shape=(1, 27, 512), dtype=float32, numpy=
array([[[ 0.28822073, -1.4294956 , -0.98417675, ..., -1.3555392 ,
          1.3087683 ,  1.0613352 ],
        [-0.2037763 ,  0.5733212 ,  0.13983761, ..., -0.27103314,
         -0.47976002, -1.5966026 ],
        [-0.20868134,  0.33820766,  0.3898036 , ...,  0.09645928,
         -0.34314552, -0.85622734],
        ...,
        [ 0.7347749 , -0.20192856, -0.6455916 , ...,  0.8365166 ,
         -0.14322701,  0.32350197],
        [-0.02274738,  0.2568301 ,  0.39510253, ..., -0.6229611 ,
          1.9619648 , -0.9404138 ],
        [ 0.2989933 , -1.4857576 , -1.4279732 , ..., -2.0946026 ,
          1.0505235 ,  0.51986885]]], dtype=float32)>

>>> np.matmul(inputs, kernel)
array([[[ 0.28822058, -1.4294958 , -0.9841767 , ..., -1.3555391 ,
          1.308768  ,  1.061335  ],
        [-0.20377651,  0.5733212 ,  0.13983764, ..., -0.27103332,
         -0.47975984, -1.5966021 ],
        [-0.20868114,  0.33820775,  0.38980353, ...,  0.09645933,
         -0.34314537, -0.8562276 ],
        ...,
        [ 0.7347748 , -0.20192836, -0.6455917 , ...,  0.83651644,
         -0.14322703,  0.3235021 ],
        [-0.02274731,  0.25683013,  0.39510256, ..., -0.6229611 ,
          1.9619651 , -0.94041383],
        [ 0.29899323, -1.4857576 , -1.4279728 , ..., -2.0946023 ,
          1.0505236 ,  0.5198687 ]]], dtype=float32)
# And in torch
>>> input.matmul(weight.t()).numpy()
array([[[ 0.28822058, -1.4294958 , -0.9841767 , ..., -1.3555391 ,
          1.308768  ,  1.061335  ]],
       [[-0.20377651,  0.5733212 ,  0.13983764, ..., -0.27103332,
         -0.47975984, -1.5966021 ]],
       [[-0.20868114,  0.33820775,  0.38980353, ...,  0.09645933,
         -0.34314537, -0.8562276 ]],
       ...,
       [[ 0.73477507, -0.20192847, -0.6455916 , ...,  0.83651686,
         -0.14322712,  0.32350177]],
       [[-0.02274727,  0.25683028,  0.39510256, ..., -0.62296104,
          1.9619647 , -0.94041365]],
       [[ 0.29899323, -1.4857576 , -1.427973  , ..., -2.0946026 ,
          1.0505236 ,  0.51986873]]], dtype=float32)
```
As we can see, the number in seventh decimal is different.  TF is 0.28822073, torch and numpy are both 0.28822058.
The function is very important.I want to know why the error appears and how to fix it. Thank you.
**Describe the expected behavior**
The results are all the same.
If someone wants the data(input & weight), I can upload.
ps: input shape is 27 * 512, weight shape is 512 * 512

"
52892,Explanation of tf.keras.layers.CategoryEncoding output_mode='multi_hot' behavior,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding

## Description of issue (what needs changing):

Need clarification on the statement below about multi hot encoding.

> ""multi_hot"": Encodes each sample in the input into a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample. Treats the last dimension as the sample dimension, if input shape is (..., sample_length), output shape will be (..., num_tokens).

## Clarification

Please help understand the definition of **multi hot encoding** of tf.keras.layers.CategoryEncoding and the behavior of ```output_mode='multi_hot'```.

### Background 
According to [What exactly is multi-hot encoding and how is it different from one-hot?](https://stats.stackexchange.com/a/467672):

> If you would use multi-hot-encoding you would first label-encode your classes, thus having only a single number which represents the presence of a class (e.g. 1 for 'dog') and then convert the numerical labels to binary vectors of size log2(5)=3.  
> Examples:
> ```
> 'cat'  = [0,0,0]  
> 'dog'  = [0,0,1]  
> 'fish' = [0,1,0]  
> 'bird' = [0,1,1]  
> 'ant'  = [1,0,0]   
> ```

### Behaviour of [tf.keras.layers.CategoryEncoding][2]

The document says ```num_tokens``` is the total number of tokens the layer should support. 

> ### args
> #### num_tokens
> The total number of tokens the layer should support. All inputs to the layer must integers in the range 0 <= value < num_tokens, or an error will be thrown.
> #### output_mode
> * ""one_hot"": Encodes each individual element in the input into an array of num_tokens size, containing a 1 at the element index. If the last dimension is size 1, will encode on that dimension. If the last dimension is not size 1, will append a new dimension for the encoded output.
> * ""multi_hot"": Encodes each sample in the input into **a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample**. Treats the last dimension as the sample dimension, if input shape is (..., sample_length), output shape will be (..., num_tokens).

According to the definitions of multi hot encoding above, I expected ```tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=""multi_hot"")``` encodes 5 tokens into an array of size 3. 

However, the document says ""multi_hot"" encodes each sample into **a single array of num_tokens size**, containing a 1 for each vocabulary term present in the sample, and behaves as such.


```
dataset = tf.data.Dataset.from_tensor_slices(tf.constant(['cat', 'dog', 'fish', 'bird']))

lookup = tf.keras.layers.StringLookup(max_tokens=5, oov_token='[UNK]')
lookup.adapt(dataset)
lookup.get_vocabulary()
---
['[UNK]', 'fish', 'dog', 'cat', 'bird']

mhe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=""multi_hot"")
print(f""cat: {mhe(lookup(tf.constant('cat'))).numpy()}"")
print(f""dog: {mhe(lookup(tf.constant('dog'))).numpy()}"")
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
```

Which has no difference from One Hot En coding.

```
ohe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=""one_hot"")
print(f""cat: {ohe(lookup(tf.constant('cat'))).numpy()}"")
print(f""dog: {ohe(lookup(tf.constant('dog'))).numpy()}"")
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
```

For multi value inputs, multi_hot only handles the 1st value.
```
print(ohe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]

print(mhe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[0. 0. 1. 1. 0.]
```

To handle multiple inputs, need to be 2D array.
```
print(mhe(lookup(tf.constant([['cat'], ['dog']]))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]
```

Apparently the definition of **mluti hot encoding** of ```tf.keras.layers.CategoryEncoding``` is not the same with the one in [What exactly is multi-hot encoding and how is it different from one-hot?](https://stats.stackexchange.com/a/467672).


  [1]: https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers
  [2]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding"
52888,Handpose model doesn't get image data from React Native Canvas,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Expo, Windows
- Xiaomi Redmi note 9 pro 5g

package.json:
`{
  ""main"": ""node_modules/expo/AppEntry.js"",
  ""scripts"": {
    ""start"": ""expo start"",
    ""android"": ""expo start --android"",
    ""ios"": ""expo start --ios"",
    ""web"": ""expo start --web"",
    ""eject"": ""expo eject""
  },
  ""dependencies"": {
    ""@react-native-async-storage/async-storage"": ""~1.15.0"",
    ""@tensorflow-models/handpose"": ""^0.0.7"",
    ""@tensorflow/tfjs"": ""^3.9.0"",
    ""@tensorflow/tfjs-backend-webgl"": ""^3.9.0"",
    ""@tensorflow/tfjs-converter"": ""^3.9.0"",
    ""@tensorflow/tfjs-core"": ""^3.9.0"",
    ""@tensorflow/tfjs-react-native"": ""^0.7.0"",
    ""async-storage"": ""^0.1.0"",
    ""base-64"": ""^1.0.0"",
    ""expo"": ""~42.0.1"",
    ""expo-camera"": ""~11.2.2"",
    ""expo-gl"": ""^10.4.2"",
    ""expo-gl-cpp"": ""~10.4.1"",
    ""expo-image-picker"": ""~10.2.2"",
    ""expo-status-bar"": ""~1.0.4"",
    ""react"": ""16.13.1"",
    ""react-dom"": ""16.13.1"",
    ""react-native"": ""https://github.com/expo/react-native/archive/sdk-42.0.0.tar.gz"",
    ""react-native-canvas"": ""^0.1.38"",
    ""react-native-fs"": ""^2.18.0"",
    ""react-native-get-real-path"": ""https://github.com/Wraptime/react-native-get-real-path.git"",
    ""react-native-web"": ""~0.13.12"",
    ""react-native-webview"": ""11.6.2"",
    ""three"": ""^0.133.1""
  },
  ""devDependencies"": {
    ""@babel/core"": ""^7.9.0"",
    ""@types/react"": ""~16.9.35"",
    ""@types/react-native"": ""~0.63.2"",
    ""@types/react-native-canvas"": ""^0.1.8"",
    ""@types/three"": ""^0.133.0"",
    ""typescript"": ""~4.0.0""
  },
  ""private"": true
}
`

code:
`
import React, {Component} from 'react';
import {Image, ScrollView, StatusBar, View, StyleSheet, Button, Platform} from 'react-native';
import * as ImagePicker from 'expo-image-picker';

import Canvas, {Image as CanvasImage} from 'react-native-canvas';

import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-react-native';
import * as handpose from ""@tensorflow-models/handpose"";
import {decode as atob, encode as btoa} from ""base-64"";

const Example = ({children}) => (
    <View style={styles.example}>
        <View style={styles.exampleLeft}>{children}</View>
    </View>
);

export default class App extends Component {
    constructor(props) {
        super(props);

        this.state = {
            image: null,
            tfIsReady: false, model: null
        };

        this.handleImageData = this.handleImageData.bind(this);
        this.pickImage = this.pickImage.bind(this);
        this.processImage = this.processImage.bind(this);
        this.canvas = React.createRef();
    }

    async componentDidMount() {
        await (async () => {
            if (Platform.OS !== 'web') {
                const {status} = await ImagePicker.requestMediaLibraryPermissionsAsync();
                if (status !== 'granted') {
                    alert('Sorry, we need camera roll permissions to make this work!');
                }
            }
        })();

        await tf.ready();
        // Signal to the app that tensorflow.js can now be used.
        this.setState({
            isTfReady: true,
        });

        console.log(""tfReady"");

        this.setState({model: await handpose.load()});

        console.log(""handposeLoad"");
    };

    handleImageData() {
        const image = this.state.image;
        const canvas = this.canvas.current;
        const model = this.state.model;

        if (canvas && image && model) {

            if (!(canvas instanceof Canvas)) {
                return;
            }

            console.log(""Drawing picture"");
            const img = new CanvasImage(canvas);
            let aspectRatio = image.width / image.height;

            let width = 200;
            let height = 200;

            canvas.width = width;
            canvas.height = height;

            const context = canvas.getContext('2d');


            img.src = 'data:image/png;base64,' + image.base64;
            img.addEventListener('load', () => {
                context.drawImage(img, 0, 0, width, height);
            });

        }
    };

    async processImage() {
        const canvas = this.canvas.current;
        const model = this.state.model;
        const image = this.state.image;

        if (model && canvas) {
            console.log()
            const context = canvas.getContext('2d');

            let imageData = await context.getImageData(0, 0, image.width, image.height);

            const hand = await model.estimateHands(imageData);
            console.log(hand);
        }
    }

    async pickImage() {
        let result = await ImagePicker.launchImageLibraryAsync({
            mediaTypes: ImagePicker.MediaTypeOptions.All,
            allowsEditing: true,
            quality: 1,
            base64: true
        });

        console.log(result.width);

        if (!result.cancelled) {
            this.setState({image: result})
        }
    };

    render() {
        return (
            <View style={styles.container}>
                <StatusBar hidden={true}/>
                <Button title=""Pick an image from camera roll"" onPress={this.pickImage}/>
                <Button title=""Draw image"" onPress={this.handleImageData}/>
                <Button title=""Process image"" onPress={this.processImage}/>
                <ScrollView style={styles.examples}>
                    <Example>
                        <Canvas ref={this.canvas}/>
                    </Example>
                </ScrollView>
            </View>
        );
    }
}

const commonStyles = StyleSheet.create({
    full: {
        left: 0,
        width: '100%',
        height: '100%',
    },
    cell: {
        flex: 1,
        padding: 10,
        justifyContent: 'center',
        alignItems: 'center',
    },
});

const styles = StyleSheet.create({
    container: {
        backgroundColor: 'white',
        ...commonStyles.full,
    },
    examples: {
        ...commonStyles.full,
        padding: 5,
        paddingBottom: 0,
    },
    example: {
        paddingBottom: 5,
        flex: 1,
        flexDirection: 'row',
    },
    exampleLeft: {
        ...commonStyles.cell,
    },
    exampleRight: {
        ...commonStyles.cell,
    },
});

`

**Describe the current behavior**
Got error: 
`[Unhandled promise rejection: Error: pixels passed to tf.browser.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData in browser, or OffscreenCanvas, ImageData in webworker or {data: Uint32
Array, width: number, height: number}, but was ImageData]
at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:8871:14 in fromPixels_
at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:5406:25 in f2
at node_modules\@tensorflow-models\handpose\dist\index.js:170:40 in tf.tidy$argument_0
at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:4394:23 in scopedRun$argument_2
at node_modules\@tensorflow\tfjs-core\dist\tf-core.node.js:4404:23 in Engine.prototype.scopedRun
at node_modules\@tensorflow-models\handpose\dist\index.js:168:32 in __generator$argument_1
at node_modules\@tensorflow-models\handpose\dist\index.js:48:17 in step
at node_modules\@tensorflow-models\handpose\dist\index.js:23:13 in <anonymous>
at node_modules\react-native\node_modules\promise\setimmediate\core.js:45:6 in tryCallTwo
at node_modules\react-native\node_modules\promise\setimmediate\core.js:200:22 in doResolve
at node_modules\react-native\node_modules\promise\setimmediate\core.js:66:11 in Promise
at node_modules\@tensorflow-models\handpose\dist\index.js:19:11 in <anonymous>
at App.tsx:98:31 in processImage
at node_modules\regenerator-runtime\runtime.js:63:36 in tryCatch
at node_modules\regenerator-runtime\runtime.js:294:29 in invoke
at node_modules\regenerator-runtime\runtime.js:63:36 in tryCatch
at node_modules\regenerator-runtime\runtime.js:155:27 in invoke
at node_modules\regenerator-runtime\runtime.js:165:18 in PromiseImpl.resolve.then$argument_0
at node_modules\react-native\node_modules\promise\setimmediate\core.js:37:13 in tryCallOne
at node_modules\react-native\node_modules\promise\setimmediate\core.js:123:24 in setImmediate$argument_0
at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:130:14 in _callTimer
at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:181:14 in _callImmediatesPass
at node_modules\react-native\Libraries\Core\Timers\JSTimers.js:441:30 in callImmediates
at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:387:6 in __callImmediates
at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:135:6 in __guard$argument_0
at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:364:10 in __guard
at node_modules\react-native\Libraries\BatchedBridge\MessageQueue.js:134:4 in flushedQueue
`

**Describe the expected behavior**
It should happen estimation hands on Image Data
"
52887,Converting from TensorFlow model input to MHLO,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.3
- Are you willing to contribute it (Yes/No):
Yes，I would like to but have no ideas

**Describe the feature and the current behavior/state.**
Have no guide on how to convert a TF model to MLIR, and hope that build up a doc that shows how to make the conversion. Meanwhile, it would be better if there are more tutorials about TF and MLIR. 

**Will this change the current api? How?**
Have no ideas

**Who will benefit with this feature?**
The people who would like to use TensorFlow as the frontend in MLIR. It will enhance the combination of
the TensorFlow eco-system and MLIR eco-system.

**Any Other info.**
"
52886,Temporal Attention on LSTM Layer?,"Hi, 

**I want to implement Attention on LSTM layer. Let me put the detail description:**

we analyze the 8 hidden states of the LSTM that represent the embeddings for the different parts of an input frame. We consider the first 7 hidden states as the historical temporal
context and learn 7 weights corresponding to these states:
past context = [h1;h2;h3;:::h7] (1) 

current = h8 (2)

transformed context = tanh(W1 ×past context + b1) (3)

weights = softmax(W2 ×transformed context + b2) (4)

final embedding = past context×weights + current (5)

b1 and b2 denote the biases in the two linear layers, and W1 and
W2 represent the 2D matrices in the linear layers. We initially
apply a linear transformation accompanied by a tanh linearity
transforming each of these seven vectors of size 128 into seven
new vectors of size 128 (Eq. 3). Another linear transformation
converts these 8 vectors each to size 1 essentially giving us
scores for each of the hidden states. These scores are then
passed through a softmax to give the final set of weights (Eq.
4). These weights are used to calculate a weighted sum of all
the 8 hidden states to give the final embedding for the past
context. This past context is added to the last hidden state
to give the final embedding for the input frame (Eq. 5). This
final embedding is used for classification.

**Please verify my code according to description. Is it right?**


    from tensorflow.keras.layers import Input, Dense, Lambda, Dot, Activation, Concatenate
    from tensorflow.keras.layers import Layer
    import tensorflow as tf
    
    
    def attention(lstm_hidden_status):  # Tensor(""lstm_1/transpose_1:0"", shape=(?, 8, 128), dtype=float32)
        hidden_size = lstm_hidden_status.get_shape().as_list()  # get all dimensions all list
        hidden_size = int(hidden_size[2]) # 128
        # feed to Forward Neural Network
        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(lstm_hidden_status) # Tensor(""last_hidden_state/strided_slice:0"", shape=(?, 128), dtype=float32)
        transformed_context = Dense(hidden_size, use_bias=True, activation='tanh', name='transformed_context_vec')(
            lstm_hidden_status) # Tensor(""transformed_context_vec/Tanh:0"", shape=(?, 8, 128), dtype=float32)

        score = Dot(axes=[1, 2], name='attention_score')([h_t, transformed_context]) # Tensor(""attention_score/Squeeze:0"", shape=(?, 8), dtype=float32)
        attention_weights = Dense(8, use_bias=True, activation='softmax', name='attention_weight')(score) # Tensor(""attention_weight/Softmax:0"", shape=(?, 8), dtype=float32)
        context_vector = Dot(axes=[1, 1], name='context_vector')([lstm_hidden_status, attention_weights]) # Tensor(""context_vector/Squeeze:0"", shape=(?, 128), dtype=float32)
        new_context_vector = context_vector + h_t # Tensor(""add:0"", shape=(?, 128), dtype=float32)
        return new_context_vector 

Specifically, I am confused here in line `score = Dot(axes=[1, 2], name='attention_score')([h_t, transformed_context])`, Why we are taking Dot product? All the debug outputs are attached with each line. "
52885,Addition of a new activation function: Mish in comps,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.** Addition of a new activation function Mish.

**Will this change the current api? How?**Yes. Addition of a new activation function in the comps API. 

**Who will benefit with this feature?**As described in the following paper, Mish acts as a better activation function in comparison to LeakyReLU and outperforming it in some scenarios. Addition of this new activation functions for user to use in various models will be a huge advantage.
[Link to paper](https://arxiv.org/pdf/1908.08681.pdf)

**Any Other info.**
"
52884,"Ability of flow, flowFromDirectory and flowFromDataFrame in ImageDataGenerator to self generate class weights","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.** Addition of new attribute to generator in order to compute class weights in case of a class imbalanced dataset.

**Will this change the current api? How?** Yes. It will add a new attribute to the flow, flowFromDirectory, flowFromDataFrame and a method that will compute class weights based on strategy provided by the user as in scikitlearn compute class weights.

**Who will benefit with this feature?** It will benefit users with an imbalanced dataset to be able to compute the class weights within TensorFlow and not rely on any other library to compute class weights. 

**Any Other info.** Will be really helpful for users.
"
52883,AttributeError in tensorflowjs wizard with tensorflow_estimator version 2.7.0 (but can be fixed by downgrading modules),"**System information**
- Manjaro Linux x86_64
- TensorFlow installed from anaconda (implicitly thorugh `pip install ""tensorflowjs[wizard]""`)
- TensorFlow version : 2.6.0
- Python version: 3.6.13
- CUDA/cuDNN version: cuda_11.4.r11.4
- GPU model and memory: NVIDIA GeForce GTX 1660 Ti, 6GB memory

**Current Behavior**
When I execute `tensorflowjs_wizard`, I get the following output:

```
Traceback (most recent call last):
  File ""/home/raka/.conda/envs/tfjs_error/bin/tensorflowjs_wizard"", line 5, in <module>
    from tensorflowjs.converters.wizard import pip_main
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/__init__.py"", line 21, in <module>
    from tensorflowjs import converters
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/__init__.py"", line 21, in <module>
    from tensorflowjs.converters.converter import convert
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/converter.py"", line 37, in <module>
    from tensorflowjs.converters import tf_saved_model_conversion_v2
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py"", line 39, in <module>
    import tensorflow_hub as hub
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_hub/__init__.py"", line 88, in <module>
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_hub/estimator.py"", line 62, in <module>
    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 10, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 27, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/home/raka/.conda/envs/tfjs_error/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 70, in <module>
    @doc_controls.inheritable_header(""""""\
AttributeError: module 'tensorflow.tools.docs.doc_controls' has no attribute 'inheritable_header'
```

**Expected Behavior**
The expected behavior is the output of the `tensorflowjs_wizard`  command as documented [here](https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/README.md)
The error seems to have been caused by `inheritable_header` which is called from `tensorflow_estimator.python.estimator` as seen by log above. [This commit](https://github.com/tensorflow/estimator/commit/1cf920e090b02c6b3186b66a2bb2e0114476db40) shows, that the call from the estimator module was recently added with the 2.7.0 version which released yesterday.

**How I currently solved it for me**

Through downgrading tensorflow-estimator to 2.6.0 and tensorflowjs to 3.9.0 I was able to get the command `tensorflowjs_wizard` to **behave as expected**. I downgraded the python packages with the following command:

```
pip install --upgrade tensorflow-estimator==2.6.0
pip install --upgrade tensorflowjs==3.9.0
```

**[Contributing](https://www.tensorflow.org/community/contribute)**

I don't know how to fix this issue because how I solved my issue was through manual downgrading. Maybe someone who knows the code better knows what to do.

**Standalone code to reproduce the issue**

```bash
# Assuming you have already installed conda and are currently in no environment
conda create --name tfjs python=3.6
conda activate tfjs
pip install tensorflowjs[wizard]
# Here you have to refresh the terminal somehow
tensorflowjs_wizard
```
[edit: forgot to add the tensorflow version]"
52882,tf.nn.batch_normalization and tf.keras.layers.BatchNormalization outputs gives a diff (10^-2) range,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04

I have implemented a classification model , which has Batchnormalization layers into it. While training the model , i used ```tf.keras.layers.BatchNormalization``` layer for training. Then manually extracted the bn weights (gamma, beta, mean , var) from checkpoint file and called the bn using ```tf.nn.batch_normalization``` layer using above statistics for inference. I observed a drop in testing accuracy (~75%) as compared to the scenario where i load the weights in the model using keras itself ```model.load_weights(chkpnt_file)``` (~92%).   
Though if i replace the ```tf.nn.batch_normalization``` call to keras bn layer call by calling ```set_weights``` function , both the cases gived 0.0 diff, which apprently led me to beleive that there is diff in both the calls while the input and bn_weights are exactly same. epsilon is also same.  

**Standalone code to reproduce the issue**
Please refer to this colab notebook(https://colab.research.google.com/drive/1nysYyCR5Ay_jkPlh1WD9A1Mf01eVvhMu? usp=sharing) for a small example. Here the error is coming to be a margin of 10^-2 (which i think is also big given all inputs are same) , but in my project there is huge diff."
52867,"Linker command not proper, as it does not look under /usr/local/lib","## URL(s) with the issue:

https://www.tensorflow.org/install/lang_c

## Description of issue (what needs changing):

In the **Linker** section, the given command is `sudo ldconfig`, but as per the description, it only links to the trusted directories (`/lib` and `/usr/lib`). This needs to be replaced with `sudo ldconfig /usr/local/lib -v` (-v is optional, only for verbose) assuming that the libtensorflow files have been extracted in `/usr/local`.

### Clear description

If this method is not used, then we get this error while building the example ""Hello World"" program:
`libtensorflow.so.2: cannot open shared object file: No such file or directory`."
52854,[Tflite] quantized ABS operator doesn't behave as intended,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master/nightly
- Python version: 3.7
- Bazel version (if compiling from source): 3.7.2

**Describe the current behavior**
(Please read the standalone code)
I have quantized tf.math.abs via int8 post-training quantization with a representative dataset of negative values only.
When running the model in tflite by giving negative values to the model the output will be zeros.
When looking at the tflite model in Netron it seems that the output_scaling and output_zero_point is erroneous (the range can only represent negative values). output_scale = 0.003921568859368563 and output_zero_point = -127

**Describe the expected behavior**
I expect the quantized tf.math.abs operator to output positive values and to have a correct output scaling and zero_point.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): depending on the complexity of the solution

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

input_data = np.array([-0.2, -0.33, -1.0, -0.4, -0.1], dtype=np.float32)

inp = tf.keras.Input(shape=(5,))
out = tf.math.abs(inp)
model = tf.keras.Model(inp, out)

# Post training quantization
def representative_dataset():
    yield [input_data]
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_quant_model = converter.convert()
tflite_model_path = ""model.tflite""
with open(tflite_model_path, ""wb"") as f:
    f.write(tflite_quant_model)

# Tflite inference with the quantized model
interpreter = tf.lite.Interpreter(tflite_model_path)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0][""index""], [input_data])
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0][""index""])

# The quantized model doesn't behave as expected:
# The quant output should be able to represent positive numbers
# However, the quant output can't represent positive numbers
# Input_scale = output_scale = 0.003921568859368563
# Input_zero_point = output_zero_point = -127
print(""input: "", input_data)  # [-0.2, -0.33, -1.0, -0.4, -0.1]
print(""expected output: "", model(input_data))  # [0.2  0.33 1.   0.4  0.1 ]
print(""quantized output: "", output_data)  # [[0. 0. 0. 0. 0.]]
```"
52852,please add tf.size support,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.4.1


**Provide the text output from tflite_convert**
```
tf.size' of is neither a custom op nor a flex op
```

**Standalone code to reproduce the issue** 
https://www.tensorflow.org/lite/tutorials/pose_classification

Also, please include a link to a GraphDef or the model if possible.

=====In Addition======
I know that the convert works if add following line,
```
converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.  --> because tf.Size is not supported by tflite
    ]
```
but, this tflite model does not work on Android even if I add ""tensorflow-lite-select-tf-ops.aar"" inside my project.
So, could you please add tf.size support, please?

Or is there another way to replace tf.size when training this pose_classification model?
there're two place using the 'tf.size' inside the tutorial:

```
pose_center = tf.broadcast_to(pose_center, 
                                [tf.size(landmarks) // (17*2), 17, 2])
```
"
52850,ubantu18.0.4   python2.7  tensorflow 1.4.0 ,"ubantu18.0.4   python2.7  tensorflow 1.4.0  when i use 
q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)

it report 

  q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 756, in __init__
    name=name)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 810, in _fifo_queue_v2
    name=name)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 343, in _apply_op_helper
    raise RuntimeError(""Unrecognized Op name "" + op_type_name)
RuntimeError: Unrecognized Op name FIFOQueueV2
"
52849,tensorflow 1.4.0  python 2.7,"ubantu18.0.4   python2.7  tensorflow 1.4.0  when i use 
q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)

it report 

  q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 756, in __init__
    name=name)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 810, in _fifo_queue_v2
    name=name)
  File ""/data/conda/app/envs/GRnet/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 343, in _apply_op_helper
    raise RuntimeError(""Unrecognized Op name "" + op_type_name)
RuntimeError: Unrecognized Op name FIFOQueueV2"
52848,tensorboard walltime and step not consistent,"Does anyone know whats happening here? The walltime and the step is not consistent, might it be the asynchronous of the SummaryWriter on CPU and the training on GPU?

![image](https://user-images.githubusercontent.com/67493438/139387653-cff79104-508e-40eb-b920-f331acb52cc1.png)

![image](https://user-images.githubusercontent.com/67493438/139387687-ddfb8b5b-c683-475d-b2c2-49ebc07ec08f.png)

![image](https://user-images.githubusercontent.com/67493438/139387752-a4291b6e-aebd-48b2-8869-a397174037a7.png)
"
52845,TensorFlow binary crashes on Apple M1 in x86_64 Docker container,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): TensorFlow 2.6.0, tf-nightly 2.8.0.dev20211028
- Python version: 3.6.9, 3.7.x, 3.8.x
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
```
dwyatte-macbookpro:~ dwyatte$ docker run tensorflow/tensorflow:latest python -c ""import tensorflow as tf""    
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
2021-10-28 22:50:41.481158: F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10)
qemu: uncaught target signal 6 (Aborted) - core dumped
```

**Describe the expected behavior**
Clean exit

**Standalone code to reproduce the issue**
Requires an Apple M1 (arm64) host OS:
`docker run tensorflow/tensorflow:latest python -c ""import tensorflow as tf""`

This was previously mentioned in https://github.com/tensorflow/tensorflow/issues/42387 but unfortunately closed. When importing TensorFlow in an x86_64 docker container on an Apple M1, TensorFlow crashes. As far as I can tell, this should work as I can import and use other Python packages in the same container without problems (including things like `numpy`).

It's unclear whether this is something that can be avoided at the TensorFlow level or an unavoidable bug in qemu ([[1]](https://github.com/docker/for-mac/issues/5342#issuecomment-779133157), [[2]](https://gitlab.com/qemu-project/qemu/-/issues/601)), but I wanted to reraise the issue."
52831,Retrieving exactly the same recommendations for all users,"## URL(s) with the issue:
https://www.tensorflow.org/recommenders/examples/basic_retrieval#making_predictions

## Description of issue (what needs changing):
When making predictions, the model is retrieving the exact same predictions for all users when using other datasets.

### Clear description
When model is trained and we're making predictions, everything works fine when using the movielens dataset which is provided in the tutorial.
But if swapping to any other dataset (have tried several datasets), the recommendation becomes the exact same for all users.

### Correct links
I cannot publish my data since it is GDPR sensitive. However, this user has used an open source data and has published his work on Github:
https://github.com/fickaz/TFRS-on-Retail-Data/blob/main/src%20a.%20Retrieval.ipynb

Unfortunately, he has not checked predictions for any other user than ""user 42"". But if you replicate his work, you will see that the predictions are same for all users.

### Parameters defined
In my dataset (recipes), I have tried to mimic the Movielens dataset as close as possible, ending up with around 1k users, 1k recipes and 100k rows. each user has interacted with about 60-300 recipes during 1 year.
Movielens dataset have 25-700 movies per user. Other than that, very similair.
"
52818,Issue created for Rollback of PR #52316: Add support of filesystem_set_configuration to tensorflow core,"Merged PR #52316 is rolled back in f75c3ecbf9ecf9837a4712e880540415ca710473.
    Please follow up with the reviewer and close this issue once its resolved."
52814,Linear regression learnt parameters are wrong when using sklearn with my specific docker instance. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary? Installed via docker pull tensorflow/tensorflow:2.4.0-jupyter
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 11.0
- GPU model and memory: NVIDIA GeForce GTX 1650 Ti/PCIe/SSE2

**Describe the current behavior**

The docker tf container is causing an issue with sklearn's linear regression function (and I'm not sure what else it is causing an issue with). 

After running a tf docker container which was pulled from one of the tf images, I seek to fit a polynomial model using sklearn. 

I find that the coefficients which are given are not the coefficients which are found if the regression problem is explicitly solved for using maximum likelihood.

I have checked and I see that when everything is run with a non-tf docker (e.g. jupyter one) or just using a conda env, the issue is no longer there.

Further checks included running the same tf docker container on a mac (instead of my ubuntu 20.04). On doing so there was no issue. 

I have also tried running the same tf docker in a virtual machine ubuntu 20.04 to see if my machine was causing the bug. But the issue persists in the virtual machine ubuntu 20.04.

Moreover, I find that when the number of data points is fewer, there is no issue. Only when the number of data points is increased is this issue there.

Hence I don't know if it is particular to this combination of ubuntu + the tf docker + use of sklearn. Or just something else.

I've included alternative ways to get at the coefficients (by training a tf model with mse loss --- this gives the correct answer), and using the statsmodels library (which gives the wrong answer), for reference.

**Describe the expected behavior**

Expected behaviour - sklearn's linear regression coefficients should match with those derived by solving the maximum likelihood estimation set-up. 


- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing): N/A

**Standalone code to reproduce the issue**

To reproduce:

1. Need to create an ubuntu 20.04 environment (e.g. through virtual box). Although you can proceed without doing this, but i don't know if the issue will then be reproduced.
2. Download the following data (https://drive.google.com/file/d/1NkQJbgn2cG-4y30AcEn1olthpROFWrLo/view?usp=sharing) and notebook  (https://drive.google.com/file/d/1XKEGgwoQ-3OyQYie9-HNI4w0mqNMPWo1/view?usp=sharing) into your working directory. 
3. You need to have docker installed and configured 
4. From the working directory run the following:
5. `docker run --name abc --user ""$(id -u):$(id -g)"" -v ""$(pwd)"":/tf -it -p 8888:8888 tensorflow/tensorflow:2.4.0-jupyter`
6. Kill this with ""ctrl+c""
7. `docker start abc`
8.  `docker exec -it abc pip install --no-cache-dir --upgrade pip`
9. `docker start -i abc`
10. Now you should be able to run the notebook and see the issue.

"
52796,Title: Input shape compatibility of ZIP-object of (image and image ImageDataGenerator objects) and CNN,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: custom code 
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: NA
-   **TensorFlow installed from (source or binary)**: binary (added as library to Anaconda )
-   **TensorFlow version (use command below)**: 2.1.0
-   **Python version**: 3.7
-   **Bazel version (if compiling from source)**: NA
-   **GCC/Compiler version (if compiling from source)**: NA
-   **CUDA/cuDNN version**: 10.1 
-   **GPU model and memory**:
- 	NVIDIA GeForce RTX 2070 with Max-Q Design
	Driver version:	30.0.14.7196
	Driver date:	8/27/2021
	DirectX version:	12 (FL 12.1)
	Physical location:	PCI bus 1, device 0, function 0
	Utilization	0%
	Dedicated GPU memory	0.0/8.0 GB
	Shared GPU memory	0.1/7.9 GB
	GPU Memory	0.1/15.9 GB-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I have 64 RGB images with their corresponding masks. I use ImageDataGenerator and flow_from_directory function to read the images and masks from the directory folder.
The folder structures are as follows:
Images
---- imgs
-------- ---ROI
----------------1.tiff
----------------2.tiff
Masks
------imgs
------- -----ROI
------------------1.tiff
------------------2.tiff
Images and masks are stored with the same file names. In my case, ImageDataGenerator is an Iterator of size 2 with a batch size of 32. I did the following:

I read the images and masks into lists named as “images” and ‘’masks’’ respectively.
size = 231 
images = []

for directory_path in glob.glob(“. /Images/imgs/ROI/""):
    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff""):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       
        img = cv2.resize(img, (size, size))
        images.append(img)
        images = np.array(images)

masks = [] 
for directory_path in glob.glob(“. /Masks/imgs/ROI/""):
    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff""):
        mask = cv2.imread(mask_path, 0)       
        mask = cv2.resize(mask, (size, size))
        masks.append(mask)     
masks = np.array(masks)
masks = np.expand_dims(masks, axis=3)
Note: I obtained images of shape (64, 231,231,3) and masks of shape (64, 231, 231, 1)

I then used keras documentation the process of transforming the images and masks together as follows:

  a.	I created a dict to specify the augmentation parameters as follows for both images and masks.
  data_gen_args_imgs = dict rotation range=90,
                       rescale = 1. /255.)
  data_gen_args_masks = dict rotation range=90)
               
  b.	The I created ImageDataGenerator objects for both images and masks as follows:
  ```
  image_datagen = ImageDataGenerator(**data_gen_args_imgs)
  mask_datagen = ImageDataGenerator(**data_gen_args_masks)
  ```
  
  c.	I use the fit and flow functions using the same seeds and keyword arguments for images and masks as follows: 
  ```
  seed = 1
  
  image_datagen.fit(images, augment=True, seed=seed)
  mask_datagen.fit(masks, augment=True, seed=seed)
  
  image_generator = image_datagen.flow_from_directory(
      'data/images',
      class_mode=None,
      shuffle= False,
      seed=seed)
  
  mask_generator = mask_datagen.flow_from_directory(
      “. /Images/imgs/”,
      class_mode=None,
      shuffle= False,
      seed=seed)
  ```
  
  d.	Then I created an iterator on the zip object to return a matched pair of images and corresponding masks as follows:
  
  `train_generator = (pair for pair in zip (image_generator, mask_generator))`
  
  e.      Then I build a CNN model as follows: 
  ```
  
  activation= 'relu'
  model = Sequential()
  model.add (Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))
  model.add (Conv2D(16, kernel_size=(3, 3),activation= activation,    padding='same'))
  model.add (Flatten())
  model.add (Dense(5))
  model.add (Dense(3))
  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  model.summary()
  
  ```
  
  e.	Then I used fit_generator to fit the model as follows: 
  
  `model.fit_generator (train_generator, steps_per_epoch= 15, epochs=2, verbose=1)`

### Source code / logs
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import glob
import cv2
import os
import glob
import tensorflow as tf
import pickle
from scipy import ndimage as nd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow .keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import sklearn.utils 
from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam

print(tf.__version__)

train_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//""
mask_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//""

############################################################################
size = 231 
images = []

for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//ROI_train//""):
    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       
        img = cv2.resize(img, (size, size))
        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        images.append(img)
        #train_labels.append(label)
        
images = np.array(images)

masks = [] 
for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//ROI_train//""):
    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):
        mask = cv2.imread(mask_path, 0)       
        mask = cv2.resize(mask, (size, size))
        #mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)
        masks.append(mask)
        #train_labels.append(label)
        
masks = np.array(masks)
masks=np.expand_dims(masks, axis=3)

#######################Example of transforming images and masks together.
import tensorflow as tf

# we create two instances with the same arguments
data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90)
                    
image_datagen = ImageDataGenerator(data_gen_args)
mask_datagen = ImageDataGenerator(data_gen_args)
# Provide the same seed and keyword arguments to the fit and flow methods
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)

image_generator = image_datagen.flow_from_directory(
    train_path,
    class_mode=None,
    shuffle= False,
    seed=seed)

mask_generator = mask_datagen.flow_from_directory(
    ""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//"",
    class_mode=None,
    shuffle= False,
    seed=seed)
# combine generators into one which yields image and masks
#train_generator = zip(image_generator, mask_generator)
train_generator = (pair for pair in zip(image_generator, mask_generator))

activation= 'relu'

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))
model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(5))
model.add(Dense(3))
model.compile( loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

model.fit_generator(train_generator, steps_per_epoch= 15, epochs=2, verbose=1)

**But I got the following error regarding input dimensionality:**

```
InvalidArgumentError:  Input to reshape is a tensor with 33554432 values, but the requested shape requires a multiple of 853776
 [[node sequential_8/flatten_8/Reshape (defined at <ipython-input-91-2871596287c2>:12) ]] [Op:__inference_distributed_function_7252]

Function call stack:
distributed_function
```

**my question is: How can I fix the problem of train_generator and CNN input shapes??**
### attachments
[summary of ImageDataGenarator_ZIP object_CNN.docx](https://github.com/tensorflow/tensorflow/files/7432552/summary.of.ImageDataGenarator_ZIP.object_CNN.docx)
[The code.txt](https://github.com/tensorflow/tensorflow/files/7432554/The.code.txt)
.
"
52795,Unnormal low gpu usage: MobileNetV3,"System information

- Have I written custom code: Custom
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow version (use command below): 2.4.0
- Python version: 3.6.10
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX 2080 ti *4

I'm training my classifier with 4 RTX 2080 ti with MobileNetV3.
Using Pytorch, GPU utility could be up to 97%, however, it is only ~25% with Tensorflow, which costs much time for one epoch.

```
+-------------------------------+----------------------+----------------------+
| 31%   42C    P2    80W / 250W |   9021MiB / 11019MiB |     24%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |
| 32%   42C    P2    93W / 250W |   9085MiB / 11019MiB |     26%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   8  GeForce RTX 208...  On   | 00000000:40:00.0 Off |                  N/A |
| 32%   42C    P2    82W / 250W |   9193MiB / 11019MiB |     24%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   9  GeForce RTX 208...  On   | 00000000:41:00.0 Off |                  N/A |
| 31%   42C    P2    99W / 250W |   9135MiB / 11019MiB |     24%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```


training code
```
def mbv3(weight='imagenet', input_shape=(224, 224, 3), num_classes=6):
    base_model = tf.keras.applications.MobileNetV3Large(input_shape=input_shape,
                                            include_top=False,
                                            weights=weight)

    base_model.trainable = True
    preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input
    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
    prediction_layer = tf.keras.layers.Dense(num_classes)
    batch_normalize_layer = tf.keras.layers.BatchNormalization()

    inputs = tf.keras.Input(input_shape)
    x = inputs
    x = preprocess_input(x)
    x = base_model(x, training=True)
    x = global_average_layer(x)
    x = batch_normalize_layer(x)

    outputs = prediction_layer(x)
    model = tf.keras.Model(inputs, outputs)
    return model

if __name__ == '__main__':
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    policy = mixed_precision.experimental.Policy('mixed_float16')

    mixed_precision.experimental.set_policy(policy)

    mixed_precision.set_global_policy('mixed_float16')
    train_ds = image_dataset_from_directory(
        directory = '../data/train/',
        batch_size = 512,
        image_size = (224, 224))
    val_ds = image_dataset_from_directory(
        directory = '../data/test/',
        batch_size = 512,
        image_size = (224,224))

    class_names = val_ds.class_names
    files_path = val_ds.file_paths

    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.prefetch(buffer_size = AUTOTUNE)
    val_ds = val_ds.prefetch(buffer_size = AUTOTUNE)

    strategy = tf.distribute.MirroredStrategy([""/GPU:0"", ""/GPU:1"", ""/GPU:2"", ""/GPU:3""])
    print('train with gpu:0,1,2,3')

    with strategy.scope():
        model = mbv3(weight = 'imagenet', input_shape=(224, 224, 3), num_classes = 6)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy']
        )


    Callbacks =[
        CustomCallback(val_ds, start_time)
    ]

    history = model.fit(train_ds, epochs=40, verbose = 1, callbacks=Callbacks, validation_data=val_ds)
```
logs
```
2021-10-28 14:31:09.721686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-10-28 14:31:11.827415: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-10-28 14:31:11.828413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-10-28 14:31:11.971195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:11.972475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:3f:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:11.973679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:40:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:11.974897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:11.974921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-10-28 14:31:11.977826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-10-28 14:31:11.977892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-10-28 14:31:11.978888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-10-28 14:31:11.979151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-10-28 14:31:11.982171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-10-28 14:31:11.982859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-10-28 14:31:11.983011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-10-28 14:31:11.992158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3
2021-10-28 14:31:11.992337: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-10-28 14:31:11.993638: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-10-28 14:31:11.994873: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-10-28 14:31:11.996082: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2021-10-28 14:31:11.998462: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-28 14:31:12.000512: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-10-28 14:31:12.622385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:3e:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:12.624492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:3f:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:12.627911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: 
pciBusID: 0000:40:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:12.631088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: 
pciBusID: 0000:41:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-10-28 14:31:12.631116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-10-28 14:31:12.631149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-10-28 14:31:12.631161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-10-28 14:31:12.631174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-10-28 14:31:12.631187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-10-28 14:31:12.631199: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-10-28 14:31:12.631212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-10-28 14:31:12.631228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-10-28 14:31:12.645693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3
2021-10-28 14:31:12.645739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-10-28 14:31:14.550404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-10-28 14:31:14.550446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 3 
2021-10-28 14:31:14.550453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N N N 
2021-10-28 14:31:14.550457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N N N 
2021-10-28 14:31:14.550461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   N N N N 
2021-10-28 14:31:14.550465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   N N N N 
2021-10-28 14:31:14.557474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10066 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5)
2021-10-28 14:31:14.561911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10066 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5)
2021-10-28 14:31:14.566345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10066 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5)
2021-10-28 14:31:14.570745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10066 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5)
Found 1384696 files belonging to 6 classes.
Found 466757 files belonging to 6 classes.
train with gpu:0,1,2,3
2021-10-28 14:32:20.799831: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: ""TensorSliceDataset/_1""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_STRING
    }
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
      }
    }
  }
}

2021-10-28 14:32:21.192299: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-10-28 14:32:21.211929: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz
Epoch 1/40
2021-10-28 14:33:35.383104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-10-28 14:33:35.885437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-10-28 14:33:45.923277: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1760 of 4096
2021-10-28 14:33:55.919233: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 3318 of 4096
2021-10-28 14:34:00.473165: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:230] Shuffle buffer filled.
2021-10-28 14:34:04.182122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
  32/2705 [..............................] - ETA: 2:20:49 - loss: 1.2214 - accuracy: 0.5887
```
I've been stuck for weeks, please help."
52794,TFLite GPU Delegate Precision loss enabled produces different/wrong result.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Galaxy Note20 Ultra(Snapdragon), Galaxy S10(Exynos)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0, 2.6.0, 2.7.0 (All same result)
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Android NDK 19.2.5345600
- CUDA/cuDNN version: -
- GPU model and memory: Snapdragon 865, Exynos 9820

**Describe the current behavior**
I have been trying to run inference of my custom model using TFLite 2.5.0 or 2.6.0 with OpenCL GPU delegate enabled.
When I set gpu_precision_loss_allowed=true, avg_error is 0.334995. But for gpu_precision_loss_allowed=false, it is 6.36335e-06. The error diff between precision_loss enabled(fp16) and precision_loss disabled(fp32) is so large. It can be understandable that if fp16 result is less than 0.003. But 0.33 is so large.

Here are outputs of the inference_diff/run_eval tool obtained using [my model](https://github.com/tensorflow/tensorflow/files/7434381/model.zip) on Galaxy Note20 Ultra:

```
➜   adb shell /data/local/tmp/run_eval \
    --model_file=/data/local/tmp/model.tflite \
    --delegate=gpu \
    --num_runs=5 \
    --gpu_precision_loss_allowed=true 
GPU delegate created.
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Replacing 623 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:144 
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Replacing 389 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 336 partitions.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 5
Reference run latency: avg=645950(us), std_dev=40910(us)
Test run latency: avg=62793.9(us), std_dev=3209(us)
OutputDiff[0]: avg_error=0.394511, std_dev=0.139682
```

```
➜   adb shell /data/local/tmp/run_eval \
    --model_file=/data/local/tmp/model.tflite \
    --delegate=gpu \
    --num_runs=5 \
    --gpu_precision_loss_allowed=false
GPU delegate created.
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Replacing 623 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.
native : lite/tools/evaluation/stages/tflite_inference_stage.cc:144 
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Replacing 389 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 336 partitions.
native : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
Num evaluation runs: 5
Reference run latency: avg=650665(us), std_dev=44175(us)
Test run latency: avg=162572(us), std_dev=2556(us)
OutputDiff[0]: avg_error=5.53458e-06, std_dev=1.72756e-06
```

**Describe the expected behavior**
The result difference between CPU and GPU result should be similar. It is expected that avg_error should less than 0.003.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
1. The custom model I attached 
2. GPU Delegate - OpenCL backend
3. Precision loss enabled

Please let me know, if you need more details/logs/code etc. Thanks in advance."
52793,"Add all the averaging methods available for Precion, Recall and F-score in sklearn, natively in tensorflow or tensorflow-addons","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.4.3
- Are you willing to contribute it (Yes/No): Yes, but I have very little knowledge on tensorflow's graph execution works. So, I can't contribute.


**Describe the feature and the current behavior/state.**
Currently the [Recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)/[Precision ](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision)metric available in tensorflow only provides binary averaged score; and [F1Score](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score) and [FBetaScore](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/FBetaScore). It would be very convenient and avoids the need to write (half-baked, at least in my case) custom metrics from user side, if other averaging methods available in sklearn, that is 'macro', 'micro' and 'samples'(for multi-label problems), are also implemented in either tensorflow or tensorflow-addons.

**Will this change the current api? How?**
I presume, No.

**Who will benefit with this feature?**
every one working on multi-class and multi-label classification problems, who like to use those averaging methods.

**Any Other info.**
All aforementioned metrics from sklearn's API docs are hyperlinked below: 
[sklearn's Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score), [sklearn's Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), [sklearn's F1Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) and [sklearn's FBeta Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score)"
52774,How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): docker image
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0-gpu-jupyter

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

```
import tensorflow as tf
from tensorflow.keras.layers import (
        Concatenate,
        DepthwiseConv2D,
        ZeroPadding2D,
        Activation,
        GlobalAveragePooling2D,
        BatchNormalization,
        Conv2D,
        Input,
)
from tensorflow.keras.models import Model

def SepConv_BN(x, filters, prefix, stride=1, kernel_size=3, rate=1, depth_activation=False, epsilon=1e-3):
    if stride == 1:
        depth_padding = ""same""
    else:
        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)
        pad_total = kernel_size_effective - 1
        pad_beg = pad_total // 2
        pad_end = pad_total - pad_beg
        x = ZeroPadding2D((pad_beg, pad_end))(x)
        depth_padding = ""valid""

    if not depth_activation:
        x = Activation(tf.nn.relu)(x)
    x = DepthwiseConv2D(
        (kernel_size, kernel_size),
        strides=(stride, stride),
        dilation_rate=(rate, rate),
        padding=depth_padding,
        use_bias=False,
        name=prefix + ""_depthwise"",
    )(x)
    x = BatchNormalization(name=prefix + ""_depthwise_BN"", epsilon=epsilon)(x)
    if depth_activation:
        x = Activation(tf.nn.relu)(x)
    x = Conv2D(filters, (1, 1), padding=""same"", use_bias=False, name=prefix + ""_pointwise"")(x)
    x = BatchNormalization(name=prefix + ""_pointwise_BN"", epsilon=epsilon)(x)
    if depth_activation:
        x = Activation(tf.nn.relu)(x)

    return x

x = Input(shape=(22, 40, 160))
b0 = Conv2D(256, (1, 1), padding=""same"", use_bias=False, name=""aspp0"")(x)
b0 = BatchNormalization(name=""aspp0_BN"", epsilon=1e-5)(b0)
b0 = Activation(tf.nn.relu, name=""aspp0_activation"")(b0)
b1 = SepConv_BN(x, 256, ""aspp1"", rate=4, depth_activation=True, epsilon=1e-5)
b2 = SepConv_BN(x, 256, ""aspp2"", rate=8, depth_activation=True, epsilon=1e-5)
b3 = SepConv_BN(x, 256, ""aspp3"", rate=12, depth_activation=True, epsilon=1e-5)
b4 = GlobalAveragePooling2D()(x)
b4 = tf.reshape(b4, (-1, 1, 1, b4.shape[-1]))
b4 = Conv2D(256, (1, 1), padding=""same"", use_bias=False, name=""image_pooling"")(b4)
b4 = BatchNormalization(name=""image_pooling_BN"", epsilon=1e-5)(b4)
b4 = Activation(tf.nn.relu)(b4)
size_before = tf.shape(x)
b4 = tf.image.resize(b4, size_before[1:3], method=tf.image.ResizeMethod.BILINEAR)
y = Concatenate(name=""concat_head"")([b4, b0, b1, b2, b3])
model = Model(x, y, name=""aspp"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
model.save(""example.h5"")
open(""example.tflite"", ""wb"").write(tflite_model)
```
The models and graphs are also provided.
[models.zip](https://github.com/tensorflow/tensorflow/files/7432732/example.zip)

![keras model using Netron](https://user-images.githubusercontent.com/86588364/139223891-45dadbd6-4960-46c1-a222-3da0db42a5d0.png)
![tflite model using Netron](https://user-images.githubusercontent.com/86588364/139223763-66e0167f-4046-42e6-84a8-3eda2a1bc238.png)



### 3. Failure after conversion
The codes is an implementation of ASPP structure as used in segmentation. In the Keras model, there is no `space_to_batch` or `batch_to_space` operations. However, in the converted tflite model, they appear, which are not supported by tflite gpu-delegate. This means the inference speed will be slowed down due to those operations. 

Is there a way to convert the keras model without  those gpu-unsupported operations?

I found a related [issue](https://github.com/tensorflow/tensorflow/issues/29509), but the issue still exists.
"
52768,No matching distribution found for tensorflow,"Used one of your Dockerfiles to build a docker image with TensorFlow installed (also I've tried to do it manually and used python docker image and latest Ubuntu image) but all the time I get same error: 

<img width=""1051"" alt=""image"" src=""https://user-images.githubusercontent.com/33292483/139205340-79248dec-c487-4589-be45-9593f0492dd8.png"">

I've tried to update pip and setuptools."
52764,MultiHeadAttention layer save and load changes weight,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: pip install tensorflow
-   **TensorFlow version (use command below)**: v2.4.0-49-g85c8b2a817f 2.4.1
-   **Python version**: 3.7.11
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: https://aws.amazon.com/ec2/instance-types/g4/
-   **GPU model and memory**: https://aws.amazon.com/ec2/instance-types/g4/
-   **Exact command to reproduce**: see below in the source code session.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

trained a small model with a MHA layer and save model and load it back, the weight of the MHA layer changed. I have checked similar thread like [this](https://github.com/tensorflow/tensorflow/issues/47722), the suggestion was to fix the seed given to the `kernel_initializer`, so did that as shown below, but does not fix the problem. Notably, if just build the model and save without training (`.fit`), loading back the model, then the outputs are consistent. 

### Source code

from tensorflow.keras.layers import MultiHeadAttention, Concatenate, Add, Dense
from tensorflow.keras.initializers import glorot_uniform
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

    def encoder_layer_simple(units, d_model, num_heads, dropout, name=""encoder_layer""):
        assert d_model % num_heads == 0
    
        depth = d_model // num_heads
        query = tf.keras.Input(shape=(None, d_model), name=""query"")
        key = tf.keras.Input(shape=(None, d_model), name=""key"")
        value = tf.keras.Input(shape=(None, d_model), name='value')
    
        attention, attn_weights = MultiHeadAttention(num_heads, key_dim=depth, value_dim=depth, dropout=dropout,
                                                     kernel_initializer=glorot_uniform(seed=0))(
            query=query, value=value, key=key, return_attention_scores=True)
        attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + value)
    
        # ffwd sub-layer
        outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
        outputs = tf.keras.layers.Dense(units=d_model)(outputs)
        outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
    
        outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention)
    
        return tf.keras.Model(inputs=[query, key, value], outputs=outputs, name=name)

#### then create an 'encoder' model and do a fit
    encoder = encoder_layer_simple(512, 400, 8, 0, name='test_encoder')
    
    query = tf.random.uniform([256, 100, 400])
    
    enc_x = {'query':query, 'value':query, 'key':query}
    enc_y = tf.random.uniform([256, 100, 400])
    encoder.compile(Adam(),'MSE')
    # if we don't do fit, then the results are consistent
    encoder.fit(x=enc_x, y=enc_y)
    
    save_path = ""path_to_model""
    encoder.save(save_path)
    loaded = tf.keras.models.load_model(save_path, compile=False)
    
    eout = encoder(enc_x)
    lout = loaded(enc_x)
    
    np.allclose(lout.numpy(), eout.numpy()) # will output False

#### also can get weight matrix from the MHA layer, close but different

    encoder.get_layer(""multi_head_attention"").get_weights()[0][1, 4, 1:10]

array([ 0.01589977,  0.01451083, -0.00762749, -0.00915873,  0.00802898,
        0.00830225, -0.01598875,  0.01182209,  0.01533959], dtype=float32)
    
    loaded.get_layer(""multi_head_attention"").get_weights()[0][1, 4, 1:10]

array([ 0.01591637,  0.01452216, -0.00764309, -0.00916362,  0.0079951 ,
        0.0082739 , -0.01598743,  0.01181626,  0.01530811], dtype=float32)
   





"
52763,Model weight and training loss turn to nan During and after training,"I'm using Keras layer to define a multi-input single output model. I've check the calculation by passing a dummy data to the layer using functional API and model.predict before calling model.fit, and the model works fine. It computes the output as a float, and calculate the loss value as expected. However, whenever I call model.fit, the training loss turns to `nan`. I tried to see the model weight before and after training, and it turns out the weight was exist before calling `model.fit`, but turns to nan after fitting the model. I assume this issue was caused by gradient calculation, but I don't know for sure.

Here is the sample of what happened:

<img width=""1104"" alt=""Screen Shot 2021-10-28 at 12 14 11"" src=""https://user-images.githubusercontent.com/38188988/139191329-f6e45e8a-1ff1-4c82-b647-d7d945881136.png"">

<img width=""1115"" alt=""Screen Shot 2021-10-28 at 12 14 33"" src=""https://user-images.githubusercontent.com/38188988/139191365-a51f54d8-4800-4b71-959e-2a01a4094438.png"">

<img width=""1119"" alt=""Screen Shot 2021-10-28 at 12 14 49"" src=""https://user-images.githubusercontent.com/38188988/139191386-afe14fb7-2489-4f4b-8c4b-bc35382effaa.png"">

<img width=""1113"" alt=""Screen Shot 2021-10-28 at 12 15 06"" src=""https://user-images.githubusercontent.com/38188988/139191407-9254f87e-e10d-4cae-af37-8b799c0e5eba.png"">

<img width=""815"" alt=""Screen Shot 2021-10-28 at 12 15 16"" src=""https://user-images.githubusercontent.com/38188988/139191424-09062211-fcf8-4fe8-a88b-86a45fad5d52.png"">

I'm experiencing this issue on multiple devices: AWS SageMaker CPU and GPU instance, and my MacOS local jupyter notebook.

**Tensorflow Version** : 2.6.0
**Tensorflow Installed from**: PyPI
**Python ver**: 3.7.10
"
52759,"Is tf1.15-gpu compatible with cudnn8,cuda10.2?","centos7
gpu 2080ti
gcc 4.8.5
bazel 0.26.1
python 3.8 
tf code  1.15
cuda 10.2
cudnn 8.0.1
i modified  **./configure.py**  and  **./third_party/gpus/find_cuda_config.py** and **WORKSPACE** When I execute **bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so** command, it has some errors:

![企业微信截图_16353867475126](https://user-images.githubusercontent.com/61573829/139174477-e5d1b8d1-0832-4f09-a1af-ada97a8c5464.png)
![企业微信截图_16353867872436](https://user-images.githubusercontent.com/61573829/139174479-c98cdfe8-f4f4-4418-9227-51d86d9ff36e.png)
![企业微信截图_16353874251878](https://user-images.githubusercontent.com/61573829/139174718-eabfe646-bf5b-4643-8696-43066fa2e98a.png)

![企业微信截图_16353866595640](https://user-images.githubusercontent.com/61573829/139174485-e5b06680-7573-4ea0-a363-32ae9f5c90b2.png)
![企业微信截图_16353865686228](https://user-images.githubusercontent.com/61573829/139174495-7398e059-8bcc-4141-bf62-acc2d3e21698.png)

Is tf1.1x-gpu can not be build with cudnn8,cuda10.2?"
52755,pycham Cannot auto replenish kersa.layers,"import numpy as np
import pandas as pd
import matplotlib as plt
import tensorflow as tf
import minst_reader


# 加载fashion_mnist数据集
x_train, y_train= minst_reader.load_mnist('fashion',kind='train')
x_test, y_test= minst_reader.load_mnist('fashion',kind='t10k')
x_train,y_train = x_train/ 255.0,y_train/255.0

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128,activation = 'relu')
    tf.keras.layers.Dense(10,activation = 'softmax')
])
![}8C4{Z5%P`2%A)JP3OB{7AN](https://user-images.githubusercontent.com/72368814/139173294-0c713a9e-9849-4bbc-a3c3-a12fe62009cf.png)
e
"
52754,ModuleNotFoundError: No module named 'tf',"import numpy as np
import pandas as pd
import matplotlib as plt
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)

# 加载fashion_mnist数据集
fashion_mnist1 = tf.keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()



output is
2.6.0
Traceback (most recent call last):
  File ""G:/workspace/py/data analize/untitled/Tensorflow2leanProject/prectise.py"", line 10, in <module>
    fashion_mnist1 = tf.keras.datasets.fashion_mnist
  File ""D:\Users\Administrator\anaconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""D:\Users\Administrator\anaconda3\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""D:\Users\Administrator\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""D:\Users\Administrator\anaconda3\lib\site-packages\keras\__init__.py"", line 25, in <module>
    from tf.keras import models
ModuleNotFoundError: No module named 'tf'
"
52752,tf.nn.ctc_loss calculated sequentially when using tf.distribute.MirroredStrategy(),"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- TensorFlow installed from (source or binary): docker pull tensorflow/tensorflow:2.5.0-gpu, running in a TFJob
- GPU model and memory: Tesla P100


**Describe the current behavior**

I am working on a ctc based model and wanted to accelerate the training by applying the MirroredStrategy to my custom training loop. I modified my code according to the tutorials, but did not observe any speed up. After digging into it with the profiler I noticed in the trace viewer that each gpu seemed to compute the loss one after another instead of concurrently (see image further below). Also, there seemed a lot of communication to be going on between the devices and the host. Moreover, the overview page stated that more than ~95% of the device time is spent on eager execution. Maybe that's related somehow?

**Describe the expected behavior**
Maybe there is some misunderstanding on my side how exactly ctc_loss and MirroredStrategy work, but I would expect that it is possible to calculate the loss concurrently on all gpus.

**Standalone code to reproduce the issue**
Here is some example code which reproduces the issue I am facing. I removed the model forward/backward pass as it is not important and did not seem to cause any problems:

```python
import tensorflow as tf

# some dummy parameters, not really important
profile_steps = 3
per_replica_batch_size = 2
max_label_seq_length = frames = 20
num_labels = 10
strategy = tf.distribute.MirroredStrategy()
GLOBAL_BATCH_SIZE = per_replica_batch_size * strategy.num_replicas_in_sync

with strategy.scope():
    def compute_loss(labels, logits, label_length, logit_length):
        per_replica_loss = tf.nn.ctc_loss(labels, logits, label_length, logit_length)
        return tf.nn.compute_average_loss(
            per_replica_loss, global_batch_size=GLOBAL_BATCH_SIZE
        )

@tf.function
def train_step():
    labels = tf.ones((GLOBAL_BATCH_SIZE, max_label_seq_length), dtype=tf.int32)
    logits = tf.random.normal(shape=(frames, GLOBAL_BATCH_SIZE, num_labels))
    label_length = tf.ones(GLOBAL_BATCH_SIZE, dtype=tf.int32) * max_label_seq_length
    logit_length = tf.ones(GLOBAL_BATCH_SIZE, dtype=tf.int32) * frames
    loss = compute_loss(labels, logits, label_length, logit_length)

tf.profiler.experimental.start(""logs/profiler"")
for step in range(profile_steps):
    with tf.profiler.experimental.Trace(""train"", step_num=step, _r=1):
        loss = strategy.run(
            train_step,
        )
tf.profiler.experimental.stop()
```

**Other info / logs** 

Here is a screenshot of the trace viewer of the above code when executed on 3 gpus.

<img width=""694"" alt=""tf_distributed"" src=""https://user-images.githubusercontent.com/36960190/139153715-0ca32c6a-30b4-43a9-93f8-18f475deb5a5.PNG"">


"
52726,Tensorflow C api is no longer supported on mac OSX High Sierra (Sierra too),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OSX High Sierra
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 2.6 -> 2.8
- Python version: None
- Installed using virtualenv? pip? conda?: None 
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory:

![Capture d’écran 2021-10-27 à 19 50 12](https://user-images.githubusercontent.com/8608052/139119271-a594bd58-8cd7-4c2d-b214-2e04acf433e2.png)

**Describe the problem**
Using the tensorflow C api, a compatibility issue appeared on High Sierra (Maybe Sierra too)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Compiling a program works well but at runtime : dyld: Symbol not found: ____chkstk_darwin (which was built for Mac OS X 10.15)

**Any other info / logs**
To fix the problem, just recompile tensorflow api with the option in bazel : --macos_minimum_os=10.13 (high sierra)
"
52705,Title: Input shape compatibility of ZIP-object of (image and image ImageDataGenerator objects) and CNN,"**System information**
Have I written custom code (as opposed to using a stock example script
provided in TensorFlow): custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
happens on a mobile device: NA
TensorFlow installed from (source or binary): binary (added as library to Anaconda )
TensorFlow version (use command below): 2.1.0
Python version: 3.7
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: 10.1
GPU model and memory:
NVIDIA GeForce RTX 2070 with Max-Q Design
Driver version: 30.0.14.7196
Driver date: 8/27/2021
DirectX version: 12 (FL 12.1)
Physical location: PCI bus 1, device 0, function 0
Utilization 0%
Dedicated GPU memory 0.0/8.0 GB
Shared GPU memory 0.1/7.9 GB
GPU Memory 0.1/15.9 GB- Exact command to reproduce:
########################################

I have 64 RGB images with their corresponding masks. I use ImageDataGenerator and flow_from_directory function to read the images and masks from the directory folder.
The folder structures are as follows:
Images
---- imgs
-------- ---ROI
----------------1.tiff
----------------2.tiff
Masks 
------imgs
------- -----ROI
------------------1.tiff
------------------2.tiff
Images and masks are stored with the same file names. In my case, ImageDataGenerator is an Iterator of size 2 with a batch size of 32. I did the following:
1.	I read the images and masks into lists named as “images” and ‘’masks’’ respectively.

```
size = 231 
images = []

for directory_path in glob.glob(“. /Images/imgs/ROI/""):
    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff""):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       
        img = cv2.resize(img, (size, size))
        images.append(img)
        images = np.array(images)

masks = [] 
for directory_path in glob.glob(“. /Masks/imgs/ROI/""):
    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff""):
        mask = cv2.imread(mask_path, 0)       
        mask = cv2.resize(mask, (size, size))
        masks.append(mask)     
masks = np.array(masks)
masks = np.expand_dims(masks, axis=3)
```
**Note: I obtained images of shape (64, 231,231,3) and masks of shape (64, 231, 231, 1)**

2.	I then used keras documentation the process of transforming the images and masks together as follows:

          a.	I created a dict to specify the augmentation parameters as follows for both images and masks.
          data_gen_args_imgs = dict rotation range=90,
                               rescale = 1. /255.)
          data_gen_args_masks = dict rotation range=90)
                       
          b.	The I created ImageDataGenerator objects for both images and masks as follows:
          ```
          image_datagen = ImageDataGenerator(**data_gen_args_imgs)
          mask_datagen = ImageDataGenerator(**data_gen_args_masks)
          ```
          
          c.	I use the fit and flow functions using the same seeds and keyword arguments for images and masks as follows: 
          ```
          seed = 1
          
          image_datagen.fit(images, augment=True, seed=seed)
          mask_datagen.fit(masks, augment=True, seed=seed)
          
          image_generator = image_datagen.flow_from_directory(
              'data/images',
              class_mode=None,
              shuffle= False,
              seed=seed)
          
          mask_generator = mask_datagen.flow_from_directory(
              “. /Images/imgs/”,
              class_mode=None,
              shuffle= False,
              seed=seed)
          ```
          
          d.	Then I created an iterator on the zip object to return a matched pair of images and corresponding masks as follows:
          
          `train_generator = (pair for pair in zip (image_generator, mask_generator))`
          
          e.      Then I build a CNN model as follows: 
          ```
          
          activation= 'relu'
          model = Sequential()
          model.add (Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))
          model.add (Conv2D(16, kernel_size=(3, 3),activation= activation,    padding='same'))
          model.add (Flatten())
          model.add (Dense(5))
          model.add (Dense(3))
          model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
          model.summary()
          
          ```
          
          e.	Then I used fit_generator to fit the model as follows: 
          
          `model.fit_generator (train_generator, steps_per_epoch= 15, epochs=2, verbose=1)`

But I got the following error regarding input dimensionality:
```
InvalidArgumentError:  Input to reshape is a tensor with 33554432 values, but the requested shape requires a multiple of 853776
 [[node sequential_8/flatten_8/Reshape (defined at <ipython-input-91-2871596287c2>:12) ]] [Op:__inference_distributed_function_7252]

Function call stack:
distributed_function
```
Following is the full code:
```
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import glob
import cv2
import os
import glob
import tensorflow as tf
import pickle
from scipy import ndimage as nd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow .keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import sklearn.utils 
from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam

print(tf.__version__)

train_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//""
mask_path=""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//""

############################################################################
size = 231 
images = []

for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_1//ROI_train//""):
    for img_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       
        img = cv2.resize(img, (size, size))
        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        images.append(img)
        #train_labels.append(label)
        
images = np.array(images)

masks = [] 
for directory_path in glob.glob(""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//ROI_train//""):
    for mask_path in glob.glob(os.path.join(directory_path, ""*.tiff"")):
        mask = cv2.imread(mask_path, 0)       
        mask = cv2.resize(mask, (size, size))
        masks.append(mask)
                
masks = np.array(masks)
masks=np.expand_dims(masks, axis=3)

#######################Example of transforming images and masks together.
import tensorflow as tf

# we create two instances with the same arguments
data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90)
                     
## what is images
## what is masks
# what is **data_gen_args

image_datagen = ImageDataGenerator(data_gen_args)
mask_datagen = ImageDataGenerator(data_gen_args)
# Provide the same seed and keyword arguments to the fit and flow methods
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)

image_generator = image_datagen.flow_from_directory(
    train_path,
    class_mode=None,
    shuffle= False,
    seed=seed)

mask_generator = mask_datagen.flow_from_directory(
    ""C://Users//Tusneem//Documents//Untitled Folder//Tusneem_Seg//SEG_OBJ1//BAS//ROI_2//"",
    class_mode=None,
    shuffle= False,
    seed=seed)
# combine generators into one which yields image and masks
#train_generator = zip(image_generator, mask_generator)
train_generator = (pair for pair in zip(image_generator, mask_generator))

activation= 'relu'

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(231,231,3),padding='same'))
model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(5))
model.add(Dense(3))
model.compile( loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

model.fit_generator(train_generator, steps_per_epoch= 15, epochs=2, verbose=1)
```
**my question is: How can I fix the problem of train_generator and CNN input shapes??**
#########attachments 
[The code.txt](https://github.com/tensorflow/tensorflow/files/7439208/The.code.txt)
[summary of ImageDataGenarator_ZIP object_CNN.docx](https://github.com/tensorflow/tensorflow/files/7439221/summary.of.ImageDataGenarator_ZIP.object_CNN.docx)


"
52702, Genrules without outputs in building process.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: N/A
- Python version: 3.9
- Installed using virtualenv? pip? conda?: NO
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 11.1/cudnn8
- GPU model and memory: V100/32GB



**Describe the problem**
Came across an issue after running `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=172
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/5d226f5ff8a38cd00f5596186870c15c488a9a67.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/471b25e217e635e058bbdbca8c693e2998380a60.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-py/archive/pypi-v0.14.1.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /root/tensorflow/WORKSPACE:23:14: in <toplevel>
  /root/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/local_config_python/BUILD:69:8: in outs attribute of genrule rule @local_config_python//:python_include: Genrules without outputs don't make sense
INFO: Repository XNNPACK instantiated at:
  /root/tensorflow/WORKSPACE:15:14: in <toplevel>
  /root/tensorflow/tensorflow/workspace2.bzl:1090:21: in workspace
  /root/tensorflow/tensorflow/workspace2.bzl:134:20: in _tf_repositories
  /root/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /root/tensorflow/third_party/repo.bzl:66:35: in <toplevel>
INFO: Repository cudnn_frontend_archive instantiated at:
  /root/tensorflow/WORKSPACE:15:14: in <toplevel>
  /root/tensorflow/tensorflow/workspace2.bzl:1090:21: in workspace
  /root/tensorflow/tensorflow/workspace2.bzl:165:20: in _tf_repositories
  /root/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /root/tensorflow/third_party/repo.bzl:66:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_python//:python_include' failed
INFO: Elapsed time: 58.903s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (430 packages loaded, 17632 targets configured)
    Fetching @cython; fetching
    Fetching ...7b8d97/external/cython; Extracting /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/cython/temp17300240480834963304/0.29.21.tar.gz
    Fetching @mkl_dnn_v1; fetching
    Fetching ...external/mkl_dnn_v1; Extracting /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn_v1/temp3837256909875719335/v2.4-rc.tar.gz`
"
52692,Detection of images using Object Detection Algorithm based on YOLO (importing tensorflow) in different setups,"Hi,

I have been running some detections of images by calling upon trained weights for YOLOv3. This is all done through either tensorflow or tensorflow-GPU.
Some of these detections are done through my own laptop (CPU bases -- Mac) and sometimes using GPU Clusters which we can only access by SSH. 
I noticed that for the very same weights being used, on different laptops and CPU workstations the detections are the same.
However, in case of using a GPU cluster or CPU cluster which does not have any display input, the detections are different.
Apart from this, the workspace is identical.

Does this have to do anything with the fact that some display drivers are not installed or some tensorflow libraries are missing??

I know this is a pretty generic query, and I'll try my best to elaborate further if required.

Thanks"
52685,libdevice not found at ./libdevice.10.bc,"**System information**
- OS Platform and Distribution: Windows 10 Enterprise
- Issue occurs on a desktop
- TensorFlow installed from (source or binary):
- TensorFlow version:  'GIT_VERSION': 'v2.6.0-rc2-32-g919f693420e'
- Python version: 3.8.12
- Installed using conda:
- CUDA/cuDNN version: V11.5/8.2
- GPU model and memory: NVIDIA GeForce RTX 3060 Ti 8Gb

Just installed TensorFlow and TensorFlow probability to go through [Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/). Everything runs fine if I disable GPU. When I enable GPU use I'm getting the following error:

```
InternalError:  libdevice not found at ./libdevice.10.bc
	 [[{{node cluster_0_1/xla_compile}}]] [Op:__inference_lbeta_369]

Function call stack:
lbeta
```

I have libdevice.10.bc file in the locations defined both in Windows PATH and via conda develop PATH definition. I can't figure out where exactly TensorFlow is looking for this file, as the traceback is not helpful. I found [the solution](https://discuss.tensorflow.org/t/libdevice-not-found-why-is-it-not-found-in-the-searched-path/3419) by Julian Moore, but it did not help in my case."
52681,"In Tensorflow 2.6, tf.keras.backend.gradients() return None","I try to customize a loss functionm, But when I run the following code:

```
pressure_grad_x = tf.keras.backend.gradients(out2, cur_x_input)[0]
pressure_grad_y = tf.keras.backend.gradients(out2, cur_y_input)[0]
pressure_grad_z = tf.keras.backend.gradients(out2, cur_z_input)[0]
pressure_grad = tf.convert_to_tensor([pressure_grad_x, pressure_grad_y, pressure_grad_z])
```

An error will be reported(The above code is in the custom function.)：

```
<ipython-input-42-23232050871c>:34 call  *
    pressure_grad = tf.convert_to_tensor([pressure_grad_x, pressure_grad_y, pressure_grad_z])
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\util\dispatch.py:206 wrapper  **
    return target(*args, **kwargs)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py:1431 convert_to_tensor_v2_with_dispatch
    value, dtype=dtype, dtype_hint=dtype_hint, name=name)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py:1441 convert_to_tensor_v2
    as_ref=False)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\profiler\trace.py:163 wrapped
    return func(*args, **kwargs)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\ops.py:1566 convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\constant_op.py:346 _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\constant_op.py:272 constant
    allow_broadcast=True)
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\constant_op.py:290 _constant_impl
    allow_broadcast=allow_broadcast))
C:\Users\dell\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\framework\tensor_util.py:553 make_tensor_proto
    ""supported type."" % (type(values), values))

TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, None, None]. Consider casting elements to a supported type.
```
When I tried to solve it, I found that the value of pressure_grad_x (or pressure_grad_y, pressure_grad_z) is None.

The model i used if LSTM model and take the custom loss function as the last layer of the model.

out2 is the outputs of LSTM model. cur_x_input, cur_y_input, cur_z_input is the inputs of LSTM model.The version of Tensorflow is 2.6.0.

I have no way to solve this problem. I hope someone can help me solve this problem."
52680,Inconsistent results on every run with GPU,"
**System information**
- OS Platform and Distribution: Colab
- TensorFlow version: 2.6.0 GPU

**Describe the current behavior**
Hello, I'm using TensorFlow 2.4.0 with GPU support on my local machine to train a small CNN model. I can get the same training results when I train it on CPU mode if I fixed the random seed, but always get inconsistent results on GPU mode.

*So I test it on Colab, which has TensorFlow 2.6.0 installed in default, but still got inconsistent results on GPU mode.*

**Describe the expected behavior**
The results should be the same.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): x

**Standalone code to reproduce the issue**
Here is the test code
[Colab](https://colab.research.google.com/drive/1Xr_p9rsKM0bk3DDt9CL3L-dN7Vvz179Q?usp=sharing)
Run it for many times with GPU enabled, you will get different results on each run.

**Other info / logs*
I found that only when I use `relu`, `leaky_relu` or `tf.maximum` as the activation function the results change on every run. But if I use `sigmoid`, `tanh`, or other non-relu-like activation functions the results will remain the same."
52679,import tensorflow.experimental.numpy as tnp,"why i cant use ""import tensorflow.experimental.numpy as tnp"" and it shows ""no module""
but import tensorflow as tf  and tf.experimental.numpy are good
"
52678,Shapes are incompatible when train pose_classification,"Error ""ValueError: Shapes (16, 1) and (16, 5) are incompatible"" occurs when I'm running ""pose_classification.ipynb"" on custom dataset at this line
```
history = model.fit(X_train, y_train,
                        epochs=200,
                        batch_size=16,
                        validation_data=(X_val, y_val),
                        callbacks=[checkpoint, earlystopping])
```

-------Update---------
It seems not a bug, the problem should be the dataset and its labeling. I'll close it now.
Sorry for inconvenience."
52677,import tensorflow.experimental.numpy as tnp,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52676,Undefined behaviour in Range,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): git HEAD
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 10.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

https://github.com/tensorflow/tensorflow/blob/0b67dee3f02e2e055230ca6dd6cc7d090af72baa/tensorflow/core/ops/math_ops.cc#L1484 has undefined behaviour when size is greater than std::numeric_limits<int64_t>::max()
This leads to the unit test RangeTest.testLargeStarts failing on AARCH64 where the g++ implements different behaviour from x86. On x86 the result of the cast is large and -ve, on AARCH64 it is large and +ve. Neither is incorrect as the behaviour of casting into a type that cannot hold the value is undefined.

**Describe the expected behavior**

The code should be written to avoid relying on undefined behaviour of the source.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):

Test the variable 'size' for exceeding the greatest possible value that can be safely cast to int64_t and throw an error if found.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

$ bazel test --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=""""  --remote_cache_proxy="""" --noremote_accept_cached --config=nonccl --verbose_failures -- //tensorflow/python/kernel_tests:init_ops_test

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

======================================================================
ERROR: testLargeStarts (__main__.RangeTest)
RangeTest.testLargeStarts
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py"", line 553, in testLargeStarts
    v = math_ops.range(start=-1e+38, limit=1)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/util/traceback_utils.py"", line 141, in error_handler
    return fn(*args, **kwargs)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/util/dispatch.py"", line 1092, in op_dispatch_handler
    return dispatch_target(*args, **kwargs)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py"", line 2113, in range
    return gen_math_ops._range(start, limit, delta, name=name)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_math_ops.py"", line 7737, in _range
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 7131, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[9223372036854775807] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Range]

"
52675,Problem with TF Object Detection API,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow version: 2.6.0
- Python version: 3.9
- GPU model and memory: RTX 2060

**Problem**
I'm trying to build and train an object detection model (using ssd_resnet50_v1_fpn) configuring a pipeline with paths to train.tfrecord, test.tfrecord and label_map.pbtxt. I get the last files from cvat.org when export the project in the TFRecord format. 

**Info / logs**
I run:
 python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v
1_fpn/pipeline.config
and below what I read:

2021-10-26 10:36:02.910336: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)
to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-26 10:36:04.117867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3967 MB memory:
 -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I1026 10:36:04.304352 16668 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: None
I1026 10:36:04.309246 16668 config_util.py:552] Maybe overwriting train_steps: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I1026 10:36:04.309246 16668 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from
_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W1026 10:36:04.327197 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\model_lib_v2.py:557: StrategyBase.experime
ntal_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['annotations\\Train.tfrecord']
I1026 10:36:04.334176 16668 dataset_builder.py:163] Reading unweighted datasets: ['annotations\\Train.tfrecord']
INFO:tensorflow:Reading record datasets for input file: ['annotations\\Train.tfrecord']
I1026 10:36:04.646700 16668 dataset_builder.py:80] Reading record datasets for input file: ['annotations\\Train.tfrecord']
INFO:tensorflow:Number of filenames to read: 1
I1026 10:36:04.646700 16668 dataset_builder.py:81] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1026 10:36:04.648099 16668 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_interleave (from tensorflow.p
ython.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti
ons.experimental_deterministic`.
W1026 10:36:04.654085 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_
interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti
ons.experimental_deterministic`.
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1.map_with_legacy_function (fr
om tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W1026 10:36:04.688991 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1
.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\util\dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.spar
se_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W1026 10:36:10.687601 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\util\dispatch.py:206: sparse_to_dense (fr
om tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\util\dispatch.py:206: sample_distorted_bounding_box (from tensorflow.p
ython.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W1026 10:36:13.237039 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\util\dispatch.py:206: sample_distorted_bo
unding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\autograph\impl\api.py:464: to_float (from tensorflow.python.ops.math_o
ps) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W1026 10:36:14.707938 16668 deprecation.py:339] From C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\autograph\impl\api.py:464: to_float (from
 tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
2021-10-26 10:36:16.954501: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Traceback (most recent call last):
  File ""D:\programmi\tensorflow\workspace\training_demo\model_main_tf2.py"", line 115, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""D:\programmi\tensorflow\workspace\training_demo\model_main_tf2.py"", line 106, in main
    model_lib_v2.train_loop(
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\model_lib_v2.py"", line 599, in train_loop
    load_fine_tune_checkpoint(
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\model_lib_v2.py"", line 394, in load_fine_tune_checkpoint
    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\object_detection\model_lib_v2.py"", line 159, in _ensure_model_is_built
    features, labels = iter(input_dataset).next()
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 689, in next
    return self.__next__()
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 693, in __next__
    return self.get_next()
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 731, in get_next
    self._iterators[i].get_next_as_list_static_shapes(new_name))
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1951, in get_next_as_list_static_shapes
    return self._format_data_list_with_options(self._iterator.get_next())
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py"", line 573, in get_next
    result.append(self._device_iterators[i].get_next())
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 814, in get_next
    return self._next_internal()
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py"", line 744, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\ops\gen_dataset_ops.py"", line 2727, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\donat\anaconda3\envs\doEnv\lib\site-packages\tensorflow\python\framework\ops.py"", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.
         [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]] [Op:IteratorGetNext]"
52674,Install error on m1 MBP (Python 3.9),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.6
- TensorFlow installed from (source or binary): Try it but not worked.
- Python version: 3.9.5
- Installed using virtualenv? pip? conda?: conda
- GPU model and memory: M1

Okay. I was try to installing on my m1 mbp (python 3.9) and i got some error like this..
<pre>


Installation script for pre-release tensorflow_macos 0.1alpha3.  Please visit https://github.com/apple/tensorflow_macos
for instructions and license information.

This script will download tensorflow_macos 0.1alpha3 and needed binary dependencies, then install them into a new
or existing Python 3.8 virtual environment.
Continue [y/N]? y

Downloading installer.
/var/folders/01/5f5ftlvn2w7c9prxbs4410780000gn/T/tmp.U2PQuGHt ~
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   641  100   641    0     0   8434      0 --:--:-- --:--:-- --:--:--  8434
100  359M  100  359M    0     0  4320k      0  0:01:25  0:01:25 --:--:-- 5176k
Extracting installer.
Path to new or existing virtual environment [default: /Users/bahk_insung/tensorflow_macos_venv/]: /Users/bahk_insung/miniforge3/envs/pycv/tensorflow_mac_venv/
##############################################################

ERROR: Error retrieving python version, or python executable /Users/bahk_insung/miniforge3/envs/pycv/bin/python3 not version 3.8.  Please specify a Python 3.8 executable with the --python option.


Error running installation script with default options.  Please fix the above errors and proceed by running

  /var/folders/01/5f5ftlvn2w7c9prxbs4410780000gn/T/tmp.U2PQuGHt/tensorflow_macos/install_venv.sh --prompt


</pre>

I think my conda environment is not fitted for tensorflow. I try it with <a href=""https://github.com/apple/tensorflow_macos"">this one</a>. plz help me why does it."
52673,No SVDF ref kernel support with all 8 bit weights and state,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 1180d389b41314e9715336b4b7b194140292d5c8

TFLM has support for this already:
https://github.com/tensorflow/tflite-micro/pull/506

If CMSIS-NN should add support for this it would be good to first have support in Tensorflow Lite, since CMSIS-NN has TFL as reference for its tests.


"
52672,Warning persists after upgrading to Mac OS 12- Could not identify NUMA node of platform GPU ID 0,"**This warning was supposed to be resolved in MacOs 12 . How to fix this?**
```
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

from tensorflow.keras import layers
from tensorflow.keras import models
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()
```
```

Metal device set to: Apple M1
2021-10-26 01:42:59.638063: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-10-26 01:42:59.638241: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
```
"
52668,Does XLA handle FusedBatchNormV3 correctly in GPUs,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):: 2.6
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4
- GPU model and memory:


**Describe the current behavior**
It appears that when tf_xla_auto_jit=2 is set and mixed precision (fp16) is used,  networks run  significantly faster (3.5x) over non-XLA .  The advantage disappears when batchnorm is disabled ( about 1.3 x) 

**Describe the expected behavior**
Should not show such marked differences  with and without batchnorm.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Just use any code  with tf.keras.layers.BatchNormalization  

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52664,Target //tensorflow/tools/pip_package:build_pip_package failed to build,"### System information

- Im trying to build tensorflow to run custom code
- Using macOS Big Sur 11.4
- Tensorflow installed from source
- Can't find Tensorflow version
- Python 3.7
- bazel 3.1.0
- Intel HD Graphics 530 1536 MB
- 16 GB 2133 MHz LPDDR3
- bazel build -c opt $COPT -k //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
6 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /Users/gwendolynsarapata/tensorflow/tensorflow/python/tools/BUILD:99:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): cc_wrapper.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_gwendolynsarapata/0ae91cddfac444443cadbf8c325d1a60/execroot/org_tensorflow && \
  exec env - \
    PATH=/Users/gwendolynsarapata/opt/anaconda3/envs/test/bin:/Users/gwendolynsarapata/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \
    PWD=/proc/self/cwd \
 
...

Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 249.809s, Critical Path: 49.62s
INFO: 612 processes: 612 local.
FAILED: Build did NOT complete successfully

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
52660,ausashet,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
52657,Numpy 1.21.2/3 Causing Feature Column Unit Test Failures,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): latest test was with https://github.com/tensorflow/tensorflow/commit/94bc26a5f71ad14f2390def5a6ecead451f7c9bc (an Oct'25 2021 commit on main branch)
- Python version: 3.8
- Bazel version (if compiling from source): ""We have bazel 3.7.2 installed.""
- GCC/Compiler version (if compiling from source): gcc-7
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
This commit bumped up numpy version. https://github.com/tensorflow/tensorflow/commit/f96917ea887d1108358132c9dc9a0a0366e7d69e
Many CI relies on the above file to run their nightlies. 
//tensorflow/python/feature_column:feature_column_test unit test failures with numpy 1.21.2 or numpy 1.21.3 with the following error message. numpy 1.19.5 was successful. 

[2021-10-25T12:40:03.025Z] ERROR: test_fills_cols_to_vars (__main__.LinearModelTest)
[2021-10-25T12:40:03.025Z] LinearModelTest.test_fills_cols_to_vars
[2021-10-25T12:40:03.025Z] ----------------------------------------------------------------------
[2021-10-25T12:40:03.025Z] Traceback (most recent call last):
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column_test.py"", line 1612, in test_fills_cols_to_vars
[2021-10-25T12:40:03.025Z]     self.assertAllEqual(cols_to_vars['bias'], [bias])
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1390, in decorated
[2021-10-25T12:40:03.025Z]     return f(*args, **kwds)
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3055, in assertAllEqual
[2021-10-25T12:40:03.025Z]     a = self._GetNdArray(a)
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2799, in _GetNdArray
[2021-10-25T12:40:03.025Z]     return np.array(a)
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 534, in __array__
[2021-10-25T12:40:03.025Z]     return np.asarray(self.numpy(), dtype=dtype)
[2021-10-25T12:40:03.025Z]   File ""/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-eigen-test/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py"", line 674, in numpy
[2021-10-25T12:40:03.025Z]     raise NotImplementedError(
[2021-10-25T12:40:03.025Z] NotImplementedError: numpy() is only available when eager execution is enabled.


**Describe the expected behavior**

The unit test should pass. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
- Downgrade numpy to ~1.19.2 as workaround. Not sure about the root-cause and fix. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Simply run the //tensorflow/python/feature_column:feature_column_test unit test. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

An example full failure log can be found in Intel's public CI page via https://tensorflow-ci.intel.com/job/tensorflow-eigen-test/492/artifact/eigen_test.log/*view*/ (subject to expiration). 
"
52656,ValueError: as_list() is not defined on an unknown TensorShape when using dataset.from_generator on model.fit(dataset),"Tensorflow 2.4.1

I'm trying to convert a generator to dataset, I'm able to create the dataset successfully and iterate through it. However when I call model.fit(dataset) I get the above error. 

````
class Generator(Sequence):

    def __init__(self, filelist, batch_size, input_size, train=True, max_ones = 1):
        self.batch_size = batch_size
        self.filelist = filelist
        self.input_size = input_size
        self.train = train
        self.max_ones = max_ones
        
        if int(len(self.filelist['1'])) == 0:
            self.max_ones = 0
            
        self.shuffle()
        

    def __len__(self):
        if int(len(self.filelist['1'])) == 0:
            return 16
        else:
            return int(len(self.filelist['1'])) 
        
    def shuffle(self):

        print('shuffling')
        self.filelist_shuffled = self.filelist.copy()
       
        random.shuffle(self.filelist_shuffled['1'])
        random.shuffle(self.filelist_shuffled['0'])
            
        print(self.filelist_shuffled['0'][:10])
        self.ind = dict()
        self.ind['1'] = 0
        self.ind['0'] = 0

    def get_cropped_image(self, label):

        while True:
            
            label = str(random.randint(0, 1))
            
            ind = self.ind[label]
            
            if label == ""1"":
                ind = random.randrange(0,len(self.filelist_shuffled[label]))
             
            coord = self.filelist_shuffled[label][ind]
            
            Icrop = np.load(coord)

            self.ind[label] = (self.ind[label] + 1) % len(self.filelist[label])

            return cv2.resize(Icrop, self.input_size), label

    def __getitem__(self, idx):

        if idx == 0:
            self.shuffle()

        X = []
        Y = []

        count = 0
        while count < self.batch_size:

            if self.train:
                I0,label_0 = self.get_cropped_image('0')
                I1,label_1 = self.get_cropped_image('1')

                X.append(I0)
                X.append(I1)

                Y.append(int(label_0))
                Y.append(int(label_1))

                count += 2
            else:
                if count < self.max_ones:
                    I1,label_1 = self.get_cropped_image('1')
                    X.append(I1)
                    Y.append(int(label_1))
                else:
                    I0,label_0 = self.get_cropped_image('0')
                    X.append(I0)
                    Y.append(int(label_0))

                count += 1
        return np.array(X), np.array(Y)

def make_gen_callable(_gen):
        def gen():
            for x in _gen:
                 yield x
        return gen
train_ = make_gen_callable(generator_train)

dataset = tf.data.Dataset.from_generator(train_, (tf.float32, tf.int32))
```
"
52654,Keras Metrics Accuracy Result is not the same to Result caculated by myself,"Here is My notebooks, Could someone give me a reason, And Help me get higher precision about keras metrcs. When I use the accuracy metrics in large images dataset and Train on multi gpus, The accuracy will be not correct.

```
accuracy1 = model.evaluate(validation_dataset)[-1]
```
```
error_count = 0
bad_error_count = 0
good_error_count = 0
batch_count = 0
count = 0
for imgs,labels in validation_dataset:
    current_len = len(imgs)
    count += current_len
    predictions = model.predict(imgs)
    predictions_index = tf.argmax(predictions,axis=1)
    labels_index = tf.argmax(labels,axis=1)

    for prediction_index,label_index in zip(predictions_index,labels_index):
        if prediction_index != label_index:
            error_count += 1
            if label_index == 0:
                bad_error_count += 1
            else:
                good_error_count+= 1
    batch_count += 1
    # if batch_count == batch_num:
        # break
        
print(error_count,
bad_error_count,
good_error_count,
batch_count)

accuracy2 = (count-error_count)/count

```
![image](https://user-images.githubusercontent.com/31404627/138727750-9607d8b8-0bac-47a4-ba62-b052bf717bad.png)

Click this link below, reproduce this issue.

https://colab.research.google.com/drive/1xNkpQTenCav5tHg8SNO5MaXRs5UI_TNx?usp=sharing"
52653,TFLite: XNNPACK delegate failed to delegate FULLY_CONNECTED node,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Source for inference, Binary for model generation
- TensorFlow version (use command below): r2.6.0 (from git tag) for the inference code.  2.4.1 for model generation.
- Python version: 3.8.5 (model generation side only)
- Bazel version (if compiling from source): N/A, I'm building TFLite via CMake
- GCC/Compiler version (if compiling from source): MSVC 19.29.30136.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When loading the attached MobileNet model via the TFLite C API, the XNNPACK delegate fails with the following output:

```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: failed to delegate FULLY_CONNECTED node #67
ERROR: Node number 83 (TfLiteXNNPackDelegate) failed to prepare.
```

**Describe the expected behavior**
The model loads correctly.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing): -

**Standalone code to reproduce the issue**

Model file: [model.fp32.tflite.zip](https://github.com/tensorflow/tensorflow/files/7409553/model.fp32.tflite.zip)

### Model generation (if not using the attached tflite model):

```python
import tensorflow as tf
from pathlib import Path
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(
        input_shape=(224,224,3), alpha=1.0, weights='imagenet',
        classes=6, include_top=False, pooling='avg')
  output = tf.keras.layers.Dense(6, name='logits', dtype='float32')(base_model.output)
  model = tf.keras.Model(base_model.inputs, output)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
Path('model.fp32.tflite').write_bytes(tflite_model)
```

### Model loading in Cpp

```cpp
#include <tensorflow/lite/c/c_api.h>
#include <tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h>

int main()
{
    auto m_model = TfLiteModelCreateFromFile(""model.fp32.tflite"");
    auto m_options = TfLiteInterpreterOptionsCreate();
    TfLiteXNNPackDelegateOptions opt = TfLiteXNNPackDelegateOptionsDefault();
    auto m_xnnpack_delegate = TfLiteXNNPackDelegateCreate(&opt);
    TfLiteInterpreterOptionsAddDelegate(m_options, m_xnnpack_delegate);
    auto m_interpreter = TfLiteInterpreterCreate(m_model, m_options); // This returns nullptr and prints error messages in the console
}
```
"
52652,"Input tensors to a Functional must come from `tf.keras.Input`. Received: tf.Tensor( [[[ 0.48200893]   [ 2.3927188 ]   [ 2.27573   ]   ...   [ 0.02205751]   [ 0.49903008]   [ 0.27605903]]   [[ 5.696963  ]   [ 2.674962  ]   [-0.40183762]   ...   [-0.9503441 ]   [-0.2546521 ]   [-0.09128924]]   [[ 6.234084  ]   [ 3.4273057 ]   [ 0.97633094]   ...   [-0.7193552 ]   [-0.17173009]   [-2.2666237 ]]   ...   [[ 5.675222  ]   [ 3.1183105 ]   [ 1.2060118 ]   ...   [ 0.40663907]   [-1.2273816 ]   [ 0.5917349 ]]   [[ 5.627962  ]   [ 2.7021565 ]   [ 0.14773515]   ...   [ 0.16272612]   [ 0.263398  ]   [-1.0668343 ]]   [[ 4.9977493 ]   [ 2.6142614 ]   [ 2.8206005 ]   ...   [-0.35571998]   [-0.05585755]   [-1.796899  ]]], shape=(341, 101, 1), dtype=float32) (missing previous layer metadata).","<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

**Important Notice**

Please note that `tf.keras` code was moved entirely to
[keras-team/keras](https://github.com/keras-team/keras) repository

You can open any code/doc bugs, performance issues, and feature requests
 in [keras-team/keras](https://github.com/keras-team/keras/issues) repository

`tf.keras` related issues opened in
[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may
not get attention as [keras-team/keras](https://github.com/keras-team/keras)
repository is dedicated for the development of `keras` code
"
52650,error: `'Py_hash_t'` does not name a type,"**System information**
- OS Platform: `Ubuntu 20.04.3 LTS`
- TensorFlow installed from (source or binary): `source`
- TensorFlow version: 487ce666eba608e3ac909db94c4211c2661f2828
- Python version: `Python 3.8.10`
- Bazel version: `bazel 3.7.2`
- GCC/Compiler version (if compiling from source): `gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0`
- CUDA/cuDNN version: `NaN`
- GPU model and memory: `NaN`

**Problem**

When I try installing `tensorflow` from source the build fails with a message saying `'error: 'npy_hash_t' does not name a type; did you mean 'npy_half'?'` which is the result of the error `'error: 'Py_hash_t' does not name a type'`

![Screenshot from 2021-10-25 11-27-41](https://user-images.githubusercontent.com/70365318/138641957-3e555ff7-291d-4b3f-a9a2-b59387814a3f.png)

**Command that I used to build tensorflow from source**

```shell
$ bazel --host_jvm_args=-Xms512m build --local_ram_resources 2048 --jobs=2 //tensorflow/tools/pip_package:build_pip_package
```

**Other info**
There's a strange behaviour that I would like to inform you, when I examine the messages from the kernel ring buffer I found the `Out of memory` issue which is strange for me because I'm limiting both the `memory` and the `number of jobs` while building `tensorflow` from source.

![Screenshot from 2021-10-25 11-21-12](https://user-images.githubusercontent.com/70365318/138641409-da4b242f-af19-49f9-be44-bf489882fda1.png)
"
52649,TFlite support for FakeQuantWithMinMaxVarsPerChannel,"Is there support for conversion of QAT model trained with per-channel quantization to TFlite models?
Thanks!"
52648,"ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
52647,"TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, None, None]. Consider casting elements to a supported type.","I have 4 params: out2, cur_x_input, cur_y_input, cur_z_input.Their shape is:

out2: [None, 1]
cur_x_input: [None, 4, 1]
cur_y_input: [None, 4, 1]
cur_z_input: [None, 4, 1]

But when I execute the following code：

pressure_grad_x = tf.keras.backend.gradients(out2, cur_x_input)[0]
pressure_grad_y = tf.keras.backend.gradients(out2, cur_y_input)[0]
pressure_grad_z = tf.keras.backend.gradients(out2, cur_z_input)[0]
pressure_grad = tf.convert_to_tensor([pressure_grad_x, pressure_grad_y, pressure_grad_z])

It will report an error：

TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, None, None]. Consider casting elements to a supported type.

I find that the value of pressure_grad_x (or pressure_grad_y, or pressure_grad_z) is None. It means that the result of tf.keras.backend.gradients(out2, cur_x_input)[0] 
(or tf.keras.backend.gradients(out2, cur_y_input)[0], or tf.keras.backend.gradients(out2, cur_z_input)[0]) is None
The version of Tensorflow is 2.6.0

How should I solve this problem？
Thank you very much! 
"
52645,xla compile op cache many tensors to out of memory .,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 3.8
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.1 (clang-1001.0.46.4)
- CUDA/cuDNN version: - 
- GPU model and memory: - 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
zsh: command not found: v1.12.1-54214-gb51e7cff1aa

**Describe the current behavior**
xla OOM


**Describe the expected behavior**
xla not OOM


**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
    flat_hash_map -> LRUCache(with large size) .

1. OOM
    XlaCompilationCache in XlaCompileOp maybe cache many tensors in ```absl::flat_hash_map<Signature, std::unique_ptr<Entry>, Signature::Hash> cache_```.
    My XlaCompileOp has an input named TruncatedNormalV2, and TruncatedNormalV2 will output a dynamic shape, so my XlaCompileOp will compile many exe file and cache lots of input tensors. 

2. LOG
  I spend 3 days to find tensor OOM, so I think  If there is too much content in cache_, at least print a warning to remind it.

I want to confirm whether there is such a problem in TF ? or I have a mistake usage ?
If you think it's a bug, I am happy to fix this problem.



"
52644,' TypeError: '<=' not supported between instances of 'list' and 'Rectangle'  ' from pose_classification.ipynb,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Github : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb

Google colab : https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb 

## Description of issue (what needs changing):

I'm following steps from that URL (Movenet yoga pose classification) by google colab and Jupyter notebook. 
When executing 'Preprocess the TRAIN dataset' code, an error occurs.

-----------------------------------------------------------------------------------

Code 
-------------

if not is_skip_step_1:
  images_in_train_folder = os.path.join(IMAGES_ROOT, 'train')
  images_out_train_folder = 'poses_images_out_train'
  csvs_out_train_path = 'train_data.csv'

  preprocessor = MoveNetPreprocessor(
      images_in_folder=images_in_train_folder,
      images_out_folder=images_out_train_folder,
      csvs_out_path=csvs_out_train_path,
  )

  preprocessor.process(per_pose_class_limit=None)

---------------------------------------------------------------------------

Error message
-------------

Preprocessing chair
  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  0%|          | 0/200 [00:00<?, ?it/s]
'---------------------------------------------------------------------------'
TypeError                                 Traceback (most recent call last)
<ipython-input-11-87cdca3ca58e> in <module>()
     10   )
     11 
---> 12   preprocessor.process(per_pose_class_limit=None)

2 frames
<__array_function__ internals> in amin(*args, **kwargs)

/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85                 return reduction(axis=axis, out=out, **passkwargs)
     86 
---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     88 
     89 

TypeError: '<=' not supported between instances of 'list' and 'Rectangle'


![image](https://user-images.githubusercontent.com/43366200/138588872-3d9f22a0-28f8-4807-a02d-e69fd99cb801.png)

How can I solve this issue?
"
52642,Frozen graph converted by freeze_graph.py is not usable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Ubuntu 20.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0
- Python version: 3.9.5
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
I tried to convert tf2 object detection into a frozen graph.  I downloaded the model from [here](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz).  
At first, i tried to use tf tool freeze_graph.py, it worked. But the converted pb is not usable, it cannot run normally as tf1 pb  downloaded [here](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz)   
Then i tried to convert tf2 ckpt to tf1 ckpt, using the script [here](https://www.tensorflow.org/guide/migrate/migrating_checkpoints#convert_tf2_checkpoint_to_tf1), but the converted ckpt was not usable either.   
Could you please tell me is there any way to convert  or maybe how to run this different pb?

**Standalone code to reproduce the issue**
- code to convert saved model to frozen graph
```shell
path=/path/to/saved_model
python tensorflow/python/tools/freeze_graph.py \
--input_saved_model_dir=$path/saved_model \
--output_node_names=StatefulPartitionedCall \
--output_graph=$path/tf2.pb
```
- error when running this converted pb
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to find function ""__inference_signature_wrapper_frozen_1938"" in function library:
```"
52641,"When using make to run the test, the error ""there should be no - m at this time. Find: the parameter format is incorrect"" is reported","F:\桌面\TyniML\tensorflow-2.4.3>make -f tensorflow/lite/micro/tools/make/Makefile 'test_hello_world_test'
process_begin: CreateProcess(NULL, uname -m, ...) failed.
此时不应有 -m。
FIND: 参数格式不正确
FIND: 参数格式不正确
make: *** No rule to make target ''test_hello_world_test''。 停止。
"
